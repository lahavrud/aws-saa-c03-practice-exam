// AWS SAA-C03 Exam Questions
// Auto-generated from JSON files in questions directory

const examQuestions = {
    test1: [
        {
            id: 1,
            text: "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)", correct: false },
                { id: 2, text: "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon Kinesis Data Streams is specifically designed for real-time streaming data ingestion and processing, making it ideal for IoT applications that need to process high-volume data streams in real-time. Kinesis can handle millions of records per second and provides built-in capabilities for real-time analytics through Kinesis Data Analytics or integration with Lambda functions. Once the analytics processing is complete, Amazon SNS is the perfect service for delivering push notifications to mobile applications. SNS supports native mobile push notification protocols including APNS (Apple Push Notification Service) for iOS devices and FCM (Firebase Cloud Messaging) for Android devices. Unlike pull-based services, SNS uses a push model where AWS directly delivers messages to mobile apps, eliminating the need for apps to continuously poll for updates. This push-based approach is more efficient, reduces latency, and conserves mobile device battery life. The combination of Kinesis for real-time streaming analytics and SNS for mobile push notifications creates a complete, scalable, and efficient solution that meets all the requirements: real-time data processing, analytics, and mobile notification delivery.\n\n**Why option 0 is incorrect:**\nWhile Amazon SQS can work with SNS, SQS is fundamentally a message queuing service designed for decoupling applications and handling discrete messages, not continuous data streams. SQS uses a pull-based model where consumers must poll the queue to retrieve messages, which introduces latency and doesn't provide the real-time streaming capabilities needed for IoT data processing. For the real-time analytics requirement, Kinesis Data Streams is purpose-built with features like data retention, multiple consumers reading the same stream, and real-time processing capabilities that SQS lacks. Using SQS in this scenario would add unnecessary complexity and latency to the data processing pipeline, making it less suitable than Kinesis for real-time streaming analytics.\n\n**Why option 1 is incorrect:**\nAmazon SES (Simple Email Service) is specifically designed for sending email notifications, not push notifications to mobile applications. The requirement explicitly asks for notifications to be sent to mobile apps, which requires a service that supports mobile push notification protocols like APNS or FCM. SES only sends emails through SMTP and cannot deliver push notifications directly to mobile devices. Even though Kinesis is correctly chosen for the streaming analytics part, pairing it with SES fails to meet the mobile notification requirement. SNS is the appropriate service for mobile push notifications, as it supports the native push protocols that mobile applications use.\n\n**Why option 3 is incorrect:**\nAmazon SQS is a pull-based message queuing service where consumers must actively poll the queue to retrieve messages. For mobile applications, this means the app would need to continuously check SQS for new messages, which is highly inefficient. This polling approach increases latency (messages aren't delivered immediately), wastes network bandwidth, and significantly drains mobile device battery life. Additionally, SQS doesn't provide the real-time streaming data processing capabilities that Kinesis offers for the analytics requirement. SNS, on the other hand, uses a true push model where AWS delivers messages directly to mobile apps without requiring polling, making it the correct choice for mobile notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nS3 One Zone-IA stores data in a single Availability Zone, providing 99.5% availability at approximately 20% lower cost than S3 Standard-IA. Since the assets are re-creatable (can be regenerated if lost), the reduced durability of One Zone-IA is acceptable. The 30-day transition period ensures assets remain in Standard storage during the first week of high access, then transition to cheaper storage after access frequency drops. This balances cost optimization with the requirement for immediate accessibility.\n\n**Why option 0 is incorrect:**\nTransitioning to S3 One Zone-IA after 7 days is too early. The scenario states assets are accessed frequently for the first few days, and transitioning too early could impact performance or increase costs if assets are still being accessed frequently. The 30-day period ensures the high-access period has passed before transitioning to cheaper storage.\n\n**Why option 1 is incorrect:**\nS3 Standard-IA stores data across multiple Availability Zones providing 99.999999999% durability (11 9's), which is overkill for re-creatable assets. It's also more expensive than One Zone-IA. Additionally, the 7-day transition is too early, potentially impacting performance during high-access periods when assets are still being frequently accessed.\n\n**Why option 3 is incorrect:**\nWhile the 30-day timing is correct, S3 Standard-IA is more expensive than One Zone-IA. For re-creatable assets that don't require maximum durability, One Zone-IA provides the best cost optimization while maintaining immediate accessibility. Standard-IA's multi-AZ durability is unnecessary for data that can be regenerated.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
            options: [
                { id: 0, text: "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: true },
                { id: 1, text: "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 2, text: "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 3, text: "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nLaunch templates are the modern, recommended way to configure Auto Scaling groups. They support mixed instance types and mixed purchasing options (On-Demand and Spot Instances) through the Mixed Instances Policy feature. This allows you to specify multiple instance types and purchasing strategies, enabling cost optimization while maintaining performance. Launch templates also support versioning and can be updated without recreating the Auto Scaling group.\n\n**Why option 1 is incorrect:**\nThis is incorrect because launch templates DO support mixed instance types with Spot Instances through Mixed Instances Policy. This feature was specifically designed for this use case and allows you to provision capacity across multiple instance types using both On-Demand and Spot Instances.\n\n**Why option 2 is incorrect:**\nLaunch configurations are legacy and do NOT support mixed instance types or Spot Instances. They only support a single instance type and On-Demand instances. This option is incorrect because it suggests launch configurations can do something they cannot. Only launch templates support the Mixed Instances Policy feature.\n\n**Why option 3 is incorrect:**\nLaunch configurations cannot support mixed instance types or Spot Instances. They are limited to a single instance type and On-Demand purchasing only. This is a fundamental limitation of launch configurations. Only launch templates support mixed instance types and purchasing options.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
            options: [
                { id: 0, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type", correct: false },
                { id: 1, text: "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed", correct: true },
                { id: 2, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance", correct: false },
                { id: 3, text: "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nLaunch configurations are immutable - they cannot be modified once created. To fix an incorrect instance type, you must create a new launch configuration with the correct instance type, then update the Auto Scaling group to use the new launch configuration. The old launch configuration can be deleted once the Auto Scaling group is updated. New instances launched by the Auto Scaling group will use the correct instance type from the new launch configuration. Existing instances will continue running with the old instance type until they are terminated and replaced.\n\n**Why option 0 is incorrect:**\nAuto Scaling groups don't have a direct instance type setting - they get this from the launch configuration. You cannot change the instance type without changing the launch configuration. The Auto Scaling group references a launch configuration, which defines the instance type. Simply modifying the Auto Scaling group won't change the instance type.\n\n**Why option 2 is incorrect:**\nAdding more instances of the wrong type doesn't solve the performance problem. The issue is that the instance type itself is incorrect and not optimized for the application workflow, not the quantity. More instances of the wrong type won't improve performance - you need the correct instance type that matches the application's requirements.\n\n**Why option 3 is incorrect:**\nLaunch configurations are immutable and cannot be modified after creation. This is a fundamental characteristic of launch configurations. You must create a new launch configuration with the correct instance type rather than trying to modify an existing one.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 1, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 2, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
                { id: 3, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 4, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nAWS CLI aws s3 sync command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets (like 1 petabyte) efficiently with parallel transfers and retry logic. The sync command automatically handles differences between buckets and can resume interrupted transfers. It's a straightforward solution for one-time copies and can be run from any machine with AWS CLI access.\n\n**Why option 2 is correct:**\nS3 batch replication can copy existing objects (not just new ones) when configured with batch replication. After setting up replication and allowing it to copy existing objects, you can delete the replication configuration once the one-time copy is complete. This provides a managed, console-based solution that handles the transfer automatically without requiring command-line tools or manual intervention.\n\n**Why option 1 is incorrect:**\nS3 Transfer Acceleration optimizes transfers from clients (like on-premises) to S3 using CloudFront edge locations. It does NOT help with bucket-to-bucket transfers within AWS. For bucket-to-bucket transfers, you should use sync or replication. Transfer Acceleration is designed for client-to-S3 transfers, not S3-to-S3 transfers.\n\n**Why option 3 is incorrect:**\nSnowball is for transferring data from on-premises to AWS, not for S3 bucket-to-bucket transfers within AWS. The data is already in S3, so Snowball is not applicable. Snowball is used when you need to physically ship data from on-premises locations, not for cloud-to-cloud transfers between S3 buckets.\n\n**Why option 4 is incorrect:**\nWhile you can copy individual objects in the S3 console, copying 1 petabyte through the console UI is not practical. The console is designed for small-scale operations, not bulk transfers. This would require thousands of manual operations and is not feasible for such a large dataset.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
            options: [
                { id: 0, text: "AWS Lambda", correct: false },
                { id: 1, text: "AWS Step Functions", correct: false },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon CloudWatch", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "**Why option 2 is correct:**\nSNS can receive messages from CloudWatch alarms and send email notifications to subscribers. This requires minimal setup - just create an SNS topic, subscribe email addresses, and configure the CloudWatch alarm to publish to the topic. No Lambda functions or custom code needed.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- Amazon CloudWatch: CloudWatch automatically monitors EC2 instance metrics including CPU utilization. You can create CloudWatch alarms that trigger when CPU utilization breaches a threshold. CloudWatch alarms can directly publish to SNS topics, requiring no custom code. This is a native AWS service integration.\n- Amazon SNS: SNS can receive messages from CloudWatch alarms and send email notifications to subscribers. This requires minimal setup - just create an SNS topic, subscribe email addresses, and configure the CloudWatch alarm to publish to the topic. No Lambda functions or custom code needed.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nWhile Lambda can be used to process CloudWatch events and send emails, it requires writing custom code, which increases development effort. CloudWatch alarms with SNS provide a no-code solution. Lambda adds unnecessary complexity for simple monitoring and email notifications.\n\n**Why option 1 is incorrect:**\nStep Functions orchestrate multiple AWS services but adds unnecessary complexity for simple monitoring and email notifications. CloudWatch + SNS is simpler and requires less development effort. Step Functions are for complex workflows, not simple alerting.\n\n**Why option 3 is incorrect:**\nSQS is a message queue service and doesn't directly send email notifications. While it could be part of a more complex solution (SQS -> Lambda -> SES), it's not needed here and adds unnecessary complexity. SQS requires a consumer to process messages.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 2, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nAuto Scaling groups manage EC2 instance capacity but don't provide low-latency routing. They don't optimize network paths or provide global distribution. Auto Scaling is about instance management, not network optimization.\n\n**Why option 1 is incorrect:**\nELB distributes traffic across instances within a region but doesn't optimize for global latency. It doesn't use AWS's global network infrastructure or provide static IP addresses. ELB is regional, not global.\n\n**Why option 2 is incorrect:**\nCloudFront is a Content Delivery Network (CDN) optimized for HTTP/HTTPS traffic and static content caching. It's not designed for UDP protocol traffic or real-time data distribution. CloudFront caches content at edge locations, which isn't suitable for live, real-time sports results.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nSNS is a pub/sub messaging service for notifications and fan-out scenarios. It doesn't support custom data processing or real-time analytics. SNS delivers messages to subscribers but doesn't provide the streaming data processing capabilities needed here.\n\n**Why option 2 is incorrect:**\n- Use Amazon Kinesis Data Firehose: Kinesis Data Firehose is designed for loading streaming data into destinations like S3, Redshift, or Elasticsearch. It doesn't support custom processing applications - it's a fully managed service that automatically delivers data to destinations. For custom processing and analysis, you need Kinesis Data Streams.\n- Use Amazon Simple Queue Service (Amazon SQS): SQS is a message queuing service for decoupling applications, but it's not designed for real-time streaming data processing. It's pull-based (consumers poll for messages) and doesn't provide the real-time processing capabilities or data retention features of Kinesis. SQS is better for discrete messages, not continuous streams.\n- Use Amazon Simple Notification Service (Amazon SNS): SNS is a pub/sub messaging service for notifications and fan-out scenarios. It doesn't support custom data processing or real-time analytics. SNS delivers messages to subscribers but doesn't provide the streaming data processing capabilities needed here.\n\n**Why option 3 is incorrect:**\n- Use Amazon Kinesis Data Firehose: Kinesis Data Firehose is designed for loading streaming data into destinations like S3, Redshift, or Elasticsearch. It doesn't support custom processing applications - it's a fully managed service that automatically delivers data to destinations. For custom processing and analysis, you need Kinesis Data Streams.\n- Use Amazon Simple Queue Service (Amazon SQS): SQS is a message queuing service for decoupling applications, but it's not designed for real-time streaming data processing. It's pull-based (consumers poll for messages) and doesn't provide the real-time processing capabilities or data retention features of Kinesis. SQS is better for discrete messages, not continuous streams.\n- Use Amazon Simple Notification Service (Amazon SNS): SNS is a pub/sub messaging service for notifications and fan-out scenarios. It doesn't support custom data processing or real-time analytics. SNS delivers messages to subscribers but doesn't provide the streaming data processing capabilities needed here.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Remove full database access for all IAM users in the organization", correct: false },
                { id: 1, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Only root user should have full database access in the organization", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nThis is impractical and would severely limit productivity. Developers need database access to work. The root user should never be used for day-to-day operations. This would create operational bottlenecks.\n\n**Why option 2 is incorrect:**\nWhile this adds oversight, it's not scalable and relies on manual processes that can be error-prone. It doesn't provide a technical safeguard against accidental deletions. This is a process solution, not an architectural one.\n\n**Why option 3 is incorrect:**\n- Remove full database access for all IAM users in the organization: This is too restrictive and would prevent legitimate work. Developers need database access to build features. The solution should prevent accidental deletions, not remove all access. This would break normal operations.\n- The CTO should review the permissions for each new developer's IAM user: While this adds oversight, it's not scalable and relies on manual processes that can be error-prone. It doesn't provide a technical safeguard against accidental deletions. This is a process solution, not an architectural one.\n- Only root user should have full database access in the organization: This is impractical and would severely limit productivity. Developers need database access to work. The root user should never be used for day-to-day operations. This would create operational bottlenecks.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to analyze API calls", correct: true },
                { id: 1, text: "Implement an IAM policy to forbid users to change Amazon S3 bucket settings", correct: false },
                { id: 2, text: "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations", correct: false },
                { id: 3, text: "Use Amazon S3 access logs to analyze user access using Athena", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nThis adds security but doesn't help identify who is making changes or why. It also doesn't address the investigation requirement. MFA prevents unauthorized access but doesn't provide audit trails for authorized users.\n\n**Why option 2 is incorrect:**\n- Implement an IAM policy to forbid users to change Amazon S3 bucket settings: This restricts user rights, which the question explicitly asks to avoid. The requirement is to figure out what's happening without restricting rights. This would prevent investigation by blocking the activity entirely.\n- Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations: This adds security but doesn't help identify who is making changes or why. It also doesn't address the investigation requirement. MFA prevents unauthorized access but doesn't provide audit trails for authorized users.\n- Use Amazon S3 access logs to analyze user access using Athena: S3 access logs (server access logs) track object-level access (GET, PUT, DELETE operations on objects), but they don't track bucket-level configuration changes like bucket policies, versioning settings, or lifecycle policies. CloudTrail is needed for API-level auditing of bucket configuration changes.\n\n**Why option 3 is incorrect:**\nS3 access logs (server access logs) track object-level access (GET, PUT, DELETE operations on objects), but they don't track bucket-level configuration changes like bucket policies, versioning settings, or lifecycle policies. CloudTrail is needed for API-level auditing of bucket configuration changes.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
            options: [
                { id: 0, text: "Amazon Redshift", correct: false },
                { id: 1, text: "Amazon RDS", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: true },
                { id: 3, text: "Amazon ElastiCache", correct: false },
                { id: 4, text: "AWS Lambda", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "**Why option 2 is correct:**\nDynamoDB is a NoSQL database that stores data as key-value pairs, which matches the data format described (key-value pairs). It provides high availability with automatic multi-AZ replication and can handle high-throughput writes (one-minute frequency is easily manageable). DynamoDB is serverless, scales automatically, and provides 99.999% availability SLA. It's ideal for time-series data like weather metrics.\n\n**Why option 4 is correct:**\nLambda can process the incoming weather data, transform it if needed, and write it to DynamoDB. It can be triggered by various sources (API Gateway, Kinesis, SQS, etc.) and provides serverless, event-driven processing. Lambda automatically scales to handle the incoming data frequency and integrates seamlessly with DynamoDB.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nElastiCache is an in-memory caching service (Redis or Memcached) designed for temporary data storage, not persistent storage. Data in ElastiCache can be lost if the cache is cleared or nodes fail. The requirement is for reliable storage with high availability, which requires persistent storage like DynamoDB.\n\n**Why option 1 is incorrect:**\n- Amazon Redshift: Redshift is a data warehouse designed for analytical queries on large datasets, not for high-frequency writes of key-value pairs. It's optimized for complex SQL queries, not simple key-value storage. Redshift would be overkill and not optimized for this use case.\n- Amazon RDS: RDS is a relational database service that requires schema definition and is optimized for relational data, not key-value pairs. While it could work, DynamoDB is better suited for key-value data and provides better scalability and availability for this use case. RDS requires more management overhead.\n- Amazon ElastiCache: ElastiCache is an in-memory caching service (Redis or Memcached) designed for temporary data storage, not persistent storage. Data in ElastiCache can be lost if the cache is cleared or nodes fail. The requirement is for reliable storage with high availability, which requires persistent storage like DynamoDB.\n\n**Why option 3 is incorrect:**\n- Amazon Redshift: Redshift is a data warehouse designed for analytical queries on large datasets, not for high-frequency writes of key-value pairs. It's optimized for complex SQL queries, not simple key-value storage. Redshift would be overkill and not optimized for this use case.\n- Amazon RDS: RDS is a relational database service that requires schema definition and is optimized for relational data, not key-value pairs. While it could work, DynamoDB is better suited for key-value data and provides better scalability and availability for this use case. RDS requires more management overhead.\n- Amazon ElastiCache: ElastiCache is an in-memory caching service (Redis or Memcached) designed for temporary data storage, not persistent storage. Data in ElastiCache can be lost if the cache is cleared or nodes fail. The requirement is for reliable storage with high availability, which requires persistent storage like DynamoDB.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
                { id: 3, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nRoute 53 uses DNS-based routing, which is subject to DNS caching on mobile devices. Mobile phones cache DNS records, so users might not see the new deployment even after DNS changes propagate. This doesn't solve the DNS caching problem mentioned in the scenario.\n\n**Why option 1 is incorrect:**\nCodeDeploy manages application deployments but doesn't control traffic routing or distribution. It deploys code to instances but doesn't help with routing traffic between blue and green environments for testing. CodeDeploy is about deployment, not traffic management.\n\n**Why option 3 is incorrect:**\nELB distributes traffic within a region but doesn't provide traffic dials or weighted routing between different deployments. ELB also doesn't solve the DNS caching issue. ELB is regional and doesn't provide the global traffic management needed.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
                { id: 1, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 2, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "**Why option 3 is correct:**\nMFA delete requires multi-factor authentication before objects can be permanently deleted. This adds an extra layer of protection against accidental deletions. Even if someone has delete permissions, they need MFA to permanently delete versioned objects, providing compliance-grade protection.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- Enable versioning: S3 versioning keeps multiple versions of objects, so if an object is deleted, you can restore a previous version. This provides protection against accidental deletion by allowing recovery of deleted objects. Versioning is a fundamental S3 feature for data protection and compliance.\n- Enable MFA delete: MFA delete requires multi-factor authentication before objects can be permanently deleted. This adds an extra layer of protection against accidental deletions. Even if someone has delete permissions, they need MFA to permanently delete versioned objects, providing compliance-grade protection.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThere is no such configuration option in S3. S3 doesn't have a built-in \"confirmation dialog\" setting. This is not a real S3 feature.\n\n**Why option 1 is incorrect:**\nWhile this provides notification of deletions, it doesn't prevent accidental deletion or allow recovery. It's reactive, not protective. The object would still be deleted before the notification is sent.\n\n**Why option 2 is incorrect:**\nThis is a process solution, not a technical safeguard. It relies on human processes that can be bypassed or forgotten. Technical controls like versioning and MFA delete are more reliable and enforceable.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 3, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nIAM roles are the recommended and secure way to grant permissions to EC2 instances. Roles provide temporary credentials that are automatically rotated, eliminating the need to store long-lived access keys. An instance profile is a container for an IAM role that allows EC2 instances to assume the role. This is the AWS best practice for EC2 access to AWS services. Credentials are automatically provided to the instance via the instance metadata service.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nThis is a security anti-pattern. Hardcoding credentials in code is insecure because credentials can be exposed in code repositories, logs, or if the instance is compromised. Credentials don't rotate automatically and must be manually updated. This violates AWS security best practices.\n\n**Why option 2 is incorrect:**\nWhile encryption adds some security, this still requires storing credentials on the instance, which is not recommended. Credentials must still be decrypted and stored in memory, and the encryption key must be managed. IAM roles eliminate the need to store credentials entirely.\n\n**Why option 3 is incorrect:**\nSimilar to the previous options, this requires storing IAM user credentials on the instance, which is insecure. IAM user credentials are long-lived and don't rotate automatically. Using IAM roles is the recommended approach.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon RDS Custom is specifically designed for applications that require customization of the database environment and underlying operating system. RDS Custom for Oracle allows DBAs to access and customize the database environment, install custom software, and configure the OS while still providing managed database services. Multi-AZ configuration provides high availability with automatic failover. This is the only RDS option that allows both customization and high availability.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nStandard RDS for Oracle doesn't allow customization of the database environment or underlying OS. Read replicas provide read scaling but don't allow the level of customization required. Standard RDS is a managed service with limited OS and database customization.\n\n**Why option 2 is incorrect:**\nWhile this allows full customization, it requires managing the database yourself, including backups, patching, and high availability setup. This increases operational overhead compared to RDS Custom, which provides managed services with customization capabilities.\n\n**Why option 3 is incorrect:**\nStandard RDS for Oracle doesn't allow customization of the database environment or underlying operating system. The requirement specifically needs customization capabilities that standard RDS doesn't provide.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
            options: [
                { id: 0, text: "Move to Amazon Redshift", correct: false },
                { id: 1, text: "Switch application code to AWS Lambda for better performance", correct: false },
                { id: 2, text: "Setup Amazon ElastiCache in front of Amazon RDS", correct: true },
                { id: 3, text: "Setup Amazon RDS Read Replicas", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nRedshift is a data warehouse designed for analytical queries on large datasets, not for transactional read/write operations. It's not suitable for real-time game data retrieval. Redshift has higher latency and is optimized for complex analytical queries, not simple lookups.\n\n**Why option 1 is incorrect:**\nLambda is a compute service and doesn't address the database read performance issue. The problem is database load, not compute performance. Lambda would still need to query the same RDS database, so it doesn't solve the problem.\n\n**Why option 3 is incorrect:**\nWhile read replicas can help distribute read traffic, they don't reduce costs - you're adding more database instances, which increases costs. Read replicas also don't reduce latency as much as caching does. ElastiCache provides better performance improvement and cost reduction.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nMacie is a data security service that uses machine learning to automatically discover, classify, and protect sensitive data in S3. It can identify sensitive data like PII, credit card numbers, and other regulated information. Macie provides visibility into data access patterns and helps ensure compliance.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nThis reverses the roles of the services. GuardDuty monitors threats, Macie identifies sensitive data. The correct pairing is GuardDuty for threats and Macie for sensitive data.\n\n**Why option 2 is incorrect:**\n- Use Amazon GuardDuty to monitor malicious activity AND identify sensitive data: GuardDuty is designed for threat detection, not data classification. It doesn't have the capability to identify sensitive data patterns. Macie is specifically designed for sensitive data discovery.\n- Use Amazon Macie to monitor malicious activity AND identify sensitive data: Macie is designed for data discovery and classification, not threat detection. It doesn't monitor for malicious activity like unauthorized access attempts or data exfiltration. GuardDuty is needed for threat monitoring.\n- Use Amazon Macie to monitor malicious activity. Use Amazon GuardDuty to identify sensitive data: This reverses the roles of the services. GuardDuty monitors threats, Macie identifies sensitive data. The correct pairing is GuardDuty for threats and Macie for sensitive data.\n\n**Why option 3 is incorrect:**\n- Use Amazon GuardDuty to monitor malicious activity AND identify sensitive data: GuardDuty is designed for threat detection, not data classification. It doesn't have the capability to identify sensitive data patterns. Macie is specifically designed for sensitive data discovery.\n- Use Amazon Macie to monitor malicious activity AND identify sensitive data: Macie is designed for data discovery and classification, not threat detection. It doesn't monitor for malicious activity like unauthorized access attempts or data exfiltration. GuardDuty is needed for threat monitoring.\n- Use Amazon Macie to monitor malicious activity. Use Amazon GuardDuty to identify sensitive data: This reverses the roles of the services. GuardDuty monitors threats, Macie identifies sensitive data. The correct pairing is GuardDuty for threats and Macie for sensitive data.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 1, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 2, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nVPC peering connects VPCs but doesn't \"share\" them. Each VPC remains separate, and resources in one VPC cannot be launched in the other VPC's subnets. VPC peering is for network connectivity, not resource sharing.\n\n**Why option 2 is incorrect:**\n- Use VPC peering to share one or more subnets: VPC peering connects entire VPCs, not individual subnets. You cannot share subnets using VPC peering - it creates a network connection between two VPCs. VPC peering doesn't allow multiple accounts to launch resources in the same subnet.\n- Use VPC peering to share a VPC: VPC peering connects VPCs but doesn't \"share\" them. Each VPC remains separate, and resources in one VPC cannot be launched in the other VPC's subnets. VPC peering is for network connectivity, not resource sharing.\n- Use VPC sharing to share a VPC: VPC sharing works at the subnet level, not the VPC level. You share specific subnets with other accounts, not entire VPCs. This allows for more granular control and better resource isolation.\n\n**Why option 3 is incorrect:**\nVPC sharing works at the subnet level, not the VPC level. You share specific subnets with other accounts, not entire VPCs. This allows for more granular control and better resource isolation.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
            options: [
                { id: 0, text: "Amazon ElastiCache for Redis/Memcached", correct: true },
                { id: 1, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon DocumentDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nDocumentDB is a MongoDB-compatible document database, not a caching solution. It doesn't cache SQL query results and is designed for document storage, not query result caching.\n\n**Why option 2 is incorrect:**\n- Amazon DynamoDB Accelerator (DAX): DAX is specifically designed as a caching layer for DynamoDB, not for relational databases or SQL queries. It's optimized for DynamoDB's NoSQL data model and cannot cache SQL query results from other databases.\n- Amazon DynamoDB: DynamoDB is a NoSQL database, not a caching solution. While it's fast, it doesn't cache SQL query results. The requirement specifically asks for caching SQL query results, which requires a caching layer, not a different database.\n- Amazon DocumentDB: DocumentDB is a MongoDB-compatible document database, not a caching solution. It doesn't cache SQL query results and is designed for document storage, not query result caching.\n\n**Why option 3 is incorrect:**\n- Amazon DynamoDB Accelerator (DAX): DAX is specifically designed as a caching layer for DynamoDB, not for relational databases or SQL queries. It's optimized for DynamoDB's NoSQL data model and cannot cache SQL query results from other databases.\n- Amazon DynamoDB: DynamoDB is a NoSQL database, not a caching solution. While it's fast, it doesn't cache SQL query results. The requirement specifically asks for caching SQL query results, which requires a caching layer, not a different database.\n- Amazon DocumentDB: DocumentDB is a MongoDB-compatible document database, not a caching solution. It doesn't cache SQL query results and is designed for document storage, not query result caching.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DMS can read data from S3 (using S3 as a source endpoint) and stream it to Kinesis Data Streams (using Kinesis as a target endpoint). DMS supports full load of existing data and ongoing change data capture (CDC) for new files. This provides a managed, efficient way to stream both existing and new S3 files to Kinesis without requiring custom code or Lambda functions. DMS handles the complexity of reading from S3 and writing to Kinesis.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nSimilar to S3 event notifications, EventBridge can trigger on S3 events but only for new objects. It doesn't handle existing files, and you would still need Lambda to read the file content and write to Kinesis, adding complexity.\n\n**Why option 2 is incorrect:**\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event: While this works for new files, it doesn't handle existing files in S3. S3 event notifications only trigger for new object creation events, not for existing objects. You would need a separate process to handle existing files.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon SNS: S3 cannot directly write to SNS. S3 can send event notifications to SNS, but these are just notifications about object creation, not the actual data. SNS also cannot directly send data to Kinesis Data Streams - you would need Lambda or another service.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3: Similar to S3 event notifications, EventBridge can trigger on S3 events but only for new objects. It doesn't handle existing files, and you would still need Lambda to read the file content and write to Kinesis, adding complexity.\n\n**Why option 3 is incorrect:**\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event: While this works for new files, it doesn't handle existing files in S3. S3 event notifications only trigger for new object creation events, not for existing objects. You would need a separate process to handle existing files.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon SNS: S3 cannot directly write to SNS. S3 can send event notifications to SNS, but these are just notifications about object creation, not the actual data. SNS also cannot directly send data to Kinesis Data Streams - you would need Lambda or another service.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3: Similar to S3 event notifications, EventBridge can trigger on S3 events but only for new objects. It doesn't handle existing files, and you would still need Lambda to read the file content and write to Kinesis, adding complexity.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 1, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nFor cross-account access, you need permissions on both sides: the Lambda function's execution role needs permissions to access S3, AND the S3 bucket policy must grant access to the Lambda function's role. The bucket policy in Account B must explicitly allow the role ARN from Account A. This is the standard pattern for cross-account resource access - both the resource policy (bucket policy) and the identity policy (role policy) must allow the access.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nMaking the bucket public is a security risk and violates the principle of least privilege. Public buckets expose data to anyone on the internet. Cross-account access should use IAM roles and bucket policies, not public access.\n\n**Why option 2 is incorrect:**\nThis is incomplete. Just creating a role with permissions isn't enough - the bucket policy in Account B must also grant access to that role. Without the bucket policy, the Lambda function will be denied access.\n\n**Why option 3 is incorrect:**\nThis is incorrect. Lambda can absolutely access resources across accounts using IAM roles and resource policies. Cross-account access is a standard AWS pattern.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 1, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 3, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nFor 5 petabytes of data, Snowball Edge Storage Optimized devices are the most cost-effective solution. Each device can hold up to 80TB. Snowball devices are shipped to your location, you copy data to them, then AWS ships them back and imports the data into S3. After import, you can use lifecycle policies to automatically transition data to Glacier for long-term archival storage. This approach avoids expensive network transfer costs and is much faster than transferring 5PB over the internet.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nVPN connections have bandwidth limitations and would take an extremely long time to transfer 5PB. VPN is also not cost-effective for such large transfers. Like Direct Connect, you cannot directly write to Glacier.\n\n**Why option 1 is incorrect:**\nDirect Connect has high setup costs and monthly fees. For a one-time migration of 5PB, the cost would be prohibitive. Direct Connect is designed for ongoing connectivity, not one-time bulk transfers. Also, you cannot directly write to Glacier - data must go to S3 first, then transition to Glacier.\n\n**Why option 3 is incorrect:**\nSnowball devices import data into S3, not directly into Glacier. You must first import to S3, then use lifecycle policies or other methods to transition to Glacier. Glacier doesn't support direct imports from Snowball.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 1, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 2, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 3, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 4, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
                { id: 5, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "**Why option 1 is correct:**\nBy default, Lambda functions run in AWS-managed VPCs with internet access. When you attach a Lambda to your VPC, it loses default internet access and needs a NAT Gateway (or NAT Instance) in a public subnet to access the internet or AWS APIs. This is a critical consideration for VPC-enabled Lambdas.\n\n**Why option 3 is correct:**\n- Lambda VPC behavior: By default, Lambda functions run in AWS-managed VPCs with internet access. When you attach a Lambda to your VPC, it loses default internet access and needs a NAT Gateway (or NAT Instance) in a public subnet to access the internet or AWS APIs. This is a critical consideration for VPC-enabled Lambdas.\n- Lambda Layers: Lambda Layers allow you to package libraries, custom runtimes, or other function dependencies separately. This promotes code reuse, reduces deployment package size, and can speed up deployments. Layers are shared across functions, making them ideal for common dependencies.\n- CloudWatch Alarms for scaling: Lambda can scale to thousands of concurrent executions very quickly. Monitoring ConcurrentExecutions and Invocations helps prevent runaway costs and ensures you're aware of scaling events. CloudWatch alarms provide proactive monitoring.\n**Why other options are incorrect:**\n\n**Why option 5 is correct:**\nLambda can scale to thousands of concurrent executions very quickly. Monitoring ConcurrentExecutions and Invocations helps prevent runaway costs and ensures you're aware of scaling events. CloudWatch alarms provide proactive monitoring.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis is incorrect. Lambda DOES support container images (up to 10GB). You can package Lambda functions as container images and deploy them. Container images are supported for Lambda.\n\n**Why option 2 is incorrect:**\nThis is incorrect. AWS recommends RIGHT-SIZING timeout settings, not over-provisioning. Over-provisioning wastes money. You should set timeouts based on actual function execution time.\n\n**Why option 4 is incorrect:**\nWhile larger packages do increase cold start time, AWS recommends using Lambda Layers for dependencies, not separate packages. Layers are the recommended approach for managing dependencies.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator", correct: false },
                { id: 1, text: "Use AWS Direct Connect", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "**Why option 3 is correct:**\nAurora read replicas can handle read traffic, offloading the primary database during spikes. This improves read performance and provides additional capacity. Read replicas can be in the same or different regions, providing geographic distribution and better resilience.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- Amazon CloudFront: CloudFront is a CDN that caches content at edge locations worldwide, reducing latency and offloading traffic from the origin (Application Load Balancer). During traffic spikes, CloudFront serves cached content from edge locations, reducing load on the backend infrastructure. It also provides DDoS protection and can handle sudden traffic increases.\n- Amazon Aurora Replica: Aurora read replicas can handle read traffic, offloading the primary database during spikes. This improves read performance and provides additional capacity. Read replicas can be in the same or different regions, providing geographic distribution and better resilience.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nGlobal Accelerator routes traffic to optimal endpoints but doesn't cache content or reduce database load. It's designed for improving connection performance, not handling traffic spikes or database load. It doesn't address the database performance issue.\n\n**Why option 1 is incorrect:**\nDirect Connect provides dedicated network connectivity but doesn't help with traffic spikes or database performance. It's for network connectivity, not application resilience or performance optimization.\n\n**Why option 2 is incorrect:**\nShield provides DDoS protection but doesn't improve application performance or handle legitimate traffic spikes. It protects against attacks but doesn't optimize for high traffic volumes.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon S3", correct: true },
                { id: 1, text: "Amazon Kinesis", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 4, text: "Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nGateway endpoints are VPC endpoints that use route tables to route traffic to AWS services. Only S3 and DynamoDB support gateway endpoints. Gateway endpoints are free and don't require NAT Gateway or Internet Gateway for access. They're added as routes in your VPC route tables and automatically route traffic to the service without going over the internet.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\nGateway endpoints are VPC endpoints that use route tables to route traffic to AWS services. Only S3 and DynamoDB support gateway endpoints. Gateway endpoints are free and don't require NAT Gateway or Internet Gateway for access. They're added as routes in your VPC route tables and automatically route traffic to the service without going over the internet.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nSNS uses interface endpoints (PrivateLink), not gateway endpoints. Only S3 and DynamoDB support the simpler gateway endpoint model.\n\n**Why option 2 is incorrect:**\n- Amazon Kinesis: Kinesis uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints are ENIs in your VPC that provide private connectivity. They're different from gateway endpoints.\n- Amazon Simple Queue Service (Amazon SQS): SQS uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints require DNS resolution and are more complex than gateway endpoints.\n- Amazon Simple Notification Service (Amazon SNS): SNS uses interface endpoints (PrivateLink), not gateway endpoints. Only S3 and DynamoDB support the simpler gateway endpoint model.\n\n**Why option 3 is incorrect:**\n- Amazon Kinesis: Kinesis uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints are ENIs in your VPC that provide private connectivity. They're different from gateway endpoints.\n- Amazon Simple Queue Service (Amazon SQS): SQS uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints require DNS resolution and are more complex than gateway endpoints.\n- Amazon Simple Notification Service (Amazon SNS): SNS uses interface endpoints (PrivateLink), not gateway endpoints. Only S3 and DynamoDB support the simpler gateway endpoint model.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAurora read replicas are separate database instances that replicate data from the primary. They have their own endpoint that applications can use for read queries. By directing read traffic to the read replica endpoint, you offload read operations from the primary database, reducing I/O contention and allowing writes to perform better. The application must be modified to use the read replica endpoint for read queries while continuing to use the primary endpoint for writes.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nAurora doesn't have a \"read-through caching\" feature. You can use ElastiCache in front of Aurora for caching, but that's a separate service, not an Aurora feature.\n\n**Why option 2 is incorrect:**\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This wording is confusing, but the key issue is that you need to modify the application to use the read replica endpoint. Simply creating a read replica isn't enough - the application must be configured to route reads to it.\n- Activate read-through caching on the Amazon Aurora database: Aurora doesn't have a \"read-through caching\" feature. You can use ElastiCache in front of Aurora for caching, but that's a separate service, not an Aurora feature.\n- Configure the application to read from the Multi-AZ standby instance: The Multi-AZ standby instance is for high availability and automatic failover, not for read scaling. It's not accessible for read queries - it's only used during failover. Read replicas are separate instances designed for read scaling.\n\n**Why option 3 is incorrect:**\nThe Multi-AZ standby instance is for high availability and automatic failover, not for read scaling. It's not accessible for read queries - it's only used during failover. Read replicas are separate instances designed for read scaling.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 2, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 3, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 4, text: "The route for the health check is misconfigured", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nThe EC2 instance's security group must allow inbound traffic from the ALB's security group (or the ALB's IP addresses). If the security group only allows traffic from specific IPs or doesn't allow ALB traffic, health checks will fail even though the website works when accessed directly via IP.\n\n**Why option 4 is correct:**\nThe ALB health check is configured with a specific path (like /health or /). If the application doesn't respond correctly to that path, or if the path is incorrect, health checks will fail. The website might work at / but fail at /health, causing the instances to be marked unhealthy.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nEBS volume mounting issues would prevent the application from running at all. If the website works when accessed directly, the volumes are mounted correctly. This wouldn't cause selective health check failures.\n\n**Why option 2 is incorrect:**\nEIPs are not required for ALB health checks. ALB routes traffic to instances using their private IP addresses. EIPs are for public internet access, not for ALB connectivity.\n\n**Why option 3 is incorrect:**\nALB works with any HTTP/HTTPS application regardless of runtime. It operates at Layer 7 (HTTP) and doesn't care about the application runtime. This is not a valid reason for health check failures.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
            options: [
                { id: 0, text: "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated", correct: false },
                { id: 1, text: "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume", correct: true },
                { id: 2, text: "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume", correct: false },
                { id: 3, text: "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWhen you launch an EC2 instance, the root EBS volume has a \"Delete on Termination\" attribute that defaults to true. This means when you terminate the instance, the root volume is automatically deleted. Additional EBS volumes attached to the instance have \"Delete on Termination\" set to false by default, so they persist. The team likely stored data on the root volume instead of a separate data volume, causing the data loss.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis is incorrect. Only the root volume is terminated by default. Additional volumes persist unless explicitly configured to be deleted.\n\n**Why option 2 is incorrect:**\nEFS is a file system service, not a backup destination. The issue is volume termination settings, not backup location.\n\n**Why option 3 is incorrect:**\n- On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated: This is incorrect. Only the root volume is terminated by default. Additional volumes persist unless explicitly configured to be deleted.\n- The Amazon EBS volumes were not backed up on Amazon S3 storage: Backing up to S3 is a good practice but not required for volumes to persist. The issue is the \"Delete on Termination\" setting, not lack of backups. Even without S3 backups, volumes can persist if configured correctly.\n- The Amazon EBS volumes were not backed up on Amazon EFS file system storage: EFS is a file system service, not a backup destination. The issue is volume termination settings, not backup location.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use Spot Instances", correct: false },
                { id: 1, text: "Use a Cluster placement group", correct: true },
                { id: 2, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nSpot Instances are about cost optimization, not network performance. They don't improve network performance between instances. Network performance depends on instance type and placement group, not purchasing option.\n\n**Why option 2 is incorrect:**\nKernel optimization might provide minor improvements but won't significantly impact network performance between instances. Placement groups are the primary way to optimize inter-instance network performance.\n\n**Why option 3 is incorrect:**\nSpread placement groups place instances on distinct hardware to minimize correlated failures. They actually REDUCE network performance compared to Cluster placement groups because instances are spread across different hardware. For high network performance, Cluster placement groups are required.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
                { id: 1, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 2, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 3, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nRedshift is a data warehouse, not suitable for transactional applications. It's designed for analytical workloads, not operational databases. It also doesn't support the same MySQL compatibility.\n\n**Why option 2 is incorrect:**\n- Use Amazon DynamoDB Global Tables: DynamoDB is a NoSQL database and would require migrating from the relational MySQL schema. The requirement explicitly states \"without moving away from the underlying relational database schema.\" DynamoDB uses a different data model.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases: This requires managing databases yourself, including backups, patching, replication, and high availability. It's not cost-effective and increases operational overhead. Aurora Global Database provides managed global replication.\n- Spin up a Amazon Redshift cluster in each AWS region: Redshift is a data warehouse, not suitable for transactional applications. It's designed for analytical workloads, not operational databases. It also doesn't support the same MySQL compatibility.\n\n**Why option 3 is incorrect:**\n- Use Amazon DynamoDB Global Tables: DynamoDB is a NoSQL database and would require migrating from the relational MySQL schema. The requirement explicitly states \"without moving away from the underlying relational database schema.\" DynamoDB uses a different data model.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases: This requires managing databases yourself, including backups, patching, replication, and high availability. It's not cost-effective and increases operational overhead. Aurora Global Database provides managed global replication.\n- Spin up a Amazon Redshift cluster in each AWS region: Redshift is a data warehouse, not suitable for transactional applications. It's designed for analytical workloads, not operational databases. It also doesn't support the same MySQL compatibility.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
            options: [
                { id: 0, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data", correct: false },
                { id: 1, text: "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object", correct: true },
                { id: 2, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data", correct: false },
                { id: 3, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 provides strong read-after-write consistency for PUT operations. When you overwrite an object and immediately read it, S3 always returns the new version. There's no eventual consistency delay for overwrite PUTs. This is critical for high-frequency trading systems where data consistency is essential. S3 guarantees that after a successful PUT, subsequent GET requests will return the new data.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\n- Until the change is fully propagated, Amazon S3 might return the previous data: This describes eventual consistency, which S3 does NOT have for overwrite PUTs. S3 has strong consistency for overwrite PUTs and DELETE operations. The \"propagation delay\" concept doesn't apply to overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 might return the new data: This is backwards - if there were a propagation delay, you'd get old data, not new data. But more importantly, there's no propagation delay for overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 does not return any data: S3 doesn't block reads during updates. It always returns data - either the old or new version. For overwrite PUTs, it always returns the new version immediately.\n\n**Why option 2 is incorrect:**\n- Until the change is fully propagated, Amazon S3 might return the previous data: This describes eventual consistency, which S3 does NOT have for overwrite PUTs. S3 has strong consistency for overwrite PUTs and DELETE operations. The \"propagation delay\" concept doesn't apply to overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 might return the new data: This is backwards - if there were a propagation delay, you'd get old data, not new data. But more importantly, there's no propagation delay for overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 does not return any data: S3 doesn't block reads during updates. It always returns data - either the old or new version. For overwrite PUTs, it always returns the new version immediately.\n\n**Why option 3 is incorrect:**\n- Until the change is fully propagated, Amazon S3 might return the previous data: This describes eventual consistency, which S3 does NOT have for overwrite PUTs. S3 has strong consistency for overwrite PUTs and DELETE operations. The \"propagation delay\" concept doesn't apply to overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 might return the new data: This is backwards - if there were a propagation delay, you'd get old data, not new data. But more importantly, there's no propagation delay for overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 does not return any data: S3 doesn't block reads during updates. It always returns data - either the old or new version. For overwrite PUTs, it always returns the new version immediately.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DataSync is designed for efficient data transfer between on-premises and AWS. It can transfer from NFS file systems to EFS. Using Direct Connect with a private VIF provides dedicated network connectivity. PrivateLink interface VPC endpoints allow private connectivity to EFS from on-premises via Direct Connect without traversing the public internet. DataSync can be scheduled to run automatically, making it operationally efficient.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\n- Transfer to an AWS VPC peering endpoint for Amazon EFS: VPC peering connects VPCs, but EFS doesn't use \"VPC peering endpoints.\" EFS uses mount targets within VPCs. You need PrivateLink interface endpoints for EFS access from on-premises via Direct Connect.\n- Transfer to an Amazon S3 bucket... then Lambda to copy to EFS: This adds unnecessary complexity and an extra step. DataSync can transfer directly from NFS to EFS, eliminating the need for S3 as an intermediate storage and Lambda processing. Direct transfer is more efficient.\n- Transfer to an Amazon S3 bucket by using public VIF: Public VIFs are for internet-routable traffic, not private connectivity. For secure, private transfers, you should use private VIFs. Also, transferring to S3 first adds unnecessary steps when DataSync can go directly to EFS.\n\n**Why option 2 is incorrect:**\n- Transfer to an AWS VPC peering endpoint for Amazon EFS: VPC peering connects VPCs, but EFS doesn't use \"VPC peering endpoints.\" EFS uses mount targets within VPCs. You need PrivateLink interface endpoints for EFS access from on-premises via Direct Connect.\n- Transfer to an Amazon S3 bucket... then Lambda to copy to EFS: This adds unnecessary complexity and an extra step. DataSync can transfer directly from NFS to EFS, eliminating the need for S3 as an intermediate storage and Lambda processing. Direct transfer is more efficient.\n- Transfer to an Amazon S3 bucket by using public VIF: Public VIFs are for internet-routable traffic, not private connectivity. For secure, private transfers, you should use private VIFs. Also, transferring to S3 first adds unnecessary steps when DataSync can go directly to EFS.\n\n**Why option 3 is incorrect:**\n- Transfer to an AWS VPC peering endpoint for Amazon EFS: VPC peering connects VPCs, but EFS doesn't use \"VPC peering endpoints.\" EFS uses mount targets within VPCs. You need PrivateLink interface endpoints for EFS access from on-premises via Direct Connect.\n- Transfer to an Amazon S3 bucket... then Lambda to copy to EFS: This adds unnecessary complexity and an extra step. DataSync can transfer directly from NFS to EFS, eliminating the need for S3 as an intermediate storage and Lambda processing. Direct transfer is more efficient.\n- Transfer to an Amazon S3 bucket by using public VIF: Public VIFs are for internet-routable traffic, not private connectivity. For secure, private transfers, you should use private VIFs. Also, transferring to S3 first adds unnecessary steps when DataSync can go directly to EFS.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
                { id: 3, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nECS with EC2 launch type runs containers on EC2 instances you manage. You pay for the EC2 instances (compute) and EBS volumes (storage) you provision, regardless of how much the containers actually use. ECS with Fargate is serverless - you only pay for the vCPU and memory resources your containers request and consume. Fargate abstracts away the underlying infrastructure, so you don't pay for EC2 instances or EBS volumes directly.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\n- Both are charged based on EC2 instances and EBS volumes: This only applies to EC2 launch type. Fargate doesn't use EC2 instances you manage, so this pricing model doesn't apply.\n- Both are charged based on vCPU and memory resources: This only applies to Fargate. EC2 launch type charges for entire EC2 instances, not just the resources containers use.\n- Both are just charged based on Elastic Container Service used per hour: There's no separate \"ECS service\" charge. ECS itself is free - you pay for the underlying compute resources (EC2 instances for EC2 launch type, or vCPU/memory for Fargate).\n\n**Why option 1 is incorrect:**\n- Both are charged based on EC2 instances and EBS volumes: This only applies to EC2 launch type. Fargate doesn't use EC2 instances you manage, so this pricing model doesn't apply.\n- Both are charged based on vCPU and memory resources: This only applies to Fargate. EC2 launch type charges for entire EC2 instances, not just the resources containers use.\n- Both are just charged based on Elastic Container Service used per hour: There's no separate \"ECS service\" charge. ECS itself is free - you pay for the underlying compute resources (EC2 instances for EC2 launch type, or vCPU/memory for Fargate).\n\n**Why option 2 is incorrect:**\n- Both are charged based on EC2 instances and EBS volumes: This only applies to EC2 launch type. Fargate doesn't use EC2 instances you manage, so this pricing model doesn't apply.\n- Both are charged based on vCPU and memory resources: This only applies to Fargate. EC2 launch type charges for entire EC2 instances, not just the resources containers use.\n- Both are just charged based on Elastic Container Service used per hour: There's no separate \"ECS service\" charge. ECS itself is free - you pay for the underlying compute resources (EC2 instances for EC2 launch type, or vCPU/memory for Fargate).",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nSSL/TLS encryption secures data while it's being transmitted between the EC2 instances and the RDS database. This provides end-to-end security for data in transit. RDS supports SSL connections, and you can require SSL for all connections. This is the standard way to secure database connections and meets the requirement for \"end-to-end security for data-in-transit.\"\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nSimilar to security groups, NACLs control network traffic but don't encrypt it. Blocking SSH doesn't address data-in-transit encryption for database connections.\n\n**Why option 1 is incorrect:**\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: Security groups control network access but don't encrypt data in transit. Blocking SSH doesn't encrypt database connections. SSH is for server access, not database connections.\n- Create a new network ACL that blocks SSH from the entire Amazon EC2 subnet: Similar to security groups, NACLs control network traffic but don't encrypt it. Blocking SSH doesn't address data-in-transit encryption for database connections.\n- Use IAM authentication to access the database instead of the database user's access credentials: IAM authentication provides authentication (who you are) but doesn't provide encryption for data in transit. You still need SSL/TLS to encrypt the connection. IAM authentication and SSL encryption are complementary, not alternatives.\n\n**Why option 2 is incorrect:**\nIAM authentication provides authentication (who you are) but doesn't provide encryption for data in transit. You still need SSL/TLS to encrypt the connection. IAM authentication and SSL encryption are complementary, not alternatives.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 1, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "**Why option 0 is correct:**\n- S3 Transfer Acceleration: Uses CloudFront's globally distributed edge locations to optimize the path from clients to S3. Data is routed through the nearest edge location, then over AWS's optimized network backbone to S3. This significantly improves upload speeds from distant locations (Europe and Asia) to US-based S3 buckets.\n- Multipart uploads: Break large files into smaller parts that are uploaded in parallel. This improves throughput, allows resuming failed uploads, and is more efficient for large files. Multipart uploads are especially beneficial for large video files.\n**Why other options are incorrect:**\n\n**Why option 1 is correct:**\nBreak large files into smaller parts that are uploaded in parallel. This improves throughput, allows resuming failed uploads, and is more efficient for large files. Multipart uploads are especially beneficial for large video files.\n**Why other options are incorrect:**\n\n**Why option 2 is incorrect:**\nVPN connections have bandwidth limitations and don't optimize the network path like Transfer Acceleration does. They also require setup at each location and don't provide the same performance improvements.\n\n**Why option 3 is incorrect:**\nGlobal Accelerator optimizes traffic routing to applications, not to S3. It's designed for application endpoints, not object storage. S3 Transfer Acceleration is specifically designed for S3 uploads.\n\n**Why option 4 is incorrect:**\n- Create multiple AWS Direct Connect connections: Direct Connect requires physical installation at each location and has high setup costs. For multiple locations (Europe and Asia), this would be extremely expensive and time-consuming. Transfer Acceleration and multipart uploads are software solutions that work immediately.\n- Use AWS Global Accelerator for faster file uploads: Global Accelerator optimizes traffic routing to applications, not to S3. It's designed for application endpoints, not object storage. S3 Transfer Acceleration is specifically designed for S3 uploads.\n- Create multiple AWS Site-to-Site VPN connections: VPN connections have bandwidth limitations and don't optimize the network path like Transfer Acceleration does. They also require setup at each location and don't provide the same performance improvements.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
            options: [
                { id: 0, text: "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once", correct: true },
                { id: 1, text: "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts", correct: false },
                { id: 2, text: "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs", correct: false },
                { id: 3, text: "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Shield Advanced has a monthly subscription fee. When you have multiple AWS accounts, each account is charged separately unless consolidated billing is enabled through AWS Organizations. With consolidated billing, the monthly Shield Advanced fee is charged once for the organization, not per account. This is a significant cost savings when you have multiple accounts.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nShield Advanced doesn't support Savings Plans. Savings Plans are for compute services (EC2, Lambda, Fargate), not security services like Shield Advanced.\n\n**Why option 2 is incorrect:**\nShield Advanced only protects AWS resources (CloudFront, ELB, Route 53, EC2, etc.). It cannot protect on-premises or non-AWS infrastructure. This wouldn't cause unexpected costs.\n\n**Why option 3 is incorrect:**\nShield Advanced includes Shield Standard features, but this doesn't cause increased costs. Shield Standard is free, and Shield Advanced replaces it, not adds to it.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda", correct: false },
                { id: 1, text: "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: false },
                { id: 2, text: "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda", correct: false },
                { id: 3, text: "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCan buffer and throttle streaming data. Kinesis Data Streams can handle high-throughput data ingestion and throttle consumers, preventing downstream systems from being overwhelmed.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nGateway endpoints are VPC endpoints for S3 and DynamoDB - they don't provide throttling. They're for private connectivity, not traffic management.\n\n**Why option 1 is incorrect:**\nSNS doesn't provide throttling or buffering - it's a pub/sub messaging service. Lambda can be throttled but doesn't throttle incoming requests. This combination doesn't provide the throttling capabilities needed.\n\n**Why option 2 is incorrect:**\nELB distributes traffic but doesn't throttle it. ELB can handle high traffic but doesn't prevent spikes from reaching backend services. API Gateway provides better throttling capabilities.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon Neptune", correct: true },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon Aurora", correct: false },
                { id: 3, text: "Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nAurora is a relational database optimized for SQL queries. While it can handle complex queries with JOINs, graph databases like Neptune are specifically designed for relationship-heavy queries and perform much better for social network-style queries.\n\n**Why option 2 is incorrect:**\n- Amazon OpenSearch Service: OpenSearch (formerly Elasticsearch) is a search and analytics engine, not a graph database. It's good for full-text search and log analytics but not optimized for relationship traversal queries like \"friends of friends.\"\n- Amazon Aurora: Aurora is a relational database optimized for SQL queries. While it can handle complex queries with JOINs, graph databases like Neptune are specifically designed for relationship-heavy queries and perform much better for social network-style queries.\n- Amazon Redshift: Redshift is a data warehouse for analytical queries on large datasets. It's not designed for transactional queries or relationship traversal. It's optimized for aggregations and analytical workloads, not graph queries.\n\n**Why option 3 is incorrect:**\nRedshift is a data warehouse for analytical queries on large datasets. It's not designed for transactional queries or relationship traversal. It's optimized for aggregations and analytical workloads, not graph queries.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
                { id: 1, text: "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 3, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n- AWS Cost Explorer Resource Optimization: Analyzes EC2 usage patterns and identifies idle or underutilized instances that can be terminated or rightsized. It provides actionable recommendations for cost savings.\n- AWS Compute Optimizer: Analyzes historical utilization metrics and recommends optimal instance types. It can suggest moving to smaller instance types or different instance families that better match workload requirements, reducing costs.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nTrusted Advisor doesn't automatically renew Reserved Instances. Also, the startup likely doesn't have Reserved Instances yet. Trusted Advisor provides recommendations but doesn't focus on idle instances or instance type optimization.\n\n**Why option 2 is incorrect:**\nCompute Optimizer recommends instance types, not purchasing options (On-Demand vs Reserved vs Spot). For purchasing options, you'd use Cost Explorer or Reserved Instance recommendations.\n\n**Why option 3 is incorrect:**\nThis is for S3 cost optimization, not EC2 or RDS. The startup's infrastructure includes EC2 and RDS, so S3 optimization doesn't address their main cost concerns.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
                { id: 1, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 2, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 3, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 4, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "**Why option 0 is correct:**\nUser data scripts run as root by default, allowing them to perform system-level operations like installing packages, modifying system files, and configuring services. This is necessary for many bootstrap operations.\n\n**Why option 1 is correct:**\n- Root privileges: User data scripts run as root by default, allowing them to perform system-level operations like installing packages, modifying system files, and configuring services. This is necessary for many bootstrap operations.\n- Runs only during first boot: User data executes once when the instance first launches, not on every restart. This is by design - user data is for initial setup, not ongoing maintenance. If you need scripts to run on every boot, you must configure that explicitly (e.g., using systemd or cron).\n**Why other options are incorrect:**\n\n**Why option 2 is incorrect:**\nUser data cannot be modified after instance launch. It's set at launch time and cannot be changed. You can view it, but not update it. To change user data, you must launch a new instance.\n\n**Why option 3 is incorrect:**\nThis is incorrect. User data scripts run as root by default, which is why they can perform system-level operations.\n\n**Why option 4 is incorrect:**\n- When an instance is running, you can update user data by using root user credentials: User data cannot be modified after instance launch. It's set at launch time and cannot be changed. You can view it, but not update it. To change user data, you must launch a new instance.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: User data runs only on first launch, not on restarts. Restarting an instance doesn't re-execute user data. This is a common misconception.\n- By default, scripts entered as user data do not have root user privileges: This is incorrect. User data scripts run as root by default, which is why they can perform system-level operations.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "AWS Glue", correct: false },
                { id: 1, text: "Amazon EMR", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: true },
                { id: 3, text: "Amazon FSx for Windows File Server", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nGlue is an ETL (Extract, Transform, Load) service for data preparation and transformation. It's not a file system and doesn't provide the high-performance file access needed for EDA applications. Glue is for data processing pipelines, not file storage.\n\n**Why option 1 is incorrect:**\nEMR is a managed Hadoop/Spark cluster service for big data processing. While it can handle large datasets, it's not optimized for the high-performance file access patterns of EDA applications. EMR is for distributed data processing, not file system performance.\n\n**Why option 3 is incorrect:**\nFSx for Windows is designed for Windows-based file shares and Active Directory integration. It's not optimized for high-performance compute workloads like EDA. Lustre is specifically designed for HPC and compute-intensive applications.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 2, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 3, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
                { id: 4, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nA Golden AMI is a pre-configured AMI with common components already installed. By including static installation components (like operating system, common libraries, frameworks) in the AMI, you eliminate the need to install them on every instance launch, dramatically reducing launch time.\n\n**Why option 2 is correct:**\n- Golden AMI: A Golden AMI is a pre-configured AMI with common components already installed. By including static installation components (like operating system, common libraries, frameworks) in the AMI, you eliminate the need to install them on every instance launch, dramatically reducing launch time.\n- User data for dynamic parts: User data scripts can handle dynamic, instance-specific configuration that needs to happen at boot time. This allows customization while leveraging the pre-configured AMI for static components. The combination reduces launch time from 45 minutes to under 2 minutes.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nWhile S3 provides fast retrieval, you still need to download and install the files, which takes time. This doesn't solve the 45-minute installation problem. Pre-installing in an AMI is much faster.\n\n**Why option 3 is incorrect:**\nInstalling everything via user data still takes 45 minutes. The solution is to pre-install static components in an AMI, not install everything at boot time.\n\n**Why option 4 is incorrect:**\nElastic Beanstalk doesn't have a \"deployment caching\" feature that would significantly reduce installation time. The solution requires AMI optimization and user data, not Beanstalk features.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
            options: [
                { id: 0, text: "Use Identity and Access Management (IAM) policies", correct: false },
                { id: 1, text: "Use Amazon S3 Bucket Policies", correct: true },
                { id: 2, text: "Use Security Groups", correct: false },
                { id: 3, text: "Use Access Control Lists (ACLs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 Bucket Policies are resource-based policies attached to S3 buckets that can grant permissions to IAM users, roles, and even other AWS accounts. Bucket policies can control both user-level access (within the same account) and account-level access (cross-account). They're the most flexible and optimized way to control S3 access, supporting complex permission scenarios including cross-account access, which is mentioned in the requirement.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nIAM policies are identity-based and attached to users, groups, or roles. While they can grant S3 access, they don't provide account-level control for cross-account scenarios. Bucket policies are needed for cross-account access control.\n\n**Why option 2 is incorrect:**\nSecurity Groups are for EC2 instances and other VPC resources, not for S3 access control. S3 is accessed via API calls, not network-level security. Security Groups don't apply to S3.\n\n**Why option 3 is incorrect:**\nACLs are legacy and less flexible than bucket policies. They don't support complex permission scenarios or cross-account access as effectively as bucket policies. Bucket policies are the recommended approach.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 1, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nInternet Gateways provide public internet access for resources in public subnets. Using an Internet Gateway would route SQS traffic over the public internet, which is what the team wants to avoid. It also requires public subnets and public IPs.\n\n**Why option 1 is incorrect:**\nVPN connections are for connecting on-premises networks to VPCs, not for accessing AWS services from within a VPC. VPN doesn't provide private connectivity to AWS services - traffic would still go over the internet.\n\n**Why option 3 is incorrect:**\nNAT instances allow private subnet resources to access the internet, but traffic still goes over the public internet. VPC endpoints provide true private connectivity without internet traversal.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nData transfer within the same AWS region is free for data transfer between AWS services. By deploying the visualization tool in the same region as the data warehouse, the 60MB query responses don't incur data transfer costs. Users accessing the visualization tool over Direct Connect only transfer the 600KB web pages, not the 60MB query results. Direct Connect has lower egress costs than internet transfer, and since query results stay within AWS (same region), there's no egress charge for them.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nWhile query results stay in-region (free), accessing the tool over the internet has higher costs than Direct Connect for the web page transfers.\n\n**Why option 2 is incorrect:**\nThis would transfer 60MB query results over Direct Connect for each query, which incurs data transfer costs. The visualization tool in AWS with same-region queries avoids this cost.\n\n**Why option 3 is incorrect:**\nInternet egress from AWS has higher costs than Direct Connect. Transferring 60MB query results over the internet would be more expensive.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 2, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 4, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 5, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "**Why option 1 is correct:**\n- Throughput limit of 3,000 messages per second: FIFO queues have a throughput limit of 3,000 messages per second (or 300 messages per second without batching). With batching, you can achieve up to 3,000 messages per second. This is a hard limit that must be considered during migration.\n- Queue name must end with .fifo: FIFO queues require the .fifo suffix in their name. This is a mandatory naming convention that distinguishes FIFO queues from standard queues.\n- Delete and recreate: Standard queues cannot be converted to FIFO queues. You must delete the standard queue and create a new FIFO queue. This is because FIFO and standard queues have fundamentally different architectures and behaviors.\n**Why other options are incorrect:**\n\n**Why option 2 is correct:**\n- Throughput limit of 3,000 messages per second: FIFO queues have a throughput limit of 3,000 messages per second (or 300 messages per second without batching). With batching, you can achieve up to 3,000 messages per second. This is a hard limit that must be considered during migration.\n- Queue name must end with .fifo: FIFO queues require the .fifo suffix in their name. This is a mandatory naming convention that distinguishes FIFO queues from standard queues.\n- Delete and recreate: Standard queues cannot be converted to FIFO queues. You must delete the standard queue and create a new FIFO queue. This is because FIFO and standard queues have fundamentally different architectures and behaviors.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\nStandard queues cannot be converted to FIFO queues. You must delete the standard queue and create a new FIFO queue. This is because FIFO and standard queues have fundamentally different architectures and behaviors.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nFIFO queues must have different names (with .fifo suffix). You cannot reuse the same name. Also, you need to delete the standard queue first.\n\n**Why option 3 is incorrect:**\nThis is the limit WITHOUT batching. With batching (which the question mentions), the limit is 3,000 messages per second.\n\n**Why option 5 is incorrect:**\nStandard queues cannot be converted to FIFO queues. They must be deleted and recreated. This is a fundamental limitation.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 1, text: "Use AWS Glue to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 2, text: "Use AWS EMR to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS DMS is a managed service designed specifically for database migration and replication. It can continuously replicate data from multiple source databases (Oracle, PostgreSQL) to Redshift with minimal configuration. DMS handles schema conversion, data transformation, and ongoing replication automatically. It requires no infrastructure management and minimal development effort - you configure source and target endpoints and DMS handles the rest.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nKinesis is for real-time streaming data, not database replication. It would require custom code to read from databases and write to Redshift. Kinesis doesn't understand database schemas or handle replication automatically.\n\n**Why option 1 is incorrect:**\nGlue is an ETL service for data transformation and preparation, not continuous replication. It's designed for batch ETL jobs, not ongoing database replication. Glue would require more development and management effort.\n\n**Why option 2 is incorrect:**\nEMR is for big data processing with Hadoop/Spark, not database replication. It would require significant development effort to build replication logic. EMR is overkill for database replication and requires infrastructure management.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 2, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 3, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nFor AWS Managed IPSec VPN connections, the Virtual Private Gateway (VGW) is an AWS-managed VPN endpoint that's attached to your VPC. The Customer Gateway is a resource in AWS that represents your on-premises VPN device. The VGW goes on the AWS side, and the Customer Gateway represents the on-premises side. This is the standard AWS VPN architecture.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nVGWs are AWS-managed and only exist on the AWS side. On-premises uses your own VPN equipment, represented by a Customer Gateway resource in AWS.\n\n**Why option 2 is incorrect:**\n- Create a VGW on the on-premises side and a Customer Gateway on the AWS side: This reverses the components. VGW is AWS-managed and goes on the AWS side. Customer Gateway represents on-premises equipment.\n- Create a Customer Gateway on both sides: Customer Gateways represent on-premises equipment. You only need one Customer Gateway (in AWS) to represent your on-premises device. The AWS side uses a VGW.\n- Create a VGW on both sides: VGWs are AWS-managed and only exist on the AWS side. On-premises uses your own VPN equipment, represented by a Customer Gateway resource in AWS.\n\n**Why option 3 is incorrect:**\n- Create a VGW on the on-premises side and a Customer Gateway on the AWS side: This reverses the components. VGW is AWS-managed and goes on the AWS side. Customer Gateway represents on-premises equipment.\n- Create a Customer Gateway on both sides: Customer Gateways represent on-premises equipment. You only need one Customer Gateway (in AWS) to represent your on-premises device. The AWS side uses a VGW.\n- Create a VGW on both sides: VGWs are AWS-managed and only exist on the AWS side. On-premises uses your own VPN equipment, represented by a Customer Gateway resource in AWS.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 2, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
                { id: 3, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nCloudWatch doesn't provide a standard metric for certificate expiration. AWS Config is the service designed for compliance and configuration monitoring, including certificate expiration.\n\n**Why option 2 is incorrect:**\nACM-created certificates are automatically managed by AWS and don't need expiration monitoring. The requirement is for imported third-party certificates.\n\n**Why option 3 is incorrect:**\n- Monitor CloudWatch metric for certificates created via ACM: ACM-created certificates are automatically renewed by AWS, so monitoring expiration isn't necessary. Also, CloudWatch doesn't have a standard metric for certificate expiration. The question specifies \"third-party\" certificates imported into ACM.\n- Leverage AWS Config managed rule for certificates created via ACM: ACM-created certificates are automatically managed by AWS and don't need expiration monitoring. The requirement is for imported third-party certificates.\n- Monitor CloudWatch metric for certificates imported into ACM: CloudWatch doesn't provide a standard metric for certificate expiration. AWS Config is the service designed for compliance and configuration monitoring, including certificate expiration.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
            options: [
                { id: 0, text: "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs", correct: false },
                { id: 1, text: "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: true },
                { id: 2, text: "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: false },
                { id: 3, text: "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon GuardDuty analyzes these specific data sources to detect threats:\n- VPC Flow Logs: Network traffic information showing source, destination, ports, and protocols. GuardDuty analyzes this for suspicious network activity.\n- DNS logs: DNS query logs from Route 53 Resolver. GuardDuty analyzes DNS queries for malicious domains, data exfiltration attempts, and other threats.\n- CloudTrail events: API calls and management events. GuardDuty analyzes API calls for unauthorized access, privilege escalation, and other security threats.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nGuardDuty doesn't analyze API Gateway logs or S3 access logs. It uses CloudTrail (which includes API calls) and DNS logs, not application-level logs.\n\n**Why option 2 is incorrect:**\n- VPC Flow Logs, API Gateway logs, S3 access logs: GuardDuty doesn't analyze API Gateway logs or S3 access logs. It uses CloudTrail (which includes API calls) and DNS logs, not application-level logs.\n- ELB logs, DNS logs, CloudTrail events: GuardDuty doesn't analyze ELB access logs. It uses VPC Flow Logs for network traffic analysis, not ELB logs.\n- CloudFront logs, API Gateway logs, CloudTrail events: GuardDuty doesn't analyze CloudFront or API Gateway logs. It focuses on VPC Flow Logs, DNS logs, and CloudTrail events.\n\n**Why option 3 is incorrect:**\nGuardDuty doesn't analyze CloudFront or API Gateway logs. It focuses on VPC Flow Logs, DNS logs, and CloudTrail events.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
            options: [
                { id: 0, text: "Use AWS_IAM authorization", correct: false },
                { id: 1, text: "Use Amazon Cognito User Pools", correct: true },
                { id: 2, text: "Use Amazon Cognito Identity Pools", correct: false },
                { id: 3, text: "Use AWS Lambda authorizer for Amazon API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nIAM authorization uses AWS IAM credentials, which are for AWS services and applications, not end users. IAM doesn't provide user management features like registration, password reset, or user profiles. It's for service-to-service authentication.\n\n**Why option 2 is incorrect:**\nIdentity Pools provide temporary AWS credentials for users, but they don't provide user management. Identity Pools work with User Pools or other identity providers. They're for granting AWS resource access, not user management.\n\n**Why option 3 is incorrect:**\nLambda authorizers allow custom authorization logic, but they don't provide user management. You'd need to build user registration, authentication, and management features yourself, which contradicts the \"built-in user management\" requirement.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 1, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDedicated Hosts are physical servers dedicated to your use. They allow you to use your existing server-bound software licenses (like Windows Server, SQL Server, etc.) because you have visibility and control over the underlying physical server. Dedicated Hosts are the most cost-effective way to use existing licenses on AWS while maintaining compliance with license terms that require physical server dedication.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nReserved instances are a purchasing option, not an instance type. They don't address the license requirement for physical server visibility. Reserved instances can be On-Demand, Dedicated Instances, or Dedicated Hosts.\n\n**Why option 2 is incorrect:**\n- Use Amazon EC2 dedicated instances: Dedicated instances run on dedicated hardware but you don't have visibility into the physical server. Many license agreements require visibility into the physical server, which Dedicated Hosts provide but Dedicated Instances don't.\n- Use Amazon EC2 on-demand instances: On-demand instances don't provide the physical server visibility needed for server-bound licenses. They're shared or dedicated at the instance level, not the host level.\n- Use Amazon EC2 reserved instances: Reserved instances are a purchasing option, not an instance type. They don't address the license requirement for physical server visibility. Reserved instances can be On-Demand, Dedicated Instances, or Dedicated Hosts.\n\n**Why option 3 is incorrect:**\n- Use Amazon EC2 dedicated instances: Dedicated instances run on dedicated hardware but you don't have visibility into the physical server. Many license agreements require visibility into the physical server, which Dedicated Hosts provide but Dedicated Instances don't.\n- Use Amazon EC2 on-demand instances: On-demand instances don't provide the physical server visibility needed for server-bound licenses. They're shared or dedicated at the instance level, not the host level.\n- Use Amazon EC2 reserved instances: Reserved instances are a purchasing option, not an instance type. They don't address the license requirement for physical server visibility. Reserved instances can be On-Demand, Dedicated Instances, or Dedicated Hosts.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 1, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 2, text: "NAT instance supports port forwarding", correct: true },
                { id: 3, text: "NAT gateway supports port forwarding", correct: false },
                { id: 4, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 5, text: "Security Groups can be associated with a NAT gateway", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "**Why option 0 is correct:**\nNAT instances are EC2 instances, so they support security groups for fine-grained network access control.\n**Why other options are incorrect:**\n\n**Why option 2 is correct:**\n- NAT instance as bastion: NAT instances are EC2 instances, so they can be used as bastion servers for SSH access to private subnet instances. You can SSH into the NAT instance, then SSH from there to private instances.\n- NAT instance supports port forwarding: NAT instances run software (like iptables) that can be configured for port forwarding. This allows you to forward specific ports to instances in private subnets.\n- Security Groups on NAT instance: NAT instances are EC2 instances, so they support security groups for fine-grained network access control.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- NAT instance as bastion: NAT instances are EC2 instances, so they can be used as bastion servers for SSH access to private subnet instances. You can SSH into the NAT instance, then SSH from there to private instances.\n- NAT instance supports port forwarding: NAT instances run software (like iptables) that can be configured for port forwarding. This allows you to forward specific ports to instances in private subnets.\n- Security Groups on NAT instance: NAT instances are EC2 instances, so they support security groups for fine-grained network access control.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nNAT gateways don't support port forwarding. They only provide basic NAT functionality (source/destination NAT). Port forwarding requires instance-level configuration.\n\n**Why option 3 is incorrect:**\n- NAT gateway can be used as a bastion server: NAT gateways are managed AWS services, not EC2 instances. You cannot SSH into them or use them as bastion servers. They're only for NAT functionality.\n- NAT gateway supports port forwarding: NAT gateways don't support port forwarding. They only provide basic NAT functionality (source/destination NAT). Port forwarding requires instance-level configuration.\n- Security Groups can be associated with a NAT gateway: NAT gateways are managed services and don't support security groups. They use NACLs for network-level control, not security groups.\n\n**Why option 5 is incorrect:**\nNAT gateways are managed services and don't support security groups. They use NACLs for network-level control, not security groups.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "It is not possible to access cross-account resources", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIAM roles are the recommended way to provide cross-account access. Users from the development account can assume a role in the production account that has the necessary permissions. The production account's role trust policy allows the development account's users/roles to assume it. This provides secure, temporary access without sharing credentials.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nIAM users are not recommended for cross-account access. Roles provide temporary credentials and better security. Users have long-lived credentials that are harder to manage across accounts.\n\n**Why option 2 is incorrect:**\nCross-account access is absolutely possible using IAM roles. This is a standard AWS pattern for multi-account architectures.\n\n**Why option 3 is incorrect:**\nSharing credentials violates security best practices. Credentials can be compromised, are hard to rotate, and don't provide audit trails. Roles are the secure way to provide cross-account access.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
            options: [
                { id: 0, text: "7", correct: false },
                { id: 1, text: "3", correct: true },
                { id: 2, text: "14", correct: false },
                { id: 3, text: "15", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSpread placement groups place instances on distinct underlying hardware to minimize correlated failures. Each Availability Zone can have a maximum of 7 running instances per spread placement group. To deploy 15 instances, you need to distribute them across multiple Availability Zones. With 7 instances per AZ maximum, you need at least 3 Availability Zones (7 + 7 + 1 = 15 instances minimum across 3 AZs).\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis doesn't account for the 7-instance-per-AZ limit. You'd need 3 AZs minimum.\n\n**Why option 2 is incorrect:**\n- 7: You can only have 7 instances per AZ in a spread placement group. With 15 instances, you need more than one AZ.\n- 14: This doesn't account for the 7-instance-per-AZ limit. You'd need 3 AZs minimum.\n- 15: You cannot put 15 instances in a single AZ with a spread placement group due to the 7-instance limit.\n\n**Why option 3 is incorrect:**\nYou cannot put 15 instances in a single AZ with a spread placement group due to the 7-instance limit.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 1, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDAX is an in-memory caching layer specifically designed for DynamoDB. It provides microsecond latency for read operations and can cache commonly accessed data. Since 90% of reads are for commonly accessed data, DAX will dramatically improve DynamoDB read performance.\n\n**Why option 0 is incorrect:**\nDAX is better than ElastiCache for DynamoDB, and CloudFront is better than ElastiCache for S3 static content. This combination doesn't use the optimal services.\n\n**Why option 2 is incorrect:**\n- ElastiCache Redis for DynamoDB: ElastiCache is a general-purpose cache, but DAX is specifically optimized for DynamoDB with better integration and performance. DAX understands DynamoDB's data model and provides better caching for DynamoDB workloads.\n- DAX for DynamoDB and ElastiCache Memcached for S3: S3 doesn't need ElastiCache - CloudFront is the appropriate caching/CDN solution for S3 static content. ElastiCache is for application-level caching, not object storage.\n- ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3: DAX is better than ElastiCache for DynamoDB, and CloudFront is better than ElastiCache for S3 static content. This combination doesn't use the optimal services.\n\n**Why option 3 is incorrect:**\n- ElastiCache Redis for DynamoDB: ElastiCache is a general-purpose cache, but DAX is specifically optimized for DynamoDB with better integration and performance. DAX understands DynamoDB's data model and provides better caching for DynamoDB workloads.\n- DAX for DynamoDB and ElastiCache Memcached for S3: S3 doesn't need ElastiCache - CloudFront is the appropriate caching/CDN solution for S3 static content. ElastiCache is for application-level caching, not object storage.\n- ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3: DAX is better than ElastiCache for DynamoDB, and CloudFront is better than ElastiCache for S3 static content. This combination doesn't use the optimal services.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
                { id: 3, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nVPC peering requires a peering connection between each pair of VPCs. For 5 VPCs, this requires 10 peering connections (A-B, A-C, A-D, A-E, B-C, B-D, B-E, C-D, C-E, D-E). This is complex to manage and doesn't scale well.\n\n**Why option 1 is incorrect:**\nInternet Gateways provide internet access, not VPC-to-VPC connectivity. Routing VPC traffic through the internet is insecure and inefficient. VPCs should communicate privately.\n\n**Why option 2 is incorrect:**\nVPC endpoints are for accessing AWS services (like S3, DynamoDB) from your VPC, not for connecting VPCs together. Endpoints don't provide VPC-to-VPC connectivity.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
            options: [
                { id: 0, text: "Distribute the static content through Amazon EFS", correct: false },
                { id: 1, text: "Distribute the dynamic content through Amazon EFS", correct: false },
                { id: 2, text: "Distribute the static content through Amazon S3", correct: true },
                { id: 3, text: "Distribute the dynamic content through Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nStatic content (images, CSS, JavaScript) should be served from S3 (ideally with CloudFront) rather than from ECS containers. This offloads 90% of network traffic from the ECS cluster, reducing costs and improving performance. S3 is designed for static content delivery and is much more cost-effective than serving static files from compute resources. This is a standard best practice for containerized applications.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nDynamic content is generated by the application and changes per request. S3 is for static, unchanging content. Dynamic content should remain in the application layer.\n\n**Why option 1 is incorrect:**\n- Distribute the static content through Amazon EFS: EFS is a network file system designed for shared storage, not static content delivery. It's more expensive than S3 for static content and doesn't provide the same performance or cost benefits. EFS is for dynamic, shared file storage.\n- Distribute the dynamic content through Amazon EFS: Dynamic content needs to be generated by the application, so it should stay in ECS. The problem is static content, not dynamic content.\n- Distribute the dynamic content through Amazon S3: Dynamic content is generated by the application and changes per request. S3 is for static, unchanging content. Dynamic content should remain in the application layer.\n\n**Why option 3 is incorrect:**\n- Distribute the static content through Amazon EFS: EFS is a network file system designed for shared storage, not static content delivery. It's more expensive than S3 for static content and doesn't provide the same performance or cost benefits. EFS is for dynamic, shared file storage.\n- Distribute the dynamic content through Amazon EFS: Dynamic content needs to be generated by the application, so it should stay in ECS. The problem is static content, not dynamic content.\n- Distribute the dynamic content through Amazon S3: Dynamic content is generated by the application and changes per request. S3 is for static, unchanging content. Dynamic content should remain in the application layer.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Database Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Glue", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\n- AWS SCT: Converts database schemas from one database engine to another. It handles complex database objects like secondary indexes, foreign keys, stored procedures, and triggers. SCT analyzes the source database and generates conversion scripts for the target database.\n- AWS DMS: Handles the actual data migration and ongoing replication. It migrates data from the source database to the target while keeping them in sync. DMS works with SCT to provide a complete migration solution.\n**Why other options are incorrect:**\n\n**Why option 2 is correct:**\n- AWS SCT: Converts database schemas from one database engine to another. It handles complex database objects like secondary indexes, foreign keys, stored procedures, and triggers. SCT analyzes the source database and generates conversion scripts for the target database.\n- AWS DMS: Handles the actual data migration and ongoing replication. It migrates data from the source database to the target while keeping them in sync. DMS works with SCT to provide a complete migration solution.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nThis is not an AWS service. Schema conversion requires understanding different database syntaxes and features, which SCT handles automatically.\n\n**Why option 3 is incorrect:**\nSnowball is for physical data transfer, not database migration. It's for moving large datasets from on-premises to S3, not for migrating databases with complex schemas.\n\n**Why option 4 is incorrect:**\nGlue is an ETL service for data transformation, not database schema conversion. It doesn't handle stored procedures, foreign keys, or other complex database objects that SCT specializes in.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 1, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nNetwork Load Balancers operate at Layer 4 (TCP/UDP) and route traffic based on IP addresses and ports. They route to instances using the instance's private IP address from the primary network interface. NLB doesn't use instance IDs, public IPs, or Elastic IPs for routing - it uses the private IP address that's assigned to the instance's primary network interface.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nNLB routes to private IP addresses, not public IPs. Public IPs are for internet-facing access, but NLB-to-instance communication uses private IPs.\n\n**Why option 2 is incorrect:**\n- Traffic is routed using the primary elastic IP address: NLB doesn't route based on Elastic IP addresses. Elastic IPs are for public internet access, but NLB routing uses private IP addresses.\n- Traffic is routed using the instance ID: Instance IDs are identifiers, not network addresses. NLB routes based on IP addresses, not instance IDs.\n- Traffic is routed using the primary public IP address: NLB routes to private IP addresses, not public IPs. Public IPs are for internet-facing access, but NLB-to-instance communication uses private IPs.\n\n**Why option 3 is incorrect:**\n- Traffic is routed using the primary elastic IP address: NLB doesn't route based on Elastic IP addresses. Elastic IPs are for public internet access, but NLB routing uses private IP addresses.\n- Traffic is routed using the instance ID: Instance IDs are identifiers, not network addresses. NLB routes based on IP addresses, not instance IDs.\n- Traffic is routed using the primary public IP address: NLB routes to private IP addresses, not public IPs. Public IPs are for internet-facing access, but NLB-to-instance communication uses private IPs.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 2, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 3, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 4, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "**Why option 1 is correct:**\nAny snapshot taken from an encrypted volume is automatically encrypted. You cannot create an unencrypted snapshot from an encrypted volume.\n\n**Why option 3 is correct:**\nData moving between the encrypted volume and the EC2 instance is encrypted. This provides end-to-end encryption.\n\n**Why option 5 is correct:**\nWhen an EBS volume is encrypted:\n- Snapshots are encrypted: Any snapshot taken from an encrypted volume is automatically encrypted. You cannot create an unencrypted snapshot from an encrypted volume.\n- Data in transit is encrypted: Data moving between the encrypted volume and the EC2 instance is encrypted. This provides end-to-end encryption.\n- Data at rest is encrypted: All data stored on the encrypted volume is encrypted at rest using AWS KMS. This meets HIPAA compliance requirements for data protection.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis is incorrect. Encrypted EBS volumes provide encryption for data in transit between the volume and instance.\n\n**Why option 2 is incorrect:**\nThis is incorrect. Snapshots from encrypted volumes are always encrypted.\n\n**Why option 4 is incorrect:**\nThis contradicts the definition of an encrypted volume. Encrypted volumes encrypt all data at rest.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "Software VPN", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "VPC Endpoint", correct: false },
                { id: 3, text: "AWS VPN CloudHub", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nThis is a generic term, not an AWS service. Software VPNs don't provide the managed hub-and-spoke connectivity that CloudHub offers.\n\n**Why option 1 is incorrect:**\nVPC peering connects VPCs, but the branch offices are connecting via VPN to a single VPC, not to separate VPCs. CloudHub is specifically designed for this VPN hub scenario.\n\n**Why option 2 is incorrect:**\nVPC endpoints are for accessing AWS services (like S3, DynamoDB) from your VPC, not for interconnecting remote locations. Endpoints don't provide connectivity between branch offices.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
            options: [
                { id: 0, text: "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
                { id: 1, text: "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis", correct: false },
                { id: 2, text: "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift", correct: true },
                { id: 3, text: "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nThis requires exporting data and doing manual analysis, which is time-consuming and doesn't provide seamless cross-referencing. Spectrum allows direct SQL joins.\n\n**Why option 1 is incorrect:**\n- Use Redshift COPY command to load S3 data into Redshift, then remove it: This requires loading data into Redshift (incurring storage costs) and then removing it, which is inefficient. You'd need to repeat this process for each query. Spectrum queries S3 directly without loading.\n- Setup access via Amazon Athena... export to flat files for cross-reference: This requires exporting data and doing manual analysis, which is time-consuming and doesn't provide seamless cross-referencing. Spectrum allows direct SQL joins.\n- Use AWS Glue ETL job to load S3 data into Redshift, then remove it: Similar to the COPY approach, this loads data into Redshift temporarily, which is inefficient and costly. Spectrum queries S3 directly without ETL.\n\n**Why option 3 is incorrect:**\nSimilar to the COPY approach, this loads data into Redshift temporarily, which is inefficient and costly. Spectrum queries S3 directly without ETL.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 1, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
                { id: 3, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nSQS doesn't provide the same real-time streaming capabilities as Kinesis. SQS is pull-based and doesn't support multiple consumers reading the same data stream efficiently. Kinesis is designed for streaming analytics.\n\n**Why option 1 is incorrect:**\nSimilar to standard queues, FIFO queues don't provide streaming data capabilities. They're for message queuing, not real-time data streams. FIFO queues also have lower throughput limits.\n\n**Why option 2 is incorrect:**\n- Swap out Kinesis Data Streams with Amazon SQS Standard queues: SQS doesn't provide the same real-time streaming capabilities as Kinesis. SQS is pull-based and doesn't support multiple consumers reading the same data stream efficiently. Kinesis is designed for streaming analytics.\n- Swap out Kinesis Data Streams with Amazon SQS FIFO queues: Similar to standard queues, FIFO queues don't provide streaming data capabilities. They're for message queuing, not real-time data streams. FIFO queues also have lower throughput limits.\n- Swap out Kinesis Data Streams with Amazon Kinesis Data Firehose: Firehose is for loading streaming data into destinations (S3, Redshift, etc.), not for multiple consumer applications. It doesn't support the multiple consumer pattern that Kinesis Data Streams provides.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCloudWatch can monitor EC2 instance status checks (system status and instance status). When a status check fails, CloudWatch alarms can trigger EC2 reboot actions directly without requiring Lambda functions. This is the most cost-effective and resource-efficient solution - CloudWatch alarms are inexpensive, and EC2 reboot actions don't require Lambda execution (no Lambda costs). It's also the simplest solution with minimal components.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nRebooting every 5 minutes regardless of status is wasteful and doesn't solve the problem. You need to detect failures first, then reboot. Also, Lambda execution incurs costs.\n\n**Why option 2 is incorrect:**\nThis adds unnecessary complexity (SNS + Lambda) when CloudWatch alarms can directly trigger EC2 reboot actions. Lambda execution costs money, while direct EC2 actions don't.\n\n**Why option 3 is incorrect:**\n- Use EventBridge events to trigger Lambda to check instance status every 5 minutes: This requires Lambda execution every 5 minutes (costs money) and custom code. CloudWatch alarms with EC2 actions are simpler and more cost-effective. Also, checking every 5 minutes might miss failures.\n- Setup CloudWatch alarm... publish to SNS... trigger Lambda to reboot: This adds unnecessary complexity (SNS + Lambda) when CloudWatch alarms can directly trigger EC2 reboot actions. Lambda execution costs money, while direct EC2 actions don't.\n- Use EventBridge events to trigger Lambda to reboot every 5 minutes: Rebooting every 5 minutes regardless of status is wasteful and doesn't solve the problem. You need to detect failures first, then reboot. Also, Lambda execution incurs costs.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test2: [
        {
            id: 1,
            text: "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 2, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: true },
                { id: 3, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3", correct: false },
                { id: 1, text: "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3", correct: true },
                { id: 2, text: "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
            options: [
                { id: 0, text: "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 1, text: "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 2, text: "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 3, text: "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 4, text: "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements, Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 1, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
            options: [
                { id: 0, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS", correct: true },
                { id: 1, text: "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS", correct: false },
                { id: 2, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS", correct: false },
                { id: 3, text: "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
            options: [
                { id: 0, text: "Different versions of a single object can have different retention modes and periods", correct: true },
                { id: 1, text: "The bucket default settings will override any explicit retention mode or period you request on an object version", correct: false },
                { id: 2, text: "You cannot place a retention period on an object version through a bucket default setting", correct: false },
                { id: 3, text: "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", correct: true },
                { id: 4, text: "When you use bucket default settings, you specify a Retain Until Date for the object version", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Different versions of a single object can have different retention modes and periods, When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The bucket default settings will override any explicit retention mode or period you request on an object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot place a retention period on an object version through a bucket default setting: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When you use bucket default settings, you specify a Retain Until Date for the object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 1, text: "AWS Glue", correct: false },
                { id: 2, text: "Amazon EMR", correct: false },
                { id: 3, text: "Amazon FSx for Lustre", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
            options: [
                { id: 0, text: "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering => Amazon S3 Standard", correct: true },
                { id: 2, text: "Amazon S3 Standard-IA => Amazon S3 One Zone-IA", correct: false },
                { id: 3, text: "Amazon S3 One Zone-IA => Amazon S3 Standard-IA", correct: true },
                { id: 4, text: "Amazon S3 Standard => Amazon S3 Intelligent-Tiering", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Amazon S3 Intelligent-Tiering => Amazon S3 Standard, Amazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA => Amazon S3 One Zone-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
            options: [
                { id: 0, text: "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources", correct: false },
                { id: 1, text: "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI", correct: false },
                { id: 2, text: "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions", correct: false },
                { id: 3, text: "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
            options: [
                { id: 0, text: "Configure the security group for the Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)", correct: false },
                { id: 2, text: "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 3, text: "Configure the security group on the Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure the security group for the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the security group on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway with AWS Lambda", correct: false },
                { id: 1, text: "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Leverage Amazon QuickSight with Amazon Redshift", correct: false },
                { id: 3, text: "Leverage Amazon Athena with Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway with AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon QuickSight with Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Athena with Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor", correct: false },
                { id: 1, text: "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets", correct: true },
                { id: 3, text: "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
            options: [
                { id: 0, text: "Encrypt the access keys and save them on Amazon S3", correct: false },
                { id: 1, text: "Create a strong password for the AWS account root user", correct: true },
                { id: 2, text: "Enable Multi Factor Authentication (MFA) for the AWS account root user account", correct: true },
                { id: 3, text: "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future", correct: false },
                { id: 4, text: "Create AWS account root user access keys and share those keys only with the business owner", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Create a strong password for the AWS account root user, Enable Multi Factor Authentication (MFA) for the AWS account root user account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the access keys and save them on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create AWS account root user access keys and share those keys only with the business owner: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
            options: [
                { id: 0, text: "Grant maximum privileges to avoid assigning privileges again", correct: false },
                { id: 1, text: "Use user credentials to provide access specific permissions for Amazon EC2 instances", correct: false },
                { id: 2, text: "Create a minimum number of accounts and share these account credentials among employees", correct: false },
                { id: 3, text: "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users", correct: true },
                { id: 4, text: "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users, Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Grant maximum privileges to avoid assigning privileges again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use user credentials to provide access specific permissions for Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a minimum number of accounts and share these account credentials among employees: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 1, text: "Only root user should have full database access in the organization", correct: false },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Remove full database access for all IAM users in the organization", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance", correct: true },
                { id: 1, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched", correct: false },
                { id: 2, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application", correct: true },
                { id: 3, text: "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously", correct: false },
                { id: 4, text: "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance, As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time", correct: false },
                { id: 1, text: "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly", correct: false },
                { id: 2, text: "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores", correct: true },
                { id: 3, text: "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
            options: [
                { id: 0, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate", correct: true },
                { id: 1, text: "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages", correct: false },
                { id: 2, text: "Use Amazon SQS standard queue to process the messages", correct: false },
                { id: 3, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS standard queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
            options: [
                { id: 0, text: "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs", correct: true },
                { id: 1, text: "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches", correct: false },
                { id: 2, text: "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share", correct: false },
                { id: 3, text: "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
            options: [
                { id: 0, text: "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket", correct: false },
                { id: 1, text: "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)", correct: true },
                { id: 2, text: "Upload the compressed file using multipart upload", correct: false },
                { id: 3, text: "Upload the compressed file in a single operation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file using multipart upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file in a single operation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
            options: [
                { id: 0, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance", correct: false },
                { id: 1, text: "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance", correct: true },
                { id: 2, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance", correct: false },
                { id: 3, text: "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
            options: [
                { id: 0, text: "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution", correct: true },
                { id: 1, text: "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: true },
                { id: 2, text: "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 3, text: "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 4, text: "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution, Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection", correct: true },
                { id: 1, text: "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region", correct: false },
                { id: 2, text: "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region", correct: false },
                { id: 3, text: "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes", correct: true },
                { id: 3, text: "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
            options: [
                { id: 0, text: "The engineering team needs to provision more servers running the Amazon SNS service", correct: false },
                { id: 1, text: "The engineering team needs to provision more servers running the AWS Lambda service", correct: false },
                { id: 2, text: "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit", correct: true },
                { id: 3, text: "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The engineering team needs to provision more servers running the Amazon SNS service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The engineering team needs to provision more servers running the AWS Lambda service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
            options: [
                { id: 0, text: "General Purpose Solid State Drive (gp2)", correct: false },
                { id: 1, text: "Throughput Optimized Hard disk drive (st1)", correct: true },
                { id: 2, text: "Instance Store", correct: false },
                { id: 3, text: "Cold Hard disk drive (sc1)", correct: true },
                { id: 4, text: "Provisioned IOPS Solid state drive (io1)", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Throughput Optimized Hard disk drive (st1), Cold Hard disk drive (sc1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose Solid State Drive (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance Store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned IOPS Solid state drive (io1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
            options: [
                { id: 0, text: "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 1, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 2, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: true },
                { id: 3, text: "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
            options: [
                { id: 0, text: "Host-based Routing", correct: false },
                { id: 1, text: "Path-based Routing", correct: true },
                { id: 2, text: "HTTP header-based routing", correct: false },
                { id: 3, text: "Query string parameter-based routing", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Path-based Routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host-based Routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- HTTP header-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Query string parameter-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time", correct: false },
                { id: 1, text: "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: true },
                { id: 2, text: "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling", correct: false },
                { id: 3, text: "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
            options: [
                { id: 0, text: "Use AWS Lambda function to run the workflow processes", correct: false },
                { id: 1, text: "Use Amazon EC2 on-demand instances to run the workflow processes", correct: false },
                { id: 2, text: "Use Amazon EC2 reserved instances to run the workflow processes", correct: false },
                { id: 3, text: "Use Amazon EC2 spot instances to run the workflow processes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 spot instances to run the workflow processes\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Lambda function to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
            options: [
                { id: 0, text: "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability", correct: false },
                { id: 1, text: "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency", correct: false },
                { id: 2, text: "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed", correct: true },
                { id: 3, text: "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously", correct: true },
                { id: 4, text: "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed, Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
            options: [
                { id: 0, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: true },
                { id: 1, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 2, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 3, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
            options: [
                { id: 0, text: "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 1, text: "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables", correct: true },
                { id: 2, text: "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 3, text: "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
            options: [
                { id: 0, text: "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue", correct: false },
                { id: 1, text: "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service", correct: true },
                { id: 2, text: "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again", correct: false },
                { id: 3, text: "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy", correct: false },
                { id: 4, text: "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service, Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 3, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet", correct: false },
                { id: 1, text: "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block", correct: false },
                { id: 2, text: "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC", correct: true },
                { id: 3, text: "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
            options: [
                { id: 0, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 1, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 2, text: "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 3, text: "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse", correct: false },
                { id: 1, text: "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes", correct: false },
                { id: 2, text: "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks", correct: false },
                { id: 3, text: "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
            options: [
                { id: 0, text: "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services", correct: false },
                { id: 1, text: "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC", correct: true },
                { id: 2, text: "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services", correct: false },
                { id: 3, text: "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
            options: [
                { id: 0, text: "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior", correct: false },
                { id: 1, text: "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration", correct: false },
                { id: 2, text: "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data", correct: true },
                { id: 3, text: "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands", correct: true },
                { id: 4, text: "Use Amazon Aurora Global Database to replicate data across regions for compatibility", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data, Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Aurora Global Database to replicate data across regions for compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
            options: [
                { id: 0, text: "Use VPC security groups to control the network traffic to and from your file system", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system", correct: false },
                { id: 2, text: "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system", correct: false },
                { id: 3, text: "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance", correct: false },
                { id: 4, text: "Use an IAM policy to control access for clients who can mount your file system with the required permissions", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use VPC security groups to control the network traffic to and from your file system, Use an IAM policy to control access for clients who can mount your file system with the required permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 1, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
                { id: 2, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket, Enable versioning on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
            options: [
                { id: 0, text: "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively", correct: false },
                { id: 1, text: "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures", correct: false },
                { id: 2, text: "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements", correct: false },
                { id: 3, text: "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
            options: [
                { id: 0, text: "The junior scientist does not need to pay any transfer charges for the image upload", correct: true },
                { id: 1, text: "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload", correct: false },
                { id: 2, text: "The junior scientist only needs to pay S3TA transfer charges for the image upload", correct: false },
                { id: 3, text: "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The junior scientist does not need to pay any transfer charges for the image upload\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay Amazon S3 transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
            options: [
                { id: 0, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 1, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon S3 Glacier Deep Archive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "It is not possible to access cross-account resources", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
            options: [
                { id: 0, text: "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EFS mount points", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with access to Amazon S3 based storage", correct: false },
                { id: 3, text: "Use Instance Store based Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Instance Store based Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EFS mount points: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with access to Amazon S3 based storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
            options: [
                { id: 0, text: "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies", correct: false },
                { id: 1, text: "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods", correct: true },
                { id: 3, text: "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
            options: [
                { id: 0, text: "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class", correct: false },
                { id: 1, text: "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class", correct: false },
                { id: 2, text: "Store the intermediary query results in Amazon S3 Standard storage class", correct: true },
                { id: 3, text: "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Store the intermediary query results in Amazon S3 Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow", correct: false },
                { id: 2, text: "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards", correct: false },
                { id: 3, text: "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
            options: [
                { id: 0, text: "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%", correct: false },
                { id: 2, text: "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%", correct: true },
                { id: 3, text: "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point", correct: true },
                { id: 1, text: "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution", correct: false },
                { id: 2, text: "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers", correct: true },
                { id: 1, text: "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers", correct: false },
                { id: 2, text: "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia", correct: false },
                { id: 3, text: "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
            options: [
                { id: 0, text: "AWS Global Accelerator", correct: true },
                { id: 1, text: "AWS Elastic Load Balancing (ELB)", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon CloudFront", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Global Accelerator\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- AWS Elastic Load Balancing (ELB): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance", correct: false },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB", correct: false },
                { id: 2, text: "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB", correct: true },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Microsoft SQL Server on AWS", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: false },
                { id: 3, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Microsoft SQL Server on AWS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume", correct: false },
                { id: 2, text: "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required", correct: true },
                { id: 3, text: "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks: SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.\n- Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
            options: [
                { id: 0, text: "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud", correct: true },
                { id: 1, text: "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
            options: [
                { id: 0, text: "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B", correct: true },
                { id: 1, text: "1 Amazon EC2 instance and 2 AMIs exist in Region B", correct: false },
                { id: 2, text: "1 Amazon EC2 instance and 1 AMI exist in Region B", correct: false },
                { id: 3, text: "1 Amazon EC2 instance and 1 snapshot exist in Region B", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- 1 Amazon EC2 instance and 2 AMIs exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 AMI exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 snapshot exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
            options: [
                { id: 0, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 1, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: true },
                { id: 2, text: "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 3, text: "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 1, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
            options: [
                { id: 0, text: "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 1, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: true },
                { id: 2, text: "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 3, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
            options: [
                { id: 0, text: "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets", correct: false },
                { id: 1, text: "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations", correct: true },
                { id: 2, text: "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files", correct: false },
                { id: 3, text: "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test3: [
        {
            id: 1,
            text: "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database. Which of the following would you recommend to securely share the database with the auditor?",
            options: [
                { id: 0, text: "Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access", correct: false },
                { id: 1, text: "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket", correct: false },
                { id: 2, text: "Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket", correct: false },
                { id: 3, text: "Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored. What do you recommend?",
            options: [
                { id: 0, text: "Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is", correct: false },
                { id: 1, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID", correct: true },
                { id: 2, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is", correct: false },
                { id: 3, text: "Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "\"An enterprise organization is expanding its cloud footprint and needs to centralize its security event data from various AWS accounts and services. The goal is to evaluate security posture across all environments and improve threat detection and response — without requiring significant custom code or manual integration. Which solution will fulfill these needs with the least development effort?",
            options: [
                { id: 0, text: "Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake", correct: true },
                { id: 1, text: "Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard", correct: false },
                { id: 2, text: "Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis", correct: false },
                { id: 3, text: "Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A manufacturing analytics company has a large collection of automated scripts that perform data cleanup, validation, and system integration tasks. These scripts are currently run by a local Linux cron scheduler and have an execution time of up to 30 minutes. The company wants to migrate these scripts to AWS without significant changes, and would prefer a containerized, serverless architecture that automatically scales and can respond to event-based triggers in the future. The solution must minimize infrastructure management. Which solution will best meet these requirements with minimal refactoring and operational overhead?",
            options: [
                { id: 0, text: "Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions", correct: false },
                { id: 1, text: "Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS", correct: true },
                { id: 2, text: "Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate", correct: false },
                { id: 3, text: "Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
            options: [
                { id: 0, text: "A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive", correct: false },
                { id: 1, text: "The health check grace period for the instance has not expired", correct: true },
                { id: 2, text: "The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)", correct: false },
                { id: 3, text: "The instance has failed the Elastic Load Balancing (ELB) health check status", correct: true },
                { id: 4, text: "A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks", correct: false },
                { id: 5, text: "The instance maybe in Impaired status", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: The health check grace period for the instance has not expired, The instance has failed the Elastic Load Balancing (ELB) health check status, The instance maybe in Impaired status\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located inus-east-1region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time. As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)",
            options: [
                { id: 0, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53", correct: false },
                { id: 1, text: "Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region", correct: false },
                { id: 2, text: "Create Amazon Aurora read replicas in the eu-west-1 region", correct: true },
                { id: 3, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53", correct: false },
                { id: 4, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Create Amazon Aurora read replicas in the eu-west-1 region, Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "Which of the following IAM policies provides read-only access to the Amazon S3 bucketmybucketand its content?",
            options: [
                { id: 0, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }", correct: true },
                { id: 1, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }", correct: false },
                { id: 2, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }", correct: false },
                { id: 3, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 weighted routing to spread traffic across different deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CodeDeploy deployment options to choose the right deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to distribute traffic across deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures. Which is the MOST cost-optimal solution for this workload?",
            options: [
                { id: 0, text: "Run the workload on Reserved Instances (RI)", correct: false },
                { id: 1, text: "Run the workload on a Spot Fleet", correct: true },
                { id: 2, text: "Run the workload on Spot Instances", correct: false },
                { id: 3, text: "Run the workload on Dedicated Hosts", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Run the workload on a Spot Fleet\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run the workload on Reserved Instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the workload on Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the workload on Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos. Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)",
            options: [
                { id: 0, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos", correct: true },
                { id: 1, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos", correct: false },
                { id: 2, text: "Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos", correct: true },
                { id: 3, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos", correct: false },
                { id: 4, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos, Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use Amazon EC2 Meta-Data", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that", correct: false },
                { id: 2, text: "Use Amazon EC2 User-Data", correct: false },
                { id: 3, text: "Use Amazon EC2 Instance Hibernate", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 Instance Hibernate\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Meta-Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 User-Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "An enterprise is building a secure business intelligence API using Amazon API Gateway to serve internal users with confidential analytics data. The API must be accessible only from a set of trusted IP addresses that are part of the organization's internal network ranges. No external IP traffic should be able to invoke the API. A solutions architect must design this access control mechanism with the least operational complexity. What should the architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list", correct: true },
                { id: 1, text: "Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs", correct: false },
                { id: 2, text: "Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses", correct: false },
                { id: 3, text: "Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms: Which is the MOST operationally efficient solution?",
            options: [
                { id: 0, text: "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation", correct: true },
                { id: 1, text: "Server-side encryption (SSE-S3) with automatic key rotation", correct: false },
                { id: 2, text: "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation", correct: false },
                { id: 3, text: "Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption (SSE-S3) with automatic key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available. As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
            options: [
                { id: 0, text: "Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)", correct: false },
                { id: 1, text: "Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)", correct: false },
                { id: 2, text: "Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)", correct: true },
                { id: 3, text: "Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ)us-east-1aas it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ)us-east-1alike so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour. Which of the following instances would be terminated per the default termination policy?",
            options: [
                { id: 0, text: "Instance C", correct: false },
                { id: 1, text: "Instance A", correct: false },
                { id: 2, text: "Instance B", correct: true },
                { id: 3, text: "Instance D", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instance B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Instance C: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance A: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance D: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform. Which of the following solutions would have the LEAST amount of downtime?",
            options: [
                { id: 0, text: "Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3", correct: false },
                { id: 1, text: "Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3", correct: true },
                { id: 2, text: "Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer", correct: false },
                { id: 3, text: "Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?",
            options: [
                { id: 0, text: "Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager", correct: true },
                { id: 1, text: "Create a VPC peering connection between all virtual private cloud (VPCs)", correct: false },
                { id: 2, text: "Create a Private Link between all the Amazon EC2 instances", correct: false },
                { id: 3, text: "Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a VPC peering connection between all virtual private cloud (VPCs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between all the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner. What could be the reason for this denial of permission for the bucket owner?",
            options: [
                { id: 0, text: "By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster", correct: true },
                { id: 1, text: "When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures", correct: false },
                { id: 2, text: "The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress", correct: false },
                { id: 3, text: "When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS. How should you configure the security groups? (Select three)",
            options: [
                { id: 0, text: "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443", correct: true },
                { id: 1, text: "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80", correct: false },
                { id: 2, text: "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80", correct: false },
                { id: 3, text: "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80", correct: true },
                { id: 4, text: "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432", correct: true },
                { id: 5, text: "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432", correct: false },
            ],
            correctAnswers: [0, 3, 4],
            explanation: "The correct answers are: The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443, The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80, The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved. As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
            options: [
                { id: 0, text: "Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations", correct: false },
                { id: 1, text: "Remove any overlapping namespaces for the private and public hosted zones", correct: false },
                { id: 2, text: "Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken", correct: false },
                { id: 3, text: "Enable DNS hostnames and DNS resolution for private hosted zones", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable DNS hostnames and DNS resolution for private hosted zones\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove any overlapping namespaces for the private and public hosted zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An enterprise uses a centralized Amazon S3 bucket to store logs and reports generated by multiple analytics services. Each service writes to and reads from a dedicated prefix (folder path) in the bucket. The company wants to enforce fine-grained access control so that each service can access only its own prefix, without being able to see or modify other services' data. The solution must support scalable and maintainable permissions management with minimal operational overhead. Which approach will best meet these requirements?",
            options: [
                { id: 0, text: "Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services", correct: false },
                { id: 1, text: "Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket", correct: true },
                { id: 2, text: "Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags", correct: false },
                { id: 3, text: "Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
            options: [
                { id: 0, text: "Enable encryption on the Amazon RDS database using the AWS Console", correct: false },
                { id: 1, text: "Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database", correct: false },
                { id: 2, text: "Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ", correct: false },
                { id: 3, text: "Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable encryption on the Amazon RDS database using the AWS Console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Purchase 70 on-demand instances and 30 reserved instances", correct: false },
                { id: 1, text: "Purchase 70 reserved instances (RIs) and 30 spot instances", correct: true },
                { id: 2, text: "Purchase 70 reserved instances and 30 on-demand instances", correct: false },
                { id: 3, text: "Purchase 70 on-demand instances and 30 spot instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Purchase 70 reserved instances (RIs) and 30 spot instances\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Purchase 70 on-demand instances and 30 reserved instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 70 reserved instances and 30 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 70 on-demand instances and 30 spot instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience. The company is looking at alternate database options and migrating database engines if required. What would you suggest?",
            options: [
                { id: 0, text: "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora", correct: true },
                { id: 1, text: "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump", correct: false },
                { id: 2, text: "Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database", correct: false },
                { id: 3, text: "Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A media production studio is building a content rendering and editing platform on AWS. The editing workstations and rendering tools require access to shared files over the SMB (Server Message Block) protocol. The studio wants a managed storage solution that is simple to set up, integrates easily with SMB clients, and minimizes ongoing operational tasks. Which solution will best meet the requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled", correct: false },
                { id: 1, text: "Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients", correct: false },
                { id: 2, text: "Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs", correct: false },
                { id: 3, text: "Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live. Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
            options: [
                { id: 0, text: "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration", correct: false },
                { id: 1, text: "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration", correct: false },
                { id: 2, text: "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration", correct: true },
                { id: 3, text: "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
                { id: 3, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage. As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
            options: [
                { id: 0, text: "Enable storage auto-scaling for Amazon RDS MySQL", correct: true },
                { id: 1, text: "Create read replica for Amazon RDS MySQL", correct: false },
                { id: 2, text: "Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required", correct: false },
                { id: 3, text: "Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable storage auto-scaling for Amazon RDS MySQL\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create read replica for Amazon RDS MySQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic. As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
            options: [
                { id: 0, text: "Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer", correct: false },
                { id: 1, text: "Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution", correct: false },
                { id: 2, text: "Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 1, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
                { id: 2, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 3, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A multinational logistics company is migrating its core systems to AWS. As part of this migration, the company has built an Amazon S3–based data lake to ingest and analyze supply chain data from external carriers and vendors. While some vendors have adopted the company’s modern REST-based APIs for S3 uploads, others operate legacy systems that rely exclusively on SFTP for file transfers. These vendors are unable or unwilling to modify their workflows to support S3 APIs. The company wants to provide these vendors with an SFTP-compatible solution that allows direct uploads to Amazon S3, and must use fully managed AWS services to avoid managing any infrastructure. It must also support identity federation so that internal teams can map vendor access securely to specific S3 buckets or prefixes. Which combination of options will provide a scalable and low-maintenance solution for this use case? (Select two)",
            options: [
                { id: 0, text: "Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access", correct: true },
                { id: 1, text: "Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions", correct: true },
                { id: 2, text: "Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes", correct: false },
                { id: 3, text: "Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint", correct: false },
                { id: 4, text: "Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access, Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas. Which of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
            options: [
                { id: 0, text: "There are data transfer charges for replicating data across AWS Regions", correct: true },
                { id: 1, text: "There are data transfer charges for replicating data within the same Availability Zone (AZ)", correct: false },
                { id: 2, text: "There are no data transfer charges for replicating data across AWS Regions", correct: false },
                { id: 3, text: "There are data transfer charges for replicating data within the same AWS Region", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: There are data transfer charges for replicating data across AWS Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- There are data transfer charges for replicating data within the same Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- There are no data transfer charges for replicating data across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- There are data transfer charges for replicating data within the same AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves theAdministratorAccessmanaged policy. How should you proceed?",
            options: [
                { id: 0, text: "For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves", correct: true },
                { id: 1, text: "Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy", correct: false },
                { id: 2, text: "Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves", correct: false },
                { id: 3, text: "Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
                { id: 1, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 2, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 3, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 4, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: By default, user data runs only during the boot cycle when you first launch an instance, By default, scripts entered as user data are executed with root user privileges\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- By default, scripts entered as user data do not have root user privileges for executing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When an instance is running, you can update user data by using root user credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A wildlife research organization uses IoT-based motion sensors attached to thousands of migrating animals to monitor their movement across regions. Every few minutes, a sensor checks for significant movement and sends updated location data to a backend application running on Amazon EC2 instances spread across multiple Availability Zones in a single AWS Region. Recently, an unexpected surge in motion data overwhelmed the application, leading to lost location records with no mechanism to replay missed data. A solutions architect must redesign the ingestion mechanism to prevent future data loss and to minimize operational overhead. What should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages", correct: false },
                { id: 1, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages", correct: true },
                { id: 2, text: "Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically", correct: false },
                { id: 3, text: "Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 36,
            text: "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key. Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
            options: [
                { id: 0, text: "Server-Side Encryption with Customer-Provided Keys (SSE-C)", correct: true },
                { id: 1, text: "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3", correct: false },
                { id: 2, text: "Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)", correct: false },
                { id: 3, text: "Server-Side Encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Server-Side Encryption with Customer-Provided Keys (SSE-C)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-Side Encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has historically operated only in theus-east-1region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into theus-west-1AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions. Which of the following represents the best solution to address these requirements?",
            options: [
                { id: 0, text: "Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region", correct: false },
                { id: 1, text: "Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets", correct: false },
                { id: 2, text: "Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region", correct: false },
                { id: 3, text: "Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "A media publishing company is migrating its legacy content management application to AWS. Currently, the application and its MySQL database run on a single on-premises virtual machine, which creates a single point of failure and limits scalability. As traffic has increased due to growing reader engagement and video uploads, the company needs to redesign the solution to ensure automatic scaling, high availability, and separation of application and database layers. The company wants to continue using a MySQL-compatible engine and needs a cost-effective, managed solution that minimizes operational overhead. Which AWS architecture will best fulfill these requirements?",
            options: [
                { id: 0, text: "Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database", correct: false },
                { id: 1, text: "Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries", correct: false },
                { id: 2, text: "Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage", correct: false },
                { id: 3, text: "Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 39,
            text: "Consider the following policy associated with an IAM group containing several users: Which of the following options is correct?",
            options: [
                { id: 0, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200", correct: false },
                { id: 1, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200", correct: true },
                { id: 2, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200", correct: false },
                { id: 3, text: "Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution. Which of the following represents the MOST cost-optimal and high-performance solution?",
            options: [
                { id: 0, text: "Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution", correct: true },
                { id: 1, text: "Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin", correct: false },
                { id: 2, text: "Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin", correct: false },
                { id: 3, text: "Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A video conferencing platform serves users worldwide through a globally distributed deployment of Amazon EC2 instances behind Network Load Balancers (NLBs) in several AWS Regions. The platform's architecture currently allows clients to connect to any Region via public endpoints, depending on how DNS resolves. However, users in regions far from the load balancers frequently experience high latency and slow connection times, especially during session initiation. The company wants to optimize the experience for global users by reducing end-to-end latency and load time while keeping the existing NLBs and EC2-based application infrastructure in place. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution", correct: false },
                { id: 1, text: "Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances", correct: false },
                { id: 2, text: "Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy", correct: false },
                { id: 3, text: "Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?",
            options: [
                { id: 0, text: "General Purpose", correct: false },
                { id: 1, text: "Bursting Throughput", correct: false },
                { id: 2, text: "Provisioned Throughput", correct: false },
                { id: 3, text: "Max I/O", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Max I/O\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Bursting Throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned Throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 44,
            text: "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
            options: [
                { id: 0, text: "Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)", correct: true },
                { id: 1, text: "Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand", correct: false },
                { id: 2, text: "Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand", correct: false },
                { id: 3, text: "Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "What does this IAM policy do?",
            options: [
                { id: 0, text: "It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region", correct: false },
                { id: 1, text: "It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region", correct: false },
                { id: 2, text: "It allows running Amazon EC2 instances anywhere but in the eu-west-1 region", correct: false },
                { id: 3, text: "It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows running Amazon EC2 instances anywhere but in the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A financial institution is transitioning its critical back-office systems to AWS. These systems currently rely on Microsoft SQL Server databases hosted on on-premises infrastructure. The data is highly sensitive and subject to regulatory compliance. The organization wants to enhance security and minimize database management tasks as part of the migration. Which solution will best meet these goals with the least operational burden?",
            options: [
                { id: 0, text: "Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data", correct: false },
                { id: 1, text: "Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key", correct: true },
                { id: 2, text: "Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption", correct: false },
                { id: 3, text: "Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture. As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)",
            options: [
                { id: 0, text: "For security group B: Add an inbound rule that allows traffic only from security group A on port 443", correct: false },
                { id: 1, text: "For security group B: Add an inbound rule that allows traffic only from all sources on port 1433", correct: false },
                { id: 2, text: "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443", correct: false },
                { id: 3, text: "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433", correct: true },
                { id: 4, text: "For security group B: Add an inbound rule that allows traffic only from security group A on port 1433", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433, For security group B: Add an inbound rule that allows traffic only from security group A on port 1433\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- For security group B: Add an inbound rule that allows traffic only from security group A on port 443: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- For security group B: Add an inbound rule that allows traffic only from all sources on port 1433: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation. Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
            options: [
                { id: 0, text: "Build a shared services Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 1, text: "Use VPCs connected with AWS Direct Connect", correct: false },
                { id: 2, text: "Use Fully meshed VPC Peering connection", correct: false },
                { id: 3, text: "Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Build a shared services Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPCs connected with AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Fully meshed VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 2, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 3, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this IAM policy do?",
            options: [
                { id: 0, text: "It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block", correct: false },
                { id: 1, text: "It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block", correct: true },
                { id: 2, text: "It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block", correct: false },
                { id: 3, text: "It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A financial services company runs a Kubernetes-based microservices application in its on-premises data center. The application uses the Advanced Message Queuing Protocol (AMQP) to interact with a message queue. The company is experiencing rapid growth and its on-prem infrastructure cannot scale fast enough. The company wants to migrate the application to AWS with minimal code changes and reduce infrastructure management overhead. The messaging component must continue using AMQP, and the solution should offer high scalability and low operational effort. Which combination of options will together meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes", correct: true },
                { id: 1, text: "Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic", correct: false },
                { id: 3, text: "Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery", correct: false },
                { id: 4, text: "Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes, Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances. As a solutions architect, which of the following actions would you recommend to stop the attacks?",
            options: [
                { id: 0, text: "Create a deny rule for the malicious IP in the Security Groups associated with each of the instances", correct: false },
                { id: 1, text: "Create an IP match condition in the AWS WAF to block the malicious IP address", correct: true },
                { id: 2, text: "Create a ticket with AWS support to take action against the malicious IP", correct: false },
                { id: 3, text: "Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an IP match condition in the AWS WAF to block the malicious IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a deny rule for the malicious IP in the Security Groups associated with each of the instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a ticket with AWS support to take action against the malicious IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A SaaS company is modernizing one of its legacy web applications by migrating it to AWS. The company aims to improve the availability of the application during both normal and peak traffic periods. Additionally, the company wants to implement protection against common web exploits and malicious traffic. The architecture must be scalable and integrate AWS WAF to secure incoming traffic. Which solution will best meet these requirements with high availability and minimal configuration complexity?",
            options: [
                { id: 0, text: "Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection", correct: false },
                { id: 1, text: "Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering", correct: false },
                { id: 2, text: "Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic", correct: false },
                { id: 3, text: "Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
            options: [
                { id: 0, text: "Create an AWS Snowball job and target a Amazon S3 Glacier Vault", correct: false },
                { id: 1, text: "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day", correct: true },
                { id: 2, text: "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day", correct: false },
                { id: 3, text: "Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Snowball job and target a Amazon S3 Glacier Vault: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 1, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute. Which of the following options would you recommend?",
            options: [
                { id: 0, text: "Set up an Amazon Aurora provisioned Database cluster", correct: false },
                { id: 1, text: "Set up an Amazon Aurora Global Database cluster", correct: true },
                { id: 2, text: "Set up an Amazon Aurora multi-master Database cluster", correct: false },
                { id: 3, text: "Set up an Amazon Aurora serverless Database cluster", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon Aurora Global Database cluster\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon Aurora provisioned Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Aurora multi-master Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Aurora serverless Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a read replica and modify the application to use the appropriate endpoint\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Activate read-through caching on the Amazon Aurora database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the application to read from the Multi-AZ standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed. Which of the following options represents the best solution for the given requirements?",
            options: [
                { id: 0, text: "Amazon Elastic File System (EFS) Standard–IA storage class", correct: true },
                { id: 1, text: "Amazon Elastic Block Store (EBS)", correct: false },
                { id: 2, text: "Amazon Elastic File System (EFS) Standard storage class", correct: false },
                { id: 3, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Elastic File System (EFS) Standard–IA storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic Block Store (EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic File System (EFS) Standard storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global media agency is developing a cultural analysis project to explore how major sports stories have evolved over the last five years. The team has collected thousands of archived news bulletins and magazine spreads stored in PDF format. These documents are rich in unstructured text and come from various sources with differing layouts and font styles. The agency wants to better understand how public tone and narrative have shifted over time. The team has chosen to use Amazon Textract for its ability to accurately extract printed and scanned text from complex PDF layouts. They need a solution that can then analyze the emotional tone and subject matter of the extracted text with the least possible operational burden, using fully managed AWS services where possible. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.", correct: false },
                { id: 1, text: "Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.", correct: false },
                { id: 2, text: "Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.", correct: true },
                { id: 3, text: "Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which of the following actions meets the given requirements?",
            options: [
                { id: 0, text: "Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user", correct: false },
                { id: 1, text: "Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account", correct: false },
                { id: 2, text: "Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled", correct: false },
                { id: 3, text: "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A mobile app allows users to submit photos, which are stored in an Amazon S3 bucket. Currently, a batch of Amazon EC2 Spot Instances is launched nightly to process all the day’s uploads. Each photo requires approximately 3 minutes and 512 MB of memory to process. To improve responsiveness and minimize costs, the company wants to shift to near real-time image processing that begins as soon as an image is uploaded. Which solution will provide the MOST cost-effective and scalable architecture to meet these new requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt", correct: false },
                { id: 1, text: "Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously", correct: true },
                { id: 2, text: "Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image", correct: false },
                { id: 3, text: "Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)",
            options: [
                { id: 0, text: "Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation", correct: true },
                { id: 1, text: "Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format", correct: false },
                { id: 2, text: "Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format", correct: true },
                { id: 3, text: "Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation", correct: false },
                { id: 4, text: "Create an AWS Lambda function based job to delete the raw zone data after 1 day", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation, Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Lambda function based job to delete the raw zone data after 1 day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines. Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
            options: [
                { id: 0, text: "Spot Instances", correct: false },
                { id: 1, text: "On-Demand Instances", correct: false },
                { id: 2, text: "Dedicated Hosts", correct: false },
                { id: 3, text: "Dedicated Instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend. Which of the following is the correct outcome during the maintenance window?",
            options: [
                { id: 0, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade", correct: false },
                { id: 1, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete", correct: true },
                { id: 2, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade", correct: false },
                { id: 3, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud. What do you recommend as a replacement for the DFSR?",
            options: [
                { id: 0, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 3, text: "Amazon FSx for Lustre", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test4: [
        {
            id: 1,
            text: "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
            options: [
                { id: 0, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold", correct: false },
                { id: 1, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold", correct: false },
                { id: 2, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold", correct: false },
                { id: 3, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
                { id: 1, text: "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: true },
                { id: 2, text: "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: false },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 2, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 4, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
                { id: 5, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue, Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
            options: [
                { id: 0, text: "Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint", correct: false },
                { id: 1, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: false },
                { id: 2, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: true },
                { id: 3, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: true },
                { id: 4, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint, Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 1, text: "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 2, text: "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 3, text: "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte. As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
            options: [
                { id: 0, text: "Use Amazon EFS with Bursting Throughput mode", correct: false },
                { id: 1, text: "Use Amazon EFS with Provisioned Throughput mode", correct: true },
                { id: 2, text: "Use Amazon DynamoDB table that is accessible by all ECS cluster instances", correct: false },
                { id: 3, text: "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EFS with Provisioned Throughput mode\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EFS with Bursting Throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB table that is accessible by all ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon EBS volume mounted to the Amazon ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead. What solution would best meet these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume", correct: true },
                { id: 1, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
                { id: 2, text: "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume", correct: false },
                { id: 3, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
            options: [
                { id: 0, text: "Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads", correct: true },
                { id: 1, text: "Use Amazon ElastiCache to improve the performance of compute-intensive workloads", correct: true },
                { id: 2, text: "Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads", correct: false },
                { id: 3, text: "Use Amazon ElastiCache to run highly complex JOIN queries", correct: false },
                { id: 4, text: "Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads, Use Amazon ElastiCache to improve the performance of compute-intensive workloads\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to run highly complex JOIN queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
            options: [
                { id: 0, text: "You can use an IP address as the custom source for the inbound rule", correct: false },
                { id: 1, text: "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule", correct: false },
                { id: 2, text: "You can use an Internet Gateway ID as the custom source for the inbound rule", correct: true },
                { id: 3, text: "You can use a security group as the custom source for the inbound rule", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: You can use an Internet Gateway ID as the custom source for the inbound rule\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You can use an IP address as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a security group as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A national logistics company has a dedicated AWS Direct Connect connection from its corporate data center to AWS. Within its AWS account, the company operates 25 Amazon VPCs in the same Region, each supporting different regional distribution services. The VPCs were configured with non-overlapping CIDR blocks and currently use private VIFs for Direct Connect access to on-premises resources. As the architecture scales, the company wants to enable communication across all VPCs and the on-premises environment. The solution must scale efficiently, support full-mesh connectivity, and reduce the complexity of maintaining separate private VIFs for each VPC. Which combination of solutions will best fulfill these requirements with the least amount of operational overhead? (Select two)",
            options: [
                { id: 0, text: "Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs", correct: false },
                { id: 1, text: "Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services", correct: false },
                { id: 2, text: "Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway", correct: true },
                { id: 3, text: "Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing", correct: true },
                { id: 4, text: "Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway, Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic", correct: false },
                { id: 1, text: "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic", correct: false },
                { id: 2, text: "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint", correct: true },
                { id: 3, text: "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A regional transportation authority operates a high-traffic public information portal hosted on AWS. The backend consists of Amazon EC2 instances behind an Application Load Balancer (ALB). In recent weeks, the operations team has observed intermittent slowdowns and performance issues. After investigation, the team suspect the application is being targeted by distributed denial-of-service (DDoS) attacks coming from a wide range of IP addresses. The team needs a solution that provides DDoS mitigation, detailed logs for audit purposes, and requires minimal changes to the existing architecture. Which solution best addresses these needs?",
            options: [
                { id: 0, text: "Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly", correct: false },
                { id: 1, text: "Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events", correct: true },
                { id: 2, text: "Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service", correct: false },
                { id: 1, text: "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users", correct: false },
                { id: 2, text: "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face", correct: true },
                { id: 3, text: "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A healthcare startup is modernizing its monolithic Python-based analytics application by transitioning to a microservices architecture on AWS. As a pilot, the team wants to refactor one module into a standalone microservice that can handle hundreds of requests per second. They are seeking an AWS-native solution that supports Python, scales automatically with traffic, and requires minimal infrastructure management and operational overhead to build, test, and deploy the service efficiently. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup", correct: false },
                { id: 1, text: "Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads", correct: true },
                { id: 2, text: "Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments", correct: false },
                { id: 3, text: "Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A company has its application servers in the public subnet that connect to the database instances in the private subnet. For regular maintenance, the database instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
            options: [
                { id: 0, text: "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC", correct: false },
                { id: 1, text: "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables", correct: false },
                { id: 2, text: "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC", correct: false },
                { id: 3, text: "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Egress-only internet gateway for the resources in the private subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
            options: [
                { id: 0, text: "General Purpose SSD (gp2)", correct: false },
                { id: 1, text: "Provisioned IOPS SSD (io1)", correct: true },
                { id: 2, text: "Throughput Optimized HDD (st1)", correct: false },
                { id: 3, text: "Cold HDD (sc1)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD (io1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose SSD (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Throughput Optimized HDD (st1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD (sc1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer", correct: false },
                { id: 1, text: "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions", correct: false },
                { id: 2, text: "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints", correct: true },
                { id: 3, text: "Configure Elastic IPs for each of the Application Load Balancers in each Region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Elastic IPs for each of the Application Load Balancers in each Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account. Which of the following solutions should a solutions architect configure for the necessary permissions?",
            options: [
                { id: 0, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly", correct: false },
                { id: 1, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly", correct: false },
                { id: 2, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role", correct: false },
                { id: 3, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance\n\nIAM roles provide temporary credentials and are the recommended way to grant permissions to AWS services and applications. They're more secure than access keys as credentials are automatically rotated and don't need to be stored.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
            options: [
                { id: 0, text: "Create a CNAME record", correct: true },
                { id: 1, text: "Create an A record", correct: false },
                { id: 2, text: "Create a PTR record", correct: false },
                { id: 3, text: "Create an Alias Record", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a CNAME record\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an A record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a PTR record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Alias Record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets", correct: false },
                { id: 1, text: "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets", correct: true },
                { id: 2, text: "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture", correct: false },
                { id: 3, text: "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 22,
            text: "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
            options: [
                { id: 0, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: false },
                { id: 1, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each", correct: false },
                { id: 2, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each", correct: false },
                { id: 3, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A financial services firm runs a containerized risk analytics tool in its on-premises data center using Docker. The tool depends on persistent data storage for maintaining customer simulation results and operates on a single host machine where the volume is locally mounted. The infrastructure team is looking to replatform the tool to AWS using a fully managed service because they want to avoid managing EC2 instances, volumes, or underlying servers. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually", correct: false },
                { id: 1, text: "Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically", correct: false },
                { id: 2, text: "Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage", correct: false },
                { id: 3, text: "Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 2, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 3, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
            options: [
                { id: 0, text: "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 1, text: "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 2, text: "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services", correct: true },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A financial services company is modernizing its analytics platform on AWS. Their legacy data processing scripts, built for both Windows and Linux environments, require shared access to a file system that supports Windows ACLs and SMB protocol for compatibility with existing Windows workloads. At the same time, their Linux-based applications need to read from and write to the same shared storage to maintain cross-platform consistency. The solutions architect needs to design a storage solution that allows both Windows and Linux EC2 instances to access the shared file system simultaneously, while preserving Windows-specific features like NTFS permissions and Active Directory (AD) integration. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms", correct: false },
                { id: 1, text: "Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances", correct: false },
                { id: 2, text: "Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads", correct: false },
                { id: 3, text: "Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
            options: [
                { id: 0, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume", correct: true },
                { id: 1, text: "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected", correct: false },
                { id: 2, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 1, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 2, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
            options: [
                { id: 0, text: "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets", correct: false },
                { id: 1, text: "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data", correct: false },
                { id: 2, text: "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively", correct: true },
                { id: 3, text: "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in theus-west-2region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of theus-west-2region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
            options: [
                { id: 0, text: "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts", correct: false },
                { id: 1, text: "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 2, text: "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 3, text: "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
            options: [
                { id: 0, text: "Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions", correct: false },
                { id: 1, text: "Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions", correct: true },
                { id: 2, text: "Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy", correct: false },
                { id: 3, text: "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use a simple scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 1, text: "Use a target tracking scaling policy based on a custom Amazon SQS queue metric", correct: true },
                { id: 2, text: "Use a step scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 3, text: "Use a scheduled scaling policy based on a custom Amazon SQS queue metric", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a target tracking scaling policy based on a custom Amazon SQS queue metric\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a simple scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a step scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a scheduled scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 1, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
                { id: 2, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
            options: [
                { id: 0, text: "Use SQS message timer to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 1, text: "Use SQS long polling to retrieve messages from your Amazon SQS queues", correct: true },
                { id: 2, text: "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 3, text: "Use SQS short polling to retrieve messages from your Amazon SQS queues", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use SQS long polling to retrieve messages from your Amazon SQS queues\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use SQS message timer to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS visibility timeout to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS short polling to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
            options: [
                { id: 0, text: "Service control policy (SCP) affects service-linked roles", correct: false },
                { id: 1, text: "Service control policy (SCP) does not affect service-linked role", correct: true },
                { id: 2, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action", correct: false },
                { id: 3, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action", correct: true },
                { id: 4, text: "Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts", correct: true },
                { id: 5, text: "Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts", correct: false },
            ],
            correctAnswers: [1, 3, 4],
            explanation: "The correct answers are: Service control policy (SCP) does not affect service-linked role, If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action, Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Service control policy (SCP) affects service-linked roles: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
            options: [
                { id: 0, text: "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: false },
                { id: 1, text: "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ", correct: false },
                { id: 2, text: "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: true },
                { id: 3, text: "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
            options: [
                { id: 0, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot", correct: false },
                { id: 1, text: "You cannot share an Amazon Machine Image (AMI) with another AWS account", correct: false },
                { id: 2, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot", correct: true },
                { id: 3, text: "You can share an Amazon Machine Image (AMI) with another AWS account", correct: true },
                { id: 4, text: "You cannot copy an Amazon Machine Image (AMI) across AWS Regions", correct: false },
                { id: 5, text: "You can copy an Amazon Machine Image (AMI) across AWS Regions", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot, You can share an Amazon Machine Image (AMI) with another AWS account, You can copy an Amazon Machine Image (AMI) across AWS Regions\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot share an Amazon Machine Image (AMI) with another AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot copy an Amazon Machine Image (AMI) across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
            options: [
                { id: 0, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy", correct: false },
                { id: 1, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy", correct: true },
                { id: 2, text: "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy", correct: false },
                { id: 3, text: "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 1, text: "Security Groups can be associated with a NAT gateway", correct: false },
                { id: 2, text: "NAT gateway supports port forwarding", correct: false },
                { id: 3, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 4, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 5, text: "NAT instance supports port forwarding", correct: true },
            ],
            correctAnswers: [0, 4, 5],
            explanation: "The correct answers are: Security Groups can be associated with a NAT instance, NAT instance can be used as a bastion server, NAT instance supports port forwarding\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 1, text: "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones", correct: true },
                { id: 2, text: "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 3, text: "Add Amazon EventBridge to decouple the complex architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon EventBridge to decouple the complex architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Glue", correct: false },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Database Migration Service (AWS DMS)", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
            options: [
                { id: 0, text: "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 1, text: "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 2, text: "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 3, text: "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 1, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs", correct: false },
                { id: 2, text: "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data. How will you implement this requirement without adding the overhead of splitting the data into logical groups?",
            options: [
                { id: 0, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: true },
                { id: 1, text: "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data", correct: false },
                { id: 2, text: "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: false },
                { id: 3, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data", correct: false },
                { id: 1, text: "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data", correct: true },
                { id: 2, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data", correct: false },
                { id: 3, text: "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
            options: [
                { id: 0, text: "Amazon Cloud Directory", correct: false },
                { id: 1, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 2, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 3, text: "Active Directory Connector", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Cloud Directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 3, text: "File Gateway Configuration of AWS Storage Gateway", correct: true },
                { id: 4, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon FSx for Windows File Server, File Gateway Configuration of AWS Storage Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic Block Store (Amazon EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A startup is building a serverless microservices architecture where client applications (web and mobile) authenticate users via a third-party OIDC-compliant identity provider. The backend APIs must validate JSON Web Tokens (JWTs) issued by this provider, enforce scope-based access control, and be cost-effective with minimal latency. The development team wants to use a fully managed service that supports JWT validation natively, without writing custom authentication logic. Which solution should the team implement to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider", correct: true },
                { id: 1, text: "Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer", correct: false },
                { id: 2, text: "Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens", correct: false },
                { id: 3, text: "Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects", correct: true },
                { id: 2, text: "Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy", correct: false },
                { id: 4, text: "Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects, Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A media streaming startup is building a set of backend APIs that will be consumed by external mobile applications. To prevent API abuse, protect downstream resources, and ensure fair usage across clients, the architecture must enforce rate limiting and throttling on a per-client basis. The team also wants to define usage quotas and apply different limits to different API consumers. Which solution should the team implement to enforce rate limiting and usage quotas at the API layer?",
            options: [
                { id: 0, text: "Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances", correct: false },
                { id: 1, text: "Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits", correct: false },
                { id: 2, text: "Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client", correct: true },
                { id: 3, text: "Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling", correct: true },
                { id: 1, text: "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas", correct: false },
                { id: 2, text: "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling", correct: false },
                { id: 3, text: "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A geospatial analytics firm recently completed a large-scale seafloor imaging project and used AWS Snowball Edge Storage Optimized devices to collect and transfer over 150 TB of raw sonar data. The firm uses a high-performance computing (HPC) cluster on AWS to process and analyze massive datasets to identify natural resource formations. The processing workloads require sub-millisecond latency, high-throughput, fast and parallel file access to the imported dataset once the Snowball Edge devices are returned to AWS and the data is ingested. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel", correct: false },
                { id: 1, text: "Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data", correct: true },
                { id: 2, text: "Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access", correct: false },
                { id: 3, text: "Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
            options: [
                { id: 0, text: "Use Amazon FSx for Windows File Server as a shared storage solution", correct: true },
                { id: 1, text: "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies", correct: false },
                { id: 2, text: "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution", correct: false },
                { id: 3, text: "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon FSx for Windows File Server as a shared storage solution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic File System (Amazon EFS) as a shared storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway of AWS Storage Gateway to create a hybrid storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 1, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
            options: [
                { id: 0, text: "If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery", correct: false },
                { id: 1, text: "Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance", correct: false },
                { id: 2, text: "During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained", correct: false },
                { id: 3, text: "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery", correct: true },
                { id: 4, text: "A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If your instance has a public IPv4 address, it retains the public IPv4 address after recovery, A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A software company manages a fleet of Amazon EC2 instances that support internal analytics applications. These instances use an IAM role with custom policies to connect to Amazon RDS and AWS Secrets Manager for secure access to credentials and database endpoints. The IT operations team wants to implement a centralized patch management solution that simplifies compliance and security tasks. Their goal is to automate OS patching across EC2 instances without disrupting the running applications. Which approach will allow the company to meet these goals with the least administrative overhead?",
            options: [
                { id: 0, text: "Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts", correct: false },
                { id: 1, text: "Enable Default Host Management Configuration in AWS Systems Manager Quick Setup", correct: true },
                { id: 2, text: "Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features", correct: false },
                { id: 3, text: "Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Default Host Management Configuration in AWS Systems Manager Quick Setup\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
            options: [
                { id: 0, text: "If a spot request is persistent, then it is opened again after your Spot Instance is interrupted", correct: true },
                { id: 1, text: "Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: true },
                { id: 2, text: "When you cancel an active spot request, it terminates the associated instance as well", correct: false },
                { id: 3, text: "Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: false },
                { id: 4, text: "If a spot request is persistent, then it is opened again after you stop the Spot Instance", correct: false },
                { id: 5, text: "When you cancel an active spot request, it does not terminate the associated instance", correct: true },
            ],
            correctAnswers: [0, 1, 5],
            explanation: "The correct answers are: If a spot request is persistent, then it is opened again after your Spot Instance is interrupted, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, When you cancel an active spot request, it does not terminate the associated instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- When you cancel an active spot request, it terminates the associated instance as well: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a spot request is persistent, then it is opened again after you stop the Spot Instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
            options: [
                { id: 0, text: "Internet Gateway (I1)", correct: true },
                { id: 1, text: "Route Table (R1)", correct: false },
                { id: 2, text: "Subnet (S1)", correct: false },
                { id: 3, text: "Network Address Translation (NAT) instance (N1)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Internet Gateway (I1)\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Route Table (R1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subnet (S1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Network Address Translation (NAT) instance (N1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 3, text: "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
            options: [
                { id: 0, text: "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job", correct: true },
                { id: 1, text: "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script", correct: false },
                { id: 2, text: "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
                { id: 3, text: "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions", correct: false },
                { id: 1, text: "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions", correct: true },
                { id: 2, text: "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions", correct: false },
                { id: 3, text: "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
            options: [
                { id: 0, text: "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data", correct: false },
                { id: 1, text: "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written", correct: false },
                { id: 2, text: "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written", correct: false },
                { id: 3, text: "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
            options: [
                { id: 0, text: "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ", correct: false },
                { id: 1, text: "Create a read replica and connect the report generation tool/application to it", correct: true },
                { id: 2, text: "Migrate from General Purpose SSD to magnetic storage to enhance IOPS", correct: false },
                { id: 3, text: "Increase the size of Amazon RDS instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a read replica and connect the report generation tool/application to it\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate from General Purpose SSD to magnetic storage to enhance IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the size of Amazon RDS instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test5: [
        {
            id: 1,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 2, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 3, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
                { id: 4, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
            options: [
                { id: 0, text: "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions", correct: true },
                { id: 2, text: "Use Amazon EC2 user data to speed up the installation process", correct: false },
                { id: 3, text: "Store the installation files in Amazon S3 for quicker retrieval", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to speed up the installation process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the installation files in Amazon S3 for quicker retrieval: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
            options: [
                { id: 0, text: "Set up Amazon DynamoDB table with a global secondary index", correct: false },
                { id: 1, text: "Set up Amazon DynamoDB table in the on-demand capacity mode", correct: true },
                { id: 2, text: "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled", correct: false },
                { id: 3, text: "Set up Amazon DynamoDB global table in the provisioned capacity mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up Amazon DynamoDB table in the on-demand capacity mode\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon DynamoDB table with a global secondary index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB global table in the provisioned capacity mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
            options: [
                { id: 0, text: "EXAMPLE.COM", correct: false },
                { id: 1, text: "example.test.com", correct: false },
                { id: 2, text: "test.example.com", correct: true },
                { id: 3, text: "example.com", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: test.example.com\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- EXAMPLE.COM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.test.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 1, text: "Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
                { id: 3, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
            options: [
                { id: 0, text: "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels", correct: true },
                { id: 1, text: "Use AWS Global Accelerator for the VPN connection to maximize the throughput", correct: false },
                { id: 2, text: "Use Transfer Acceleration for the VPN connection to maximize the throughput", correct: false },
                { id: 3, text: "Create a virtual private gateway with equal cost multipath routing and multiple channels", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transfer Acceleration for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway with equal cost multipath routing and multiple channels: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Dedicated hosts for the minimum capacity", correct: false },
                { id: 1, text: "Set the minimum capacity to 3", correct: false },
                { id: 2, text: "Use Reserved Instances (RIs) for the minimum capacity", correct: true },
                { id: 3, text: "Set the minimum capacity to 2", correct: true },
                { id: 4, text: "Set the minimum capacity to 1", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Reserved Instances (RIs) for the minimum capacity, Set the minimum capacity to 2\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Dedicated hosts for the minimum capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 1: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
            options: [
                { id: 0, text: "Use a wildcard Secure Sockets Layer certificate (SSL certificate)", correct: false },
                { id: 1, text: "Use Secure Sockets Layer certificate (SSL certificate) with SNI", correct: true },
                { id: 2, text: "Change the Elastic Load Balancing (ELB) SSL Security Policy", correct: false },
                { id: 3, text: "Use an HTTP to HTTPS redirect", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Secure Sockets Layer certificate (SSL certificate) with SNI\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a wildcard Secure Sockets Layer certificate (SSL certificate): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the Elastic Load Balancing (ELB) SSL Security Policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an HTTP to HTTPS redirect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Spot Instances", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon EC2 Reserved Instances (RIs)", correct: false },
                { id: 4, text: "Amazon EC2 On-Demand Instances", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Amazon EC2 Spot Instances, Amazon Simple Queue Service (Amazon SQS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Reserved Instances (RIs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
            options: [
                { id: 0, text: "The Time To Live (TTL) is still in effect", correct: true },
                { id: 1, text: "The health checks are failing", correct: false },
                { id: 2, text: "The Alias Record is misconfigured", correct: false },
                { id: 3, text: "The CNAME Record is misconfigured", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The Time To Live (TTL) is still in effect\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The health checks are failing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Alias Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CNAME Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size10.0.1.0/24and the Auto Scaling group is deployed in a subnet of size10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
            options: [
                { id: 0, text: "Add a rule to authorize the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "Add a rule to authorize the CIDR 10.0.1.0/24", correct: false },
                { id: 2, text: "Add a rule to authorize the security group of the Auto Scaling group", correct: false },
                { id: 3, text: "Add a rule to authorize the CIDR 10.0.4.0/22", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Add a rule to authorize the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Add a rule to authorize the CIDR 10.0.1.0/24: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the security group of the Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the CIDR 10.0.4.0/22: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A fintech startup hosts its real-time transaction metadata in Amazon DynamoDB tables. During a recent system maintenance event, a junior engineer accidentally deleted a production table, resulting in major service downtime and irreversible data loss. Leadership has mandated an immediate solution that prevents future data loss from human error, while requiring minimal ongoing maintenance or manual effort from the engineering team. Which approach best addresses these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Enable deletion protection on DynamoDB tables", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3", correct: false },
                { id: 2, text: "Enable point-in-time recovery (PITR) on each DynamoDB table", correct: false },
                { id: 3, text: "Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable deletion protection on DynamoDB tables\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable point-in-time recovery (PITR) on each DynamoDB table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare provider is experiencing rapid data growth in its on-premises servers due to increased patient imaging and record retention requirements. The organization wants to extend its storage capacity to AWS in a way that preserves quick access to critical records, including from its local file systems. The company must optimize bandwidth usage during migration and avoid any retrieval fees or delays when accessing the data in the cloud. The provider wants a hybrid cloud solution that requires minimal application reconfiguration, allows frequent local access to key datasets, and ensures that cloud storage costs remain predictable without paying extra for data retrieval. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates", correct: false },
                { id: 1, text: "Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement", correct: false },
                { id: 2, text: "Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3", correct: true },
                { id: 3, text: "Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A genomics research firm is processing sporadic bursts of data-intensive workloads using Amazon EC2 instances. The shared storage must support unpredictable spikes in file operations, but the average daily throughput demand remains relatively low. The team has selected Amazon Elastic File System (EFS) for its scalability and wants to ensure optimal cost and performance during bursts without provisioning throughput manually. Which approach should the team take to best meet these requirements?",
            options: [
                { id: 0, text: "Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads", correct: false },
                { id: 1, text: "Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class", correct: true },
                { id: 2, text: "Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode", correct: false },
                { id: 3, text: "Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use a Cluster placement group", correct: true },
                { id: 1, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 2, text: "Use Spot Instances", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
            options: [
                { id: 0, text: "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment", correct: true },
                { id: 1, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration", correct: false },
                { id: 2, text: "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration", correct: false },
                { id: 3, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
            options: [
                { id: 0, text: "Use a Network Address Translation gateway (NAT gateway)", correct: false },
                { id: 1, text: "Use a VPC peering connection", correct: true },
                { id: 2, text: "Use an AWS Direct Connect connection", correct: false },
                { id: 3, text: "Use an Internet Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a VPC peering connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use a Network Address Translation gateway (NAT gateway): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Direct Connect connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Internet Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 18,
            text: "A transportation logistics company runs a shipment tracking application on Amazon EC2 instances with an Amazon Aurora MySQL database cluster. The application is experiencing rapid growth due to increased demand from mobile app users querying package delivery statuses. Although the compute layer (EC2) has remained stable, the Aurora DB cluster is under growing read pressure, especially from frequent repeated queries about package locations and delivery history. The company added an Aurora read replica, which temporarily alleviated the load, but read traffic continues to spike as user queries grow. The company wants to reduce the repeated reads pressure on the DB cluster. Which solution will best meet these requirements in a cost-effective manner?",
            options: [
                { id: 0, text: "Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas", correct: false },
                { id: 1, text: "Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database", correct: true },
                { id: 2, text: "Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions", correct: false },
                { id: 3, text: "Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect – Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
            options: [
                { id: 0, text: "Use field level encryption in Amazon CloudFront to protect sensitive data for specific content", correct: true },
                { id: 1, text: "Amazon CloudFront can route to multiple origins based on the price class", correct: false },
                { id: 2, text: "Use geo restriction to configure Amazon CloudFront for high-availability and failover", correct: false },
                { id: 3, text: "Amazon CloudFront can route to multiple origins based on the content type", correct: true },
                { id: 4, text: "Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content", correct: false },
                { id: 5, text: "Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover", correct: true },
            ],
            correctAnswers: [0, 3, 5],
            explanation: "The correct answers are: Use field level encryption in Amazon CloudFront to protect sensitive data for specific content, Amazon CloudFront can route to multiple origins based on the content type, Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon CloudFront can route to multiple origins based on the price class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use geo restriction to configure Amazon CloudFront for high-availability and failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
            options: [
                { id: 0, text: "Amazon ElastiCache", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 3, text: "Amazon Neptune", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Neptune: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
            options: [
                { id: 0, text: "Pilot Light", correct: false },
                { id: 1, text: "Warm Standby", correct: true },
                { id: 2, text: "Multi Site", correct: false },
                { id: 3, text: "Backup and Restore", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Warm Standby\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Pilot Light: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi Site: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Backup and Restore: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
            options: [
                { id: 0, text: "Access control list (ACL)", correct: false },
                { id: 1, text: "Trust policy", correct: true },
                { id: 2, text: "Permissions boundary", correct: false },
                { id: 3, text: "AWS Organizations Service Control Policies (SCP)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Trust policy\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Access control list (ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Permissions boundary: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Organizations Service Control Policies (SCP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon Kinesis", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon S3", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Amazon DynamoDB, Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
                { id: 1, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 2, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 3, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 4, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 5, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are: Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold, By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
            options: [
                { id: 0, text: "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years", correct: true },
                { id: 1, text: "Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes", correct: false },
                { id: 2, text: "Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs", correct: false },
                { id: 3, text: "Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
            options: [
                { id: 0, text: "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases", correct: false },
                { id: 1, text: "Use AWS CloudFormation to manage Amazon RDS databases", correct: true },
                { id: 2, text: "Attach an IAM policy to interns preventing them from creating an Amazon RDS database", correct: false },
                { id: 3, text: "Store your recommendations in a custom AWS Trusted Advisor rule", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation to manage Amazon RDS databases\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy to interns preventing them from creating an Amazon RDS database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store your recommendations in a custom AWS Trusted Advisor rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator", correct: true },
                { id: 1, text: "Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer", correct: false },
                { id: 3, text: "Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs):AZ-AandAZ-B. Cross-zone load balancing is disabled.AZ-Ahas four targets andAZ-Bhas six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
            options: [
                { id: 0, text: "Each of the four targets in AZ-A receives 12.5% of the traffic", correct: true },
                { id: 1, text: "Each of the six targets in AZ-B receives 10% of the traffic", correct: false },
                { id: 2, text: "Each of the four targets in AZ-A receives 10% of the traffic", correct: false },
                { id: 3, text: "Each of the four targets in AZ-A receives 8% of the traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Each of the four targets in AZ-A receives 12.5% of the traffic\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Each of the six targets in AZ-B receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 8% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 30,
            text: "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
            options: [
                { id: 0, text: "Use Amazon RDS Read Replicas", correct: true },
                { id: 1, text: "Use Amazon DynamoDB", correct: false },
                { id: 2, text: "Use Amazon ElastiCache", correct: false },
                { id: 3, text: "Use Amazon RDS Multi-AZ feature", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon RDS Read Replicas\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Multi-AZ feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A global pharmaceutical company operates a hybrid cloud network. Its primary AWS workloads run in the us-west-2 Region, connected to its on-premises data center via an AWS Direct Connect connection. After acquiring a biotech firm headquartered in Europe, the company must integrate the biotech’s workloads, which are hosted in several VPCs in the eu-central-1 Region and connected to the biotech's on-premises facility through a separate Direct Connect link. All CIDR blocks are non-overlapping, and the business requires full connectivity between both data centers and all VPCs across the two Regions. The company also wants a scalable solution that minimizes manual network configuration and long-term operational overhead. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication", correct: false },
                { id: 1, text: "Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions", correct: true },
                { id: 2, text: "Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic", correct: false },
                { id: 3, text: "Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon OpenSearch Service", correct: false },
                { id: 1, text: "Amazon Redshift", correct: false },
                { id: 2, text: "Amazon Neptune", correct: true },
                { id: 3, text: "Amazon Aurora", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
            options: [
                { id: 0, text: "Change the Amazon EC2 instance type to something much smaller", correct: false },
                { id: 1, text: "Keep the Amazon EBS volume to io1 and reduce the IOPS", correct: false },
                { id: 2, text: "Convert the Amazon EC2 instance EBS volume to gp2", correct: true },
                { id: 3, text: "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Convert the Amazon EC2 instance EBS volume to gp2\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Change the Amazon EC2 instance type to something much smaller: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Keep the Amazon EBS volume to io1 and reduce the IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs).AZ-Ahas 3 Amazon EC2 instances andAZ-Bhas 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
            options: [
                { id: 0, text: "A random instance in the AZ-A will be terminated", correct: false },
                { id: 1, text: "A random instance will be terminated in AZ-B", correct: false },
                { id: 2, text: "An instance in the AZ-A will be created", correct: false },
                { id: 3, text: "The instance with the oldest launch template or launch configuration will be terminated in AZ-B", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The instance with the oldest launch template or launch configuration will be terminated in AZ-B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- A random instance in the AZ-A will be terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A random instance will be terminated in AZ-B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance in the AZ-A will be created: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
            options: [
                { id: 0, text: "Use a Classic Load Balancer with an Auto Scaling Group", correct: false },
                { id: 1, text: "Use an Application Load Balancer with an Auto Scaling Group", correct: false },
                { id: 2, text: "Use a Network Load Balancer with an Auto Scaling Group", correct: true },
                { id: 3, text: "Use an Auto Scaling Group with Dynamic Elastic IPs attachment", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use a Network Load Balancer with an Auto Scaling Group\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a Classic Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling Group with Dynamic Elastic IPs attachment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
            options: [
                { id: 0, text: "Use a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 1, text: "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 2, text: "Use permissions boundary to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 3, text: "Use a bucket policy to grant permission to users in its account as well as to users in another account", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a bucket policy to grant permission to users in its account as well as to users in another account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use permissions boundary to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "You have an Amazon S3 bucket that contains files in two different folders -s3://my-bucket/imagesands3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
            options: [
                { id: 0, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days", correct: false },
                { id: 1, text: "Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days", correct: false },
                { id: 2, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days", correct: false },
                { id: 3, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days", correct: true },
                { id: 4, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days, Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
            options: [
                { id: 0, text: "Deploy AWS Lambda in a VPC", correct: false },
                { id: 1, text: "Attach an AWS Identity and Access Management (IAM) role to AWS Lambda", correct: true },
                { id: 2, text: "Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL", correct: true },
                { id: 3, text: "Restrict the Amazon RDS database security group to the AWS Lambda's security group", correct: false },
                { id: 4, text: "Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Attach an AWS Identity and Access Management (IAM) role to AWS Lambda, Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Lambda in a VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Restrict the Amazon RDS database security group to the AWS Lambda's security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A ride-hailing startup has launched a mobile app that matches passengers with nearby drivers based on real-time GPS coordinates. The application backend uses an Amazon RDS for PostgreSQL instance with read replicas to store the latitude and longitude of drivers and passengers. As the service scales, the backend experiences performance bottlenecks during peak hours, especially when thousands of updates and reads occur per second to keep location data current. The company expects its user base to double in the next few months and needs a high-performance, scalable solution that can handle frequent write and read operations with minimal latency. What do you recommend?",
            options: [
                { id: 0, text: "Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration", correct: false },
                { id: 1, text: "Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards", correct: false },
                { id: 2, text: "Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy", correct: true },
                { id: 3, text: "Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 1, text: "The route for the health check is misconfigured", correct: true },
                { id: 2, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 3, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 4, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: The route for the health check is misconfigured, The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A digital media platform is preparing to launch a new interactive content service that is expected to receive sudden spikes in user engagement, especially during live events and media releases. The backend uses an Amazon Aurora PostgreSQL Serverless v2 cluster to handle dynamic workloads. The architecture must be capable of scaling both compute and storage performance to maintain low latency and avoid bottlenecks under load. The engineering team is evaluating storage configuration options and wants a solution that will scale automatically with traffic, optimize I/O performance, and remain cost-effective without manual provisioning or tuning. Which configuration will best meet these requirements?",
            options: [
                { id: 0, text: "Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage", correct: false },
                { id: 1, text: "Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes", correct: false },
                { id: 2, text: "Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks", correct: false },
                { id: 3, text: "Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 2, text: "Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon MQ", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
            options: [
                { id: 0, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time", correct: false },
                { id: 1, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3", correct: true },
                { id: 2, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3", correct: false },
                { id: 3, text: "Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
            options: [
                { id: 0, text: "Latency-based routing", correct: false },
                { id: 1, text: "Geolocation routing", correct: false },
                { id: 2, text: "Geoproximity routing", correct: true },
                { id: 3, text: "Weighted routing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Geoproximity routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Latency-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Geolocation routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Weighted routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A financial services company is implementing two separate data retention policies to comply with regulatory standards: Policy A: Critical transaction records must be immediately available for audit and must not be deleted or overwritten for 7 years. Policy B: Archived compliance data must be stored in a low-cost, long-term storage solution and locked from deletion or modification for at least 10 years. As a solutions architect, which combination of AWS features should you recommend to enforce these policies effectively?",
            options: [
                { id: 0, text: "Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B", correct: false },
                { id: 1, text: "Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B", correct: true },
                { id: 2, text: "Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention", correct: false },
                { id: 3, text: "Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
            options: [
                { id: 0, text: "Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables", correct: false },
                { id: 1, text: "Contact AWS support to map your VPC with subnet", correct: false },
                { id: 2, text: "Check if the security groups allow ping from the source", correct: true },
                { id: 3, text: "Disable Source / Destination check on the Amazon EC2 instance", correct: false },
                { id: 4, text: "Check if the route table is configured with internet gateway", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Check if the security groups allow ping from the source, Check if the route table is configured with internet gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Contact AWS support to map your VPC with subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable Source / Destination check on the Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
            options: [
                { id: 0, text: "Enable Amazon API Gateway Caching", correct: true },
                { id: 1, text: "Switch to using an Application Load Balancer", correct: false },
                { id: 2, text: "Add Amazon Aurora Read Replicas", correct: false },
                { id: 3, text: "Enable AWS Lambda In Memory Caching", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon API Gateway Caching\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Switch to using an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Aurora Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Lambda In Memory Caching: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy?",
            options: [
                { id: 0, text: "It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket", correct: false },
                { id: 1, text: "It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket", correct: true },
                { id: 2, text: "It ensures Amazon EC2 instances that have inherited a security group can access the bucket", correct: false },
                { id: 3, text: "It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures Amazon EC2 instances that have inherited a security group can access the bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Egress Only Internet Gateway as a backup connection", correct: false },
                { id: 1, text: "Use AWS Site-to-Site VPN as a backup connection", correct: true },
                { id: 2, text: "Use AWS Direct Connect connection as a primary connection", correct: true },
                { id: 3, text: "Use AWS Site-to-Site VPN as a primary connection", correct: false },
                { id: 4, text: "Use AWS Direct Connect connection as a backup connection", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use AWS Site-to-Site VPN as a backup connection, Use AWS Direct Connect connection as a primary connection\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Egress Only Internet Gateway as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Site-to-Site VPN as a primary connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect connection as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this AWS CloudFormation snippet do? (Select three)",
            options: [
                { id: 0, text: "It lets traffic flow from one IP on port 22", correct: true },
                { id: 1, text: "It configures a security group's outbound rules", correct: false },
                { id: 2, text: "It configures a security group's inbound rules", correct: true },
                { id: 3, text: "It configures the inbound rules of a network access control list (network ACL)", correct: false },
                { id: 4, text: "It only allows the IP 0.0.0.0 to reach HTTP", correct: false },
                { id: 5, text: "It allows any IP to pass through on the HTTP port", correct: true },
                { id: 6, text: "It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1", correct: false },
            ],
            correctAnswers: [0, 2, 5],
            explanation: "The correct answers are: It lets traffic flow from one IP on port 22, It configures a security group's inbound rules, It allows any IP to pass through on the HTTP port\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- It configures a security group's outbound rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It configures the inbound rules of a network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It only allows the IP 0.0.0.0 to reach HTTP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "An e-commerce analytics company is preparing to archive several years of transaction records and customer analytics reports in Amazon S3 for long-term storage. To meet compliance requirements, the archived data must be encrypted at rest. Additionally, the solution must be cost-effective and ensure that key rotation occurs automatically every 12 months to comply with the company’s internal data governance policy. Which solution will meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket", correct: true },
                { id: 1, text: "Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow", correct: false },
                { id: 2, text: "Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings", correct: false },
                { id: 3, text: "Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 1, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 2, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 3, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
                { id: 4, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
            options: [
                { id: 0, text: "Select an Elastic Inference accelerator while launching Amazon EC2 instances", correct: false },
                { id: 1, text: "Select the appropriate capacity reservation while launching Amazon EC2 instances", correct: false },
                { id: 2, text: "Select dedicated instance tenancy while launching Amazon EC2 instances", correct: false },
                { id: 3, text: "Select a cluster placement group while launching Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Select a cluster placement group while launching Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Select an Elastic Inference accelerator while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select the appropriate capacity reservation while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select dedicated instance tenancy while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
            options: [
                { id: 0, text: "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure", correct: true },
                { id: 1, text: "Schedule manual backups using Redis append-only file (AOF)", correct: false },
                { id: 2, text: "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure", correct: false },
                { id: 3, text: "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Schedule manual backups using Redis append-only file (AOF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Schedule daily automatic backups at a time when you expect low resource utilization for your cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 1, text: "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks", correct: true },
                { id: 2, text: "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 3, text: "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 56,
            text: "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
            options: [
                { id: 0, text: "Use AWS Snowball", correct: false },
                { id: 1, text: "Use multi-part upload feature of Amazon S3", correct: true },
                { id: 2, text: "Use AWS Direct Connect to provide extra bandwidth", correct: false },
                { id: 3, text: "Use Amazon S3 Versioning", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use multi-part upload feature of Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Snowball: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to provide extra bandwidth: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Versioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 57,
            text: "A media company operates a video rendering pipeline on Amazon EKS, where containerized jobs are scheduled using Kubernetes deployments. The application experiences bursty traffic patterns, particularly during peak streaming hours. The platform uses the Kubernetes Horizontal Pod Autoscaler (HPA) to scale pods based on CPU utilization. A solutions architect observes that the total number of EC2 worker nodes remains constant during traffic spikes, even when all nodes are at maximum resource utilization. The company needs a solution that enables automatic scaling of the underlying compute infrastructure when pod demand exceeds cluster capacity. Which solution should the architect implement to resolve this issue with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure", correct: false },
                { id: 1, text: "Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands", correct: true },
                { id: 2, text: "Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group", correct: false },
                { id: 3, text: "Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two)",
            options: [
                { id: 0, text: "Use Amazon EMR for serverless orchestration of the containerized services", correct: false },
                { id: 1, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 3, text: "Use Amazon SageMaker for serverless orchestration of the containerized services", correct: false },
                { id: 4, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services, Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EMR for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
            options: [
                { id: 0, text: "Use an Amazon CloudFront distribution in front of your website", correct: false },
                { id: 1, text: "Use an Auto Scaling Group", correct: true },
                { id: 2, text: "Use an Amazon Route 53 Multi Value record", correct: false },
                { id: 3, text: "Deploy the website on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon CloudFront distribution in front of your website: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Route 53 Multi Value record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the website on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A streaming media company operates a high-traffic content delivery platform on AWS. The application backend is deployed on Amazon EC2 instances within an Auto Scaling group across multiple Availability Zones in a VPC. The team has observed that workloads follow predictable usage patterns, such as higher viewership on weekends and in the evenings, along with occasional real-time spikes due to viral content. To reduce cost and improve responsiveness, the team wants an automated scaling approach that can forecast future demand using historical usage patterns, scale in advance based on those predictions, and react quickly to unplanned usage surges in real time. Which scaling strategy should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments", correct: false },
                { id: 1, text: "Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes", correct: true },
                { id: 2, text: "Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours", correct: false },
                { id: 3, text: "Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture", correct: false },
                { id: 1, text: "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture", correct: false },
                { id: 3, text: "Use Amazon EventBridge to decouple the system architecture", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EventBridge to decouple the system architecture\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) for effective decoupling of system architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A research organization is running a high-performance computing (HPC) workload using Amazon EC2 instances that are distributed across multiple Availability Zones (AZs) within a single AWS Region. The workload requires access to a shared file system with the lowest possible latency for frequent reads and writes.The team decides to use Amazon Elastic File System (Amazon EFS) for its scalability and simplicity. To ensure optimal performance and reduce network latency, the solution architect must design the architecture so that each EC2 instance can access the file system with the least possible delay. Which of the following is the most appropriate solution to meet these requirements?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones", correct: false },
                { id: 1, text: "Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target", correct: false },
                { id: 2, text: "Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances", correct: false },
                { id: 3, text: "Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A global insurance company is modernizing its infrastructure by migrating multiple line-of-business applications from its on-premises data centers to AWS. These applications will be deployed across several AWS accounts, all governed under a centralized AWS Organizations structure. The company manages all user identities, groups, and access policies within its on-premises Microsoft Active Directory and wants to continue doing so. The goal is to enable seamless single sign-in across all AWS accounts without duplicating user identity stores or manually provisioning accounts. Which solution best meets these requirements in the most operationally efficient manner?",
            options: [
                { id: 0, text: "Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts", correct: false },
                { id: 1, text: "Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider", correct: false },
                { id: 2, text: "Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts", correct: true },
                { id: 3, text: "Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Streams", correct: false },
                { id: 1, text: "Use Amazon DynamoDB DAX", correct: true },
                { id: 2, text: "Use Amazon DynamoDB Global Tables", correct: false },
                { id: 3, text: "Use Amazon ElastiCache", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon DynamoDB DAX\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test6: [
        {
            id: 1,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
            options: [
                { id: 0, text: "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch", correct: true },
                { id: 1, text: "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3", correct: false },
                { id: 2, text: "Disable the Termination from the Auto Scaling Group any time a user reports an issue", correct: false },
                { id: 3, text: "Make a snapshot of the Amazon EC2 instance just before it gets terminated", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable the Termination from the Auto Scaling Group any time a user reports an issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make a snapshot of the Amazon EC2 instance just before it gets terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too", correct: false },
                { id: 1, text: "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data", correct: false },
                { id: 2, text: "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures", correct: true },
                { id: 3, text: "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
            options: [
                { id: 0, text: "The application will be down until the primary database has recovered itself", correct: false },
                { id: 1, text: "The URL to access the database will change to the standby database", correct: false },
                { id: 2, text: "An email will be sent to the System Administrator asking for manual intervention", correct: false },
                { id: 3, text: "The CNAME record will be updated to point to the standby database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The CNAME record will be updated to point to the standby database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- The application will be down until the primary database has recovered itself: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The URL to access the database will change to the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An email will be sent to the System Administrator asking for manual intervention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A healthcare startup runs a lightweight reporting application on a single Amazon EC2 On-Demand instance. The application is designed to be stateless, fault-tolerant, and optimized for fast rendering of analytics dashboards. During major health events or news cycles, the team observes latency issues and occasional 5xx errors due to traffic spikes. To meet growing demand without over-provisioning resources during off-peak hours, the company wants to implement a cost-effective, scalable solution that ensures consistent performance even under unpredictable load. Which approach best meets the requirements while minimizing costs?",
            options: [
                { id: 0, text: "Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%", correct: false },
                { id: 1, text: "Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly", correct: false },
                { id: 2, text: "Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load", correct: false },
                { id: 3, text: "Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two)",
            options: [
                { id: 0, text: "The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic", correct: true },
                { id: 1, text: "The subnet has been configured to be public and has no access to the internet", correct: false },
                { id: 2, text: "The instance's subnet is not associated with any route table", correct: false },
                { id: 3, text: "The route table in the instance’s subnet should have a route to an Internet Gateway", correct: true },
                { id: 4, text: "The instance's subnet is associated with multiple route tables with conflicting configurations", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic, The route table in the instance’s subnet should have a route to an Internet Gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The subnet has been configured to be public and has no access to the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is not associated with any route table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is associated with multiple route tables with conflicting configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A digital content production company has transitioned all of its media assets to Amazon S3 in an effort to reduce storage costs. However, the rendering engine used in production continues to run in an on-premises data center and requires frequent and low-latency access to large media files. The company wants to implement a storage solution that maintains application performance while keeping costs low. Which approach should the company choose to meet these requirements in the most cost-effective way?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket", correct: false },
                { id: 1, text: "Set up an Amazon S3 File Gateway to provide storage for the on-premises application", correct: true },
                { id: 2, text: "Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel", correct: false },
                { id: 3, text: "Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 File Gateway to provide storage for the on-premises application\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
            options: [
                { id: 0, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 1, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation", correct: true },
                { id: 2, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A digital media startup allows users to submit images through its web portal. These images are uploaded directly into an Amazon S3 bucket. On average, around 200 images are uploaded daily. The company wants to automatically generate a smaller preview version (thumbnail) of each new image and store the resulting thumbnails in a separate Amazon S3 bucket. The team prefers a design that is low-cost, requires minimal infrastructure management, and automatically reacts to new uploads. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail", correct: false },
                { id: 1, text: "Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket", correct: true },
                { id: 2, text: "Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket", correct: false },
                { id: 3, text: "Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A digital publishing platform stores large volumes of media assets (such as images and documents) in an Amazon S3 bucket. These assets are accessed frequently during business hours by internal editors and content delivery tools. The company has strict encryption policies and currently uses AWS KMS to handle server-side encryption. The cloud operations team notices that AWS KMS request costs are increasing significantly due to the high frequency of object uploads and accesses. The team is now looking for a way to maintain the same encryption method but reduce the cost of KMS usage, especially for frequent access patterns. Which solution meets the company's encryption and cost optimization goals?",
            options: [
                { id: 0, text: "Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object", correct: true },
                { id: 1, text: "Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control", correct: false },
                { id: 2, text: "Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges", correct: false },
                { id: 3, text: "Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
            options: [
                { id: 0, text: "The database user credentials (username and password) configured for the application are incorrect", correct: false },
                { id: 1, text: "The database user credentials (username and password) configured for the application do not have the required privilege for the given database", correct: false },
                { id: 2, text: "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers", correct: true },
                { id: 3, text: "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The database user credentials (username and password) configured for the application are incorrect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The database user credentials (username and password) configured for the application do not have the required privilege for the given database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A biomedical research firm operates a file exchange system for external research partners to upload and download experimental data. Currently, the system runs on two Amazon EC2 Linux instances, each configured with Elastic IP addresses to allow access from trusted IPs. File transfers use the SFTP protocol, and Linux user accounts are manually provisioned to enforce file-level access control. Data is stored on a shared file system mounted to both EC2 instances. The firm wants to modernize the solution to a fully managed, serverless model with high IOPS, fine-grained user permission control, and strict IP-based access restrictions. They also want to reduce operational overhead without sacrificing performance or security. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings", correct: false },
                { id: 1, text: "Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses", correct: false },
                { id: 2, text: "Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF", correct: false },
                { id: 3, text: "Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "An organization operates a legacy reporting tool hosted on an Amazon EC2 instance located within a public subnet of a VPC. This tool aggregates scanned PDF reports from field devices and temporarily stores them on an attached Amazon EBS volume. At the end of each day, the tool transfers the accumulated files to an Amazon S3 bucket for archival. A solutions architect identifies that the files are being uploaded over the internet using S3's public endpoint. To improve security and avoid exposing data traffic to the public internet, the architect needs to reconfigure the setup so that uploads to Amazon S3 occur privately without using the public S3 endpoint. Which solution will fulfill these requirements?",
            options: [
                { id: 0, text: "Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data", correct: false },
                { id: 1, text: "Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint", correct: true },
                { id: 2, text: "Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately", correct: false },
                { id: 3, text: "Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A fintech company currently operates a real-time search and analytics platform on-premises. This platform ingests streaming data from multiple data-producing systems and provides immediate search capabilities and interactive visualizations for end users. As part of its cloud migration strategy, the company wants to rearchitect the solution using AWS-native services. Which of the following represents the most efficient solution?",
            options: [
                { id: 0, text: "Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations", correct: false },
                { id: 1, text: "Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights", correct: false },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data", correct: false },
                { id: 3, text: "Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A global e-commerce platform currently operates its order processing system in a single on-premises data center located in Europe. As the company grows its customer base across Asia and North America, it plans to deploy the application across multiple AWS Regions to improve availability and reduce latency. The company requires that updates to the central order database be completed in under one second with global consistency. The application layer will be deployed separately in each Region, but the order management data must remain centrally managed and globally synchronized. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region", correct: false },
                { id: 1, text: "Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.", correct: false },
                { id: 2, text: "Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions", correct: false },
                { id: 3, text: "Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose", correct: false },
                { id: 1, text: "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 2, text: "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 3, text: "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
            options: [
                { id: 0, text: "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed", correct: false },
                { id: 1, text: "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page", correct: false },
                { id: 2, text: "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: true },
                { id: 3, text: "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A retail enterprise is expanding its hybrid IT infrastructure and plans to securely connect its on-premises corporate network to its AWS environment. The company wants to ensure that all data exchanged between on-premises systems and AWS is encrypted at both the network and session layers. Additionally, the solution must incorporate granular security controls that restrict unnecessary or unauthorized access between the cloud and on-premises environments. A solutions architect must recommend a scalable and secure approach that supports these goals. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems", correct: true },
                { id: 1, text: "Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed", correct: false },
                { id: 2, text: "Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies", correct: false },
                { id: 3, text: "Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
            options: [
                { id: 0, text: "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type", correct: true },
                { id: 1, text: "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type", correct: false },
                { id: 2, text: "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type", correct: false },
                { id: 3, text: "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
            options: [
                { id: 0, text: "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service", correct: false },
                { id: 1, text: "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage", correct: false },
                { id: 2, text: "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises", correct: false },
                { id: 3, text: "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure Amazon EFS to provide a fast, cost-effective and sharable storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
            options: [
                { id: 0, text: "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option", correct: false },
                { id: 3, text: "Use Amazon EC2 instances with Instance Store as the storage option", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 instances with Instance Store as the storage option\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 3, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A logistics company runs a two-step job handling process on AWS. The first step quickly receives job submissions from clients, while the second step requires longer processing time to complete each job. Currently, both steps run on separate Amazon EC2 Auto Scaling groups. However, during high-demand hours, the job processing stage falls behind, and there is concern that jobs may be lost due to instance termination during scaling events. A solutions architect needs to design a more scalable and reliable architecture that preserves job data and accommodates fluctuating demand in both stages. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue", correct: true },
                { id: 1, text: "Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue", correct: false },
                { id: 2, text: "Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior", correct: false },
                { id: 3, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
                { id: 1, text: "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: true },
                { id: 2, text: "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: false },
                { id: 3, text: "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A global enterprise is modernizing its hybrid IT infrastructure to improve both availability and network performance. The company operates a TCP-based application hosted on Amazon EC2 instances that are deployed across multiple AWS Regions, while a secondary UDP-based component of the application is hosted in its on-premises data centers. These application components must be accessed by customers around the world with minimal latency and consistent uptime. Which combination of options should a solutions architect implement for the given use case? (Select two)",
            options: [
                { id: 0, text: "Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers", correct: true },
                { id: 1, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups", correct: false },
                { id: 2, text: "Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover", correct: false },
                { id: 3, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups", correct: true },
                { id: 4, text: "Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers, Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A SaaS analytics company is deploying a microservices-based application on Amazon ECS using the Fargate launch type. The application requires access to a shared, POSIX-compliant file system that is available across multiple Availability Zones for redundancy and availability. To meet compliance requirements, the system must support regional backups and cross-Region data recovery with a recovery point objective (RPO) of no more than 8 hours. A backup strategy will be implemented using AWS Backup to automate replication across Regions. As the lead cloud architect, you are evaluating file storage solutions that align with these requirements. Which option best meets the application’s availability, durability, and RPO objectives?",
            options: [
                { id: 0, text: "Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically", correct: false },
                { id: 1, text: "Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata", correct: false },
                { id: 2, text: "Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region", correct: false },
                { id: 3, text: "Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
                { id: 1, text: "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets", correct: false },
                { id: 2, text: "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection", correct: false },
                { id: 3, text: "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances", correct: false },
                { id: 4, text: "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files, Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A financial auditing firm uses Amazon S3 to store sensitive client records that are subject to write-once-read-many (WORM) regulations to prevent alteration or deletion of records for a specific retention period. The firm wants to enforce immutable storage, such that even administrators cannot overwrite or delete the records during the lock duration. They also need audit-friendly enforcement to prevent accidental or malicious deletion. Which configuration of S3 Object Lock will ensure that the retention policy is strictly enforced, and no user (including root or administrators) can override or delete protected objects during the lock period?",
            options: [
                { id: 0, text: "Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period", correct: true },
                { id: 1, text: "Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period", correct: false },
                { id: 2, text: "Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period", correct: false },
                { id: 3, text: "Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 2, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 3, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 4, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Data moving between the volume and the instance is encrypted, Any snapshot created from the volume is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An enterprise is developing an internal compliance framework for its cloud infrastructure hosted on AWS. The enterprise uses AWS Organizations to group accounts under various organizational units (OUs) based on departmental function. As part of its governance controls, the security team mandates that all Amazon EC2 instances must be tagged to indicate the level of data classification — either 'confidential' or 'public'. Additionally, the organization must ensure that IAM users cannot launch EC2 instances without assigning a classification tag, nor should they be able to remove the tag from running instances. A solutions architect must design a solution to meet these compliance controls while minimizing operational overhead. Which combination of steps will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations", correct: true },
                { id: 1, text: "Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected", correct: false },
                { id: 2, text: "Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts", correct: true },
                { id: 3, text: "Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning", correct: false },
                { id: 4, text: "Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations, Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A financial data processing company runs a workload on Amazon EC2 instances that fetch and process real-time transaction batches from an Amazon SQS queue. The application needs to scale based on unpredictable message volume, which fluctuates significantly throughout the day. The system must process messages with minimal delay and no downtime, even during peak spikes. The company is seeking a solution that balances cost-efficiency with availability and elasticity. Which EC2 purchasing strategy best meets these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes", correct: false },
                { id: 1, text: "Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume", correct: true },
                { id: 2, text: "Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations", correct: false },
                { id: 3, text: "Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB for distributed in-memory cache based session management", correct: false },
                { id: 1, text: "Use Amazon Elasticache for distributed in-memory cache based session management", correct: true },
                { id: 2, text: "Use Amazon RDS for distributed in-memory cache based session management", correct: false },
                { id: 3, text: "Use Application Load Balancer sticky sessions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Elasticache for distributed in-memory cache based session management\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer sticky sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations", correct: true },
                { id: 1, text: "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed", correct: false },
                { id: 2, text: "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user", correct: false },
                { id: 3, text: "Set up an Amazon Route 53 geoproximity routing policy to route traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Route 53 geoproximity routing policy to route traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "A tech company runs a web application that includes multiple internal services deployed across Amazon EC2 instances within a VPC. These services require communication with a third-party SaaS provider's API for analytics and billing, which is also hosted on the AWS infrastructure. The company is concerned about minimizing public internet exposure while maintaining secure and reliable connectivity. The solution must ensure private access without allowing unsolicited incoming traffic from the SaaS provider. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC", correct: true },
                { id: 1, text: "Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider", correct: false },
                { id: 2, text: "Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication", correct: false },
                { id: 3, text: "Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
            options: [
                { id: 0, text: "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 1, text: "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone", correct: false },
                { id: 2, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 3, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A mobile-based e-learning platform is migrating its backend storage layer to Amazon DynamoDB to support a rapidly increasing number of student users and learning transactions. The platform must ensure seamless availability and minimal disruption for a global user base. The DynamoDB design must provide low-latency performance, high availability, and automatic fault tolerance across geographies with the lowest possible operational overhead and cost. Which solution will fulfill these needs in the most cost-efficient manner?",
            options: [
                { id: 0, text: "Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions", correct: false },
                { id: 1, text: "Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions", correct: false },
                { id: 2, text: "Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability", correct: true },
                { id: 3, text: "Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output", correct: false },
                { id: 1, text: "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments", correct: true },
                { id: 2, text: "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
            options: [
                { id: 0, text: "Both the Auto Scaling group and Application Load Balancer are using ALB based health check", correct: false },
                { id: 1, text: "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check", correct: false },
                { id: 2, text: "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check", correct: false },
                { id: 3, text: "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Both the Auto Scaling group and Application Load Balancer are using ALB based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A fintech company recently conducted a security audit and discovered that some IAM roles and Amazon S3 buckets might be unintentionally shared with external accounts or publicly accessible. The security team wants to identify these overly permissive resources and ensure that only intended principals (within their AWS Organization or specific AWS accounts) have access. They need a solution that can analyze IAM policies and resource policies to detect unintended access paths to AWS resources such as S3 buckets, IAM roles, KMS keys, and SNS topics. Which solution should the team use to meet this requirement?",
            options: [
                { id: 0, text: "Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment", correct: false },
                { id: 1, text: "Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization", correct: true },
                { id: 2, text: "Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access", correct: false },
                { id: 3, text: "Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "An enterprise runs a critical Oracle database workload in its on-premises environment. The company now plans to replicate both existing records and continuous transactional changes to a managed Oracle environment in AWS. The target database will run on Amazon RDS for Oracle. Data transfer volume is expected to fluctuate throughout the day, and the team wants the solution to provision compute resources automatically based on actual workload requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected", correct: false },
                { id: 1, text: "Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.", correct: false },
                { id: 2, text: "Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.", correct: false },
                { id: 3, text: "Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two)",
            options: [
                { id: 0, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions", correct: true },
                { id: 1, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region", correct: false },
                { id: 2, text: "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage", correct: false },
                { id: 3, text: "Use cross-Region Read Replicas", correct: true },
                { id: 4, text: "Use the database cloning feature of the Amazon RDS Database cluster", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions, Use cross-Region Read Replicas\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the database cloning feature of the Amazon RDS Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A DevOps team is tasked with enabling secure and temporary SSH access to Amazon EC2 instances for developers during deployments. The team wants to avoid distributing long-term SSH key pairs and instead prefers ephemeral access that can be audited and revoked immediately after the session ends. The team wants direct access via the AWS Management Console. What do you recommend?",
            options: [
                { id: 0, text: "Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address", correct: true },
                { id: 1, text: "Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint", correct: false },
                { id: 2, text: "Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions", correct: false },
                { id: 3, text: "Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through", correct: false },
                { id: 1, text: "Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances", correct: false },
                { id: 2, text: "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through", correct: true },
                { id: 3, text: "Use Application Load Balancer geo match statement listing the countries that you want to block", correct: false },
                { id: 4, text: "Use AWS WAF geo match statement listing the countries that you want to block", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through, Use AWS WAF geo match statement listing the countries that you want to block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer geo match statement listing the countries that you want to block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A digital design company has migrated its project archiving platform to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group that spans multiple Availability Zones. Designers upload and retrieve high-resolution image files from a shared file system, which is currently configured to use Amazon EFS Standard-IA. Metadata for these files is stored and indexed in an Amazon RDS for PostgreSQL database. The company's cloud engineering team has been asked to optimize storage costs for the image archive without compromising reliability. They are open to refactoring the application to use managed AWS services when necessary. Which solution offers the most cost-effective architecture?",
            options: [
                { id: 0, text: "Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path", correct: false },
                { id: 1, text: "Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost", correct: false },
                { id: 2, text: "Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 1, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 3, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A data analytics team at a global media firm is building a new analytics platform to process large volumes of both historical and real-time data. This data is stored in Amazon S3. The team wants to implement a serverless solution that allows them to query the data directly using SQL. Additionally, the solution must ensure that all data is encrypted at rest and automatically replicated to another AWS Region to support business continuity. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data", correct: true },
                { id: 2, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls", correct: false },
                { id: 1, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
                { id: 2, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls", correct: true },
                { id: 3, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company hosts a Microsoft SQL Server database on Amazon EC2 instances with attached Amazon EBS volumes. The operations team takes daily snapshots of these EBS volumes as backups. However, a recent incident occurred in which an automated script designed to clean up expired snapshots accidentally deleted all available snapshots, leading to potential data loss. The company wants to improve the backup strategy to avoid permanent data loss while still ensuring that old snapshots are eventually removed to optimize cost. A solutions architect needs to implement a mechanism that prevents immediate and irreversible deletion of snapshots. Which solution will best meet these requirements with the least development effort?",
            options: [
                { id: 0, text: "Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots", correct: true },
                { id: 1, text: "Set up the IAM policy of the user to deny EBS snapshot deletion", correct: false },
                { id: 2, text: "Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery", correct: false },
                { id: 3, text: "Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up the IAM policy of the user to deny EBS snapshot deletion: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 1, text: "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
            options: [
                { id: 0, text: "Use long polling to handle message processing failures", correct: false },
                { id: 1, text: "Use a dead-letter queue to handle message processing failures", correct: true },
                { id: 2, text: "Use a temporary queue to handle message processing failures", correct: false },
                { id: 3, text: "Use short polling to handle message processing failures", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a dead-letter queue to handle message processing failures\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use long polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a temporary queue to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use short polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting aProvisionedThroughputExceededExceptionexception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
            options: [
                { id: 0, text: "Decrease the Stream retention duration", correct: false },
                { id: 1, text: "Use batch messages", correct: true },
                { id: 2, text: "Increase the number of shards", correct: false },
                { id: 3, text: "Use Exponential Backoff", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use batch messages\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Decrease the Stream retention duration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the number of shards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Exponential Backoff: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two)",
            options: [
                { id: 0, text: "Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target", correct: false },
                { id: 1, text: "Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica", correct: true },
                { id: 2, text: "Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target", correct: true },
                { id: 3, text: "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes", correct: false },
                { id: 4, text: "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica, Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "An enterprise SaaS provider is currently operating a legacy web application hosted on a single Amazon EC2 instance within a public subnet. The same instance also hosts a MySQL database. DNS records for the application are configured through Amazon Route 53. As part of a modernization initiative, the company wants to rearchitect this application for high availability and scalability. In addition, the company wants to improve read performance on the database layer to handle increasing user traffic. Which combination of solutions will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: false },
                { id: 1, text: "Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages", correct: false },
                { id: 2, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: true },
                { id: 3, text: "Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions", correct: false },
                { id: 4, text: "Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly, Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
            options: [
                { id: 0, text: "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 1, text: "Use Amazon Athena to run SQL based analytics against Amazon S3 data", correct: true },
                { id: 2, text: "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 3, text: "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Athena to run SQL based analytics against Amazon S3 data\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A media streaming company expects a major increase in user activity during the launch of a highly anticipated live event. The streaming platform is deployed on AWS and uses Amazon EC2 instances for the application layer and Amazon RDS for persistent storage. The operations team needs to proactively monitor system performance to ensure a smooth user experience during the event. Their monitoring setup must provide data visibility with intervals of no more than 2 minutes, and the team prefers a solution that is quick to implement and low-maintenance. Which solution should the team implement?",
            options: [
                { id: 0, text: "Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena", correct: false },
                { id: 1, text: "Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics", correct: false },
                { id: 2, text: "Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics", correct: false },
                { id: 3, text: "Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
            options: [
                { id: 0, text: "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications", correct: true },
                { id: 1, text: "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications", correct: false },
                { id: 2, text: "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates", correct: false },
                { id: 3, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 1, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
                { id: 3, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A streaming service provider collects user experience feedback through embedded feedback forms in their mobile and web apps. Feedback submissions frequently spike to thousands per hour during content launches or service outages. Currently, the feedback is sent via email to the operations team for manual review. The company now wants to automate feedback collection and sentiment analysis so that insights can be generated quickly and stored for a full year for trend analysis. Which solution provides the most scalable and automated approach to meet these requirements?",
            options: [
                { id: 0, text: "Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item", correct: true },
                { id: 1, text: "Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table", correct: false },
                { id: 2, text: "Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months", correct: false },
                { id: 3, text: "Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
            options: [
                { id: 0, text: "Use Amazon Route 53 with Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Use AWS Firewall Manager with CloudFront distribution", correct: false },
                { id: 2, text: "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution", correct: true },
                { id: 3, text: "Use AWS Security Hub with Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Firewall Manager with CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Security Hub with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A financial analytics firm runs performance-intensive modeling software on Amazon EC2 instances backed by Amazon EBS volumes. The production data resides on EBS volumes attached to EC2 instances in the same AWS Region where the testing environment is hosted. To maintain data integrity, any changes made during testing must not affect production data. The development team needs to frequently create clones of this production data for simulations. The modeling software requires high and consistent I/O performance, and the firm wants to minimize the time required to provision test data. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment", correct: true },
                { id: 1, text: "Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance", correct: false },
                { id: 2, text: "Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations", correct: false },
                { id: 3, text: "Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A multinational logistics company operates its shipment tracking platform from Amazon EC2 instances deployed in the AWS us-west-2 Region. The platform exposes a set of APIs over HTTPS, which are used by logistics partners and customers around the world to retrieve real-time tracking data. The company has observed that users from Europe and Asia experience latency issues and inconsistent API response times when accessing the service. As a cloud architect, you have been tasked to propose the most cost-effective solution to improve performance for these international users without migrating the application. Which solution should you recommend?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups", correct: true },
                { id: 1, text: "Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2", correct: false },
                { id: 2, text: "Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region", correct: false },
                { id: 3, text: "Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A media company operates a web application that enables users to upload photos. These uploads are stored in an Amazon S3 bucket located in the eu-west-2 Region. To enhance performance and provide secure access under a custom domain name, the company wants to integrate Amazon CloudFront for uploads to the S3 bucket. The architecture must support secure HTTPS connections using a custom domain, and the upload process must ensure optimal speed and security. Which combination of actions will fulfill these requirements? (Select two)",
            options: [
                { id: 0, text: "Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution", correct: false },
                { id: 1, text: "Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations", correct: false },
                { id: 2, text: "Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL", correct: false },
                { id: 3, text: "Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)", correct: true },
                { id: 4, text: "Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC), Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A digital media company runs its content rendering service on Amazon EC2 instances that are registered with an Application Load Balancer (ALB) using IP-based target groups. The company relies on AWS Systems Manager to manage and patch these instances regularly. According to new compliance requirements, EC2 instances must be safely removed from production traffic during patching to prevent user disruption and maintain application integrity. However, during the most recent patch cycle, the operations team noticed application failures and API timeouts, even though patching succeeded on the instances. You are asked to suggest a reliable and scalable way to ensure safe patching while preserving service availability. Which solution will best meet the new compliance and operational requirements? (Select two)",
            options: [
                { id: 0, text: "Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window", correct: true },
                { id: 1, text: "Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata", correct: false },
                { id: 2, text: "Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching", correct: true },
                { id: 3, text: "Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes", correct: false },
                { id: 4, text: "Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window, Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "Use Amazon ElastiCache for Memcached", correct: false },
                { id: 2, text: "Use AWS Global Accelerator", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Redis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ElastiCache for Redis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache for Memcached: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test7: [
        {
            id: 1,
            text: "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
            options: [
                { id: 0, text: "On-Demand Instances", correct: false },
                { id: 1, text: "Spot Instances", correct: false },
                { id: 2, text: "Dedicated Instances", correct: true },
                { id: 3, text: "Dedicated Hosts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
            options: [
                { id: 0, text: "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication", correct: false },
                { id: 1, text: "Use the Amazon EC2 instances private IP for the replication", correct: true },
                { id: 2, text: "Create a Private Link between the two Amazon EC2 instances", correct: false },
                { id: 3, text: "Use an Elastic Fabric Adapter (EFA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Amazon EC2 instances private IP for the replication\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between the two Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Elastic Fabric Adapter (EFA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
            options: [
                { id: 0, text: "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: true },
                { id: 1, text: "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity", correct: true },
                { id: 2, text: "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted", correct: false },
                { id: 3, text: "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: false },
                { id: 4, text: "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification, A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
            options: [
                { id: 0, text: "Create a policy and assign it manually to the 50 users", correct: false },
                { id: 1, text: "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA", correct: false },
                { id: 2, text: "Create a group, attach the policy to the group and place the users in the group", correct: true },
                { id: 3, text: "Update the Amazon S3 bucket policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a group, attach the policy to the group and place the users in the group\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a policy and assign it manually to the 50 users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Update the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
            options: [
                { id: 0, text: "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation", correct: false },
                { id: 1, text: "You can specify instance store volumes for an instance when you launch or restart it", correct: false },
                { id: 2, text: "An instance store is a network storage type", correct: false },
                { id: 3, text: "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved", correct: true },
                { id: 4, text: "You can't detach an instance store volume from one instance and attach it to a different instance", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved, You can't detach an instance store volume from one instance and attach it to a different instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can specify instance store volumes for an instance when you launch or restart it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance store is a network storage type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
            options: [
                { id: 0, text: "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process", correct: false },
                { id: 1, text: "Use AWS CLI to run the user data scripts only once while launching the instance", correct: false },
                { id: 2, text: "Run the custom scripts as user data scripts on the Amazon EC2 instances", correct: true },
                { id: 3, text: "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Run the custom scripts as user data scripts on the Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CLI to run the user data scripts only once while launching the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the custom scripts as instance metadata scripts on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions. Which solution best meets these requirements while ensuring the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3", correct: true },
                { id: 1, text: "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx", correct: false },
                { id: 2, text: "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes", correct: false },
                { id: 3, text: "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application. Which solution will most effectively address the performance issues with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users", correct: true },
                { id: 1, text: "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket", correct: false },
                { id: 2, text: "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency", correct: false },
                { id: 3, text: "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
            options: [
                { id: 0, text: "Use AWS Shield Advanced and setup a rate-based rule", correct: false },
                { id: 1, text: "Configure Sticky Sessions on the Application Load Balancer", correct: false },
                { id: 2, text: "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule", correct: true },
                { id: 3, text: "Define a network access control list (network ACL) on your Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Shield Advanced and setup a rate-based rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Sticky Sessions on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define a network access control list (network ACL) on your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: false },
                { id: 1, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 3, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
            options: [
                { id: 0, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "AWS Global Accelerator", correct: false },
                { id: 2, text: "Amazon ElastiCache for Redis", correct: false },
                { id: 3, text: "Amazon ElastiCache for Memcached", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon ElastiCache for Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache for Redis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
            options: [
                { id: 0, text: "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 1, text: "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 2, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2", correct: true },
                { id: 3, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
            options: [
                { id: 0, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: true },
                { id: 3, text: "Amazon Redshift", correct: false },
                { id: 4, text: "Amazon DynamoDB Accelerator (DAX)", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon ElastiCache, Amazon DynamoDB Accelerator (DAX)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
            options: [
                { id: 0, text: "Create a Spot Fleet request", correct: false },
                { id: 1, text: "Assign an Amazon EC2 Instance Role to perform the necessary API calls", correct: true },
                { id: 2, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1", correct: true },
                { id: 3, text: "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group", correct: false },
                { id: 4, text: "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it", correct: true },
                { id: 5, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are: Assign an Amazon EC2 Instance Role to perform the necessary API calls, Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1, Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Spot Fleet request: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
            options: [
                { id: 0, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 1, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 2, text: "Active Directory Connector", correct: false },
                { id: 3, text: "AWS Transit Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Transit Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon Kinesis Data Streams to ingest the data", correct: true },
                { id: 1, text: "Set up AWS Fargate with Amazon ECS to process the data", correct: true },
                { id: 2, text: "Set up AWS Database Migration Service (AWS DMS) to ingest the data", correct: false },
                { id: 3, text: "Set up AWS Lambda with AWS Step Functions to process the data", correct: false },
                { id: 4, text: "Provision Amazon EC2 instances in an Auto Scaling group to process the data", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Set up Amazon Kinesis Data Streams to ingest the data, Set up AWS Fargate with Amazon ECS to process the data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up AWS Database Migration Service (AWS DMS) to ingest the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Lambda with AWS Step Functions to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision Amazon EC2 instances in an Auto Scaling group to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work. Which solution should the company implement to meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access", correct: false },
                { id: 1, text: "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically", correct: false },
                { id: 2, text: "Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets", correct: true },
                { id: 3, text: "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure", correct: true },
                { id: 1, text: "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones", correct: false },
                { id: 2, text: "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.", correct: false },
                { id: 3, text: "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency. Which solution best satisfies these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint", correct: false },
                { id: 1, text: "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway", correct: false },
                { id: 2, text: "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances", correct: false },
                { id: 3, text: "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
            options: [
                { id: 0, text: "Elastic Fabric Adapter (EFA)", correct: true },
                { id: 1, text: "Elastic IP Address (EIP)", correct: false },
                { id: 2, text: "Elastic Network Adapter (ENA)", correct: false },
                { id: 3, text: "Elastic Network Interface (ENI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Elastic Fabric Adapter (EFA)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Elastic IP Address (EIP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Adapter (ENA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Interface (ENI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
            options: [
                { id: 0, text: "enableVpcHostnames", correct: false },
                { id: 1, text: "enableVpcSupport", correct: false },
                { id: 2, text: "enableDnsHostnames", correct: true },
                { id: 3, text: "enableDnsSupport", correct: true },
                { id: 4, text: "enableDnsDomain", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: enableDnsHostnames, enableDnsSupport\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- enableVpcHostnames: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableVpcSupport: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableDnsDomain: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Migrate the Amazon Redshift underlying storage to Amazon S3 IA", correct: false },
                { id: 1, text: "Analyze the cold data with Amazon Athena", correct: true },
                { id: 2, text: "Create a smaller Amazon Redshift Cluster with the cold data", correct: false },
                { id: 3, text: "Move the data to Amazon S3 Glacier Deep Archive after 30 days", correct: false },
                { id: 4, text: "Move the data to Amazon S3 Standard IA after 30 days", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Analyze the cold data with Amazon Athena, Move the data to Amazon S3 Standard IA after 30 days\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Migrate the Amazon Redshift underlying storage to Amazon S3 IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a smaller Amazon Redshift Cluster with the cold data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move the data to Amazon S3 Glacier Deep Archive after 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region. The firm has defined strict DR objectives: Recovery Point Objective (RPO): ≤ 5 minutes Recovery Time Objective (RTO): ≤ 15 minutes Leadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
            options: [
                { id: 0, text: "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes", correct: false },
                { id: 1, text: "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover", correct: false },
                { id: 2, text: "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events", correct: false },
                { id: 3, text: "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
            options: [
                { id: 0, text: "Amazon Aurora Serverless", correct: true },
                { id: 1, text: "Amazon DynamoDB with On-Demand Capacity", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: false },
                { id: 3, text: "Amazon DynamoDB with Provisioned Capacity and Auto Scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Aurora Serverless\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB with On-Demand Capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB with Provisioned Capacity and Auto Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
            options: [
                { id: 0, text: "Set up AWS Direct Connect", correct: false },
                { id: 1, text: "Set up an AWS Site-to-Site VPN connection", correct: true },
                { id: 2, text: "Set up an Internet Gateway between the on-premises data center and AWS cloud", correct: false },
                { id: 3, text: "Set up a bastion host on Amazon EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an AWS Site-to-Site VPN connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Internet Gateway between the on-premises data center and AWS cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host on Amazon EC2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
            options: [
                { id: 0, text: "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group", correct: false },
                { id: 1, text: "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy", correct: true },
                { id: 2, text: "Make the Amazon S3 bucket public", correct: false },
                { id: 3, text: "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make the Amazon S3 bucket public: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed. Which solution meets these requirements most cost-effectively?",
            options: [
                { id: 0, text: "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation", correct: false },
                { id: 1, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: false },
                { id: 2, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: true },
                { id: 3, text: "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Amazon EFS Infrequent Access", correct: true },
                { id: 1, text: "Amazon EFS Standard", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Standard-IA", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EFS Infrequent Access\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon EFS Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
            options: [
                { id: 0, text: "AWS Firewall Manager", correct: false },
                { id: 1, text: "AWS Shield Advanced", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: true },
                { id: 3, text: "AWS Web Application Firewall (AWS WAF)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon GuardDuty\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Firewall Manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Web Application Firewall (AWS WAF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Run on AWS Lambda", correct: false },
                { id: 1, text: "Run on an Application Load Balancer", correct: false },
                { id: 2, text: "Run on Amazon EMR", correct: false },
                { id: 3, text: "Run on a Spot Instance with a persistent request type", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Run on a Spot Instance with a persistent request type\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run on AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application. Which combination of steps will enable secure S3 integration for this workload? (Select two)",
            options: [
                { id: 0, text: "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3", correct: true },
                { id: 1, text: "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID", correct: false },
                { id: 2, text: "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access", correct: false },
                { id: 3, text: "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate", correct: true },
                { id: 4, text: "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3, Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage. Which solution will help improve the application’s responsiveness and scalability during peak load periods?",
            options: [
                { id: 0, text: "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue", correct: false },
                { id: 1, text: "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput", correct: false },
                { id: 2, text: "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue", correct: true },
                { id: 3, text: "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Store the images using the Amazon S3 Standard-IA storage class", correct: false },
                { id: 1, text: "Store the images using the Amazon S3 Intelligent-Tiering storage class", correct: true },
                { id: 2, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
                { id: 3, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Store the images using the Amazon S3 Intelligent-Tiering storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Store the images using the Amazon S3 Standard-IA storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold. What is the MOST operationally efficient way to achieve this?",
            options: [
                { id: 0, text: "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders", correct: true },
                { id: 1, text: "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly", correct: false },
                { id: 2, text: "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops", correct: false },
                { id: 3, text: "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) temporary queues", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS) dead-letter queues", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) delay queues", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS) FIFO queues", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Simple Queue Service (Amazon SQS) temporary queues\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) dead-letter queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) delay queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
            options: [
                { id: 0, text: "Create a Read Replica in another Region as the Master database and point the analytics workload there", correct: false },
                { id: 1, text: "Create a Read Replica in the same Region as the Master database and point the analytics workload there", correct: true },
                { id: 2, text: "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database", correct: false },
                { id: 3, text: "Migrate the analytics application to AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Read Replica in the same Region as the Master database and point the analytics workload there\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Read Replica in another Region as the Master database and point the analytics workload there: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the analytics application to AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans. Which combination of savings plans will satisfy these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly", correct: false },
                { id: 1, text: "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances", correct: true },
                { id: 2, text: "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads", correct: false },
                { id: 3, text: "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type", correct: false },
                { id: 4, text: "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances, Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
            options: [
                { id: 0, text: "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: true },
                { id: 1, text: "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 2, text: "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 3, text: "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
            options: [
                { id: 0, text: "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes", correct: false },
                { id: 1, text: "Storage class analysis only provides recommendations for Standard to Standard IA classes", correct: true },
                { id: 2, text: "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes", correct: false },
                { id: 3, text: "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Storage class analysis only provides recommendations for Standard to Standard IA classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
            options: [
                { id: 0, text: "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby", correct: true },
                { id: 1, text: "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database", correct: false },
                { id: 2, text: "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync", correct: false },
                { id: 3, text: "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason", correct: true },
                { id: 4, text: "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby, Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practices—like mandatory logging and guardrails—when new accounts are created. The firm prefers a low-maintenance, governance-driven setup. Which solution best meets these goals while minimizing operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually", correct: false },
                { id: 1, text: "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance", correct: false },
                { id: 2, text: "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs", correct: false },
                { id: 3, text: "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
            options: [
                { id: 0, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region", correct: false },
                { id: 1, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region", correct: true },
                { id: 2, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region", correct: false },
                { id: 3, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection. Which configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
            options: [
                { id: 0, text: "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center", correct: true },
                { id: 1, text: "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect", correct: false },
                { id: 2, text: "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing", correct: false },
                { id: 3, text: "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
            options: [
                { id: 0, text: "Require HTTPS for communication between Amazon CloudFront and your S3 origin", correct: false },
                { id: 1, text: "Require HTTPS for communication between Amazon CloudFront and your custom origin", correct: false },
                { id: 2, text: "Use Amazon CloudFront signed URLs", correct: true },
                { id: 3, text: "Use Amazon CloudFront signed cookies", correct: true },
                { id: 4, text: "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Amazon CloudFront signed URLs, Use Amazon CloudFront signed cookies\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Require HTTPS for communication between Amazon CloudFront and your S3 origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Require HTTPS for communication between Amazon CloudFront and your custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in read request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 1, text: "Use AWS Global Accelerator", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use AWS Direct Connect", correct: false },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
            options: [
                { id: 0, text: "VPC Route Table", correct: false },
                { id: 1, text: "Amazon Inspector", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: false },
                { id: 3, text: "AWS Shield Advanced", correct: true },
                { id: 4, text: "AWS Web Application Firewall (AWS WAF)", correct: true },
                { id: 5, text: "VPC Security Group", correct: true },
            ],
            correctAnswers: [3, 4, 5],
            explanation: "The correct answers are: AWS Shield Advanced, AWS Web Application Firewall (AWS WAF), VPC Security Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- VPC Route Table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Inspector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon GuardDuty: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon MQ", correct: true },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) Standard", correct: false },
                { id: 3, text: "Amazon SQS FIFO (First-In-First-Out)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SQS FIFO (First-In-First-Out): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 48,
            text: "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning. Which solution will meet these requirements in the MOST efficient and scalable way?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically", correct: false },
                { id: 1, text: "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time", correct: true },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals", correct: false },
                { id: 3, text: "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "VPC Endpoint", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "AWS VPN CloudHub", correct: true },
                { id: 3, text: "Software VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic", correct: true },
                { id: 1, text: "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 2, text: "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store", correct: false },
                { id: 1, text: "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones", correct: false },
                { id: 2, text: "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances", correct: false },
                { id: 3, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 52,
            text: "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
            options: [
                { id: 0, text: "AWS Transit Gateway", correct: true },
                { id: 1, text: "VPC Peering Connection", correct: false },
                { id: 2, text: "Virtual private gateway (VGW)", correct: false },
                { id: 3, text: "AWS PrivateLink", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Transit Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Peering Connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Virtual private gateway (VGW): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS PrivateLink: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring. Which combination of actions will best support this migration approach? (Select three)",
            options: [
                { id: 0, text: "Launch a cutover instance after completing testing and confirming that replication is up-to-date", correct: true },
                { id: 1, text: "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use", correct: false },
                { id: 2, text: "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover", correct: true },
                { id: 3, text: "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs", correct: true },
                { id: 4, text: "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations", correct: false },
                { id: 5, text: "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation", correct: false },
            ],
            correctAnswers: [0, 2, 3],
            explanation: "The correct answers are: Launch a cutover instance after completing testing and confirming that replication is up-to-date, Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover, Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
            options: [
                { id: 0, text: "Throughput Optimized HDD Amazon EBS volumes", correct: false },
                { id: 1, text: "Provisioned IOPS SSD Amazon EBS volumes", correct: true },
                { id: 2, text: "General-purpose SSD-based Amazon EBS volumes", correct: false },
                { id: 3, text: "Cold HDD Amazon EBS volumes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD Amazon EBS volumes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Throughput Optimized HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- General-purpose SSD-based Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 1, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
            options: [
                { id: 0, text: "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS", correct: false },
                { id: 1, text: "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index", correct: false },
                { id: 2, text: "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS", correct: true },
                { id: 3, text: "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone", correct: false },
                { id: 1, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: false },
                { id: 2, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: true },
                { id: 3, text: "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
            options: [
                { id: 0, text: "Client Side Encryption", correct: true },
                { id: 1, text: "Server-side encryption with AWS KMS keys (SSE-KMS)", correct: false },
                { id: 2, text: "Server-side encryption with customer-provided keys (SSE-C)", correct: false },
                { id: 3, text: "Server-side encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Client Side Encryption\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption with AWS KMS keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root user’s email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users. Which solution meets these requirements in the most secure and scalable way?",
            options: [
                { id: 0, text: "Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications", correct: true },
                { id: 1, text: "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts", correct: false },
                { id: 2, text: "Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses", correct: false },
                { id: 3, text: "Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
            options: [
                { id: 0, text: "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC", correct: false },
                { id: 1, text: "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC", correct: true },
                { id: 2, text: "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC", correct: false },
                { id: 3, text: "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 1, text: "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 2, text: "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 3, text: "Use delay queues to postpone the delivery of new messages to the queue for a few seconds", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use delay queues to postpone the delivery of new messages to the queue for a few seconds\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 1, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 2, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue", correct: true },
                { id: 3, text: "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort. What is the most efficient solution to meet this goal with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule", correct: false },
                { id: 1, text: "Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed", correct: true },
                { id: 2, text: "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds", correct: false },
                { id: 3, text: "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
            options: [
                { id: 0, text: "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica", correct: true },
                { id: 1, text: "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
                { id: 2, text: "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica", correct: false },
                { id: 3, text: "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets", correct: false },
                { id: 1, text: "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled", correct: true },
                { id: 2, text: "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket", correct: false },
                { id: 3, text: "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test8: [
        {
            id: 1,
            text: "A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. A Solutions Architect has been instructed to create a cost-effective solution without affecting the performance of the application.Which dynamic scaling policy should be used to satisfy this requirement?",
            options: [
                { id: 0, text: "Use simple scaling.", correct: false },
                { id: 1, text: "Use scheduled scaling.", correct: false },
                { id: 2, text: "Use suspend and resume scaling.", correct: false },
                { id: 3, text: "Use target tracking scaling.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nWith a target tracking scaling policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over-provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern. This makes it ideal for cost optimization without affecting performance, as it automatically scales based on actual demand rather than requiring manual threshold configuration.\n\n**Why option 0 is incorrect:**\nSimple scaling requires you to wait for the cooldown period to complete before initiating additional scaling activities. After a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. This can lead to over-provisioning during rapid load changes, which is exactly what the scenario wants to avoid.\n\n**Why option 1 is incorrect:**\nScheduled scaling is mainly used for predictable traffic patterns where you know in advance when traffic will increase or decrease. Since the scenario mentions dynamic scaling and concerns about over-provisioning due to unpredictable load changes, scheduled scaling would not be appropriate. Target tracking scaling is better suited for optimizing costs with unpredictable traffic patterns.\n\n**Why option 2 is incorrect:**\nSuspend and resume scaling is used to temporarily pause scaling activities triggered by your scaling policies and scheduled actions. This is not a scaling policy itself but rather a control mechanism to pause scaling. It does not help optimize costs or prevent over-provisioning.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "A financial services company plans to migrate its trading application from on-premises Microsoft Windows Server to Amazon Web Services (AWS).The solution must ensure high availability across multiple Availability Zones and offer low-latency access to block storage.Which of the following solutions will fulfill these requirements?",
            options: [
                { id: 0, text: "Deploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for Windows File Server for shared storage.", correct: false },
                { id: 1, text: "Deploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon Elastic File System (Amazon EFS) to provide shared storage between the instances. Configure Amazon EFS with cross-region replication to sync data across Availability Zones.", correct: false },
                { id: 2, text: "Configure the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for NetApp ONTAP to create a Multi-AZ file system and access the data via iSCSI protocol.", correct: true },
                { id: 3, text: "Configure the trading application on Amazon EC2 Windows instances across two Availability Zones. Use Amazon Simple Storage Service (Amazon S3) for storage and configure cross-region replication to sync data between S3 buckets in each Availability Zone.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon FSx for NetApp ONTAP is a fully managed AWS service that provides high-performance, scalable file storage based on NetApp's ONTAP file system. It features Multi-AZ file systems designed to ensure continuous availability across AWS Availability Zones, providing high availability for Windows Server workloads. It offers consistent sub-millisecond file operation latencies with SSD storage, essential for block storage workloads in Windows environments. FSx for NetApp ONTAP fully supports block storage protocols like iSCSI, commonly used in Windows Server settings, making it the ideal solution for a trading application that requires low-latency block storage access across multiple Availability Zones.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server only provides shared, low-latency file storage for Windows environments. It doesn't support low-latency access to shared block storage, which is a key requirement for the trading application. The scenario specifically requires block storage access via iSCSI protocol, which FSx for Windows File Server cannot provide.\n\n**Why option 1 is incorrect:**\nWhile Amazon EFS provides high availability across Availability Zones, it is not optimized for Windows workloads and doesn't offer low-latency block storage. EFS is primarily designed for Linux-based file systems and uses NFS protocol, not iSCSI. Additionally, EFS cross-region replication is not the same as Multi-AZ deployment within a region, and the scenario requires block storage, not file storage.\n\n**Why option 3 is incorrect:**\nAmazon S3 is primarily an object storage service, not block storage. It doesn't offer the low-latency access required for a trading application. S3 is designed for storing and retrieving objects over HTTP/HTTPS, not for mounting as a block device or providing iSCSI access. While S3 can provide high availability, it cannot meet the low-latency block storage requirement.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 3,
            text: "A company is using AWS Fargate to run a batch job whenever an object is uploaded to an Amazon S3 bucket. The minimum ECS task count is initially set to 1 to save on costs and should only be increased based on new objects uploaded to the S3 bucket.Which is the most suitable option to implement with the LEAST amount of effort?",
            options: [
                { id: 0, text: "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to a Lambda function that will run theStartTaskAPI command.", correct: false },
                { id: 1, text: "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task.", correct: true },
                { id: 2, text: "Set up an alarm in Amazon CloudWatch to monitor S3 object-level operations that are recorded on CloudTrail. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers the ECS cluster when new CloudTrail events are detected.", correct: false },
                { id: 3, text: "Set up an alarm in CloudWatch to monitor S3 object-level operations recorded on CloudTrail. Set two alarm actions to update the ECS task count to scale-out/scale-in depending on the S3 event.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon EventBridge (Amazon CloudWatch Events) is a serverless event bus that makes it easy to connect applications together. You can use EventBridge to run Amazon ECS tasks when certain AWS events occur. You can set up an EventBridge rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using the Amazon S3 PUT operation. EventBridge can directly target an ECS cluster to run a new task, which requires the least amount of effort since no intermediate Lambda function is needed. This is the simplest and most direct approach to trigger ECS tasks based on S3 events.\n\n**Why option 0 is incorrect:**\nAlthough this solution meets the requirement, creating your own Lambda function for this scenario is not really necessary. It is much simpler to control ECS tasks directly as targets for the CloudWatch Event rule. The scenario asks for a solution that is the easiest to implement with the least amount of effort, and using EventBridge to directly target ECS is simpler than adding a Lambda intermediary.\n\n**Why option 2 is incorrect:**\nUsing CloudTrail and CloudWatch Alarm creates unnecessary complexity. Amazon EventBridge can directly target an ECS task on the Targets section when you create a new rule, eliminating the need for CloudTrail monitoring and CloudWatch alarms. This approach adds multiple layers of complexity that are not needed for the simple requirement of triggering an ECS task when an S3 object is uploaded.\n\n**Why option 3 is incorrect:**\nYou cannot directly set CloudWatch Alarms to update the ECS task count. CloudWatch alarms are designed to monitor metrics and trigger actions like SNS notifications or Auto Scaling policies, but they cannot directly scale ECS tasks. This approach would not work for the requirement.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 1,
            text: "A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances with different instance types and sizes. The application is extensively used during office hours from 9 in the morning to 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.Which of the following is the MOST operationally efficient solution to implement to ensure the application works properly at the beginning of the day?",
            options: [
                { id: 0, text: "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization.", correct: false },
                { id: 1, text: "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization.", correct: false },
                { id: 2, text: "Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day.", correct: true },
                { id: 3, text: "Configure a Predictive scaling policy for the Auto Scaling group to automatically adjust the number of Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nScaling based on a schedule allows you to scale your application in response to predictable load changes. Since the application is extensively used during office hours from 9 AM to 5 PM, and users complain about slow performance at the start of the day, a scheduled scaling policy can launch new instances before the start of the day (e.g., at 8:45 AM). This ensures that the instances are already scaled up and ready before the application is used the most, preventing the slow performance issue. Scheduled scaling is the most operationally efficient solution when you know the exact peak hours of your application, as it proactively scales resources before demand increases.\n\n**Why option 0 is incorrect:**\nAlthough dynamic scaling based on CPU utilization is a valid solution, it is reactive rather than proactive. By the time CPU utilization hits a peak and triggers scaling, the application already has performance issues, which is exactly what users are experiencing. Since the peak hours are predictable (9 AM to 5 PM), scheduled scaling is more operationally efficient as it ensures resources are ready before the demand arrives.\n\n**Why option 1 is incorrect:**\nSimilar to CPU-based dynamic scaling, memory-based dynamic scaling is reactive and will only trigger after memory utilization reaches a threshold. This means users will already experience slow performance before the scaling occurs. Since the traffic pattern is predictable (office hours), scheduled scaling is more efficient as it prepares resources in advance.\n\n**Why option 3 is incorrect:**\nPredictive scaling can be used in this scenario, but it is not the most operationally efficient option. The scenario mentions that the Auto Scaling group consists of EC2 instances with different instance types and sizes. Predictive scaling assumes that your Auto Scaling group is homogeneous, meaning all EC2 instances are of equal capacity. The forecasted capacity can be inaccurate if you are using a variety of EC2 instance sizes and types, making scheduled scaling a more reliable choice for this heterogeneous environment.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A company has a highly available architecture consisting of an Elastic Load Balancer and multiple Amazon EC2 instances configured with Auto Scaling across three Availability Zones. The company needs to monitor EC2 instances based on a specific metric that is not readily available in Amazon CloudWatch.Which of the following is a custom metric in CloudWatch that requires manual setup?",
            options: [
                { id: 0, text: "Memory Utilizationof an EC2 instance", correct: true },
                { id: 1, text: "CPU Utilizationof an EC2 instance", correct: false },
                { id: 2, text: "Disk Read activityof an EC2 instance", correct: false },
                { id: 3, text: "Network packets outof an EC2 instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon CloudWatch has Amazon EC2 Metrics available for monitoring by default, including CPU Utilization, Network Utilization, and Disk Read activity. However, Memory Utilization is not readily available in CloudWatch and requires manual setup. You need to prepare a custom metric using CloudWatch Monitoring Scripts (written in Perl) or install the CloudWatch Agent to collect more system-level metrics from Amazon EC2 instances. Memory utilization is one of the custom metrics that requires this manual configuration, making it the correct answer for a metric that is not readily available.\n\n**Why option 1 is incorrect:**\nCPU Utilization is typically available by default in CloudWatch and does not require a custom metric setup. CloudWatch automatically collects CPU metrics for EC2 instances without any additional configuration, so this option does not meet the requirement for a metric that requires manual setup.\n\n**Why option 2 is incorrect:**\nDisk Read activity is already included in CloudWatch's default EC2 metrics. Only certain storage-related metrics, such as disk space utilization, require custom monitoring. Since disk read activity is available by default, it does not meet the requirement for a custom metric that requires manual setup.\n\n**Why option 3 is incorrect:**\nNetwork packets out (Network Utilization) is typically included in CloudWatch's default network utilization monitoring. This metric is available by default for EC2 instances, so there is no need to configure a custom metric for it.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "A company is using Amazon S3 to store frequently accessed data. When an object is created or deleted, the S3 bucket will send an event notification to the Amazon SQS queue. A solutions architect needs to create a solution that will notify the development and operations team about the created or deleted objects.Which of the following would satisfy this requirement?",
            options: [
                { id: 0, text: "Set up another SQS queue for the other team. Grant S3 permission to send a notification to the second SQS queue.", correct: false },
                { id: 1, text: "Create a new Amazon SNS FIFO topic for the other team. Grant S3 permission to send the notification to the second SNS topic.", correct: false },
                { id: 2, text: "Set up an Amazon SNS topic and configure two SQS queues to poll the SNS topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.", correct: false },
                { id: 3, text: "Create an Amazon SNS topic and configure two SQS queues to subscribe to the topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon S3 event notifications are designed to be delivered at least once and to one destination only. You cannot attach two or more SNS topics or SQS queues directly to S3 event notifications. To send notifications to multiple subscribers (development and operations teams), you must use the message fanout pattern with Amazon SNS and Amazon SQS. By creating an SNS topic and configuring two SQS queues to subscribe to the topic, S3 can send notifications to the SNS topic, which will then fan out the message to both subscribed SQS queues. This allows both teams to receive identical notifications without requiring multiple S3 notification configurations.\n\n**Why option 0 is incorrect:**\nYou can only add 1 SQS queue or 1 SNS topic at a time for Amazon S3 event notifications. S3 does not support multiple destinations in a single notification configuration. If you need to send events to multiple subscribers, you should implement a message fanout pattern with Amazon SNS and Amazon SQS, where S3 sends to one SNS topic, and multiple SQS queues subscribe to that topic.\n\n**Why option 1 is incorrect:**\nSimilar to option 0, you can only configure one SNS topic for S3 event notifications. Additionally, you cannot poll Amazon SNS. Instead of configuring queues to poll Amazon SNS, you should configure each Amazon SQS queue to subscribe to the SNS topic, allowing SNS to push messages to the queues. Also, neither Amazon SNS FIFO topic nor Amazon SQS FIFO queue is warranted in this scenario since strict message ordering and deduplication are not required.\n\n**Why option 2 is incorrect:**\nYou cannot poll Amazon SNS. SNS uses a push model where it delivers messages to subscribed endpoints. Instead of configuring queues to poll Amazon SNS, you should configure each Amazon SQS queue to subscribe to the SNS topic, allowing SNS to automatically push messages to both queues when S3 sends a notification.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
            options: [
                { id: 0, text: "Use Amazon FSx For Lustre and the Provisioned IOPS SSD (io1) volumes of Amazon EBS for hot and cold storage respectively.", correct: false },
                { id: 1, text: "Use Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.", correct: true },
                { id: 2, text: "Use Amazon Elastic File System and Amazon S3 for hot and cold storage respectively.", correct: false },
                { id: 3, text: "Use Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon FSx For Lustre is a high-performance file system for fast processing of workloads. Lustre is a popular open-source parallel file system which stores data across multiple network file servers to maximize performance and reduce bottlenecks, making it ideal for concurrent processing of training datasets. For the cold storage requirement, Amazon S3 offers cost-effective storage tiers including Amazon S3 Glacier and Glacier Deep Archive for rarely accessed archived data. S3 provides industry-leading scalability and cost optimization for infrequently accessed datasets, making it perfect for archiving datasets that yield low profit. The combination of FSx for Lustre (high-performance parallel hot storage) and S3 with Glacier tiers (cost-effective cold storage) meets both requirements.\n\n**Why option 0 is incorrect:**\nProvisioned IOPS SSD (io1) volumes of Amazon EBS are primarily designed for storing hot data used in I/O-intensive workloads, not cold storage. EBS has a storage option called \"Cold HDD,\" but due to its price, it is not ideal for data archiving. EBS Cold HDD is much more expensive than Amazon S3 Glacier or Glacier Deep Archive and is often utilized in applications where sequential cold data is read less frequently, not for cost-effective archiving of large datasets.\n\n**Why option 2 is incorrect:**\nAlthough Amazon EFS supports concurrent access to data, it does not have the high-performance parallel file system capabilities that are typically required for machine learning workloads. EFS is designed for general-purpose file storage with shared access, but it lacks the parallel processing architecture that Lustre provides for high-performance computing and ML training workloads that require concurrent processing of thousands of datasets.\n\n**Why option 3 is incorrect:**\nAmazon FSx For Windows File Server does not have a parallel file system, unlike Lustre. FSx for Windows File Server is designed for Windows-based file shares with SMB protocol support, but it does not provide the parallel file system architecture needed for concurrent processing of large ML training datasets. It is optimized for Windows file sharing, not high-performance parallel computing.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement?",
            options: [
                { id: 0, text: "Create an API using Amazon API Gateway and use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic in seconds.", correct: false },
                { id: 1, text: "Create an API using Amazon API Gateway and use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic in seconds.", correct: false },
                { id: 2, text: "Create an API using Amazon API Gateway and use AWS Lambda to handle the bursts of traffic in seconds.", correct: true },
                { id: 3, text: "Create an API using Amazon API Gateway and use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic in seconds.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Lambda can scale faster than the regular Auto Scaling feature of Amazon EC2, Amazon Elastic Beanstalk, or Amazon ECS. Lambda functions can absorb reasonable bursts of traffic for approximately 15-30 minutes. For an initial burst of traffic, Lambda's cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region. Under the hood, Lambda can run your code on thousands of available AWS-managed EC2 instances (that could already be running) within seconds to accommodate traffic. This is much faster than the Auto Scaling process of launching new EC2 instances, which could take a few minutes. Since the scenario requires handling bursts of traffic within seconds whenever there is a new product announcement, Lambda is the ideal choice. Combined with Amazon API Gateway to create the API, this solution provides both global API access and rapid scaling to handle traffic spikes.\n\n**Why option 0 is incorrect:**\nThe processing time of Amazon EC2 Auto Scaling to provision new resources takes minutes. When using ECS with Service Auto Scaling, new tasks need to be launched on EC2 instances, which involves instance provisioning, container image pulling, and task startup. This process takes several minutes, which is too slow for the scenario's requirement of handling bursts of traffic within seconds.\n\n**Why option 1 is incorrect:**\nJust like option 0, Amazon Elastic Beanstalk with Auto Scaling has a delay of a few minutes as it launches new EC2 instances. The Auto Scaling process involves launching new instances, waiting for health checks, and deploying the application, which takes significantly longer than Lambda's near-instant scaling. This does not meet the requirement for handling traffic bursts within seconds.\n\n**Why option 3 is incorrect:**\nAn Auto Scaling group of Amazon EC2 instances has the same limitation as the previous options. The processing time of Amazon EC2 Auto Scaling to provision new resources takes minutes. EC2 instances need to be launched, pass health checks, and have the application deployed before they can handle traffic. This multi-minute process cannot meet the requirement for handling traffic bursts within seconds.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using Amazon DynamoDB. The application requires improved database performance by distributing the workload evenly and utilizing the provisioned throughput efficiently.Which of the following should be implemented for the DynamoDB table?",
            options: [
                { id: 0, text: "Reduce the number of partition keys in the DynamoDB table.", correct: false },
                { id: 1, text: "Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.", correct: true },
                { id: 2, text: "Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.", correct: false },
                { id: 3, text: "Avoid using a composite primary key, which is composed of a partition key and a sort key.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThe partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. Provisioned I/O capacity for the table is divided evenly among these physical partitions. The more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. High-cardinality attributes have a large number of distinct values, which means more unique partition keys and better distribution of I/O requests across partitions. This prevents \"hot\" partitions that result in throttling and ensures efficient utilization of provisioned throughput. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases.\n\n**Why option 0 is incorrect:**\nReducing the number of partition keys is the exact opposite of what you should do. The more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, fewer distinct partition key values means less even distribution across partitions, which effectively slows performance and creates hot partitions that can cause throttling.\n\n**Why option 2 is incorrect:**\nUsing partition keys with low-cardinality attributes (which have few distinct values) is the exact opposite of the correct answer. Low-cardinality means fewer unique partition key values, which leads to less even distribution of requests across partitions. This creates hot partitions where most traffic goes to a few partitions, causing throttling and inefficient use of provisioned throughput.\n\n**Why option 3 is incorrect:**\nA composite primary key (composed of a partition key and a sort key) actually provides more partitioning opportunities and improves performance. The sort key allows for better data organization within each partition, and the combination helps distribute workload more evenly. Avoiding composite keys would limit your ability to optimize partition distribution and query performance.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand Amazon EC2 instances that use Amazon Aurora as its database. Currently, the system stores the file documents that users upload in one of the attached Amazon EBS volumes. The system's performance has been observed to be slow, and the manager has instructed the team to improve the architecture.In this scenario, which solution should be implemented to achieve a scalable, highly available, POSIX-compliant shared file system?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket and use this as the storage for the CMS", correct: false },
                { id: 1, text: "Use Amazon EFS to provide a shared file system for concurrent access to data", correct: true },
                { id: 2, text: "Upgrade your existing EBS volumes to Provisioned IOPS SSD volumes", correct: false },
                { id: 3, text: "Leverage Amazon ElastiCache to cache frequently accessed data and reduce latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage that can be accessed concurrently by multiple Amazon EC2 instances across multiple Availability Zones. When mounted on EC2 instances, EFS provides a standard POSIX-compliant file system interface, allowing seamless integration with existing applications. In this scenario, the current architecture stores file documents on EBS volumes attached to individual instances, which doesn't provide parallel shared access. Although an EBS Volume can be attached to multiple EC2 instances, you can only do so on instances within a single Availability Zone. EFS solves this by providing highly available storage that spans multiple Availability Zones, allowing all EC2 instances in the auto-scaled fleet to access the same file documents concurrently.\n\n**Why option 0 is incorrect:**\nAmazon S3 is an object storage service, not a file storage system. It does not provide the required POSIX-compliant file system interface and file locking mechanisms needed by a CMS. S3 uses REST APIs for object operations and cannot be mounted as a traditional file system, making it unsuitable for applications that require standard file system semantics.\n\n**Why option 2 is incorrect:**\nSimply upgrading EBS volumes to Provisioned IOPS SSD volumes does not address the requirement for a shared, POSIX-compliant file system. EBS volumes are block storage devices that can only be attached to a single EC2 instance at a time (or multiple instances within the same AZ with limitations). They cannot provide the concurrent shared access across multiple instances and Availability Zones that the scenario requires.\n\n**Why option 3 is incorrect:**\nAmazon ElastiCache is an in-memory data store (Redis or Memcached) that improves application performance through caching, but it is not a file storage solution. ElastiCache cannot store file documents or provide a POSIX-compliant file system interface. It is designed for caching frequently accessed data in memory, not for persistent file storage that multiple instances need to access concurrently.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "An e-commerce company runs a highly scalable web application that depends on an Amazon Aurora database. As the number of users increases, the read replica faces difficulties keeping up with the increasing read traffic, causing performance bottlenecks during peak periods.Which of the following will resolve the issue with the most cost-effective solution?",
            options: [
                { id: 0, text: "Increase the size of the Aurora DB cluster.", correct: false },
                { id: 1, text: "Use automatic scaling for the Aurora read replica using Aurora Auto Scaling.", correct: true },
                { id: 2, text: "Implement read scaling with Aurora Global Database.", correct: false },
                { id: 3, text: "Set up a read replica that can operate across different regions.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAurora Auto Scaling automatically adjusts the capacity of your Aurora database cluster based on the workload. It ensures that your database cluster scales up or down as needed without manual intervention, making it particularly useful for businesses with fluctuating workloads. In this scenario, Aurora Auto Scaling allows the system to dynamically manage read replica resources, effectively addressing the surge in read traffic during peak periods. This dynamic management ensures that the company pays only for the extra resources when they are genuinely required, making it the most cost-effective solution. The read replicas scale up automatically during peak periods and scale down during off-peak periods, optimizing costs while maintaining performance.\n\n**Why option 0 is incorrect:**\nIt's not economical to permanently increase the size of the Aurora DB cluster just to alleviate bottlenecks during peak periods. A static increase in the DB cluster size results in constant costs, regardless of whether your database's resources are being fully utilized during off-peak periods or not. This means you would pay for larger instances even when traffic is low, which is not cost-effective compared to auto-scaling that only uses resources when needed.\n\n**Why option 2 is incorrect:**\nAmazon Aurora Global Database is primarily designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS Regions. While this can provide global availability and read scaling, it introduces additional complexity and can be more expensive due to infrastructure and data transfer costs. Since the scenario asks for the most cost-effective solution and the issue is within a single region (read replica performance), Aurora Auto Scaling is a better choice.\n\n**Why option 3 is incorrect:**\nSetting up a read replica that operates across different regions can provide read scalability and load-balancing benefits, but it is not the most cost-effective solution. It incurs additional costs associated with inter-region data replication and data transfer. Moreover, the issue is not related to cross-region availability but rather the read replica's performance within the current region during peak periods. Aurora Auto Scaling addresses this more cost-effectively by scaling replicas within the same region.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A popular social network is hosted in AWS and is using a Amazon DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email.Which of the following is the most suitable solution to implement to meet the requirement?",
            options: [
                { id: 0, text: "Using the Amazon Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using Amazon SNS.", correct: false },
                { id: 1, text: "Create an AWS Lambda function that uses DynamoDB Streams Amazon Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an Amazon SNS Topic that will notify the subscribers via email when there is an update made by a particular user.", correct: false },
                { id: 2, text: "Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and an AWS Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using Amazon SNS.", correct: false },
                { id: 3, text: "Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to Amazon SNS Topic that will notify the subscribers via email.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nA DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items. Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers —pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. Hence, the correct answer is: Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to Amazon SNS Topic that will notify the subscribers via email .\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\njust like in the above, you have to manually enable DynamoDB Streams first before you can use its endpoint.\n\n**Why option 2 is incorrect:**\nthe DynamoDB Accelerator (DAX) feature is primarily used to significantly improve the in-memory read performance of your database, and not to capture the time-ordered sequence of item-level modifications. You should use DynamoDB Streams in this scenario instead.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A retail company receives raw.csvdata files into its Amazon S3 bucket from multiple sources on an hourly basis, with an average file size of 2 GB.An automated process must be implemented to convert these.csvfiles into the more efficient Apache Parquet format and store the converted files in another S3 bucket. Additionally, the conversion process must be automatically initiated each time a new file is uploaded into the S3 bucket.Which of the following options must be implemented to meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use an AWS Lambda function triggered by anS3 PUTevent to convert the.csvfiles to Parquet format. Use the AWS Transfer Family with SFTP service to move the output files to the target S3 bucket.", correct: false },
                { id: 1, text: "Utilize an AWS Glue extract, transform, and load (ETL) job to process and convert the.csvfiles to Apache Parquet format and then store the output files into the target S3 bucket. Set up an S3 Event Notification to track everyS3 PUTevent and invoke the ETL job in Glue through Amazon SQS.", correct: true },
                { id: 2, text: "Set up an Apache Spark job running in an Amazon EC2 instance and create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitorS3 PUTevents in the S3 bucket. Configure AWS Lambda to invoke the Spark job for every new.csvfile added via a Function URL.", correct: false },
                { id: 3, text: "Create an ETL (Extract, Transform, Load) job and a Data Catalog table in AWS Glue. Configure the Glue crawler to run on a schedule to check for new files in the S3 bucket every hour and convert them to Parquet format.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Glue is a powerful ETL service that easily moves data between different data stores. By using AWS Glue, you can easily create and manage ETL jobs to transfer data from various sources, such as Amazon S3, Amazon RDS, and Amazon Redshift. Additionally, AWS Glue enables you to transform your data as needed to fit your specific needs. One of the key advantages of AWS Glue is its automatic schema discovery and mapping, which allows you to easily map data from different sources with different schemas. When working with big data processing, it is often necessary to convert data from one format to another to optimize processing efficiency. Apache Parquet is a columnar storage format that is designed to provide higher efficiency and performance for big data processing. By storing and processing large amounts of data with high compression rates and faster query times, Parquet can offer significant benefits to the company. Fortunately, Parquet is compatible with many data processing frameworks such as Spark, Hive, and Hadoop, making it a versatile format for big data processing. By using AWS Glue and other AWS services, you can easily convert their .csv files to the more efficient Apache Parquet format and store the output files in an S3 bucket, making it easy to access and process large amounts of data. Hence the correct answer is: Utilize an AWS Glue extract, transform, and load (ETL) job to process and convert the .csv files to Apache Parquet format and then store the output files into the target S3 bucket. Set up an S3 Event Notification to track every S3 PUT event and invoke the ETL job in Glue through Amazon SQS.\n\n**Why option 0 is incorrect:**\nThe conversion of CSV files to Parquet format using a Lambda function and S3 event notification would work; however, this is not the most efficient solution when handling large amounts of data. The Lambda function has a maximum execution time limit (15 minutes) which means that converting large 2 GB files may result in timeout issues. Additionally, Lambda has memory limits, requiring data to be delivered via a data stream, which entails additional effort compared with using AWS Glue. Using AWS Transfer Family with SFTP service to move output files to the target S3 bucket is unnecessary since Glue can write directly to S3.\n\n**Why option 2 is incorrect:**\nRunning Spark on EC2 instances requires manual provisioning, monitoring, and maintenance, leading to time and additional costs. Additionally, using Amazon EventBridge (CloudWatch Events) to trigger the Spark job through a Function URL adds complexity and potential points of failure. This option introduces unnecessary complexity and operational overhead, which contradicts the requirement for the solution with the least operational overhead.\n\n**Why option 3 is incorrect:**\nAlthough it is correct to create an ETL job using AWS Glue, simply triggering the job on a scheduled basis (every hour) rather than being triggered automatically by a new file upload is not ideal. It is not as efficient as using an S3 event trigger to initiate the conversion process immediately upon file upload. Scheduled execution means files may wait up to an hour before processing, and it doesn't meet the requirement for automatic initiation each time a new file is uploaded.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead.In this scenario, how can you protect the backend systems of the platform from traffic spikes?",
            options: [
                { id: 0, text: "Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.", correct: false },
                { id: 1, text: "Enable throttling limits and result caching in API Gateway.", correct: true },
                { id: 2, text: "Use CloudFront in front of the API Gateway to act as a cache.", correct: false },
                { id: 3, text: "Move the Lambda function in a VPC.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon API Gateway provides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any request over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence, enabling throttling limits and result caching in API Gateway is the correct answer. You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.\n\n**Why option 0 is incorrect:**\nSwitching from AWS Lambda and API Gateway to EC2 instances, ELB, and Auto Scaling is incorrect since there is no need to transfer your applications to other services. Lambda and API Gateway are already highly scalable and can handle traffic spikes effectively when properly configured with throttling and caching.\n\n**Why option 2 is incorrect:**\nUsing CloudFront in front of the API Gateway to act as a cache is incorrect because CloudFront only speeds up content delivery which provides a better latency experience for your users. It does not help much for protecting the backend systems from traffic spikes, which is the primary concern in this scenario.\n\n**Why option 3 is incorrect:**\nMoving the Lambda function in a VPC is incorrect because this answer is irrelevant to what is being asked. A VPC is your own virtual private cloud where you can launch AWS services, but placing Lambda in a VPC does not protect the backend from traffic spikes and may actually introduce additional latency.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A company is using a combination of API Gateway and AWS Lambda for the web services of an online web portal that is accessed by hundreds of thousands of clients each day. The company will be announcing a new revolutionary product, and it is expected that the web portal will receive a massive number of visitors from all around the globe.How can the back-end systems and applications be protected from traffic spikes?",
            options: [
                { id: 0, text: "Use throttling limits in API Gateway", correct: true },
                { id: 1, text: "API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything.", correct: false },
                { id: 2, text: "Manually upgrade the Amazon EC2 instances being used by API Gateway", correct: false },
                { id: 3, text: "Deploy Multi-AZ in API Gateway with Read Replica", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon API Gateway provides throttling at multiple levels including global and by a service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence, the correct answer is: Use throttling limits in API Gateway.\n\n**Why option 1 is incorrect:**\nThe option that says API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything is incorrect. Although it can scale using AWS Edge locations, you still need to configure the throttling, typically to further manage the bursts of your APIs.\n\n**Why option 2 is incorrect:**\nThe option that says manually upgrade the Amazon EC2 instances being used by API Gateway is incorrect because API Gateway is a fully managed service and hence, you do not have access to its underlying resources.\n\n**Why option 3 is incorrect:**\nThe option that says deploying Multi-AZ in API Gateway with Read Replica is incorrect because only RDS has Multi-AZ and Read Replica capabilities, and not API Gateway.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "An online learning company hosts its Microsoft .NET e-Learning application on a Windows Server in its on-premises data center. The application uses an Oracle Database Standard Edition as its backend database.The company wants a high-performing solution to migrate this workload to the AWS cloud to take advantage of the cloud’s high availability. The migration process should minimize development changes, and the environment should be easier to manage.Which of the following options should be implemented to meet the company requirements? (Select TWO.)",
            options: [
                { id: 0, text: "Perform a homogeneous migration by moving the Oracle database to Amazon RDS for Oracle in a Multi-AZ deployment using AWS Database Migration Service (AWS DMS).", correct: true },
                { id: 1, text: "Refactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.", correct: false },
                { id: 2, text: "Use AWS Application Migration Service (AWS MGN) to migrate the on-premises Oracle database server to a new Amazon EC2 instance.", correct: false },
                { id: 3, text: "Rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment which runs in multiple Availability Zones.", correct: true },
                { id: 4, text: "Provision and replatform the application to Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. Use the Windows Server Amazon Machine Image (AMI) and deploy the .NET application using to the ECS cluster via the ECS Anywhere service.", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "**Why option 0 is correct:**\nAWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud or between combinations of cloud and on-premises setups. With AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync. If you want to migrate to a different database engine, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform. You then use AWS DMS to migrate the data. Performing a homogeneous migration by moving the Oracle database to Amazon RDS for Oracle in a Multi-AZ deployment using AWS DMS minimizes development changes since it keeps the same database engine, and the Multi-AZ deployment ensures high availability.\n\n**Why option 3 is correct:**\nAWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application. AWS Elastic Beanstalk for .NET makes it easier to deploy, manage, and scale your ASP.NET web applications that use Amazon Web Services. Elastic Beanstalk for .NET is available to anyone who is developing or hosting a web application that uses IIS. Rehosting the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment which runs in multiple Availability Zones requires minimal code changes and provides high availability.\n\n**Why option 1 is incorrect:**\nThe option that says refactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate is incorrect. This will take significant changes to the application as you will refactor, or do a code change to, the codebase in order for it to become a serverless container application. Remember that the scenario explicitly mentioned that the migration process should minimize development changes. A better solution is to simply rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment, which doesn't require any code changes.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Application Migration Service (AWS MGN) to migrate the on-premises Oracle database server to a new Amazon EC2 instance is incorrect. Amazon RDS primarily supports standard Oracle databases so it would be better to use AWS DMS for the database migration, not AWS MGN.\n\n**Why option 4 is incorrect:**\nThe option that says provision and replatform the application to Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. Use the Windows Server Amazon Machine Image (AMI) and deploy the .NET application using to the ECS cluster via the ECS Anywhere service is incorrect. This may be possible, but it is not recommended for this scenario because you will have to manage the underlying EC2 instances of your Amazon ECS cluster that will run the application. It would be better just to use Elastic Beanstalk to take care of provisioning the resources for your .NET application. Keep in mind that doing a replatform-type migration like this one entails significant development changes, which is not suitable with the requirements given in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "A popular social media website uses a Amazon CloudFront web distribution to serve static content to millions of users around the globe. Recently, the website has received a number of complaints about long login times. Additionally, there are instances where users encounter HTTP 504 errors. The manager has instructed the team to significantly reduce login time and further optimize the system.Which of the following options should be used together to set up a cost-effective solution that improves the application's performance? (Select TWO.)",
            options: [
                { id: 0, text: "Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your AWS Lambda functions to execute the authentication process in AWS locations closer to the users.", correct: true },
                { id: 1, text: "Establish multiple Amazon VPCs in different AWS regions and configure a transit VPC to interconnect all of your resources. To handle the requests faster, set up AWS Lambda functions in each region with the AWS Serverless Application Model (SAM) service.", correct: false },
                { id: 2, text: "Configure your origin to add aCache-Control max-agedirective to your objects, and specify the longest practical value formax-ageto increase the cache hit ratio of your CloudFront distribution.", correct: false },
                { id: 3, text: "Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.", correct: false },
                { id: 4, text: "Implement an origin failover by creating an origin group that includes two origins. Assign one as the primary origin and the other as secondary, which enables CloudFront to automatically switch to if the primary origin encounters specific HTTP status code failure responses.", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nLambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points: - After CloudFront receives a request from a viewer (viewer request) - Before CloudFront forwards the request to the origin (origin request) - After CloudFront receives the response from the origin (origin response) - Before CloudFront forwards the response to the viewer (viewer response) In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users, significantly reducing login times for global users.\n\n**Why option 4 is correct:**\nYou can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing by providing automatic failover to a secondary origin when the primary encounters failures.\n\n**Why option 1 is incorrect:**\nThe option that says establish multiple Amazon VPCs in different AWS regions and configure a transit VPC to interconnect all of your resources. To handle the requests faster, set up AWS Lambda functions in each region with the AWS Serverless Application Model (SAM) service is incorrect because of the same reason provided above. Although setting up multiple VPCs across various regions which are just connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead.\n\n**Why option 2 is incorrect:**\nThe option that says configure your origin to add a Cache-Control max-age directive to your objects, and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can only improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the sluggish authentication process of your global users and not just the caching of the static objects.\n\n**Why option 3 is incorrect:**\nThe option that says deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with minimal cost.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A healthcare organization wants to build a system that can predict drug prescription abuse. The organization will gather real-time data from multiple sources, which include Personally Identifiable Information (PII). It's crucial that this sensitive information is anonymized prior to landing in a NoSQL database for further processing.Which solution would meet the requirements?",
            options: [
                { id: 0, text: "Create a data lake in Amazon S3 and use it as the primary storage for patient health data. Use an S3 trigger to run an AWS Lambda function that performs anonymization. Send the anonymized data to Amazon DynamoDB.", correct: false },
                { id: 1, text: "Stream the data in an Amazon DynamoDB table. Enable DynamoDB Streams, and configure an AWS Lambda function withAmazonDynamoDBFullAccesspermissions to perform anonymization on newly written items.", correct: false },
                { id: 2, text: "Deploy an Amazon Data Firehose stream to capture and transform the streaming data. Deliver the anonymized data to Amazon Redshift for analysis.", correct: false },
                { id: 3, text: "Ingest real-time data using Amazon Kinesis Data Stream. Use an AWS Lambda function to anonymize the PII, then store it in Amazon DynamoDB.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. Kinesis Data Streams integrates seamlessly with AWS Lambda, which can be utilized to transform and anonymize Personally Identifiable Information (PII) in transit before it is stored in any system. This ensures that sensitive information is anonymized immediately, preventing unanonymized PII from being stored in any storage system, as required. The anonymized data is then stored in Amazon DynamoDB, a NoSQL database suitable for handling the processed data for further analysis, such as predicting drug prescription abuse. Hence, the correct answer is: Ingest real-time data using Amazon Kinesis Data Stream. Use an AWS Lambda function to anonymize the PII, then store it in Amazon DynamoDB.\n\n**Why option 0 is incorrect:**\nThe option that says create a data lake in Amazon S3 and use it as the primary storage for patient health data. Use an S3 trigger to run an AWS Lambda function that performs anonymization. Send the anonymized data to Amazon DynamoDB is incorrect. This approach stores unanonymized PII in Amazon S3 before the Lambda function anonymizes it. This simply violates the requirement that PII be anonymized before landing in any storage system. Storing sensitive data in S3, even temporarily, only increases the risk of exposure and does not comply with the privacy requirements.\n\n**Why option 1 is incorrect:**\nThe option that says stream the data in an Amazon DynamoDB table. Enable DynamoDB Streams, and configure an AWS Lambda function with AmazonDynamoDBFullAccess permissions to perform anonymization on newly written items is incorrect. DynamoDB Streams processes changes to already written data, meaning unanonymized PII would be stored in DynamoDB before anonymization, violating the requirement. Additionally, using AmazonDynamoDBFullAccess violates the principle of least privilege, as it primarily grants more permissions than necessary.\n\n**Why option 2 is incorrect:**\nThe option that says deploy an Amazon Data Firehose stream to capture and transform the streaming data. Deliver the anonymized data to Amazon Redshift for analysis is incorrect. The requirement specifies that the anonymized data must be stored in a NoSQL database. Amazon Redshift is a relational data warehousing solution, not a NoSQL database, making this option unsuitable.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company has a web application that uses Internet Information Services (IIS) for Windows Server. A file share is used to store the application data on the network-attached storage of the company’s on-premises data center. To achieve a highly available system, the company plans to migrate the application and file share to AWS.Which of the following can be used to fulfill this requirement?",
            options: [
                { id: 0, text: "Migrate the existing file share configuration to AWS Storage Gateway.", correct: false },
                { id: 1, text: "Migrate the existing file share configuration to Amazon FSx for Windows File Server.", correct: true },
                { id: 2, text: "Migrate the existing file share configuration to Amazon EFS.", correct: false },
                { id: 3, text: "Migrate the existing file share configuration to Amazon EBS.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon FSx for Windows File Server provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. Amazon FSx for Windows File Server has the features, performance, and compatibility to easily lift and shift enterprise applications to the AWS Cloud. It is accessible from Windows, Linux, and macOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently. In this scenario, you need to migrate your existing file share configuration to the cloud. Among the options given, the best possible answer is Amazon FSx. A file share is a specific folder in your file system, including the folder's subfolders, which you make accessible to your compute instances via the SMB protocol. To migrate file share configurations from your on-premises file system, you must migrate your files first to Amazon FSx before migrating your file share configuration. Hence, the correct answer is: Migrate the existing file share configuration to Amazon FSx for Windows File Server.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the existing file share configuration to AWS Storage Gateway is incorrect because AWS Storage Gateway is primarily used to integrate your on-premises network to AWS but not for migrating your applications. Using a file share in Storage Gateway implies that you will still keep your on-premises systems, and not entirely migrate it.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the existing file share configuration to Amazon EFS is incorrect because it is stated in the scenario that the company is using a file share that runs on a Windows server. Remember that Amazon EFS only supports Linux workloads.\n\n**Why option 3 is incorrect:**\nThe option that says migrate the existing file share configuration to Amazon EBS is incorrect because EBS is primarily used as block storage for EC2 instances and not as a shared file system. A file share is a specific folder in a file system that you can access using a server message block (SMB) protocol. Amazon EBS does not support SMB protocol.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. The Solutions Architect must determine the fastest way to aggregate all the data.Which of the following options can satisfy the given requirement?",
            options: [
                { id: 0, text: "Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.", correct: true },
                { id: 1, text: "Upload the data to the closest Amazon S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket.", correct: false },
                { id: 2, text: "Use AWS Snowball Edge to transfer large amounts of data.", correct: false },
                { id: 3, text: "Set up a Site-to-Site VPN connection.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon S3 is object storage built to store and retrieve any amount of data from anywhere on the Internet. It's a simple storage service that offers industry-leading durability, availability, performance, security, and virtually unlimited scalability at very low costs. Amazon S3 is also designed to be highly flexible. Store any type and amount of data that you want; read the same piece of data a million times or only for emergency disaster recovery; build a simple FTP application or a sophisticated web application. Since the weather forecasting application is located in N.Virginia, you need to transfer all the data in the same AWS Region. With Amazon S3 Transfer Acceleration, you can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Multipart upload allows you to upload a single object as a set of parts. After all the parts of your object are uploaded, Amazon S3 then presents the data as a single object. This approach is the fastest way to aggregate all the data. Hence, the correct answer is: Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.\n\n**Why option 1 is incorrect:**\nThe option that says upload the data to the closest Amazon S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket is incorrect because replicating the objects to the destination bucket typically takes about 15 minutes. Take note that the requirement in the scenario is to aggregate the data in the fastest way.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Snowball Edge to transfer large amounts of data is incorrect because the end-to-end time to transfer up to 80 TB of data into AWS Snowball Edge is only approximately one week. This is far too slow for the requirement of aggregating data in the fastest way possible.\n\n**Why option 3 is incorrect:**\nThe option that says set up a Site-to-Site VPN connection is incorrect because setting up a VPN connection is not needed in this scenario. Site-to-Site VPN is just used for establishing secure connections between an on-premises network and Amazon VPC. Also, this approach is not the fastest way to transfer your data. You must use Amazon S3 Transfer Acceleration.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A company wishes to query data that resides in multiple AWS accounts from a central data lake. Each account has its own Amazon S3 bucket that stores data unique to its business function. Access to the data lake must be granted based on user roles.Which solution will minimize overhead and costs while meeting the required access patterns?",
            options: [
                { id: 0, text: "Use AWS Lake Formation to consolidate data from multiple accounts into a single account.", correct: true },
                { id: 1, text: "Use Amazon Data Firehose to consolidate data from multiple accounts into a single account.", correct: false },
                { id: 2, text: "Create a scheduled AWS Lambda function using Amazon EventBridge for transferring data from multiple accounts to the S3 buckets of the central account.", correct: false },
                { id: 3, text: "Use AWS Control Tower to centrally manage each account's S3 buckets.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions. Amazon S3 forms the storage layer for Lake Formation. If you already use S3, you typically begin by registering existing S3 buckets that contain your data. Lake Formation creates new buckets for the data lake and imports data into them. AWS always stores this data in your account, and only you have direct access to it. AWS Lake Formation is integrated with AWS Glue which you can use to create a data catalog that describes available datasets and their appropriate business applications. Lake Formation lets you define policies and control data access with simple grant and revoke permissions to data sets at granular levels. You can assign permissions to IAM users, roles, groups, and Active Directory users using federation. You specify permissions on catalog objects (like tables and columns) rather than on buckets and objects. Thus, the correct answer is: Use AWS Lake Formation to consolidate data from multiple accounts into a single account.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Data Firehose to consolidate data from multiple accounts into a single account is incorrect because setting up a Data Firehose in each and every account to move data into a single location is just costly and impractical. A better approach is to set up cross-account sharing which is free with AWS Lake Formation.\n\n**Why option 2 is incorrect:**\nThe option that says create a scheduled AWS Lambda function using Amazon EventBridge for transferring data from multiple accounts to the S3 buckets of the central account is incorrect. This could be done by utilizing the AWS SDK, but implementation would be difficult and quite challenging to manage. Remember that the scenario explicitly mentioned that the solution must minimize management overhead.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Control Tower to centrally manage each account's S3 buckets is incorrect because the AWS Control Tower service is primarily used to manage and govern multiple AWS accounts and not just S3 buckets. Using the AWS Lake Formation service is a more suitable choice.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 19,
            text: "A global IT company with offices around the world has multiple AWS accounts. To improve efficiency and drive costs down, the Chief Information Officer (CIO) wants to set up a solution that centrally manages their AWS resources. This will allow them to procure AWS resources centrally and share resources such as AWS Transit Gateways, AWS License Manager configurations, or Amazon Route 53 Resolver rules across their various accounts.As the Solutions Architect, which combination of options should you implement in this scenario? (Select TWO.)",
            options: [
                { id: 0, text: "Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.", correct: true },
                { id: 1, text: "Use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts.", correct: false },
                { id: 2, text: "Use AWS Control Tower to easily and securely share your resources with your AWS accounts.", correct: false },
                { id: 3, text: "Consolidate all of the company's accounts using AWS Organizations.", correct: true },
                { id: 4, text: "Consolidate all of the company's accounts using AWS ParallelCluster.", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "**Why option 0 is correct:**\nAWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. Many organizations use multiple accounts to create administrative or billing isolation, and limit the impact of errors. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge. You can procure AWS resources centrally, and use RAM to share resources such as subnets or License Manager configurations with other accounts. This eliminates the need to provision duplicate resources in every account in a multi-account environment, reducing the operational overhead of managing those resources in every account.\n\n**Why option 3 is correct:**\nAWS Organizations is an account management service that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage. With Organizations, you can create member accounts and invite existing accounts to join your organization. You can organize those accounts into groups and attach policy-based controls. Consolidating all of the company's accounts using AWS Organizations provides the foundation for centralized management and resource sharing.\n\n**Why option 1 is incorrect:**\nThe option that says use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts is incorrect. Although you can delegate access to resources that are in different AWS accounts using IAM, this process is extremely tedious and entails a lot of operational overhead since you have to manually set up cross-account access to each and every AWS account of the company. A better solution is to use AWS Resources Access Manager instead.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Control Tower to easily and securely share your resources with your AWS accounts is incorrect because AWS Control Tower simply offers the easiest way to set up and govern a new, secure, multi-account AWS environment. This is not the most suitable service to use to securely share your resources across AWS accounts or within your Organization. You have to use AWS Resources Access Manager (RAM) instead.\n\n**Why option 4 is incorrect:**\nThe option that says consolidate all of the company's accounts using AWS ParallelCluster is incorrect because AWS ParallelCluster is simply an AWS-supported open-source cluster management tool that makes it easy for you to deploy and manage High-Performance Computing (HPC) clusters on AWS. In this particular scenario, it is more appropriate to use AWS Organizations to consolidate all of your AWS accounts.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A car dealership website hosted in Amazon EC2 stores car listings in an Amazon Aurora database managed by Amazon RDS. Once a vehicle has been sold, its data must be removed from the current listings and forwarded to a distributed processing system.Which of the following options can satisfy the given requirement?",
            options: [
                { id: 0, text: "Create an RDS event subscription and send the notifications to Amazon SQS. Configure the SQS queues to fan out the event notifications to multiple Amazon SNS topics. Process the data using AWS Lambda functions.", correct: false },
                { id: 1, text: "Create an RDS event subscription and send the notifications to AWS Lambda. Configure the Lambda function to fanout the event notifications to multiple Amazon SQS queues to update the target groups.", correct: false },
                { id: 2, text: "Create an RDS event subscription and send the notifications to Amazon SNS. Configure the SNS topic to fan out the event notifications to multiple Amazon SQS queues. Process the data using AWS Lambda functions.", correct: false },
                { id: 3, text: "Create a native function or a stored procedure that invokes an AWS Lambda function. Configure the Lambda function to send event notifications to an Amazon SQS queue for the processing system to consume.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nYou can invoke an AWS Lambda function from an Amazon Aurora MySQL-Compatible Edition DB cluster with a native function or a stored procedure. This approach can be useful when you want to integrate your database running on Aurora MySQL with other AWS services. For example, you might want to capture data changes whenever a row in a table is modified in your database. In the scenario, you can trigger a Lambda function whenever a listing is deleted from the database. You can then write the logic of the function to send the listing data to an SQS queue and have different processes consume it. Hence, the correct answer is: Create a native function or a stored procedure that invokes an AWS Lambda function. Configure the Lambda function to send event notifications to an Amazon SQS queue for the processing system to consume.\n\n**Why option 0 is incorrect:**\nThe option that says create an RDS event subscription and send the notifications to Amazon SQS. Configure the SQS queues to fan out the event notifications to multiple Amazon SNS topics. Process the data using AWS Lambda functions is incorrect because RDS event subscriptions typically notify about operational changes rather than data modifications. This method does not capture database modifications like INSERT, DELETE, or UPDATE.\n\n**Why option 1 is incorrect:**\nThe option that says create an RDS event subscription and send the notifications to AWS Lambda. Configure the Lambda function to fan out the event notifications to multiple Amazon SQS queues to update the target groups is incorrect because RDS event subscriptions primarily focus on operational-level changes rather than capturing direct data modifications.\n\n**Why option 2 is incorrect:**\nThe option that says create an RDS event subscription and send the notifications to Amazon SNS. Configure the SNS topic to fan out the event notifications to multiple Amazon SQS queues. Process the data using AWS Lambda functions is incorrect because RDS event subscriptions only track infrastructure-related events and not actual database changes.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
            options: [
                { id: 0, text: "Use a DynamoDB VPC endpoint and an S3 VPC endpoint to route all access to these services via private endpoints.", correct: true },
                { id: 1, text: "Use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints.", correct: false },
                { id: 2, text: "Enable DynamoDB Encryption at Rest with the default AWS-managed key and S3 Server-Side Encryption with the default AWS KMS key to route all traffic to DynamoDB and S3 via private endpoints.", correct: false },
                { id: 3, text: "Use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA VPC endpoint allows you to privately connect your VPC to supported AWS and VPC endpoint services powered by AWS PrivateLink without needing an Internet gateway, NAT computer, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. In the scenario, you are asked to configure private endpoints to send data to Amazon DynamoDB and Amazon S3 without accessing the public Internet. Among the options given, VPC endpoint is the most suitable service that will allow you to use private IP addresses to access both DynamoDB and S3 without any exposure to the public internet. Hence, the correct answer is: Use a DynamoDB VPC endpoint and an S3 VPC endpoint to route all access to these services via private endpoints.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints is incorrect because AWS VPN CloudHub is typically used to provide secure communication between remote sites and not for creating a private endpoint to access Amazon S3 and DynamoDB within the Amazon network.\n\n**Why option 2 is incorrect:**\nThe option that says enable DynamoDB Encryption at Rest with the default AWS-managed key and S3 Server-Side Encryption with the default AWS KMS key to route all traffic to DynamoDB and S3 via private endpoints is incorrect because encryption at rest does not affect the traffic routing. Encryption manages data security but does not control how traffic is routed between services.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints is incorrect because AWS Direct Connect is primarily used to establish a dedicated network connection from your premises to AWS. The scenario didn't say that the company is using its on-premises server or has a hybrid cloud architecture.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 1,
            text: "There was an incident in a production environment where user data stored in an Amazon S3 bucket was accidentally deleted by a Junior DevOps Engineer. The issue was escalated to management, and after a few days, an instruction was given to improve the security and protection of AWS resources.What combination of the following options will protect the S3 objects in the bucket from both accidental deletion and overwriting? (Select TWO.)",
            options: [
                { id: 0, text: "Enable Versioning", correct: true },
                { id: 1, text: "Provide access to S3 data strictly through pre-signed URL only", correct: false },
                { id: 2, text: "Disallow S3 Delete using an IAM bucket policy", correct: false },
                { id: 3, text: "Enable S3 Intelligent-Tiering", correct: false },
                { id: 4, text: "Enable Multi-Factor Authentication Delete", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nBy using Versioning and enabling MFA (Multi-Factor Authentication) Delete, you can secure and recover your S3 objects from accidental deletion or overwrite. Versioning is a means of keeping multiple variants of an object in the same bucket. Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.\n\n**Why option 4 is correct:**\nYou can also optionally add another layer of security by configuring a bucket to enable MFA (Multi-Factor Authentication) Delete, which requires additional authentication for either of the following operations: - Change the versioning state of your bucket - Permanently delete an object version MFA Delete requires two forms of authentication together: - Your security credentials - The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device.\n\n**Why option 1 is incorrect:**\nThe option that says enable Cross-Region Replication (CRR) is incorrect because CRR is used to replicate objects across different AWS Regions for disaster recovery and compliance purposes. While it provides redundancy, it does not prevent accidental deletion or overwrite of objects in the source bucket. Versioning and MFA Delete are specifically designed to protect against accidental deletions.\n\n**Why option 2 is incorrect:**\nThe option that says enable server-side encryption is incorrect because encryption protects data at rest from unauthorized access, but it does not prevent accidental deletion or overwrite of objects. Encryption ensures data confidentiality, not data recovery from accidental deletion.\n\n**Why option 3 is incorrect:**\nThe option that says enable S3 Object Lock is incorrect because while Object Lock can prevent deletion and overwriting, it is typically used for compliance and regulatory requirements with retention periods. For general protection against accidental deletion, Versioning combined with MFA Delete is more appropriate and flexible.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "An application that records weather data every minute is deployed in a fleet of Amazon EC2 Spot instances and uses a MySQL RDS database instance. Currently, there is only one Amazon RDS instance running in one Availability Zone. The database needs to be improved to ensure high availability by enabling synchronous data replication to another RDS instance.Which of the following performs synchronous data replication in RDS?",
            options: [
                { id: 0, text: "RDS DB instance running as a Multi-AZ deployment", correct: true },
                { id: 1, text: "RDS Read Replica", correct: false },
                { id: 2, text: "Amazon DynamoDB Read Replica", correct: false },
                { id: 3, text: "Amazon CloudFront running as a Multi-AZ deployment", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nWhen you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync and protect your latest database updates against DB instance failure. Therefore, the correct answer is: RDS DB instance running as a Multi-AZ deployment.\n\n**Why option 1 is incorrect:**\nThe option that says RDS Read Replica is incorrect as a Read Replica primarily provides an asynchronous replication instead of synchronous. Read Replicas are designed for read scaling and disaster recovery scenarios where slight replication lag is acceptable, but they do not provide synchronous replication required for high availability with zero data loss.\n\n**Why option 2 is incorrect:**\nThe option that says Amazon DynamoDB Read Replica is incorrect since DynamoDB does not offer a Read Replica feature. It typically uses global tables to replicate data across multiple AWS Regions, but this is asynchronous replication, not synchronous replication within a single region.\n\n**Why option 3 is incorrect:**\nThe option that says Amazon CloudFront running as a Multi-AZ deployment is incorrect as CloudFront does not have a Read Replica feature. It simply caches content at edge locations rather than replicating data in the database. CloudFront is a content delivery network service, not a database replication solution.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "An online cryptocurrency exchange platform is hosted in AWS, utilizing an Amazon ECS Cluster and Amazon RDS in a Multi-AZ Deployments configuration. The application heavily uses the RDS instance to process complex read and write database operations. To maintain reliability, availability, and performance, it is necessary to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of CPU bandwidth and total memory consumed by each process.Which of the following is the most suitable solution to monitor the database properly?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to monitor the CPU Utilization of your database.", correct: false },
                { id: 1, text: "Create a script that collects and publishes custom metrics to Amazon CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.", correct: false },
                { id: 2, text: "Enable Enhanced Monitoring in RDS.", correct: true },
                { id: 3, text: "Check theCPU%andMEM%metrics which are readily available in the RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon RDS offers a powerful feature known as Enhanced Monitoring, which provides detailed metrics in real-time about the operating system (OS) underlying your database instances. This feature allows users to monitor performance at a granular level through the AWS Management Console or by accessing the Enhanced Monitoring JSON output via CloudWatch Logs. By default, these metrics are retained in CloudWatch Logs for 30 days, but this retention period can be adjusted by modifying the retention settings for the RDSOSMetrics log group in CloudWatch. Enhanced Monitoring differs from standard CloudWatch metrics in that it gathers data directly from an agent installed on the instance, rather than from the hypervisor, which is used by CloudWatch. This distinction can lead to slight variations between the two sets of metrics. For instance, CloudWatch provides CPU utilization metrics based on the hypervisor's view, while Enhanced Monitoring captures detailed insights from the instance itself, offering a more accurate representation of resource usage at the OS level. This feature is particularly beneficial for users who need in-depth visibility into how individual processes or threads on a DB instance utilize CPU resources. The differences in metric data may become more pronounced when using smaller instance classes, as multiple virtual machines are often managed by the same hypervisor, affecting the accuracy of hypervisor-based metrics. Hence, the correct answer is: Enable Enhanced Monitoring in RDS.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon CloudWatch to monitor the CPU Utilization of your database is incorrect because standard CloudWatch metrics only provide basic CPU utilization metrics at the hypervisor level, not the detailed process-level and thread-level metrics required by the scenario. CloudWatch cannot show the percentage of CPU bandwidth and total memory consumed by each process, which is what Enhanced Monitoring provides.\n\n**Why option 1 is incorrect:**\nThe option that says create a script that collects and publishes custom metrics to Amazon CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics is incorrect because this approach requires significant operational overhead and manual scripting. Enhanced Monitoring is a built-in RDS feature that automatically provides the detailed OS-level metrics needed, including per-process CPU and memory consumption, without requiring custom scripts.\n\n**Why option 3 is incorrect:**\nThe option that says check the CPU% and MEM% metrics which are readily available in the RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance is incorrect because the CPU% and MEM% metrics are not readily available in the Amazon RDS console. These detailed per-process metrics are only available through Enhanced Monitoring, which must be explicitly enabled.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 4,
            text: "A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in an on-premises data center and uses an Oracle database. Due to a recent cooling problem in its data center, the company urgently needs to migrate its infrastructure to AWS to improve the performance of its applications. As the Solutions Architect, the responsibility is to ensure that the database is properly migrated and remains available in case of database server failure in the future, following AWS Prescriptive Guidance for database migration and high availability.Which combination of actions would meet the requirement? (Select TWO.)",
            options: [
                { id: 0, text: "Launch an Oracle database instance in Amazon RDS with Recovery Manager (RMAN) enabled.", correct: false },
                { id: 1, text: "Convert the database schema using the AWS Schema Conversion Tool.", correct: false },
                { id: 2, text: "Create an Oracle database in Amazon RDS with Multi-AZ deployments.", correct: true },
                { id: 3, text: "Migrate the Oracle database to a non-cluster Amazon Aurora with a single instance.", correct: false },
                { id: 4, text: "Migrate the Oracle database to AWS using the AWS Database Migration Service", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "**Why option 2 is correct:**\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora) so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention. In this scenario, the best RDS configuration to use is an Oracle database in RDS with Multi-AZ deployments to ensure high availability even if the primary database instance goes down.\n\n**Why option 4 is correct:**\nYou can use AWS DMS to move the on-premises database to AWS with minimal downtime and zero data loss. It supports over 20 engines, including Oracle to Aurora MySQL, MySQL to RDS for MySQL, SQL Server to Aurora PostgreSQL, MongoDB to DocumentDB, Oracle to Redshift, and S3. AWS DMS can perform continuous data replication, allowing you to migrate your Oracle database from on-premises to AWS while keeping the source database operational, minimizing downtime.\n\n**Why option 0 is incorrect:**\nThe option that says launch an Oracle database instance in Amazon RDS with Recovery Manager (RMAN) enabled is incorrect because Oracle RMAN (Recovery Manager) is not supported in RDS. RMAN is Oracle's native backup and recovery tool, but RDS manages backups automatically and does not allow direct RMAN access. RDS provides its own automated backup and snapshot capabilities instead.\n\n**Why option 1 is incorrect:**\nThe option that says convert the database schema using the AWS Schema Conversion Tool is incorrect because the AWS Schema Conversion Tool (SCT) is used to convert database schemas from one database engine to another (e.g., Oracle to PostgreSQL). However, the scenario requires migrating an Oracle database to AWS while maintaining Oracle as the database engine, not converting it to a different engine. SCT would be unnecessary in this case.\n\n**Why option 3 is incorrect:**\nThe option that says migrate the Oracle database to a non-cluster Amazon Aurora with a single instance is incorrect because Aurora does not support Oracle database engine. Aurora supports MySQL and PostgreSQL engines only. Since the scenario requires migrating an Oracle database, Aurora is not a viable option. Additionally, a single-instance Aurora deployment would not provide high availability, which is a requirement in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store several types of files. Based on user trends, files that are older than 2 years must be stored in a different storage class. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement? (Select TWO.)",
            options: [
                { id: 0, text: "Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Glacier after 2 years.", correct: true },
                { id: 1, text: "Use Amazon EFS and create a lifecycle policy that will move the objects to EFS-IA after 2 years.", correct: false },
                { id: 2, text: "Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Standard-IA after 2 years.", correct: true },
                { id: 3, text: "Use Amazon EBS volumes to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.", correct: false },
                { id: 4, text: "Use a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nAmazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file. To store a file in Amazon S3, you upload it to a bucket. When you upload a file as an object, you can set permissions on the object and any metadata. Buckets are containers for objects. You can have one or more buckets. You can control access for each bucket, deciding who can create, delete, and list objects in it. You can also choose the geographical region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects. To move a file to a different storage class, you can use Amazon S3 or Amazon EFS. Both services have lifecycle configurations. Take note that Amazon EFS can only transition a file to the IA storage class after 90 days. Since you need to move the files that are older than 2 years to a more cost-effective and scalable solution, you should use the Amazon S3 lifecycle configuration. With S3 lifecycle rules, you can transition files to S3 Standard IA or S3 Glacier. Using S3 Glacier expedited retrieval, you can quickly access your files within 1-5 minutes. S3 Glacier provides the most cost-effective storage for archival data that is rarely accessed, making it ideal for files older than 2 years.\n\n**Why option 2 is correct:**\nAmazon S3 Standard-IA (Infrequent Access) is designed for data that is accessed less frequently but requires rapid access when needed. It offers the same durability, availability, and scalability as S3 Standard, but at a lower storage price and higher retrieval cost. For files older than 2 years that still need to be accessible but are accessed infrequently, S3 Standard-IA provides a cost-effective solution while maintaining high availability and durability.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon EFS and create a lifecycle policy that will move the objects to EFS-IA after 2 years is incorrect because EFS (Elastic File System) is a file storage service designed for use with EC2 instances, not for general object storage. EFS-IA (Infrequent Access) can only transition files after 90 days, not 2 years. Additionally, EFS is not as scalable or cost-effective as S3 for storing large numbers of files uploaded by users globally. S3 is better suited for this use case.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon EBS volumes to store the files and configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years is incorrect because EBS volumes are block storage attached to EC2 instances, not suitable for storing files uploaded by users globally. EBS volumes have size limitations, are not accessible from multiple instances simultaneously, and DLM snapshots are for backup purposes, not for transitioning data to different storage classes. This approach does not provide the scalability or cost-effectiveness required.\n\n**Why option 4 is incorrect:**\nThe option that says use a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files and configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years is incorrect because RAID 0 is a storage configuration that provides no redundancy and increases the risk of data loss if any volume fails. Additionally, EBS volumes are not suitable for storing user-uploaded files globally, and DLM snapshots are for backup, not for transitioning to different storage classes. This approach lacks scalability, cost-effectiveness, and proper lifecycle management.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "An e-commerce company utilizes a regional Amazon API Gateway to host its public REST APIs. The API Gateway endpoint is accessed through a custom domain name set up with an Amazon Route 53 alias record. To support continuous improvement, the company intends to launch a new version of its APIs with enhanced features and performance optimizations.How can the company reduce customer disruption and ensure MINIMAL data loss during the update process in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Create a new API Gateway with the updated version of the APIs in OpenAPI JSON or YAML file format, but keep the same custom domain name for the new API Gateway.", correct: false },
                { id: 1, text: "Implement a canary release deployment strategy for the API Gateway. Deploy the latest version of the APIs to a canary stage and direct a portion of the user traffic to this stage. Verify the new APIs. Gradually increase the traffic percentage, monitor for any issues, and, if successful, promote the canary stage to production.", correct: true },
                { id: 2, text: "Modify the existing API Gateway with the updated version of the APIs, but keep the same custom domain name for the new API Gateway by using the import-to-update operation in either overwrite or merge mode.", correct: false },
                { id: 3, text: "Implement a blue-green deployment strategy for the API Gateway, deploying the latest version of the APIs to the green environment. Route some user traffic to it, validate the new APIs, and once thoroughly validated, promote the green environment to production.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It is a front door for your APIs, enabling you to design and implement scalable, highly available, and secure APIs. With Amazon API Gateway, you can create RESTful APIs that any HTTP client, such as web browsers and mobile devices, can consume. Implementing a canary release deployment strategy for the API Gateway is a great way to ensure your APIs remain stable and reliable. This strategy involves releasing a new version of your API to a small subset of users, allowing you to test the latest version in a controlled environment. If the new version performs well, you can gradually roll out the update to the rest of your users. This approach lets you catch any issues before they affect your entire user base, minimizing the impact on your customers. By using Amazon API Gateway, you can quickly implement a canary release deployment strategy, ensuring that your APIs are always up-to-date and performing at their best. Canary deployments allow you to gradually shift traffic from the old version to the new version, monitor for issues, and roll back if necessary, all while maintaining service availability and minimizing data loss.\n\n**Why option 0 is incorrect:**\nThe option that says create a new API Gateway with the updated version of the APIs in OpenAPI JSON or YAML file format, but keep the same custom domain name for the new API Gateway is incorrect because upgrading to a new API Gateway using an updated version of the APIs in OpenAPI JSON or YAML file format while keeping the same custom domain name can typically result in downtime and confusion during the switch. This is because of DNS propagation delays, which can negatively affect users and even lead to data loss. Additionally, creating a completely new API Gateway requires managing two separate gateways, which increases operational overhead and costs.\n\n**Why option 2 is incorrect:**\nThe option that says modify the existing API Gateway with the updated version of the APIs, but keep the same custom domain name for the new API Gateway by using the import-to-update operation in either overwrite or merge mode is incorrect because using the import-to-update operation in either overwrite or merge mode may not provide enough isolation and control testing for the new version of the APIs. If something goes wrong during the update process, it could just lead to data loss on the existing API Gateway, potentially affecting all customers simultaneously. This approach does not allow for gradual rollout or easy rollback.\n\n**Why option 3 is incorrect:**\nThe option that says implement a blue-green deployment strategy for the API Gateway, deploying the latest version of the APIs to the green environment, route some user traffic to it, validate the new APIs, and once thoroughly validated, promote the green environment to production is incorrect because in a blue-green deployment, the blue (existing) and green (updated) environments must be provisioned and maintained. This adds complexity and cost to the update process, which breaks the cost requirement that's explicitly mentioned in the scenario. Additionally, directing some user traffic to the green environment may only lead to issues for those users, especially if there are undiscovered bugs or performance problems in the updated APIs. Canary deployments are more cost-effective as they use a single API Gateway with traffic splitting.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "A company plans to migrate its on-premises workload to AWS. The current architecture is composed of a Microsoft SharePoint server that uses a Windows shared file storage. The Solutions Architect needs to use a cloud storage solution that is highly available and can be integrated with Active Directory for access control and authentication.Which of the following options can satisfy the given requirement?",
            options: [
                { id: 0, text: "Launch an Amazon EC2 Windows Server to mount a new Amazon S3 bucket as a file volume.", correct: false },
                { id: 1, text: "Create a file system using Amazon EFS and join it to an Active Directory domain.", correct: false },
                { id: 2, text: "Create a Network File System (NFS) file share using AWS Storage Gateway.", correct: false },
                { id: 3, text: "Create a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. Amazon FSx is accessible from Windows, Linux, and MacOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently. Amazon FSx works with Microsoft Active Directory to integrate with your existing Microsoft Windows environments. You have two options to provide user authentication and access control for your file system: AWS Managed Microsoft Active Directory and Self-managed Microsoft Active Directory. Take note that after you create an Active Directory configuration for a file system, you can't change that configuration. However, you can create a new file system from a backup and change the Active Directory integration configuration for that file system. These configurations allow the users in your domain to use their existing identity to access the Amazon FSx file system and to control access to individual files and folders. Hence, the correct answer is: Create a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS.\n\n**Why option 0 is incorrect:**\nThe option that says launch an Amazon EC2 Windows Server to mount a new Amazon S3 bucket as a file volume is incorrect because you can't integrate Amazon S3 with your existing Active Directory to provide authentication and access control. S3 is an object storage service, not a file system, and while you can mount S3 buckets using third-party tools, it does not natively support Active Directory integration or SMB protocol required for Windows shared file storage.\n\n**Why option 1 is incorrect:**\nThe option that says create a file system using Amazon EFS and join it to an Active Directory domain is incorrect because Amazon EFS does not support Windows systems, only Linux OS. EFS uses the NFS (Network File System) protocol, which is designed for Linux/Unix systems. You should use Amazon FSx for Windows File Server instead to satisfy the requirement in the scenario, which requires Windows shared file storage with Active Directory integration.\n\n**Why option 2 is incorrect:**\nThe option that says create a Network File System (NFS) file share using AWS Storage Gateway is incorrect because NFS file share is primarily used for Linux systems. Remember that the requirement in the scenario is to use a Windows shared file storage. Therefore, you must use an SMB file share instead, which supports Windows OS and Active Directory configuration. Alternatively, you can also use the Amazon FSx for Windows File Server file system, which is the fully managed solution designed specifically for Windows file storage.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A company has a cloud architecture composed of Linux and Windows Amazon EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, the Solutions Architect must create a solution that enables monitoring of memory and disk utilization metrics for all instances.Which of the following is the most suitable monitoring solution to implement?",
            options: [
                { id: 0, text: "Use the default Amazon CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all the EC2 instances.", correct: false },
                { id: 1, text: "Install the Amazon CloudWatch agent to all the EC2 instances that gather the memory and disk utilization data. View the custom metrics in the CloudWatch console.", correct: true },
                { id: 2, text: "Enable the Enhanced Monitoring option in EC2 and install Amazon CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.", correct: false },
                { id: 3, text: "Use Amazon Inspector and install the Inspector agent to all EC2 instances.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon CloudWatch has available Amazon EC2 Metrics for you to use for monitoring CPU utilization, Network utilization, Disk performance, and Disk Reads/Writes. In case you need to monitor the below items, you need to prepare a custom metric using a Perl or other shell script, as there are no ready-to-use metrics for the following: Memory utilization Disk swap utilization Disk space utilization Page file utilization Log collection Take note that there is a multi-platform CloudWatch agent which can be installed on both Linux and Windows-based instances. You can use a single agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. This agent supports both Windows Server and Linux and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. It is recommended that you use the new agent instead of the older monitoring scripts to collect metrics and logs. Hence, the correct answer is: Install the Amazon CloudWatch agent to all the EC2 instances that gather the memory and disk utilization data. View the custom metrics in the CloudWatch console.\n\n**Why option 0 is incorrect:**\nThe option that says use the default Amazon CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available, and install the AWS Systems Manager (SSM) Agent to all the EC2 instances is incorrect because, by default, CloudWatch does not automatically provide memory and disk utilization metrics of your instances. You have to simply set up custom CloudWatch metrics to monitor the memory, disk swap, disk space, and page file utilization of your instances. The SSM Agent is used for Systems Manager operations, not for collecting CloudWatch metrics.\n\n**Why option 2 is incorrect:**\nThe option that says enable the Enhanced Monitoring option in EC2 and install Amazon CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard is incorrect because Enhanced Monitoring is only a feature of Amazon RDS, not EC2. By default, Enhanced Monitoring metrics are only stored for 30 days in the CloudWatch Logs. EC2 instances do not have an \"Enhanced Monitoring\" option.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Inspector and install the Inspector agent to all EC2 instances is incorrect because Amazon Inspector is primarily an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. It does not provide a custom metric to track the memory and disk utilization of each and every EC2 instance in your VPC.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "An organization requires a persistent block storage volume to support its mission-critical workloads. The backup data will be stored in an object storage service and, after 30 days, transitioned to an archival storage service for long-term retention.What should be done to meet the above requirement?",
            options: [
                { id: 0, text: "Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.", correct: true },
                { id: 1, text: "Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA.", correct: false },
                { id: 2, text: "Attach an instance store volume in your existing Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.", correct: false },
                { id: 3, text: "Attach an instance store volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon Elastic Block Store (EBS) is an easy-to-use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. A broad range of workloads, such as relational and non-relational databases, enterprise applications, containerized applications, big data analytics engines, file systems, and media workflows are widely deployed on Amazon EBS. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. In an S3 Lifecycle configuration, you can define rules to transition objects from one storage class to another to save on storage costs. Amazon S3 supports a waterfall model for transitioning between storage classes. In this scenario, three services are required to implement this solution. The mission-critical workloads mean that you need to have a persistent block storage volume and the designed service for this is Amazon EBS volumes. The second workload needs to have an object storage service, such as Amazon S3, to store your backup data. Amazon S3 enables you to configure the lifecycle policy from S3 Standard to different storage classes. For the last one, it needs archive storage such as Amazon S3 Glacier Flexible Retrieval. Hence, the correct answer is: Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.\n\n**Why option 1 is incorrect:**\nThe option that says attach an Amazon EBS volume in your Amazon EC2 instance, use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA is incorrect because this lifecycle policy will transition your objects into an infrequently accessed storage class and not a storage class for data archiving. S3 One Zone-IA is designed for infrequently accessed data that requires rapid access, but it is not suitable for long-term archival retention. S3 Glacier Flexible Retrieval is specifically designed for archival storage with lower costs.\n\n**Why option 2 is incorrect:**\nThe option that says attach an instance store volume in your existing Amazon EC2 instance, use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval is incorrect because an Instance Store volume is simply a temporary block-level storage for EC2 instances. Also, you can't attach instance store volumes to an instance after you've launched it. You can specify the instance store volumes for your instance only when you launch it. Instance store volumes are ephemeral and data is lost when the instance stops or terminates, making them unsuitable for mission-critical workloads.\n\n**Why option 3 is incorrect:**\nThe option that says attach an instance store volume in your Amazon EC2 instance, use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA is incorrect. Just like the previous option, the use of instance store volume is not suitable for mission-critical workloads because the data can be lost if the underlying disk drive fails, the instance stops, or if the instance is terminated. In addition, Amazon S3 Glacier Flexible Retrieval is a more suitable option for data archival instead of Amazon S3 One Zone-IA.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A suite of web applications is hosted in an Auto Scaling group of Amazon EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application.Which EC2 instance will be the first one to be terminated by the Auto Scaling group?",
            options: [
                { id: 0, text: "The EC2 instance which has the least number of user sessions", correct: false },
                { id: 1, text: "The EC2 instance which has been running for the longest time", correct: false },
                { id: 2, text: "The EC2 instance launched from the oldest launch template.", correct: true },
                { id: 3, text: "The instance will be randomly selected by the Auto Scaling group", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows: 1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch template. 2. Determine which unprotected instances in the selected Availability Zone use the oldest launch template. If there is one such instance, terminate it. 3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it. 4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random. The following flow diagram illustrates how the default termination policy works: Hence, the correct answer is: The EC2 instance launched from the oldest launch template.\n\n**Why option 0 is incorrect:**\nThe option that says the EC2 instance which has the least number of user sessions is incorrect because the number of user sessions is not typically a factor considered by Amazon EC2 Auto Scaling groups when deciding which instances to terminate during a scale-in event. Auto Scaling groups do not have visibility into application-level metrics like user sessions unless custom termination policies are configured.\n\n**Why option 1 is incorrect:**\nThe option that says the EC2 instance which has been running for the longest time is incorrect because the duration for which an EC2 instance has been running is not primarily a factor considered by Amazon EC2 Auto Scaling groups when deciding which instances to terminate during a scale-in event. The default termination policy prioritizes launch template age and billing hour proximity, not instance runtime duration.\n\n**Why option 3 is incorrect:**\nThe option that says the instance will be randomly selected by the Auto Scaling group is incorrect because Amazon EC2 Auto Scaling groups do not randomly select instances for termination during a scale-in event. The default termination policy follows a specific set of rules to ensure Availability Zone balance, prioritize instances with the oldest launch template, and maximize billing hour usage before resorting to random selection only as a last step when multiple instances meet all other criteria.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A logistics company plans to automate its order management application. The company wants to use SFTP file transfer for uploading business-critical documents. Since the files are confidential, encryption at rest is required, and high availability must be ensured. Additionally, each file must be automatically deleted one month after creation.Which of the following options should be implemented to meet the company’s requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket with encryption enabled. Configure AWS Transfer for SFTP to securely upload files to the S3 bucket. Configure the retention policy on the SFTP server to delete files after a month.", correct: false },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) and enable encryption. Configure AWS Transfer for SFTP to securely upload files to the EFS file system. Apply an EFS lifecycle policy to delete files after 30 days.", correct: false },
                { id: 2, text: "Provision an Amazon EC2 instance and install the SFTP service. Mount an encrypted Amazon EFS file system on the EC2 instance to store the uploaded files. Add a cron job to delete the files older than a month.", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with encryption enabled. Launch an AWS Transfer for SFTP endpoint to securely upload files to the S3 bucket. Configure an S3 lifecycle rule to delete files after a month.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Transfer for SFTP enables you to easily move your file transfer workloads that use the Secure Shell File Transfer Protocol (SFTP) to AWS without needing to modify your applications or manage any SFTP servers. To get started with AWS Transfer for SFTP (AWS SFTP) you create an SFTP server and map your domain to the server endpoint, select authentication for your SFTP clients using service-managed identities, or integrate your own identity provider, and select your Amazon S3 buckets to store the transferred data. Your existing users can continue to operate with their existing SFTP clients or applications. Data uploaded or downloaded using SFTP is available in your Amazon S3 bucket, and can be used for archiving or processing in AWS. An Amazon S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them. Expiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf. Therefore, the correct answer is: Create an Amazon S3 bucket with encryption enabled. Launch an AWS Transfer for SFTP endpoint to securely upload files to the S3 bucket. Configure an S3 lifecycle rule to delete files after a month. You can use S3 as the storage service for your AWS Transfer SFTP-enabled server.\n\n**Why option 0 is incorrect:**\nThe option that says create an Amazon S3 bucket with encryption enabled. Configure AWS Transfer for SFTP to securely upload files to the S3 bucket. Configure the retention policy on the SFTP server to delete files after a month is incorrect. The 30-day retention policy must be primarily configured on the Amazon S3 bucket. There is no retention policy option on AWS Transfer for SFTP.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon Elastic File System (Amazon EFS) and enable encryption. Configure AWS Transfer for SFTP to securely upload files to the EFS file system. Apply an EFS lifecycle policy to delete files after 30 days is incorrect. This may be possible, however, the EFS lifecycle management doesn't delete objects. It can only transition files in and out of the Infrequent Access tier.\n\n**Why option 2 is incorrect:**\nThe option that says provision an Amazon EC2 instance and install the SFTP service. Mount an encrypted Amazon EFS file system on the EC2 instance to store the uploaded files. Add a cron job to delete the files older than a month is incorrect. This option is possible, however, it entails greater operational overhead since you need to manage the EC2 instance and SFTP service.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "An online shopping platform is hosted on an Auto Scaling group of Amazon EC2 Spot instances and utilizes Amazon Aurora PostgreSQL as its database. It is required to optimize database workloads in the cluster by directing the production traffic to high-capacity instances and routing the reporting queries from the internal staff to the low-capacity instances.Which is the most suitable configuration for the application as well as the Aurora database cluster to achieve this requirement?",
            options: [
                { id: 0, text: "Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.", correct: false },
                { id: 1, text: "In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries.", correct: false },
                { id: 2, text: "Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.", correct: true },
                { id: 3, text: "Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available. For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic. Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance. The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances. Hence, the correct answer is: Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.\n\n**Why option 0 is incorrect:**\nThe option that says configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas is incorrect. Although it is true that a reader endpoint enables your Aurora database to automatically perform load-balancing among all the Aurora Replicas, it is quite limited to doing read operations only. You still need to use a custom endpoint to load-balance the database connections based on the specified criteria.\n\n**Why option 1 is incorrect:**\nThe option that says in your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries is incorrect because a cluster endpoint (also known as a writer endpoint) for an Aurora DB cluster simply connects to the current primary DB instance for that DB cluster. This endpoint can perform write operations in the database such as DDL statements, which is perfect for handling production traffic but not suitable for handling queries for reporting since there will be no write database operations that will be sent. Moreover, the endpoint does not point to lower-capacity or high-capacity instances as per the requirement. A better solution for this is to use a custom endpoint.\n\n**Why option 3 is incorrect:**\nThe option that says do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances is incorrect because Aurora does not do this by default. You have to create custom endpoints in order to accomplish this requirement.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company is experiencing repeated outages in the Availability Zone where its Amazon RDS database instance is deployed, resulting in a complete loss of access to the database during each incident.Which solution should be implemented to prevent losing database access if this occurs again?",
            options: [
                { id: 0, text: "Make a snapshot of the database", correct: false },
                { id: 1, text: "Enable Multi-AZ failover", correct: true },
                { id: 2, text: "Increase the database instance size", correct: false },
                { id: 3, text: "Create a read replica", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon RDS Multi-AZ deployments are designed to enhance the availability and durability of database instances, making them well-suited for production workloads. When you enable Multi-AZ failover, Amazon RDS automatically creates a standby replica in a different Availability Zone (AZ) and synchronously replicates data from the primary instance. This ensures high data consistency and protection against infrastructure-related disruptions. Each AZ operates on physically distinct infrastructure, which adds fault isolation and resilience. In the event of planned maintenance or an unexpected failure such as an Availability Zone outage or hardware issue, Amazon RDS automatically performs a failover to the standby instance. This process is fully managed by Amazon RDS and does not require manual intervention, helping to minimize downtime and maintain business continuity. For Amazon Aurora, the failover involves promoting a replica to become the new writer instance. Hence, the correct answer is: Enable Multi-AZ failover.\n\n**Why option 0 is incorrect:**\nThe option that says make a snapshot of the database is incorrect because snapshots are typically used for backup and disaster recovery, not for maintaining high availability. A snapshot allows you to restore a database to a specific point in time, but it does not prevent downtime or provide continuous access during an Availability Zone outage.\n\n**Why option 2 is incorrect:**\nThe option that says increase the database instance size is incorrect because this action just improves the instance's compute and memory capacity, which may help performance but does not address Availability Zone-level failures. The database would still be a single point of failure within the same zone, leaving it vulnerable to the same type of outage described in the question.\n\n**Why option 3 is incorrect:**\nThe option that says create a read replica is incorrect because read replicas are primarily intended to handle read-heavy workloads and do not automatically take over in the event of a failure. Promoting a read replica to a standalone instance requires manual intervention, which simply does not meet the goal of preventing loss of access during unplanned outages.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company has recently migrated its microservices-based application to Amazon Elastic Kubernetes Service (Amazon EKS). As part of the migration, the company must ensure that all sensitive configuration data and credentials, such as database passwords and API keys, are stored securely and encrypted within the Amazon EKS cluster's etcd key-value store.What is the most suitable solution to meet the company's requirements?",
            options: [
                { id: 0, text: "Enable secret encryption with a new AWS KMS key on an existing Amazon EKS cluster to encrypt sensitive data stored in the EKS cluster's etcd key-value store.", correct: true },
                { id: 1, text: "Use AWS Secrets Manager with a new AWS KMS key to securely manage and store sensitive data within the EKS cluster's etcd key-value store.", correct: false },
                { id: 2, text: "Enable default Amazon EBS volume encryption for the account with a new AWS KMS key to ensure encryption of sensitive data within the Amazon EKS cluster.", correct: false },
                { id: 3, text: "Use Amazon EKS default options and the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on to securely store sensitive data within the Amazon EKS cluster.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nEnabling secret encryption with a new AWS Key Management Service (KMS) key in an existing Amazon Elastic Kubernetes Service (EKS) cluster is critical to securing sensitive data stored in the cluster's etcd key-value store. Amazon EKS is a service for running and managing containerized applications, storing configuration data and secrets, etc., and is a distributed data store. By default, these secrets are not encrypted, posing potential security risks. Integrating AWS KMS with Amazon EKS allows for the encryption of these secrets, leveraging AWS KMS's capabilities to manage cryptographic keys and control their use across AWS services and applications. The process involves creating an AWS KMS key specifically for the EKS cluster and configuring the cluster to encrypt secrets before they are saved in etcd. This setup ensures that all sensitive information within the etcd database is encrypted at rest, enhancing data security. By adopting this approach, organizations can significantly improve their security posture, ensuring that sensitive data and credentials are protected according to industry standards and compliance requirements, thus maintaining data confidentiality and integrity within their Kubernetes environments. Hence, the correct answer is: Enable secret encryption with a new AWS KMS key on an existing Amazon EKS cluster to encrypt sensitive data stored in the EKS cluster's etcd key-value store.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS Secrets Manager with a new AWS KMS key to securely manage and store sensitive data within the EKS cluster's etcd key-value store is incorrect. AWS Secrets Manager is a powerful tool for managing secrets but it doesn't directly address encrypting data within the etcd key-value store of an EKS cluster. Secrets Manager is more about managing and retrieving secrets rather than encrypting data within etcd.\n\n**Why option 2 is incorrect:**\nThe option that says enable default Amazon EBS volume encryption for the account with a new AWS KMS key to ensure encryption of sensitive data within the Amazon EKS cluster is incorrect. Enabling default Amazon EBS volume encryption is a way to ensure that data at rest in EBS volumes is encrypted. However, the EBS volumes are primarily used for persistent storage of the worker nodes. They are not directly related to the storage of sensitive configuration data and credentials within the EKS cluster's etcd key-value store.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon EKS default options and the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on to securely store sensitive data within the Amazon EKS cluster is incorrect. Amazon EBS CSI driver enables Amazon Elastic Block Store (EBS) volumes as persistent storage for Kubernetes applications running on the Amazon EKS. While this can provide secure persistent storage for your microservices, it does not address the specific requirement of securely storing sensitive data within the EKS cluster's etcd key-value store.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A telecommunications company is planning to grant AWS Console access to its developers. Company policy requires the use of identity federation and role-based access control. Currently, the roles are already assigned via groups in the corporate Active Directory.In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)",
            options: [
                { id: 0, text: "AWS Directory Service AD Connector", correct: true },
                { id: 1, text: "AWS Directory Service Simple AD", correct: false },
                { id: 2, text: "IAM Groups", correct: false },
                { id: 3, text: "IAM Roles", correct: true },
                { id: 4, text: "AWS Lambda", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "**Why option 0 is correct:**\nAWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)–aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access. Considering that the company is using a corporate Active Directory, it is best to use AWS Directory Service AD Connector for easier integration. The AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory, enabling identity federation.\n\n**Why option 3 is correct:**\nSince the roles are already assigned using groups in the corporate Active Directory, it would be better to also use IAM Roles. Take note that you can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector. This enables role-based access control as required by the company policy.\n\n**Why option 1 is incorrect:**\nThe option that says AWS Directory Service Simple AD is incorrect because this only provides a subset of the features offered by AWS Managed Microsoft AD, including the ability to manage user accounts and group memberships, create and apply group policies, securely connect to Amazon EC2 instances, and provide Kerberos-based single sign-on (SSO). In this scenario, the more suitable component to use is the AD Connector since it is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory.\n\n**Why option 2 is incorrect:**\nThe option that says IAM Groups is incorrect because this is just a collection of IAM users. Groups let you specify permissions for multiple users, which can make it easier to manage the permissions for those users. In this scenario, the more suitable one to use is IAM Roles in order for permissions to create AWS Directory Service resources and enable role-based access control.\n\n**Why option 4 is incorrect:**\nThe option that says AWS Lambda is incorrect because this is primarily used for serverless computing and is not related to identity federation or role-based access control for AWS Console access.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "An application consists of multiple Amazon EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective?",
            options: [
                { id: 0, text: "Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone", correct: true },
                { id: 1, text: "Create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.", correct: false },
                { id: 2, text: "Create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.", correct: false },
                { id: 3, text: "Create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA NAT Gateway is a highly available, managed Network Address Translation (NAT) service for your resources in a private subnet to access the Internet. NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. You must create a NAT gateway on a public subnet to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances. If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway's Availability Zone is down, resources in the other Availability Zones lose Internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. Hence, the correct answer is: Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone.\n\n**Why option 1 is incorrect:**\nThe option that says create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone is incorrect because you should configure the route table in the private subnet and not the public subnet to associate the right instances in the private subnet.\n\n**Why option 2 is incorrect:**\nThe option that says create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone is incorrect because you are primarily required to set up the NAT Gateway in the private subnet to allow outbound internet access for private instances. Additionally, creating two NAT Gateways per AZ is unnecessary and not cost-effective.\n\n**Why option 3 is incorrect:**\nThe option that says create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone is incorrect because the only necessity here is ensuring outbound traffic for private instances, and adding multiple NAT Gateways does not align with cost optimization unless explicitly required for high availability.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A Solutions Architect is designing a highly available relational database solution to mitigate the risk of a multi-region failure. The database must meet a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute. The architect needs a disaster recovery plan that enables automatic cross-region replication with minimal data loss and rapid recovery in the event of a failure.Which AWS feature best fulfills this requirement?",
            options: [
                { id: 0, text: "Amazon DynamoDB Global table", correct: false },
                { id: 1, text: "Amazon RDS for PostgreSQL with cross-region read replicas", correct: false },
                { id: 2, text: "Amazon Timestream for Analytics", correct: false },
                { id: 3, text: "Amazon Aurora Global Database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Aurora Global Database supports storage-based replication that has a latency of less than 1 second. If there is an unplanned outage, one of the secondary regions you assigned can be promoted to read and write capabilities in less than 1 minute. This feature is called Cross-Region Disaster Recovery. An RPO of 1 second and an RTO of less than 1 minute provide you a strong foundation for a global business continuity plan. Hence, the correct answer is: Amazon Aurora Global Database.\n\n**Why option 0 is incorrect:**\nThe option that says Amazon DynamoDB Global table is incorrect because while this supports multi-region, fully replicated tables with low latency, it's more suitable for NoSQL workloads, not relational databases.\n\n**Why option 1 is incorrect:**\nThe option that says Amazon RDS for PostgreSQL with cross-region read replicas is incorrect. While this option can help with disaster recovery, it simply doesn't meet the specified RPO and RTO requirements in the scenario. Replication lag in cross-region read replicas can take several minutes to complete, which could prevent the company from meeting the RPO of 1 second.\n\n**Why option 2 is incorrect:**\nThe option that says Amazon Timestream for Analytics is incorrect because it is primarily a serverless time series database service that is commonly used for IoT and operational applications. The most suitable solution for this scenario is to use the Amazon Aurora Global Database since it can provide the required RPO and RTO.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A company needs to deploy at least two Amazon EC2 instances to support the normal workloads of its application and automatically scale up to six EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.As a Solutions Architect, what should be done to meet this requirement?",
            options: [
                { id: 0, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A.", correct: false },
                { id: 1, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.", correct: true },
                { id: 2, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ.", correct: false },
                { id: 3, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon EC2 Auto Scaling helps ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can also specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size. To achieve highly available and fault-tolerant architecture for your applications, you must deploy all your instances in different Availability Zones. This will help you isolate your resources if an outage occurs. Take note that to achieve fault tolerance, you need to have redundant resources in place to avoid any system degradation in the event of a server fault or an Availability Zone outage. Having a fault-tolerant architecture entails an extra cost in running additional resources than what is usually needed. This is to ensure that the mission-critical workloads are processed. Since the scenario requires at least 2 instances to handle regular traffic, you should have 2 instances running all the time even if an AZ outage occurred. You can use an Auto Scaling Group to automatically scale your compute resources across two or more Availability Zones. You have to specify the minimum capacity to 4 instances and the maximum capacity to 6 instances. If each AZ has 2 instances running, even if an AZ fails, your system will still run a minimum of 2 instances. Hence, the correct answer is: Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.\n\n**Why option 0 is incorrect:**\nThe option that says create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A is incorrect because the instances are only deployed in a single Availability Zone. It cannot protect your applications and data from datacenter or AZ failures.\n\n**Why option 2 is incorrect:**\nThe option that says create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ is incorrect. It is required to have 2 instances running all the time. If an AZ outage happened, ASG will launch a new instance on the unaffected AZ. This provisioning does not happen instantly, which means that for a certain period of time, there will only be 1 running instance left.\n\n**Why option 3 is incorrect:**\nThe option that says create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B is incorrect. Although this fulfills the requirement of at least 2 EC2 instances and high availability, the maximum capacity setting is wrong. It should be set to 6 to properly handle the peak load. If an AZ outage occurs and the system is at its peak load, the number of running instances in this setup will only be 4 instead of 6 and this will affect the performance of your application.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A company has a hybrid cloud architecture that connects its on-premises data center and cloud infrastructure in AWS. It requires a durable storage backup for its corporate documents stored on-premises and a local cache that provides low-latency access to recently accessed data to reduce data egress charges. The documents must be stored on and retrieved from AWS via the Server Message Block (SMB) protocol. These files must be immediately accessible within minutes for six months and archived for another decade to meet data compliance.Which of the following is the best and most cost-effective approach to implement in this scenario?",
            options: [
                { id: 0, text: "Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.", correct: true },
                { id: 1, text: "Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.", correct: false },
                { id: 2, text: "Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an Amazon S3 bucket, and then later to Glacier for archival.", correct: false },
                { id: 3, text: "Use AWS DataSync to transfer all files from the on-premises network directly to an Amazon S3 bucket, and set up a lifecycle policy to move the data into Glacier for archival.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor. The gateway provides access to objects in S3 as files or file share mount points. With a file gateway, you can do the following: - You can store and retrieve files directly using the NFS version 3 or 4.1 protocol. - You can store and retrieve files directly using the SMB file system version, 2 and 3 protocol. - You can access your data directly in Amazon S3 from any AWS Cloud application or service. - You can manage your Amazon S3 data using lifecycle policies, cross-region replication, and versioning. You can think of a file gateway as a file system mount on S3. AWS Storage Gateway supports the Amazon S3 Standard, Amazon S3 Standard-Infrequent Access, Amazon S3 One Zone-Infrequent Access and Amazon Glacier storage classes. When you create or update a file share, you have the option to select a storage class for your objects. You can either choose the Amazon S3 Standard or any of the infrequent access storage classes such as S3 Standard IA or S3 One Zone IA. Objects stored in any of these storage classes can be transitioned to Amazon Glacier using a Lifecycle Policy. Although you can write objects directly from a file share to the S3-Standard-IA or S3-One Zone-IA storage class, it is recommended that you use a Lifecycle Policy to transition your objects rather than write directly from the file share, especially if you're expecting to update or delete the object within 30 days of archiving it. The file gateway also provides a local cache that provides low-latency access to recently accessed data to reduce data egress charges. Therefore, the correct answer is: Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.\n\n**Why option 1 is incorrect:**\nThe option that says launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival is incorrect because although tape gateways provide cost-effective and durable archive backup data in Amazon Glacier, it does not meet the criteria of being retrievable immediately within minutes. It also doesn't maintain a local cache that provides low latency access to the recently accessed data and reduce data egress charges. Thus, it is still better to set up a file gateway instead.\n\n**Why option 2 is incorrect:**\nThe option that says establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an Amazon S3 bucket, and then later to Glacier for archival is incorrect because EBS Volumes are not only less durable compared with S3 and it would be more cost-efficient if you directly store the documents to an S3 bucket. An alternative solution is to use AWS Direct Connect with AWS Storage Gateway to create a connection for high-throughput workload needs, providing a dedicated network connection between your on-premises file gateway and AWS. But this solution is using EBS, hence, this option is still wrong.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS DataSync to transfer all files from the on-premises network directly to an Amazon S3 bucket, and set up a lifecycle policy to move the data into Glacier for archival is incorrect because DataSync is primarily a data migration service that facilitates bulk transfers, but it does not offer local caching for low-latency access to recently accessed files. Without caching, the company may incur higher data egress charges, making this option less cost-effective.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 1,
            text: "A financial application consists of an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer, and a MySQL RDS instance set up in a Multi-AZ Deployment configuration. To protect customers' confidential data, it must be ensured that the Amazon RDS database is only accessible using an authentication token specific to the profile credentials of EC2 instances.Which of the following actions should be taken to meet this requirement?",
            options: [
                { id: 0, text: "Enable the IAM DB Authentication.", correct: true },
                { id: 1, text: "Configure SSL in your application to encrypt the database connection to RDS.", correct: false },
                { id: 2, text: "Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance.", correct: false },
                { id: 3, text: "Use a combination of IAM and STS to enforce restricted access to your RDS instance using a temporary authentication token.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication. IAM database authentication provides the following benefits: Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL). You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance. For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security Hence, the correct answer is: Enable the IAM DB Authentication .\n\n**Why option 1 is incorrect:**\nan SSL connection is not just using an authentication token from IAM. Although configuring SSL to your application can improve the security of your data in flight, it is still not a suitable option to use in this scenario.\n\n**Why option 2 is incorrect:**\nalthough you can create and assign an IAM Role to your EC2 instances, you still need to configure your RDS to use IAM DB Authentication.\n\n**Why option 3 is incorrect:**\nyou have to use IAM DB Authentication for this scenario, and not simply a combination of an IAM and STS. Although STS is used to send temporary tokens for authentication, this is not a compatible use case for RDS.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company.Which S3 encryption technique should the Architect use?",
            options: [
                { id: 0, text: "Use S3 client-side encryption with an AWS KMS key.", correct: false },
                { id: 1, text: "Use S3 client-side encryption with a client-side master key.", correct: true },
                { id: 2, text: "Use S3 server-side encryption with an AWS KMS key.", correct: false },
                { id: 3, text: "Use S3 server-side encryption with customer provided key.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nClient-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options: - Use an AWS KMS key. - Use a client-side master key. When using an AWS KMS key to enable client-side data encryption, you provide an AWS KMS key identifier (KeyId) to AWS. On the other hand, when you use client-side master key for client-side data encryption, your client-side master keys and your unencrypted data are never sent to AWS. It's important that you safely manage your encryption keys because if you lose them, you can't decrypt your data. This is how client-side encryption using a client-side master key works: When uploading an object - You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. The process works like this: 1. The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally. It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object. 2. The client encrypts the data encryption key using the master key that you provide. The client uploads the encrypted data key and its material description as part of the object metadata. The client uses the material description to determine which client-side master key to use for decryption. 3. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (x-amz-meta-x-amz-key) in Amazon S3. When downloading an object - The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object. Hence, the correct answer is: Use S3 client-side encryption with a client-side master key.\n\n**Why option 0 is incorrect:**\nThe option that says use S3 client-side encryption with an AWS KMS key is incorrect because, in client-side encryption with a KMS key, you provide an AWS KMS key identifier (KeyId) to AWS. The scenario clearly indicates that both the master keys and the unencrypted data should never be sent to AWS.\n\n**Why option 2 is incorrect:**\nThe option that says use S3 server-side encryption with an AWS KMS key is incorrect because the scenario mentioned that the unencrypted data should never be sent to AWS, which means that you have to use client-side encryption in order to encrypt the data first before sending to AWS. In this way, you can only ensure that there is no unencrypted data being uploaded to AWS. In addition, the master key used by Server-Side Encryption with AWS KMS Key (SSE-KMS) is uploaded and managed by AWS, which directly violates the requirement of not uploading the master key.\n\n**Why option 3 is incorrect:**\nThe option that says use S3 server-side encryption with customer provided key is incorrect because, just as mentioned above, you have to use client-side encryption in this scenario instead of server-side encryption. For the S3 server-side encryption with a customer-provided key (SSE-C), you actually provide the encryption key as part of your request to upload the object to S3. Using this key, Amazon S3 manages both the encryption (as it writes to disks) and decryption (when you access your objects).",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A Solutions Architect is hosting a website in an Amazon S3 bucket namedtutorialsdojo. The users load the website using the following URL:http://tutorialsdojo.s3-website-us-east-1.amazonaws.com. A new requirement has been introduced to add JavaScript on the webpages to make authenticated HTTPGETrequests against the same bucket using the S3 API endpoint (tutorialsdojo.s3.amazonaws.com). However, upon testing, the web browser blocks JavaScript from allowing those requests.Which of the following options is the MOST suitable solution to implement for this scenario?",
            options: [
                { id: 0, text: "Enable cross-account access.", correct: false },
                { id: 1, text: "Enable Cross-Zone Load Balancing.", correct: false },
                { id: 2, text: "Enable Cross-origin resource sharing (CORS) configuration in the bucket.", correct: true },
                { id: 3, text: "Enable Cross-Region Replication (CRR).", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. Suppose that you are hosting a website in an Amazon S3 bucket named your-website and your users load the website endpoint http://your-website.s3-website-us-east-1.amazonaws.com. Now you want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket, your-website.s3.amazonaws.com. A browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to explicitly enable cross-origin requests from your-website.s3-website-us-east-1.amazonaws.com. Hence, the correct answer is: Enable Cross-origin resource sharing (CORS) configuration in the bucket.\n\n**Why option 0 is incorrect:**\nThe option that says enable cross-account access is incorrect because cross-account access is just a feature in IAM and not in Amazon S3.\n\n**Why option 1 is incorrect:**\nThe option that says enable Cross-Zone Load Balancing is incorrect because Cross-Zone Load Balancing is only used in ELB and not in S3.\n\n**Why option 3 is incorrect:**\nThe option that says enable Cross-Region Replication (CRR) is incorrect because CRR is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. To secure session data and ensure that Cloud Engineers must authenticate before executing Redis commands, specificallyMULTI EXECcommands, the system should enforce strong authentication by requiring users to enter a password. Additionally, access should be managed with long-lived credentials while supporting robust security practices.Which of the following actions should be taken to meet the above requirement?",
            options: [
                { id: 0, text: "Generate an IAM authentication token using AWS credentials and provide this token as a password.", correct: false },
                { id: 1, text: "Set up a Redis replication group and enable theAtRestEncryptionEnabledparameter.", correct: false },
                { id: 2, text: "Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the--transit-encryption-enabledand--auth-tokenparameters enabled.", correct: true },
                { id: 3, text: "Enable the in-transit encryption for Redis replication groups.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing Redis AUTH command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server. Hence, the correct answer is: Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the --transit-encryption-enabled and --auth-token parameters enabled. To require that users enter a password on a password-protected Redis server, include the parameter --auth-token with the correct password when you create your replication group or cluster and on all subsequent commands to the replication group or cluster.\n\n**Why option 0 is incorrect:**\nThe option that says generate an IAM authentication token using AWS credentials and provide this token as a password is incorrect. IAM authentication is simply not supported for executing Redis commands like MULTI EXEC, and IAM tokens expire every 12 hours, which does not align with the need for long-lived credentials.\n\n**Why option 1 is incorrect:**\nThe option that says set up a Redis replication group and enable the AtRestEncryptionEnabled parameter is incorrect because the Redis At-Rest Encryption feature only secures the data inside the in-memory data store. You have to use Redis AUTH option instead.\n\n**Why option 3 is incorrect:**\nThe option that says enable the in-transit encryption for Redis replication groups is incorrect. Although in-transit encryption is part of the solution, it is missing the most important thing which is the Redis AUTH option.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and handle frequent schema changes. The application should not have any downtime or performance issues whenever there is a schema change in the database. It should also provide a low latency response to high-traffic queries.Which is the most suitable database solution to use to achieve this requirement?",
            options: [
                { id: 0, text: "An Amazon RDS instance in Multi-AZ Deployments configuration", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "An Amazon Aurora database with Read Replicas", correct: false },
                { id: 3, text: "Redshift", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBefore we proceed in answering this question, we must first be clear with the actual definition of a \"schema\". Basically, the english definition of a schema is: a representation of a plan or theory in the form of an outline or model. Just think of a schema as the \"structure\" or a \"model\" of your data in your database. Since the scenario requires that the schema, or the structure of your data, changes frequently, then you have to pick a database which provides a non-rigid and flexible way of adding or removing new types of data. This is a classic example of choosing between a relational database and non-relational (NoSQL) database. A relational database is known for having a rigid schema, with a lot of constraints and limits as to which (and what type of) data can be inserted or not. It is primarily used for scenarios where you have to support complex queries which fetch data across a number of tables. It is best for scenarios where you have complex table relationships but for use cases where you need to have a flexible schema, this is not a suitable database to use. For NoSQL, it is not as rigid as a relational database because you can easily add or remove rows or elements in your table/collection entry. It also has a more flexible schema because it can store complex hierarchical data within a single item which, unlike a relational database, does not entail changing multiple related tables. Hence, the best answer to be used here is a NoSQL database, like DynamoDB. When your business requires a low-latency response to high-traffic queries, taking advantage of a NoSQL system generally makes technical and economic sense. Amazon DynamoDB helps solve the problems that limit the relational system scalability by avoiding them. In DynamoDB, you design your schema specifically to make the most common and important queries as fast and as inexpensive as possible. Your data structures are tailored to the specific requirements of your business use cases. Remember that a relational database system does not scale well for the following reasons: - It normalizes data and stores it on multiple tables that require multiple queries to write to disk. - It generally incurs the performance costs of an ACID-compliant transaction system. - It uses expensive joins to reassemble required views of query results. For DynamoDB, it scales well due to these reasons: - Its schema flexibility lets DynamoDB store complex hierarchical data within a single item. DynamoDB is not a totally schemaless database since the very definition of a schema is just the model or structure of your data. - Composite key design lets it store related items close together on the same table. Hence, the correct answer is: Amazon DynamoDB.\n\n**Why option 0 is incorrect:**\nThe option that says an Amazon RDS instance in Multi-AZ Deployments configuration is incorrect because RDS is a type of relational database with a rigid schema. Relational databases require schema changes that can cause downtime and performance issues, which directly conflicts with the requirement for frequent schema changes without downtime or performance issues.\n\n**Why option 2 is incorrect:**\nThe option that says an Amazon Aurora database with Read Replicas is incorrect because Aurora is also a type of relational database. Like RDS, it has a rigid schema structure that requires careful planning for schema changes and may cause downtime or performance degradation during schema modifications.\n\n**Why option 3 is incorrect:**\nThe option that says Redshift is incorrect because Redshift is primarily used for OLAP (Online Analytical Processing) systems and data warehousing. It is not designed for frequent schema changes or low-latency responses to high-traffic queries. Redshift is optimized for complex analytical queries on large datasets, not for operational workloads requiring flexible schemas.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A software development company is using serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. The company has a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform, and also uses a third-party API to fetch certain data for its application. One of the developers was instructed to create the environment variables for the MongoDB database hostname, username, and password, as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT, and PROD environments.Considering that the Lambda function is storing sensitive database and API credentials, how can this information be secured to prevent other developers on the team, or anyone, from seeing these credentials in plain text? Select the best option that provides maximum security.",
            options: [
                { id: 0, text: "There is no need to do anything because, by default, Lambda already encrypts the environment variables using the AWS Key Management Service.", correct: false },
                { id: 1, text: "Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information.", correct: false },
                { id: 2, text: "Lambda does not provide encryption for the environment variables. Deploy your code to an Amazon EC2 instance instead.", correct: false },
                { id: 3, text: "Create a new AWS KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nWhen you create or update Lambda functions that use environment variables, AWS Lambda encrypts them using the AWS Key Management Service. When your Lambda function is invoked, those values are decrypted and made available to the Lambda code. The first time you create or update Lambda functions that use environment variables in a region, a default service key is created for you automatically within AWS KMS. This key is used to encrypt environment variables. However, if you wish to use encryption helpers and use KMS to encrypt environment variables after your Lambda function is created, you must create your own AWS KMS key and choose it instead of the default key. The default key will give errors when chosen. Creating your own key gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data. Hence, the correct answer is: Create a new AWS KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.\n\n**Why option 0 is incorrect:**\nThe option that says there is no need to do anything because, by default, Lambda already encrypts the environment variables using the AWS Key Management Service is incorrect. Although Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables.\n\n**Why option 1 is incorrect:**\nThe option that says enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information is also incorrect since enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Typically, AWS KMS is the recommended choice for encrypting sensitive data at rest.\n\n**Why option 2 is incorrect:**\nThe option that says Lambda does not provide encryption for the environment variables. Deploy your code to an Amazon EC2 instance instead is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A payment processing company plans to migrate its on-premises application to an Amazon EC2 instance. An IPv6 CIDR block is attached to the company’s Amazon VPC. Strict security policy mandates that the production VPC must only allow outbound communication over IPv6 between the instance and the internet but should prevent the internet from initiating an inbound IPv6 connection. The new architecture should also allow traffic flow inspection and traffic filtering.What should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Launch the EC2 instance to a public subnet and attach an Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use Traffic Mirroring to set up the required rules for traffic inspection and traffic filtering.", correct: false },
                { id: 1, text: "Launch the EC2 instance to a private subnet and attach AWS PrivateLink interface endpoint to the VPC to control outbound IPv6 communication to the internet. Use Amazon GuardDuty to set up the required rules for traffic inspection and traffic filtering.", correct: false },
                { id: 2, text: "Launch the EC2 instance to a private subnet and attach a NAT Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Firewall Manager to set up the required rules for traffic inspection and traffic filtering.", correct: false },
                { id: 3, text: "Launch the EC2 instance to a private subnet and attach an Egress-Only Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAn egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet and prevents it from initiating an IPv6 connection with your instances. IPv6 addresses are globally unique and are therefore public by default. If you want your instance to be able to access the internet, but you want to prevent resources on the internet from initiating communication with your instance, you can use an egress-only internet gateway. A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet and a private subnet for resources that won't be connected to the internet. AWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs). The service can be set up with just a few clicks and scales automatically with your network traffic, so you don't have to worry about deploying and managing any infrastructure. AWS Network Firewall includes features that provide protection from common network threats. AWS Network Firewall's stateful firewall can incorporate context from traffic flows, like tracking connections and protocol identification, to enforce policies such as preventing your VPCs from accessing domains using an unauthorized protocol. AWS Network Firewall's intrusion prevention system (IPS) provides active traffic flow inspection so you can identify and block vulnerability exploits using signature-based detection. AWS Network Firewall also offers web filtering that can stop traffic to known bad URLs and monitor fully qualified domain names. In this scenario, you can use an egress-only internet gateway to allow outbound IPv6 communication to the internet and then use the AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering. Hence, the correct answer is: Launch the EC2 instance to a private subnet and attach an Egress-Only Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.\n\n**Why option 0 is incorrect:**\nThe option that says launch the EC2 instance to a public subnet and attach an Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use Traffic Mirroring to set up the required rules for traffic inspection and traffic filtering is incorrect because an Internet Gateway does not limit or control any outgoing IPv6 connection. Take note that the requirement is to prevent the Internet from initiating an inbound IPv6 connection to your instance. This solution allows all kinds of traffic to initiate a connection to your EC2 instance hence, this option is wrong. In addition, the use of Traffic Mirroring is not appropriate as well. This is just an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type interface, not to filter or inspect the incoming/outgoing traffic.\n\n**Why option 1 is incorrect:**\nThe option that says launch the EC2 instance to a private subnet and attach AWS PrivateLink interface endpoint to the VPC to control outbound IPv6 communication to the internet. Use Amazon GuardDuty to set up the required rules for traffic inspection and traffic filtering is incorrect because the AWS PrivateLink (which is also known as VPC Endpoint) is just a highly available, scalable technology that enables you to privately connect your VPC to the AWS services as if they were in your VPC. This service is not capable of controlling outbound IPv6 communication to the Internet. Furthermore, the Amazon GuardDuty service doesn't have the features to do traffic inspection or filtering.\n\n**Why option 2 is incorrect:**\nThe option that says launch the EC2 instance to a private subnet and attach a NAT Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Firewall Manager to set up the required rules for traffic inspection and traffic filtering is incorrect. While NAT Gateway has a NAT64 feature that translates an IPv6 address to IPv4, it will not prevent inbound IPv6 traffic from reaching the EC2 instance. You have to use the egress-only Internet Gateway instead. Moreover, the AWS Firewall Manager is neither capable of doing traffic inspection nor traffic filtering.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A pharmaceutical company has resources hosted on both its on-premises network and in the AWS cloud. The company requires all Software Architects to access resources in both environments using on-premises credentials, which are stored in Active Directory.In this scenario, which of the following can be used to fulfill this requirement?",
            options: [
                { id: 0, text: "Set up SAML 2.0-Based Federation by using a Web Identity Federation.", correct: false },
                { id: 1, text: "Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service.", correct: true },
                { id: 2, text: "Use IAM users", correct: false },
                { id: 3, text: "Use Amazon VPC", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSince the company is using Microsoft Active Directory which implements Security Assertion Markup Language (SAML), you can set up a SAML-Based Federation for API Access to your AWS cloud. In this way, you can easily connect to AWS using the login credentials of your on-premises network. AWS supports identity federation with SAML 2.0, an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS APIs without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code. Before you can use SAML 2.0-based federation as described in the preceding scenario and diagram, you must configure your organization's IdP and your AWS account to trust each other. The general process for configuring this trust is described in the following steps. Inside your organization, you must have an IdP that supports SAML 2.0, like Microsoft Active Directory Federation Service (AD FS, part of Windows Server), Shibboleth, or another compatible SAML 2.0 provider. Hence, the correct answer is: Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service.\n\n**Why option 0 is incorrect:**\nThe option that says set up SAML 2.0-Based Federation by using a Web Identity Federation is incorrect because this is primarily used to let users sign in via a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google. It does not utilize Active Directory.\n\n**Why option 2 is incorrect:**\nThe option that says use IAM users is incorrect because the situation requires you to use the existing credentials stored in their Active Directory, and not user accounts that will be generated by IAM.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon VPC is incorrect because this only lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. This has nothing to do with user authentication or Active Directory.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A Solutions Architect needs to make sure that the On-Demand Amazon EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection.Which configuration below will satisfy this requirement?",
            options: [
                { id: 0, text: "Security Group Inbound Rule: Protocol – TCP, Port Range – 22, Source 110.238.98.71/32", correct: true },
                { id: 1, text: "Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32", correct: false },
                { id: 2, text: "Security Group Outbound Rule: Protocol – TCP, Port Range – 22, Destination 110.238.98.71/32", correct: false },
                { id: 3, text: "Security Group Outbound Rule: Protocol – UDP, Port Range – 22, Destination 0.0.0.0/0", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups. The requirement is to only allow the individual IP of the client and not the entire network. The /32 CIDR notation denotes a single IP address. Take note that the SSH protocol uses TCP, not UDP, and runs on port 22 (default). In the scenario, we can create a security group with an inbound rule allowing incoming traffic from the specified IP address on port 22. Security groups are stateful, meaning they automatically allow return traffic associated with the client who initiated the connection to the instance. Therefore, any return traffic from the specified IP address on port 22 will be allowed to pass through the security group, regardless of whether or not there is an explicit outbound rule allowing it. Hence, the correct answer is: Security Group Inbound Rule: Protocol – TCP, Port Range – 22, Source 110.238.98.71/32\n\n**Why option 1 is incorrect:**\nThe option that says Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32 is incorrect because SSH runs over the TCP protocol, so specifying UDP would not allow the desired access.\n\n**Why option 2 is incorrect:**\nThe option that says Security Group Outbound Rule: Protocol – TCP, Port Range – 22, Destination 110.238.98.71/32 is incorrect because it's an outbound rule, not an inbound rule. Outbound rules control traffic leaving the instance. In the scenario, we need to limit inbound traffic coming from a specific address.\n\n**Why option 3 is incorrect:**\nThe option that says Security Group Outbound Rule: Protocol – UDP, Port Range – 22, Destination 0.0.0.0/0 is incorrect because it is an outbound rule rather than an inbound rule. Moreover, SSH connections only require TCP.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for the storage of their personal documents.Which of the following will you need to consider so you can set up a solution that incorporates a single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.)",
            options: [
                { id: 0, text: "Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.", correct: false },
                { id: 1, text: "Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.", correct: true },
                { id: 2, text: "Map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents.", correct: false },
                { id: 3, text: "Configure an IAM role and an IAM Policy to access the bucket.", correct: true },
                { id: 4, text: "Set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket.", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "**Why option 1 is correct:**\nThe question refers to one of the common scenarios for temporary credentials in AWS. Temporary credentials are useful in scenarios that involve identity federation, delegation, cross-account access, and IAM roles. In this example, it is called enterprise identity federation, considering that you also need to set up a single sign-on (SSO) capability. In an enterprise identity federation, you can authenticate users in your organization's network, and then provide those users access to AWS without creating new AWS identities for them and requiring them to sign in with a separate user name and password. This is known as the single sign-on (SSO) approach to temporary access. AWS STS supports open standards like Security Assertion Markup Language (SAML) 2.0, with which you can use Microsoft AD FS to leverage your Microsoft Active Directory. You can also use SAML 2.0 to manage your own solution for federating user identities.\n\n**Why option 3 is correct:**\nTo restrict access for each individual user to a designated user folder in an S3 bucket, you need to configure an IAM role and an IAM Policy. The IAM policy can use policy variables (like ${aws:username}) to dynamically map each federated user to their specific folder path in the S3 bucket, ensuring that users can only access their designated folders.\n\n**Why option 0 is incorrect:**\nThe option that says use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others is incorrect since you don't have to use 3rd party solutions to provide the access. AWS already provides the necessary tools that you can use in this situation, including AWS STS and IAM roles.\n\n**Why option 2 is incorrect:**\nThe option that says map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents is incorrect as there is no direct way of integrating Amazon S3 with Amazon WorkDocs for this particular scenario. Amazon WorkDocs is simply a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content. And because it's stored centrally on AWS, you can access it from anywhere on any device.\n\n**Why option 4 is incorrect:**\nThe option that says set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket is incorrect since creating that many IAM users would be unnecessary and operationally burdensome. Also, you want the account to integrate with your AD or LDAP directory, hence, IAM Users does not fit these criteria. Using federation with STS and IAM roles is the recommended approach.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A business has recently migrated its applications to AWS. The audit team must be able to assess whether the services the company is using meet common security and regulatory standards. A solutions architect needs to provide the team with a report of all compliance-related documents for their account.Which action should a solutions architect consider?",
            options: [
                { id: 0, text: "Run an Amazon Inspector assessment job to download all of the AWS compliance-related information.", correct: false },
                { id: 1, text: "Use AWS Artifact to view the security reports as well as other AWS compliance-related information.", correct: true },
                { id: 2, text: "Run an Amazon Macie job to view the Service Organization Control (SOC), Payment Card Industry (PCI), and other compliance reports from AWS Certificate Manager (ACM).", correct: false },
                { id: 3, text: "View all of the AWS security compliance reports from AWS Security Hub.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS' security and compliance reports and select online agreements. Reports available in AWS Artifact include our Service Organization Control (SOC) reports, Payment Card Industry (PCI) reports, and certifications from accreditation bodies across geographies and compliance verticals that validate the implementation and operating effectiveness of AWS security controls. Agreements available in AWS Artifact include the Business Associate Addendum (BAA) and the Nondisclosure Agreement (NDA). All AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their accounts by agreeing to the associated terms and conditions. You will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact while restricting access to other services and resources within your AWS Account. Hence, the correct answer in this scenario is: Use AWS Artifact to view the security reports as well as other AWS compliance-related information.\n\n**Why option 0 is incorrect:**\nThe option that says run an Amazon Inspector assessment job to download all of the AWS compliance-related information is incorrect. Amazon Inspector is simply a security tool for detecting vulnerabilities in AWS workloads. For this scenario, it is better to use the readily-available security reports in AWS Artifact instead.\n\n**Why option 2 is incorrect:**\nThe option that says run an Amazon Macie job to view the Service Organization Control (SOC), Payment Card Industry (PCI), and other compliance reports from AWS Certificate Manager (ACM) is incorrect because ACM is just a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. This service does not store certifications or compliance-related documents.\n\n**Why option 3 is incorrect:**\nThe option that says view all of the AWS security compliance reports from AWS Security Hub is incorrect because AWS Security Hub only provides you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts. It does not provide access to AWS compliance-related documents like SOC reports, PCI reports, or certifications.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company has a web application that uses Amazon CloudFront to distribute its images, videos, and other static content stored in its Amazon S3 bucket to users around the world. The company has recently introduced a new member-only access feature for some of its high-quality media files. There is a requirement to provide access to multiple private media files only to paying subscribers without having to change the current URLs.Which of the following is the most suitable solution to implement to satisfy this requirement?",
            options: [
                { id: 0, text: "Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.", correct: false },
                { id: 1, text: "Create a Signed URL with a custom policy which only allows the members to see the private files.", correct: false },
                { id: 2, text: "Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.", correct: false },
                { id: 3, text: "Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the requiredSet-Cookieheaders to the viewer which will unlock the content only to them.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nMany companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee. To securely serve this private content by using CloudFront, you can do the following: - Require that your users access your private content by using special CloudFront signed URLs or signed cookies. - Require that your users access your content by using CloudFront URLs, not URLs that access content directly on the origin server (for example, Amazon S3 or a private HTTP server). Requiring CloudFront URLs isn't necessary, but we recommend it to prevent users from bypassing the restrictions that you specify in signed URLs or signed cookies. CloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. If you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following: Use signed URLs for the following cases: - You want to use an RTMP distribution. Signed cookies aren't supported for RTMP distributions. - You want to restrict access to individual files, for example, an installation download for your application. - Your users are using a client (for example, a custom HTTP client) that doesn't support cookies. Use signed cookies for the following cases: - You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of a website. - You don't want to change your current URLs. Hence, the correct answer is: Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required Set-Cookie headers to the viewer which will unlock the content only to them.\n\n**Why option 0 is incorrect:**\nThe option that says configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member is incorrect because a Match Viewer is an Origin Protocol Policy that configures CloudFront to communicate with your origin using HTTP or HTTPS, depending on the protocol of the viewer request. CloudFront caches the object only once even if viewers make requests using both HTTP and HTTPS protocols. It does not provide access control based on user membership status.\n\n**Why option 1 is incorrect:**\nThe option that says create a Signed URL with a custom policy which only allows the members to see the private files is incorrect. Signed URLs are primarily used for providing access to individual files, as shown in the above explanation. In addition, the scenario explicitly says that they don't want to change their current URLs which is why implementing Signed Cookies is more suitable than Signed URLs.\n\n**Why option 2 is incorrect:**\nThe option that says configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members is incorrect because Field-Level Encryption only allows you to securely upload user-submitted sensitive information to your web servers. It does not provide access control to download multiple private files or restrict access based on membership status.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A Solutions Architect identified a series of DDoS attacks while monitoring the Amazon VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
            options: [
                { id: 0, text: "Use AWS Shield Advanced to detect and mitigate DDoS attacks.", correct: true },
                { id: 1, text: "Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks, and other DDoS attacks.", correct: false },
                { id: 2, text: "Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.", correct: false },
                { id: 3, text: "A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nFor higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 charges. Hence, the correct answer is: Use AWS Shield Advanced to detect and mitigate DDoS attacks.\n\n**Why option 1 is incorrect:**\nThe option that says using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks and other DDoS attacks is incorrect because AWS Firewall Manager is mainly used to simplify your AWS WAF administration and maintenance tasks across multiple accounts and resources. It does not protect your VPC against DDoS attacks.\n\n**Why option 2 is incorrect:**\nThe option that says set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic is incorrect. Even though AWS WAF can help you block common attack patterns to your VPC such as SQL injection or cross-site scripting, this is still not enough to withstand DDoS attacks. It is just better to use AWS Shield in this scenario.\n\n**Why option 3 is incorrect:**\nThe option that says a combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC is incorrect. Although using a combination of Security Groups and NACLs are valid to provide security to your VPC, this is not enough to mitigate a DDoS attack. You should use AWS Shield for better security protection.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A travel photo-sharing website is using Amazon S3 to serve high-quality photos to visitors. After a few days, it was discovered that other travel websites are linking to and using these photos. This has resulted in financial losses for the business.What is the MOST effective method to mitigate this issue?",
            options: [
                { id: 0, text: "Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.", correct: true },
                { id: 1, text: "Use Amazon CloudFront distributions for your photos.", correct: false },
                { id: 2, text: "Block the IP addresses of the offending websites using NACL.", correct: false },
                { id: 3, text: "Store and privately serve the high-quality photos on Amazon WorkDocs instead.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nIn Amazon S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and the expiration date and time. The pre-signed URLs are valid only for the specified duration. Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL. Hence, the correct answer is: Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon CloudFront distributions for your photos is incorrect. CloudFront is primarily a content delivery network service that speeds up the delivery of content to your customers. It does not prevent other websites from linking to and using your photos if the bucket is publicly accessible.\n\n**Why option 2 is incorrect:**\nThe option that says block the IP addresses of the offending websites using NACL is also incorrect. Blocking IP addresses using NACLs is not a very efficient method because a quick change in IP address would easily bypass this configuration. Additionally, NACLs operate at the subnet level and cannot effectively block external websites from linking to S3 objects.\n\n**Why option 3 is incorrect:**\nThe option that says store and privately serve the high-quality photos on Amazon WorkDocs instead is incorrect as WorkDocs is simply a fully managed, secure content creation, storage, and collaboration service. It is not a suitable service for storing static content. Amazon WorkDocs is more often used to easily create, edit, and share documents for collaboration and not for serving object data like Amazon S3.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company uses an Application Load Balancer (ALB) for its public-facing multi-tier web applications. The security team has recently reported that there has been a surge of SQL injection attacks lately, which causes critical data discrepancy issues. The same issue is also encountered by its other web applications in other AWS accounts that are behind an ALB. An immediate solution is required to prevent the remote injection of unauthorized SQL queries and protect their applications hosted across multiple accounts.As a Solutions Architect, what solution would you recommend?",
            options: [
                { id: 0, text: "Use AWS Network Firewall to filter web vulnerabilities and brute force attacks using stateful rule groups across all Application Load Balancers on all AWS accounts. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment.", correct: false },
                { id: 1, text: "Use AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.", correct: true },
                { id: 2, text: "Use Amazon Macie to scan for vulnerabilities and unintended network exposure. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment. Utilize the AWS Audit Manager to reuse the security assessment across all AWS accounts.", correct: false },
                { id: 3, text: "Use Amazon GuardDuty and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer and utilize the AWS Security Hub service to reuse the managed rules across all the AWS accounts", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS WAF is a web application firewall that lets you monitor the HTTP(S) requests that are forwarded to an Amazon CloudFront distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. -Web ACLs – You use a web access control list (ACL) to protect a set of AWS resources. You create a web ACL and define its protection strategy by adding rules. Rules define criteria for inspecting web requests and specify how to handle requests that match the criteria. You set a default action for the web ACL that indicates whether to block or allow through those requests that pass the rules inspections. -Rules – Each rule contains a statement that defines the inspection criteria and an action to take if a web request meets the criteria. When a web request meets the criteria, that's a match. You can configure rules to block matching requests, allow them through, count them, or run CAPTCHA controls against them. -Rules groups – You can use rules individually or in reusable rule groups. AWS Managed Rules and AWS Marketplace sellers provide managed rule groups for your use. You can also define your own rule groups. AWSManagedRulesSQLiRuleSet - The SQL database rule group contains rules to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. This can help prevent remote injection of unauthorized queries. Evaluate this rule group for use if your application interfaces with an SQL database. AWS WAF is easy to deploy and protect applications deployed on either Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts all your origin servers, Amazon API Gateway for your REST APIs, or AWS AppSync for your GraphQL APIs. There is no additional software to deploy, DNS configuration, SSL/TLS certificate to manage, or need for a reverse proxy setup. With AWS Firewall Manager integration, you can centrally define and manage your rules and reuse them across all the web applications that you need to protect. Therefore, the correct answer is: Use AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Network Firewall to filter web vulnerabilities and brute force attacks using stateful rule groups across all Application Load Balancers on all AWS accounts. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment is incorrect because AWS Network Firewall is a managed service that is primarily used to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs) and not particularly to your Application Load Balancers. Take note that the AWS Network Firewall is account-specific by default and needs to be integrated with the AWS Firewall Manager to easily share the firewall across your other AWS accounts. In addition, refactoring the web application will require an immense amount of time.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon Macie to scan for vulnerabilities and unintended network exposure. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment. Utilize the AWS Audit Manager to reuse the security assessment across all AWS accounts is incorrect because Amazon Macie is only used for data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data. Just like before, refactoring the web application will require an immense amount of time. The use of the AWS Audit Manager is not relevant as well. The AWS Audit Manager simply helps you continually audit your AWS usage to simplify how you manage risk and compliance with regulations and industry standards.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon GuardDuty and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer and utilize the AWS Security Hub service to reuse the managed rules across all the AWS accounts is incorrect because Amazon GuardDuty is only a threat detection service and cannot directly be integrated with the Application Load Balancer.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A company has 3 DevOps engineers that are handling its software development and infrastructure management processes. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service.What can the DevOps engineers do to prevent this from happening again?",
            options: [
                { id: 0, text: "Use S3 Infrequently Accessed storage to store the data.", correct: false },
                { id: 1, text: "Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.", correct: true },
                { id: 2, text: "Set up a signed URL for all users.", correct: false },
                { id: 3, text: "Create an IAM bucket policy that disables delete operation.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTo enhance data protection and meet best practices for securing stored objects, AWS recommends implementing safeguards against accidental deletions. To avoid accidental deletion in an Amazon S3 bucket, you can: - Enable Versioning - Enable MFA (Multi-Factor Authentication) Delete Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. If the MFA (Multi-Factor Authentication) Delete is enabled, it requires additional authentication for either of the following operations: - Change the versioning state of your bucket - Permanently delete an object version Hence, the correct answer is: Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.\n\n**Why option 0 is incorrect:**\nThe option that says use S3 Infrequently Accessed storage to store the data is incorrect. Switching your storage class to S3 Infrequent Access won't help mitigate accidental deletions. Storage classes only affect cost and retrieval times, not protection against deletion.\n\n**Why option 2 is incorrect:**\nThe option that says set up a signed URL for all users is incorrect. Signed URLs give you more control over access to your content, so this feature primarily deals more with accessing rather than deletion. Signed URLs are used to grant temporary access to objects, not to prevent deletion.\n\n**Why option 3 is incorrect:**\nThe option that says create an IAM bucket policy that disables delete operation is incorrect. If you create a bucket policy preventing deletion, other users won't be able to delete objects that should be deleted. You only want to prevent accidental deletion, not disable the action itself. Versioning and MFA Delete provide a better balance between protection and operational flexibility.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A company requires all the data stored in the cloud to be encrypted at rest. To easily integrate this with other AWS services, they must have full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS. The solution should also be able to audit the key usage independently of AWS CloudTrail.Which of the following options will meet this requirement?",
            options: [
                { id: 0, text: "Use AWS Key Management Service to create AWS owned Keys and store the non-extractable key material in AWS CloudHSM.", correct: false },
                { id: 1, text: "Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in Amazon S3.", correct: false },
                { id: 2, text: "Use AWS Key Management Service to create AWS managed keys and store the non-extractable key material in AWS CloudHSM.", correct: false },
                { id: 3, text: "Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in AWS CloudHSM.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe AWS Key Management Service (KMS) custom key store feature combines the controls provided by AWS CloudHSM with the integration and ease of use of AWS KMS. You can configure your own CloudHSM cluster and authorize AWS KMS to use it as a dedicated key store for your keys rather than the default AWS KMS key store. When you create keys in AWS KMS you can choose to generate the key material in your CloudHSM cluster. KMS Keys that are generated in your custom key store never leave the HSMs in the CloudHSM cluster in plaintext and all AWS KMS operations that use those KMS keys are only performed on your HSMs. AWS KMS can help you integrate with other AWS services to encrypt the data that you store in these services and control access to the keys that decrypt it. To immediately remove the key material from AWS KMS, you can use a custom key store. Take note that each custom key store is associated with an AWS CloudHSM cluster in your AWS account. Therefore, when you create an AWS KMS Key in a custom key store, AWS KMS generates and stores the non-extractable key material for the KMS key in an AWS CloudHSM cluster that you own and manage. This is also suitable if you want to be able to audit the usage of all your keys independently of AWS KMS or AWS CloudTrail. Since you control your AWS CloudHSM cluster, you have the option to manage the lifecycle of your KMS keys independently of AWS KMS. Here are the criteria why you might find a custom key store useful: You have encryption keys that must be safeguarded within a dedicated hardware security module (HSM) under your direct control, adhering to strict single-tenancy requirements. You require the capability to promptly and independently revoke and remove key material from AWS KMS, exercising complete control over the key lifecycle. Your compliance obligations mandate independent auditing and monitoring of all key usage activities, beyond the logging provided by AWS KMS and AWS CloudTrail. Hence, the correct answer in this scenario is: Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in AWS CloudHSM.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Key Management Service to create AWS owned Keys and store the non-extractable key material in AWS CloudHSM is incorrect because AWS owned Keys are managed by AWS. The scenario primarily requires you to have full control over the encryption of the created key. Moreover, this option does not allow you to audit the key usage independently of AWS CloudTrail.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in Amazon S3 is incorrect. Amazon S3 is primarily for general storage purposes and does not provide the required level of security and control needed for cryptographic key management. You have to use AWS CloudHSM instead.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Key Management Service to create AWS managed Keys and store the non-extractable key material in AWS CloudHSM is incorrect because AWS managed Keys are managed by AWS. The scenario primarily requires you to have full control over the encryption of the created key. Moreover, this option does not allow you to audit the key usage independently of AWS CloudTrail.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company hosted an e-commerce website on an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer. The Solutions Architect noticed that the website is receiving a high number of illegitimate external requests from multiple systems with frequently changing IP addresses. To address the performance issues, the Solutions Architect must implement a solution that would block these requests while having minimal impact on legitimate traffic.Which of the following options fulfills this requirement?",
            options: [
                { id: 0, text: "Create a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer.", correct: false },
                { id: 1, text: "Create a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests.", correct: false },
                { id: 2, text: "Create a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.", correct: true },
                { id: 3, text: "Create a custom rule in the security group of the Application Load Balancer to block the offending requests.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync – services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn't come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on regional services, such as Application Load Balancer, Amazon API Gateway, and AWS AppSync, your rules run in the region and can be used to protect Internet-facing resources as well as internal resources. A rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. Based on the given scenario, the requirement is to limit the number of requests from the illegitimate requests without affecting the genuine requests. To accomplish this requirement, you can use AWS WAF web ACL. There are two types of rules in creating your own web ACL rule: regular and rate-based rules. You need to select the latter to add a rate limit to your web ACL. After creating the web ACL, you can associate it with ALB. When the rule action triggers, AWS WAF applies the action to additional requests from the IP address until the request rate falls below the limit. Hence, the correct answer is: Create a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.\n\n**Why option 0 is incorrect:**\nThe option that says create a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer is incorrect because a regular rule typically matches the statement defined in the rule. If you need to add a rate limit to your rule, you should create a rate-based rule.\n\n**Why option 1 is incorrect:**\nThe option that says create a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests is incorrect. Although NACLs can help you block incoming traffic, this option wouldn't be able to limit the number of requests from a single IP address that is dynamically changing.\n\n**Why option 3 is incorrect:**\nThe option that says create a custom rule in the security group of the Application Load Balancer to block the offending requests is incorrect because the security group can only allow incoming traffic. Remember that you can't deny traffic using security groups. In addition, it is not capable of limiting the rate of traffic to your application unlike AWS WAF.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A government agency plans to store confidential tax documents on AWS. Due to the sensitive information in the files, the Solutions Architect must restrict the data access requests made to the storage solution to a specific Amazon VPC only. The solution should also prevent the files from being deleted or overwritten to meet the regulatory requirement of having a write-once-read-many (WORM) storage model.Which combination of the following options should the Architect implement? (Select TWO.)",
            options: [
                { id: 0, text: "Set up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific VPC.", correct: false },
                { id: 1, text: "Configure an Amazon S3 Access Point for the S3 bucket to restrict data access to a particular VPC only.", correct: true },
                { id: 2, text: "Create a new Amazon S3 bucket with the S3 Object Lock feature enabled. Store the documents in the bucket and set the Legal Hold option for object retention.", correct: true },
                { id: 3, text: "Store the tax documents in the Amazon S3 Glacier Instant Retrieval storage class. Use thePutBucketPolicyAPI to apply a bucket policy that restricts access requests to a specific VPC.", correct: false },
                { id: 4, text: "Enable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement.", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "**Why option 1 is correct:**\nAmazon S3 access points simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as GetObject and PutObject. Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. Each access point enforces a customized access point policy that works in conjunction with the bucket policy that is attached to the underlying bucket. You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.\n\n**Why option 2 is correct:**\nWith S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require WORM storage, or to simply add another layer of protection against object changes and deletion. Before locking any objects, it is essential to enable S3 Object Lock on a bucket. Once S3 Object Lock is enabled on a bucket, it allows you to lock objects within that bucket to prevent them from being deleted or overwritten for a fixed amount of time or indefinitely. Legal Hold is an option that can be set on objects to prevent deletion or overwriting, independent of retention periods.\n\n**Why option 0 is incorrect:**\nThe option that says set up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific VPC is incorrect because you cannot directly use an AWS Network Firewall to restrict S3 bucket data access requests to a specific Amazon VPC only. You have to use an Amazon S3 Access Point instead for this particular use case. An AWS Network Firewall is commonly integrated to your Amazon VPC and not to an S3 bucket.\n\n**Why option 3 is incorrect:**\nThe option that says store the tax documents in the Amazon S3 Glacier Instant Retrieval storage class. Use the PutBucketPolicy API to apply a bucket policy that restricts access requests to a specific VPC is incorrect because Amazon S3 Glacier Instant Retrieval is just an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. Additionally, using a bucket policy to restrict access from a VPC is less efficient compared to using an S3 Access Point.\n\n**Why option 4 is incorrect:**\nThe option that says enable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement is incorrect. Although the Object Lock feature does provide write-once-read-many (WORM) storage, the Object Versioning feature must also be enabled in order for this to work. In fact, you cannot manually disable the Object Versioning feature if you have already selected the Object Lock option.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A medical records company is planning to store sensitive clinical trial data in an Amazon S3 repository with the object-level versioning feature enabled. The Solutions Architect is tasked with ensuring that no object can be overwritten or deleted by any user in a period of one year only. To meet the strict compliance requirements, the root user of the company’s AWS account must also be restricted from making any changes to an object in the S3 bucket.Which of the following is the most secure way of storing the data in S3?",
            options: [
                { id: 0, text: "Enable S3 Object Lock in governance mode with a retention period of one year.", correct: false },
                { id: 1, text: "Enable S3 Object Lock in compliance mode with a retention period of one year.", correct: true },
                { id: 2, text: "Enable S3 Object Lock in governance mode with a legal hold of one year.", correct: false },
                { id: 3, text: "Enable S3 Object Lock in compliance mode with a legal hold of one year.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWith S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require WORM storage or to simply add another layer of protection against object changes and deletion. Before you lock any objects, you have to enable a bucket to use S3 Object Lock. You enable Object Lock when you create a bucket. After you enable Object Lock on a bucket, you can lock objects in that bucket. When you create a bucket with Object Lock enabled, you can't disable Object Lock or suspend versioning for that bucket. S3 Object Lock provides two retention modes: -Governance mode -Compliance mode These retention modes apply different levels of protection to your objects. You can apply either retention mode to any object version that is protected by Object Lock. In governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary. You can also use governance mode to test retention-period settings before creating a compliance-mode retention period. In compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period. To override or remove governance-mode retention settings, a user must have the s3:BypassGovernanceRetention permission and must explicitly include x-amz-bypass-governance-retention:true as a request header with any request that requires overriding governance mode. Legal Hold vs. Retention Period With Object Lock, you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the s3:PutObjectLegalHold permission. Legal holds are independent from retention periods. As long as the bucket that contains the object has Object Lock enabled, you can place and remove legal holds regardless of whether the specified object version has a retention period set. Placing a legal hold on an object version doesn't affect the retention mode or retention period for that object version. For example, suppose that you place a legal hold on an object version while the object version is also protected by a retention period. If the retention period expires, the object doesn't lose its WORM protection. Rather, the legal hold continues to protect the object until an authorized user explicitly removes it. Similarly, if you remove a legal hold while an object version has a retention period in effect, the object version remains protected until the retention period expires. Hence, the correct answer is: Enable S3 Object Lock in compliance mode with a retention period of one year.\n\n**Why option 0 is incorrect:**\nThe option that says enable S3 Object Lock in governance mode with a retention period of one year is incorrect because in the governance mode, users typically can't overwrite or delete an object version or alter its lock settings unless they have special permissions or if a user has access to the root AWS user account. A better option to choose here is to use the compliance mode, which prevents even the root user from modifying objects.\n\n**Why option 2 is incorrect:**\nThe option that says enable S3 Object Lock in governance mode with a legal hold of one year is incorrect. You cannot set a time period for a legal hold. You can only do this using the \"retention period\" option. Take note that a legal hold will still restrict users from changing the S3 objects even after the one-year retention period has elapsed. In addition, a governance mode will allow the root user to modify your S3 objects and override any existing settings.\n\n**Why option 3 is incorrect:**\nThe option that says enable S3 Object Lock in compliance mode with a legal hold of one year is incorrect. Although the choice of using the compliance mode is right, you still cannot set a one-year time period for the legal hold option. Keep in mind that the legal hold is independent of the retention period.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A government entity is conducting a population and housing census in the city. Each household information uploaded on their online portal is stored in encrypted files in Amazon S3. The government assigned its Solutions Architect to set compliance policies that verify data containing personally identifiable information (PII) in a manner that meets their compliance standards. They should also be alerted if there are potential policy violations with the privacy of their S3 buckets.Which of the following should the Architect implement to satisfy this requirement?",
            options: [
                { id: 0, text: "Set up and configure Amazon Macie to monitor their Amazon S3 data.", correct: true },
                { id: 1, text: "Set up and configure Amazon Kendra to monitor malicious activity on their Amazon S3 data", correct: false },
                { id: 2, text: "Set up and configure Amazon Polly to scan for usage patterns on Amazon S3 data", correct: false },
                { id: 3, text: "Set up and configure Amazon Fraud Detector to send out alert notifications whenever a security violation is detected on their Amazon S3 data.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon Macie is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization. Amazon Macie generates two categories of findings: policy findings and sensitive data findings. A policy finding is a detailed report of a potential policy violation or issue with the security or privacy of an Amazon S3 bucket. Macie generates these findings as part of its ongoing monitoring activities for your Amazon S3 data. A sensitive data finding is a detailed report of sensitive data in an S3 object. Macie generates these findings when it discovers sensitive data in S3 objects that you configure a sensitive data discovery job to analyze. Hence, the correct answer is: Set up and configure Amazon Macie to monitor their Amazon S3 data.\n\n**Why option 1 is incorrect:**\nThe option that says set up and configure Amazon Kendra to monitor malicious activity on their Amazon S3 data is incorrect. Amazon Kendra is just an enterprise search service that allows developers to add search capabilities to their applications. This enables their end users to discover information stored within the vast amount of content spread across their company, but not monitor malicious activity on their S3 buckets.\n\n**Why option 2 is incorrect:**\nThe option that says set up and configure Amazon Polly to scan for usage patterns on Amazon S3 data is incorrect because Amazon Polly is simply a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. Polly can't be used to scan usage patterns on your S3 data.\n\n**Why option 3 is incorrect:**\nThe option that says set up and configure Amazon Fraud Detector to send out alert notifications whenever a security violation is detected on their Amazon S3 data is incorrect because the Amazon Fraud Detector is only a fully managed service for identifying potentially fraudulent activities and for catching more online fraud faster. It does not check any S3 data containing personally identifiable information (PII), unlike Amazon Macie.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that are used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.{ \n\"Version\": \"2012-10-17\", \n\"Statement\": [ \n{ \n\"Effect\": \"Allow\", \n\"Action\": [ \n\"s3:Get*\", \n\"s3:List*\" \n], \n\"Resource\": \"*\" \n}, \n{ \n\"Effect\": \"Allow\", \n\"Action\": \"s3:PutObject\", \n\"Resource\": \"arn:aws:s3:::boracay/*\" \n} \n] \n}What does the above IAM policy allow? (Select THREE.)",
            options: [
                { id: 0, text: "An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.", correct: true },
                { id: 1, text: "An IAM user with this IAM policy is allowed to write objects into theboracayS3 bucket.", correct: true },
                { id: 2, text: "An IAM user with this IAM policy is allowed to change access rights for theboracayS3 bucket.", correct: false },
                { id: 3, text: "An IAM user with this IAM policy is allowed to read objects in theboracayS3 bucket but not allowed to list the objects in the bucket.", correct: false },
                { id: 4, text: "An IAM user with this IAM policy is allowed to read objects from theboracayS3 bucket.", correct: true },
                { id: 5, text: "An IAM user with this IAM policy is allowed to read and delete objects from theboracayS3 bucket.", correct: false },
            ],
            correctAnswers: [0, 1, 4],
            explanation: "**Why option 0 is correct:**\nThe first statement in the IAM policy grants `s3:Get*` and `s3:List*` actions with `Resource: \"*\"`. The wildcard `\"*\"` means this applies to all S3 buckets owned by the account. The `s3:Get*` action includes all Get operations like `GetObject`, `GetObjectVersion`, `GetObjectAcl`, etc., which allows reading objects from any bucket. The `s3:List*` action includes `ListBucket`, `ListBucketVersions`, etc., which allows listing objects in any bucket.\n\n**Why option 1 is correct:**\nThe second statement in the IAM policy grants `s3:PutObject` action with `Resource: \"arn:aws:s3:::boracay/*\"`. This specifically allows writing (uploading) objects to the `boracay` S3 bucket. The `s3:PutObject` action enables submitting PUT object requests to store data in the bucket.\n\n**Why option 4 is correct:**\nSince the first statement grants `s3:Get*` with `Resource: \"*\"`, this includes the ability to read objects from all buckets, including the `boracay` bucket. The `s3:Get*` action covers all Get operations, allowing reading objects from the `boracay` S3 bucket.\n\n**Why option 2 is incorrect:**\nThe option that says an IAM user with this IAM policy is allowed to change access rights for the boracay S3 bucket is incorrect because the template does not have any statements which allow the user to change access rights in the bucket. Actions like `s3:PutBucketAcl`, `s3:PutBucketPolicy`, or `s3:PutBucketCors` are not included in the policy.\n\n**Why option 3 is incorrect:**\nThe option that says an IAM user with this IAM policy is allowed to read objects in the boracay S3 bucket but not allowed to list the objects in the bucket is incorrect because it can clearly be seen in the template that there is a `s3:List*` which permits the user to list objects. The first statement grants `s3:List*` with `Resource: \"*\"`, which includes the ability to list objects in the `boracay` bucket.\n\n**Why option 5 is incorrect:**\nThe option that says an IAM user with this IAM policy is allowed to read and delete objects from the boracay S3 bucket is incorrect. Although you can read objects from the bucket (via `s3:Get*`), you cannot delete any objects. The policy does not include `s3:DeleteObject` or `s3:DeleteObjectVersion` actions.",
            domain: "Design Secure Architectures",
        },
    ],
    test9: [
        {
            id: 0,
            text: "A company has a website hosted on AWS The website is behind an Application Load Balancer \n(ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward \nall requests to the website so that the requests will use HTTPS. \nWhat should a solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Update the ALB's network ACL to accept only HTTPS traffic", correct: false },
                { id: 1, text: "Create a rule that replaces the HTTP in the URL with HTTPS.", correct: false },
                { id: 2, text: "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.", correct: true },
                { id: 3, text: "Replace the ALB with a Network Load Balancer configured to use Server Name Indication", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nTo redirect HTTP traffic to HTTPS, a solutions architect should create a listener rule on the ALB to redirect HTTP traffic to HTTPS. Application Load Balancers support listener rules that can redirect HTTP requests to HTTPS automatically. This is the standard AWS-recommended approach for enforcing HTTPS on web applications. The ALB listener rule can be configured to redirect all HTTP traffic (port 80) to HTTPS (port 443), ensuring all requests use encrypted connections. This solution requires minimal configuration and provides seamless redirection without requiring changes to the application code.\n\n**Why option 0 is incorrect:**\nThe option that says update the ALB's network ACL to accept only HTTPS traffic is incorrect because network ACLs operate at the subnet level and control traffic flow based on IP addresses and ports, but they do not have the ability to redirect traffic from one protocol to another. Network ACLs can only allow or deny traffic, not perform protocol-level redirection. Additionally, blocking HTTP traffic entirely would prevent users from accessing the site, rather than redirecting them to HTTPS.\n\n**Why option 1 is incorrect:**\nThe option that says create a rule that replaces the HTTP in the URL with HTTPS is incorrect because simply replacing text in URLs does not actually redirect traffic or enforce HTTPS connections. This approach would not work at the network level and would require application-level changes. The ALB listener rule for redirection is the proper way to handle this at the load balancer level.\n\n**Why option 3 is incorrect:**\nThe option that says replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is incorrect because Network Load Balancers operate at Layer 4 (TCP/UDP) and do not have the ability to handle HTTP/HTTPS redirection at Layer 7. While NLB supports SNI for SSL/TLS termination, it cannot perform HTTP to HTTPS redirection like an Application Load Balancer can. NLB is designed for high-performance, low-latency traffic routing, not for application-level features like HTTP redirection.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 1,
            text: "A company is developing a two-tier web application on AWS. The company's developers have \ndeployed the application on an Amazon EC2 instance that connects directly to a backend \nAmazon RDS database. The company must not hardcode database credentials in the application. \nThe company must also implement a solution to automatically rotate the database credentials on \na regular basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the database credentials in the instance metadata.", correct: false },
                { id: 1, text: "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket.", correct: false },
                { id: 2, text: "Store the database credentials as a secret in AWS Secrets Manager.", correct: true },
                { id: 3, text: "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Secrets Manager is a service that enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Secrets Manager supports automatic rotation of secrets, which is a key differentiator from AWS Systems Manager Parameter Store. By storing the database credentials as a secret in Secrets Manager, you can ensure that they are not hardcoded in the application and that they are automatically rotated on a regular basis. To grant the EC2 instance access to the secret, you can attach the required IAM permissions to the EC2 instance role. This will allow the application to retrieve the secret from Secrets Manager as needed. Secrets Manager integrates with RDS databases and can automatically rotate credentials without application downtime, providing the least operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says store the database credentials in the instance metadata and use Amazon EventBridge rules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same time is incorrect because instance metadata is not designed for storing sensitive credentials securely. Additionally, this approach requires significant operational overhead to manage Lambda functions, EventBridge rules, and coordinate credential updates between RDS and instance metadata. Secrets Manager provides automatic rotation without requiring custom Lambda functions or manual coordination.\n\n**Why option 1 is incorrect:**\nThe option that says store the database credentials in a configuration file in an encrypted Amazon S3 bucket and use EventBridge rules to run a scheduled Lambda function that updates the RDS credentials and the credentials in the configuration file is incorrect because this approach requires managing S3 bucket encryption, versioning, Lambda functions, and EventBridge rules, which adds significant operational overhead. Additionally, the application would need to be modified to retrieve credentials from S3, and there's no built-in automatic rotation capability like Secrets Manager provides.\n\n**Why option 3 is incorrect:**\nThe option that says store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store and turn on automatic rotation is incorrect because Parameter Store does not support automatic rotation of secrets. While Parameter Store can store encrypted parameters using AWS KMS, it lacks the built-in rotation capabilities that Secrets Manager provides. Parameter Store is better suited for configuration data and non-sensitive parameters, while Secrets Manager is specifically designed for secrets that require rotation.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A company is deploying a new public web application to AWS. The application will run behind an \nApplication Load Balancer (ALB).  \nThe application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by \nan external certificate authority (CA). \nThe certificate must be rotated each year before the certificate expires. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.", correct: false },
                { id: 1, text: "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.", correct: false },
                { id: 2, text: "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS", correct: false },
                { id: 3, text: "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Certificate Manager (ACM) allows you to import SSL/TLS certificates that are issued by external certificate authorities (CAs). When you import a certificate into ACM, you provide the certificate, private key, and certificate chain. However, ACM does not manage the renewal process for imported certificates. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. To meet the requirement of rotating the certificate each year, you can use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration, and then manually rotate the certificate by importing the new certificate from your external CA. This is the correct approach when you need to use certificates from an external CA.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate, apply it to the ALB, and use the managed renewal feature to automatically rotate the certificate is incorrect because ACM can only issue certificates through AWS Certificate Manager's public CA or private CA, not from external certificate authorities. If the requirement specifies that the certificate must be issued by an external CA, ACM-issued certificates would not meet this requirement.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate, import the key material from the certificate, apply it to the ALB, and use the managed renewal feature is incorrect because this is contradictory - you cannot both issue a certificate from ACM and import key material from an external certificate. Additionally, ACM-issued certificates are automatically renewed by AWS, but the scenario requires certificates from an external CA.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA, apply it to the ALB, and use the managed renewal feature is incorrect because ACM Private CA issues certificates from AWS's private certificate authority, not from an external certificate authority. The scenario explicitly requires certificates issued by an external CA, so Private CA certificates would not meet this requirement.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its \ndocument management application. The company intends to create a product that converts \nlarge .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to \nstore the original files and the converted files. A solutions architect must design a scalable \nsolution to accommodate demand that will grow rapidly over time. \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Save the .pdf files to Amazon S3.", correct: true },
                { id: 1, text: "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS", correct: false },
                { id: 2, text: "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2", correct: false },
                { id: 3, text: "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSaving PDF files to Amazon S3 and configuring an S3 PUT event to invoke an AWS Lambda function to convert the files to JPG format and store them back in Amazon S3 is the most cost-effective and scalable solution. S3 provides virtually unlimited storage capacity and can handle millions of objects. Lambda functions are serverless and only charge for compute time used, making them highly cost-effective for file conversion tasks. The S3 event-driven architecture automatically triggers the conversion process when files are uploaded, eliminating the need to manage servers or infrastructure. This solution can scale automatically to handle the growing demand from 700,000 users without requiring manual scaling or infrastructure management.\n\n**Why option 1 is incorrect:**\nThe option that says save the PDF files to Amazon DynamoDB and use DynamoDB Streams to invoke a Lambda function to convert files and store them back in DynamoDB is incorrect because DynamoDB has a 400KB item size limit, which is insufficient for PDF files averaging 5MB in size. Additionally, DynamoDB is designed for structured data storage, not for storing large binary files like PDFs and images. Using DynamoDB for file storage would be both technically infeasible and cost-ineffective compared to S3.\n\n**Why option 2 is incorrect:**\nThe option that says upload PDF files to an AWS Elastic Beanstalk application with EC2 instances, EBS storage, and Auto Scaling groups, and use a program in EC2 instances to convert files is incorrect because Elastic Beanstalk requires managing EC2 instances, which incurs ongoing costs even when not processing files. EBS storage is more expensive than S3 for large-scale file storage, and this approach requires managing servers, scaling groups, and application deployment, resulting in higher operational overhead and costs compared to the serverless S3 + Lambda approach.\n\n**Why option 3 is incorrect:**\nThe option that says upload PDF files to an AWS Elastic Beanstalk application with EC2 instances, EFS storage, and Auto Scaling groups is incorrect for similar reasons as option 2. While EFS provides shared file storage, it is more expensive than S3 for large-scale storage, and the EC2-based approach requires ongoing server management and costs. The serverless S3 + Lambda solution is more cost-effective and requires less operational overhead.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A company has more than 5 TB of file data on Windows file servers that run on premises. Users \nand applications interact with the data each day. \nThe company is moving its Windows workloads to AWS. As the company continues this process, \nthe company requires access to AWS and on-premises file storage with minimum latency. The \ncompany needs a solution that minimizes operational overhead and requires no significant \nchanges to the existing file access patterns. The company uses an AWS Site-to-Site VPN \nconnection for connectivity to AWS. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Deploy and configure Amazon FSx for Windows File Server on AWS.", correct: false },
                { id: 1, text: "Deploy and configure an Amazon S3 File Gateway on premises.", correct: false },
                { id: 2, text: "Deploy and configure an Amazon S3 File Gateway on premises.", correct: false },
                { id: 3, text: "Deploy and configure Amazon FSx for Windows File Server on AWS.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon FSx File Gateway (FSx File Gateway) is a File Gateway type that provides low latency and efficient access to in-cloud FSx for Windows File Server file shares from your on-premises facility. This solution allows the company to deploy FSx for Windows File Server on AWS while maintaining access from both AWS and on-premises environments. The FSx File Gateway deployed on-premises provides seamless access to the cloud-based FSx file shares, maintaining the existing SMB protocol and file access patterns. This minimizes operational overhead since FSx is fully managed, and the gateway handles the hybrid connectivity automatically. The solution provides minimum latency for on-premises users while allowing cloud workloads to access the same file shares directly.\n\n**Why option 0 is incorrect:**\nThe option that says deploy and configure Amazon FSx for Windows File Server on AWS, move the on-premises file data to FSx, and reconfigure workloads to use FSx on AWS is incorrect because this approach only provides access from AWS, not from on-premises. The scenario requires access to both AWS and on-premises file storage with minimum latency. Simply moving data to AWS and reconfiguring workloads would not provide the hybrid access pattern required.\n\n**Why option 1 is incorrect:**\nThe option that says deploy and configure an Amazon S3 File Gateway on-premises, move the on-premises file data to the S3 File Gateway, and reconfigure workloads to use the S3 File Gateway is incorrect because S3 File Gateway provides access to S3 buckets, not Windows file shares. The scenario involves Windows file servers and requires maintaining Windows file access patterns (SMB protocol). S3 File Gateway would not support the existing Windows file access patterns and would require significant changes to the application.\n\n**Why option 2 is incorrect:**\nThe option that says deploy and configure an Amazon S3 File Gateway on-premises, move the on-premises file data to Amazon S3, and reconfigure workloads to use either S3 directly or the S3 File Gateway is incorrect because S3 is object storage, not file storage. Windows applications expect file system access with SMB protocol, not object storage APIs. This approach would require rewriting applications to use S3 APIs instead of file system calls, which violates the requirement of \"no significant changes to the existing file access patterns.\"",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The \nhospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n5 \nformat. The hospital needs to modify the Lambda code to identify protected health information \n(PHI) in the reports. Which solution will meet these requirements with the LEAST operational \noverhead?",
            options: [
                { id: 0, text: "Use existing Python libraries to extract the text from the reports and to identify the PHI from the", correct: false },
                { id: 1, text: "Use Amazon Textract to extract the text from the reports.", correct: false },
                { id: 2, text: "Use Amazon Textract to extract the text from the reports.", correct: true },
                { id: 3, text: "Use Amazon Rekognition to extract the text from the reports.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing Amazon Textract to extract the text from the reports and Amazon Comprehend Medical to identify the PHI from the extracted text is the most efficient solution with the least operational overhead. Amazon Textract is a fully managed machine learning service specifically designed for extracting text and data from documents, including PDFs and images. Amazon Comprehend Medical is a specialized natural language processing service that can accurately identify protected health information (PHI) in medical text, such as patient names, medical conditions, medications, and other sensitive health data. Both services are fully managed, requiring no infrastructure provisioning or model training. This solution integrates seamlessly with Lambda functions, requires minimal code changes, and automatically scales to handle varying workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use existing Python libraries to extract text from reports and identify PHI from the extracted text is incorrect because this approach requires significant operational overhead. You would need to maintain and update Python libraries, handle different document formats, implement PHI detection algorithms, and manage the infrastructure to run these libraries. Additionally, custom PHI detection would be less accurate than Comprehend Medical, which is specifically trained on medical text and HIPAA-compliant.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Textract to extract text and Amazon SageMaker to identify PHI is incorrect because SageMaker is a machine learning platform that requires building, training, and deploying custom models. This approach would involve significant operational overhead including model training, hyperparameter tuning, model deployment, and ongoing maintenance. Comprehend Medical is a pre-trained, fully managed service that already understands medical terminology and PHI patterns, eliminating the need for custom model development.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Rekognition to extract text from reports and Amazon Comprehend Medical to identify PHI is incorrect because Amazon Rekognition is designed for image and video analysis, not for text extraction from documents. While Rekognition can detect text in images, it is not optimized for document text extraction like Textract. Textract is specifically designed for extracting structured and unstructured text from documents with higher accuracy for document-based workflows.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A company has an application that generates a large number of files, each approximately 5 MB in \nsize. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 \nyears before they can be deleted. Immediate accessibility is always required as the files contain \ncritical business data that is not easy to reproduce. The files are frequently accessed in the first \n30 days of the object creation but are rarely accessed after the first 30 days. \nWhich storage solution is MOST cost-effective?",
            options: [
                { id: 0, text: "Create an S3 bucket lifecycle policy to move Mm from S3 Standard to S3 Glacier 30 days from", correct: false },
                { id: 1, text: "Create an S3 bucket lifecycle policy to move tiles from S3 Standard to S3 One Zone-infrequent", correct: false },
                { id: 2, text: "Create an S3 bucket lifecycle policy to move files from S3 Standard-infrequent Access (S3", correct: true },
                { id: 3, text: "Create an S3 bucket Lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCreating an S3 bucket lifecycle policy to move files from S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation is the most cost-effective solution that meets all requirements. S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers the same high durability, high throughput, and low latency as S3 Standard, but at a lower storage price. Since the files are frequently accessed in the first 30 days but rarely accessed afterward, transitioning to Standard-IA after 30 days provides immediate accessibility (unlike Glacier, which has retrieval delays) while reducing storage costs. The files can remain in Standard-IA for the 4-year retention period, and then be deleted via lifecycle policy.\n\n**Why option 0 is incorrect:**\nThe option that says create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation is incorrect because Glacier storage classes (except Glacier Instant Retrieval) have retrieval delays ranging from minutes to hours, which does not meet the requirement for immediate accessibility. The scenario explicitly states that immediate accessibility is always required for critical business data. Unless Glacier Instant Retrieval is specifically mentioned, standard Glacier retrieval times would violate the immediate access requirement.\n\n**Why option 1 is incorrect:**\nThe option that says create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation is incorrect because S3 One Zone-IA stores data in a single Availability Zone, which provides less durability than Standard-IA (which stores data across multiple AZs). While One Zone-IA is cheaper, the scenario mentions that files contain critical business data that is not easy to reproduce, making durability a concern. Standard-IA provides better durability while still being cost-effective.\n\n**Why option 3 is incorrect:**\nThe option that says create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-IA 30 days from object creation, then move files to S3 Glacier 4 years after object creation is incorrect because moving files to Glacier after 4 years would violate the immediate accessibility requirement. The scenario states that immediate accessibility is always required, and Glacier (unless Instant Retrieval) does not provide immediate access. Additionally, the policy requires files to be stored for 4 years before deletion, not moved to Glacier.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company hosts an application on multiple Amazon EC2 instances. The application processes \nmessages from an Amazon SQS queue writes to an Amazon RDS table and deletes the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n6 \nmessage from the queue Occasional duplicate records are found in the RDS table. The SQS \nqueue does not contain any duplicate messages. \n \nWhat should a solutions architect do to ensure messages are being processed once only?",
            options: [
                { id: 0, text: "Use the CreateQueue API call to create a new queue", correct: false },
                { id: 1, text: "Use the Add Permission API call to add appropriate permissions", correct: false },
                { id: 2, text: "Use the ReceiveMessage API call to set an appropriate wail time", correct: false },
                { id: 3, text: "Use the ChangeMessageVisibility APi call to increase the visibility timeout", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe visibility timeout begins when Amazon SQS returns a message to a consumer. During this time, the consumer processes the message and should delete it. However, if the consumer fails before deleting the message and your system doesn't call the DeleteMessage action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again, leading to duplicate processing. To ensure messages are processed only once, you should use the ChangeMessageVisibility API call to increase the visibility timeout if the message processing takes longer than the default visibility timeout. This gives the consumer enough time to complete processing and delete the message before it becomes visible to other consumers. For applications that write to databases, increasing the visibility timeout ensures that database operations complete before the message can be reprocessed.\n\n**Why option 0 is incorrect:**\nThe option that says use the CreateQueue API call to create a new queue is incorrect because creating a new queue does not solve the duplicate message processing problem. The issue is that messages are being processed multiple times from the existing queue, not that a new queue is needed. Creating a new queue would not prevent duplicate processing of messages already in the original queue.\n\n**Why option 1 is incorrect:**\nThe option that says use the Add Permission API call to add appropriate permissions is incorrect because permissions control who can access the queue, not how messages are processed. Adding permissions would not prevent duplicate message processing or ensure messages are processed only once. The duplicate processing issue is related to message visibility timeout, not permissions.\n\n**Why option 2 is incorrect:**\nThe option that says use the ReceiveMessage API call to set an appropriate wait time is incorrect because the wait time (long polling) controls how long the ReceiveMessage call waits for messages to arrive, not how long messages remain invisible to other consumers. The wait time does not affect message visibility timeout or prevent duplicate processing. The visibility timeout is what controls how long a message remains hidden from other consumers after being received.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A solutions architect is designing a new hybrid architecture to extend a company s on-premises \ninfrastructure to AWS. The company requires a highly available connection with consistent low \nlatency to an AWS Region. The company needs to minimize costs and is willing to accept slower \ntraffic if the primary connection fails. \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Provision an AWS Direct Connect connection to a Region.", correct: true },
                { id: 1, text: "Provision a VPN tunnel connection to a Region for private connectivity.", correct: false },
                { id: 2, text: "Provision an AWS Direct Connect connection to a Region.", correct: false },
                { id: 3, text: "Provision an AWS Direct Connect connection to a Region.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nProvisioning an AWS Direct Connect connection to a Region and provisioning a VPN connection as a backup provides a highly available, cost-effective hybrid connectivity solution. AWS Direct Connect provides dedicated network connections from on-premises to AWS with consistent low latency and high bandwidth. However, Direct Connect connections can fail, so having a Site-to-Site VPN as a backup ensures continuity. VPN connections are much more cost-effective than a second Direct Connect connection, and while VPN may have slower performance than Direct Connect, it meets the requirement of accepting slower traffic if the primary connection fails. This solution minimizes costs by using VPN as backup instead of a second Direct Connect, while still providing the high availability and low latency required.\n\n**Why option 1 is incorrect:**\nThe option that says provision a VPN tunnel connection to a Region and provision a second VPN tunnel as backup is incorrect because VPN connections, while cost-effective, do not provide the same consistent low latency as Direct Connect. The scenario requires \"consistent low latency,\" which is a key characteristic of Direct Connect. VPN connections have variable latency depending on internet conditions, which may not meet the consistent low latency requirement.\n\n**Why option 2 is incorrect:**\nThe option that says provision an AWS Direct Connect connection and provision a second Direct Connect connection to the same Region as backup is incorrect because this approach is expensive. While it would provide high availability and low latency, it does not minimize costs as required. A second Direct Connect connection requires additional port fees and data transfer costs, making it significantly more expensive than using a VPN as backup.\n\n**Why option 3 is incorrect:**\nThe option that says provision an AWS Direct Connect connection and use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection is incorrect because there is no such \"failover attribute\" in the AWS CLI that automatically creates backup connections. Direct Connect does not have built-in automatic failover to create new connections. Failover must be configured manually using routing protocols (BGP) and requires pre-provisioned backup connections.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A company is running a business-critical web application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances are in an Auto Scaling group. The application \nuses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The \ncompany wants the application to be highly available with minimum downtime and minimum loss \nof data. \n \nWhich solution will meet these requirements with the LEAST operational effort?",
            options: [
                { id: 0, text: "Place the EC2 instances in different AWS Regions.", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use multiple Availability Zones.", correct: true },
                { id: 2, text: "Configure the Auto Scaling group to use one Availability Zone.", correct: false },
                { id: 3, text: "Configure the Auto Scaling group to use multiple AWS Regions.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy configuring the Auto Scaling group to use multiple Availability Zones, the application will be able to continue running even if one Availability Zone goes down, providing high availability for the EC2 instances. Configuring the Aurora PostgreSQL database as Multi-AZ ensures that the database remains available in the event of a failure in one Availability Zone, with automatic failover to a standby replica in another AZ. Using Amazon RDS Proxy for the database allows the application to automatically route traffic to healthy database instances, manage connection pooling, and handle failover transparently, further increasing the availability of the application. This solution meets the requirements for high availability with minimum downtime and minimum data loss (Multi-AZ provides synchronous replication) with minimal operational effort, as all components are managed services.\n\n**Why option 0 is incorrect:**\nThe option that says place EC2 instances in different AWS Regions, use Route 53 health checks to redirect traffic, and use Aurora PostgreSQL Cross-Region Replication is incorrect because cross-region replication for Aurora is asynchronous, which could result in data loss during failover. Additionally, placing instances in different regions introduces significant latency between the application and database, and Route 53 health check-based failover takes time, resulting in more downtime than Multi-AZ failover. This approach also requires more operational effort to manage cross-region replication and Route 53 configurations.\n\n**Why option 2 is incorrect:**\nThe option that says configure the Auto Scaling group to use one Availability Zone, generate hourly snapshots, and recover from snapshots in the event of failure is incorrect because using a single Availability Zone creates a single point of failure. If that AZ goes down, both the application and database would be unavailable. Hourly snapshots mean potential data loss of up to an hour, which does not meet the \"minimum loss of data\" requirement. Additionally, recovering from snapshots requires manual intervention and takes significant time, resulting in extended downtime.\n\n**Why option 3 is incorrect:**\nThe option that says configure the Auto Scaling group to use multiple AWS Regions, write data to S3, and use S3 Event Notifications to launch Lambda functions to write to the database is incorrect because this architecture introduces significant complexity and operational overhead. Writing to S3 and then using Lambda to write to the database adds latency and potential points of failure. This approach does not provide the same level of high availability as Multi-AZ deployments and requires significant operational effort to manage the S3-Lambda-database pipeline.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group \nis configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the \nweb service. \n \nThe company notices that the NLB is not detecting HTTP errors for the application. These errors \nrequire a manual restart of the EC2 instances that run the web service. The company needs to \nimprove the application's availability without writing custom scripts or code. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Enable HTTP health checks on the NLB. supplying the URL of the company's application.", correct: false },
                { id: 1, text: "Add a cron job to the EC2 instances to check the local application's logs once each minute.", correct: false },
                { id: 2, text: "Replace the NLB with an Application Load Balancer.", correct: true },
                { id: 3, text: "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nReplacing the NLB with an Application Load Balancer (ALB) is the correct solution because ALB operates at Layer 7 (application layer) and can detect HTTP errors, while Network Load Balancer (NLB) operates at Layer 4 (transport layer) and only handles TCP/UDP traffic. ALB can perform HTTP health checks that examine the HTTP response codes and content, allowing it to detect application-level errors and automatically remove unhealthy instances from the target group. This provides automatic failover without requiring custom scripts or code. ALB's health checks can be configured to check specific URLs, expected HTTP response codes, and response body content, making it ideal for detecting HTTP application errors.\n\n**Why option 0 is incorrect:**\nThe option that says enable HTTP health checks on the NLB by supplying the URL of the company's application is incorrect because Network Load Balancers do not support HTTP health checks. NLB operates at Layer 4 (TCP/UDP) and can only perform TCP health checks, which verify that a TCP connection can be established but cannot detect HTTP-level errors or application issues. NLB cannot examine HTTP response codes or content to determine if the application is functioning correctly.\n\n**Why option 1 is incorrect:**\nThe option that says add a cron job to the EC2 instances to check the local application's logs once each minute is incorrect because this approach requires writing and maintaining custom scripts, which violates the requirement of improving availability \"without writing custom scripts or code.\" Additionally, checking logs does not automatically restart unhealthy instances or remove them from the load balancer target group. This would require additional automation to integrate with the load balancer.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon CloudWatch alarm that monitors the UnhealthyHostCount metric for the NLB is incorrect because while CloudWatch can monitor metrics, it cannot automatically remove unhealthy instances from the target group or restart them. CloudWatch alarms can trigger notifications or actions, but you would still need to write custom automation (Lambda functions, Systems Manager automation, etc.) to actually handle the unhealthy instances, which violates the \"without writing custom scripts or code\" requirement. Additionally, NLB's health checks are TCP-based and may not detect HTTP errors effectively.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company runs a shopping application that uses Amazon DynamoDB to store customer \ninformation. In case of data corruption, a solutions architect needs to design a solution that meets \na recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. \n \nWhat should the solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure DynamoDB global tables.", correct: false },
                { id: 1, text: "Configure DynamoDB point-in-time recovery.", correct: true },
                { id: 2, text: "Export the DynamoDB data to Amazon S3 Glacier on a daily basis.", correct: false },
                { id: 3, text: "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDynamoDB Point-in-Time Recovery (PITR) provides continuous backups of your DynamoDB table data. With PITR enabled, you can restore your table to any point in time within the last 35 days. PITR provides a recovery point objective (RPO) of less than 1 second in most cases, easily meeting the 15-minute RPO requirement. The recovery time objective (RTO) of 1 hour can be met by restoring the table to a point in time before the corruption occurred. PITR is a fully managed service that requires no manual intervention for backups and provides fast recovery capabilities. This solution is operationally efficient as it's automatically enabled and managed by AWS.\n\n**Why option 0 is incorrect:**\nThe option that says configure DynamoDB global tables and point the application to a different AWS Region for RPO recovery is incorrect because global tables provide multi-region replication for high availability, but they do not protect against data corruption. If data is corrupted in one region, that corruption will be replicated to all regions. Global tables are designed for disaster recovery and low-latency access, not for point-in-time recovery from data corruption.\n\n**Why option 2 is incorrect:**\nThe option that says export DynamoDB data to Amazon S3 Glacier on a daily basis and import from Glacier for RPO recovery is incorrect because daily exports do not meet the 15-minute RPO requirement. Daily backups mean you could lose up to 24 hours of data, far exceeding the 15-minute RPO. Additionally, importing data from Glacier requires retrieval time (which can take hours), and the import process itself takes time, likely exceeding the 1-hour RTO requirement.\n\n**Why option 3 is incorrect:**\nThe option that says schedule Amazon EBS snapshots for the DynamoDB table every 15 minutes is incorrect because DynamoDB is a managed NoSQL database service that does not use EBS volumes. You cannot take EBS snapshots of DynamoDB tables as DynamoDB's storage is abstracted and managed by AWS. EBS snapshots are only applicable to EC2 instances with attached EBS volumes, not DynamoDB tables.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A company runs a photo processing application that needs to frequently upload and download \npictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect \nhas noticed an increased cost in data transfer fees and needs to implement a solution to reduce \nthese costs. \n \nHow can the solutions architect meet this requirement?",
            options: [
                { id: 0, text: "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls", correct: false },
                { id: 1, text: "Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to", correct: false },
                { id: 2, text: "Deploy the application Into a public subnet and allow it to route through an internet gateway to", correct: false },
                { id: 3, text: "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows", correct: false },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nBy deploying an S3 VPC gateway endpoint into the VPC and attaching an endpoint policy that allows access to the S3 buckets, the application can access the S3 buckets over a private network connection within the VPC, eliminating the need for data transfer over the internet. VPC endpoints provide private connectivity between your VPC and AWS services without requiring internet gateways, NAT devices, VPN connections, or AWS Direct Connect connections. This eliminates data transfer charges for traffic between your VPC and S3 within the same region, significantly reducing costs. The endpoint policy can be used to specify which S3 buckets the application has access to, providing fine-grained access control. This solution improves both cost efficiency and performance by keeping traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nThe option that says deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it is incorrect because API Gateway is not designed to route S3 calls and would not reduce data transfer costs. API Gateway is used for creating RESTful APIs, not for routing S3 traffic. Additionally, routing through API Gateway would add unnecessary complexity and costs without addressing the data transfer fee issue.\n\n**Why option 1 is incorrect:**\nThe option that says deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to S3 buckets is incorrect because NAT gateways are used to allow resources in private subnets to access the internet, but they still route traffic through the internet gateway, which incurs data transfer charges. NAT gateways do not provide private connectivity to S3 like VPC endpoints do. Additionally, NAT gateways themselves incur hourly charges and data processing charges.\n\n**Why option 2 is incorrect:**\nThe option that says deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets is incorrect because routing through an internet gateway means traffic goes over the public internet, which incurs data transfer charges. This approach does not reduce costs and actually increases them by requiring internet gateway routing. VPC endpoints provide private connectivity that avoids internet routing entirely.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 13,
            text: "A company recently launched Linux-based application instances on Amazon EC2 in a private \nsubnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet \nof a VPC. A solutions architect needs to connect from the on-premises network, through the \ncompany's internet connection, to the bastion host, and to the application servers. The solutions \narchitect must make sure that the security groups of all the EC2 instances will allow that access. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Replace the current security group of the bastion host with one that only allows inbound access", correct: false },
                { id: 1, text: "Replace the current security group of the bastion host with one that only allows inbound access", correct: false },
                { id: 2, text: "Replace the current security group of the bastion host with one that only allows inbound access", correct: true },
                { id: 3, text: "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.", correct: true },
                { id: 4, text: "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "**Why option 2 is correct:**\nThe bastion host security group should allow inbound access from the external IP range for the company because connections from on-premises networks go through the internet, so the bastion host will see the public IP addresses of the on-premises resources. The bastion host is in a public subnet and accessible via the internet gateway, so it needs to accept connections from the company's external/public IP addresses.\n\n**Why option 3 is correct:**\nThe application instances security group should allow inbound SSH access from only the private IP address of the bastion host because the bastion host and application instances are in the same VPC. When the bastion host connects to the application instances, it uses its private IP address (not public IP) since both are within the same VPC. This provides secure access by only allowing connections from the bastion host's private IP, following the principle of least privilege.\n\n**Why option 0 is incorrect:**\nThe option that says replace the bastion host security group with one that only allows inbound access from the application instances is incorrect because this would prevent the on-premises network from connecting to the bastion host. The bastion host needs to accept connections from on-premises (via internet) first, before it can connect to the application instances.\n\n**Why option 1 is incorrect:**\nThe option that says replace the bastion host security group with one that only allows inbound access from the internal IP range for the company is incorrect because when connecting from on-premises through the internet, the bastion host will see the public/external IP addresses of the on-premises resources, not their internal IP addresses. Internal IP ranges are not routable over the internet.\n\n**Why option 4 is incorrect:**\nThe option that says replace the application instances security group with one that allows inbound SSH access from only the public IP address of the bastion host is incorrect because within the same VPC, instances communicate using private IP addresses, not public IP addresses. The bastion host's public IP is only used for internet routing to reach the bastion, but VPC-internal communication uses private IPs.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A solutions architect is designing a two-tier web application. The application consists of a public-\nfacing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft \nSQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the \ncompany. \nHow should security groups be configured in this situation? (Choose two.)",
            options: [
                { id: 0, text: "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.", correct: true },
                { id: 1, text: "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.", correct: false },
                { id: 2, text: "Configure the security group for the database tier to allow inbound traffic on port 1433 from the", correct: false },
                { id: 3, text: "Configure the security group for the database tier to allow outbound traffic on ports 443 and", correct: false },
                { id: 4, text: "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nConfigure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0 is correct because the web tier is public-facing and needs to accept HTTPS connections from the internet. Port 443 is the standard port for HTTPS traffic. Allowing traffic from 0.0.0.0/0 means accepting connections from any IP address on the internet, which is necessary for a public-facing web application. Security groups are stateful, meaning that return traffic is automatically allowed, so you don't need to explicitly create outbound rules for responses.\n\n**Why option 2 is correct:**\nConfigure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier is correct because port 1433 is the default port for Microsoft SQL Server. The database should only accept connections from the web tier, not from the internet. By referencing the web tier's security group instead of IP addresses, you create a more secure and flexible configuration that automatically adapts if the web tier instances change. This follows the principle of least privilege by only allowing necessary database access from the web tier.\n\n**Why option 1 is incorrect:**\nThe option that says configure the web tier security group to allow outbound traffic on port 443 from 0.0.0.0/0 is incorrect because outbound rules control traffic leaving the instance, not incoming traffic. For a public-facing web application, you need inbound rules to accept incoming HTTPS requests. Additionally, security groups are stateful, so return traffic for established connections is automatically allowed without needing explicit outbound rules.\n\n**Why option 3 is incorrect:**\nThe option that says configure the database tier security group to allow outbound traffic on ports 443 and 1433 to the web tier is incorrect because outbound rules are not needed for the database to respond to queries from the web tier. Security groups are stateful, so return traffic for established connections is automatically allowed. Additionally, port 443 (HTTPS) is not used for SQL Server communication - port 1433 is the correct port for SQL Server.\n\n**Why option 4 is incorrect:**\nThe option that says configure the database tier security group to allow inbound traffic on ports 443 and 1433 from the web tier is incorrect because port 443 (HTTPS) is not used for SQL Server database connections. SQL Server uses port 1433 for database communication. Allowing port 443 on the database tier would be unnecessary and could create a security risk if not properly secured.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company wants to move a multi-tiered application from on premises to the AWS Cloud to \nimprove the application's performance. The application consists of application tiers that \ncommunicate with each other by way of RESTful services. Transactions are dropped when one \ntier becomes overloaded. A solutions architect must design a solution that resolves these issues \nand modernizes the application. \n \nWhich solution meets these requirements and is the MOST operationally efficient?",
            options: [
                { id: 0, text: "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the", correct: true },
                { id: 1, text: "Use Amazon CloudWatch metrics to analyze the application performance history to determine", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between", correct: false },
                { id: 3, text: "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing Amazon API Gateway and directing transactions to AWS Lambda functions as the application layer, with Amazon SQS as the communication layer between application services, is the most operationally efficient solution. API Gateway provides a fully managed API service that handles request routing, authentication, and throttling. Lambda functions are serverless and automatically scale to handle varying loads, eliminating the need to manage servers. SQS decouples the application tiers, preventing transaction drops when one tier becomes overloaded by buffering messages in a queue. This serverless architecture modernizes the application by eliminating server management, provides automatic scaling, and ensures high availability. The solution requires minimal operational overhead as all services are fully managed.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon CloudWatch metrics to analyze application performance history and increase EC2 instance sizes is incorrect because this approach requires manual analysis and intervention, which is not operationally efficient. Additionally, simply increasing instance sizes does not prevent transaction drops when tiers become overloaded - it only provides more capacity. This approach also doesn't modernize the application architecture and requires ongoing capacity planning and management.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon SNS to handle messaging between EC2 servers in an Auto Scaling group and use CloudWatch to monitor SNS queue length is incorrect because SNS is a pub/sub messaging service, not a queuing service. SNS does not buffer messages like SQS does, so it cannot prevent transaction drops when a tier becomes overloaded. SNS delivers messages immediately to all subscribers, and if a subscriber is overloaded, messages may be lost. Additionally, SNS doesn't have a \"queue length\" metric like SQS does.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon SQS to handle messaging between EC2 servers in an Auto Scaling group and use CloudWatch to monitor SQS queue length and scale up when communication failures are detected is partially correct but less operationally efficient than the serverless approach. While SQS can help prevent transaction drops, using EC2 instances requires managing servers, Auto Scaling groups, and scaling policies. The serverless Lambda + API Gateway approach eliminates server management entirely and provides better operational efficiency.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company receives 10 TB of instrumentation data each day from several machines located at a \nsingle factory. The data consists of JSON files stored on a storage area network (SAN) in an on-\npremises data center located within the factory. The company wants to send this data to Amazon \nS3 where it can be accessed by several additional systems that provide critical near-real-lime \nanalytics.  \nA secure transfer is important because the data is considered sensitive.  \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n11 \nWhich solution offers the MOST reliable data transfer?",
            options: [
                { id: 0, text: "AWS DataSync over public internet", correct: false },
                { id: 1, text: "AWS DataSync over AWS Direct Connect", correct: true },
                { id: 2, text: "AWS Database Migration Service (AWS DMS) over public internet", correct: false },
                { id: 3, text: "AWS Database Migration Service (AWS DMS) over AWS Direct Connect", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DataSync over AWS Direct Connect provides the most reliable data transfer solution for moving large volumes of data from on-premises to S3. DataSync is specifically designed for data migration and can rapidly move active datasets over the network into Amazon S3, Amazon EFS, or FSx for Windows File Server. DataSync includes automatic encryption and data integrity validation to ensure data arrives securely, intact, and ready to use. Using Direct Connect provides a dedicated network connection with consistent bandwidth and low latency, which is essential for transferring 10 TB of data daily. Direct Connect also provides more reliable connectivity than public internet, reducing the risk of transfer failures. The combination of DataSync's optimized transfer protocol and Direct Connect's dedicated connection ensures the most reliable transfer for critical near-real-time analytics requirements.\n\n**Why option 0 is incorrect:**\nThe option that says AWS DataSync over public internet is incorrect because while DataSync provides encryption and integrity validation, using the public internet introduces variability in bandwidth, latency, and reliability. For 10 TB of daily transfers of sensitive data requiring critical near-real-time analytics, the public internet may experience congestion, packet loss, or outages that could interrupt transfers. Direct Connect provides a dedicated connection that is more reliable and predictable for large-scale data transfers.\n\n**Why option 2 is incorrect:**\nThe option that says AWS Database Migration Service (AWS DMS) over public internet is incorrect because DMS is designed for migrating databases, not for transferring files from a SAN. The scenario involves JSON files stored on a SAN, not a database. DMS would not be the appropriate service for this use case. Additionally, using public internet has the same reliability concerns as option 0.\n\n**Why option 3 is incorrect:**\nThe option that says AWS Database Migration Service (AWS DMS) over AWS Direct Connect is incorrect because DMS is designed for database migration, not for file transfer from SAN storage. While Direct Connect provides reliable connectivity, DMS is not the right tool for transferring JSON files from a SAN to S3. DataSync is specifically designed for file-based data migration scenarios like this one.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A company needs to configure a real-time data ingestion architecture for its application. The \ncompany needs an API, a process that transforms data as the data is streamed, and a storage \nsolution for the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data", correct: false },
                { id: 1, text: "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue.", correct: false },
                { id: 2, text: "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream.", correct: true },
                { id: 3, text: "Configure an Amazon API Gateway API to send data to AWS Glue.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nConfiguring an Amazon API Gateway API to send data to an Amazon Kinesis data stream, creating a Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source, using AWS Lambda functions to transform the data, and using the Kinesis Data Firehose delivery stream to send the data to Amazon S3 provides a fully serverless, managed solution with minimal operational overhead. API Gateway provides the API layer without managing servers. Kinesis Data Streams provides real-time data streaming capabilities. Lambda functions can transform data as it streams through, and Kinesis Data Firehose automatically delivers the transformed data to S3. All components are fully managed services that scale automatically and require no infrastructure management.\n\n**Why option 0 is incorrect:**\nThe option that says deploy an Amazon EC2 instance to host an API that sends data to a Kinesis data stream, then use Firehose and Lambda is incorrect because deploying EC2 instances requires managing servers, operating systems, scaling, patching, and monitoring, which adds significant operational overhead. The serverless API Gateway approach eliminates all server management requirements.\n\n**Why option 1 is incorrect:**\nThe option that says deploy an Amazon EC2 instance to host an API that sends data to AWS Glue is incorrect because AWS Glue is an ETL (Extract, Transform, Load) service designed for batch processing and data cataloging, not for real-time streaming data ingestion. Glue is not suitable for real-time streaming scenarios. Additionally, using EC2 instances adds operational overhead for server management.\n\n**Why option 3 is incorrect:**\nThe option that says configure an Amazon API Gateway API to send data to AWS Glue, use Lambda functions to transform data, and use Glue to send data to S3 is incorrect because AWS Glue is designed for batch ETL jobs, not for real-time streaming data ingestion. Glue cannot handle real-time data streams like Kinesis can. The architecture described would not support real-time data ingestion requirements.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A company needs to keep user transaction data in an Amazon DynamoDB table. \nThe company must retain the data for 7 years. \nWhat is the MOST operationally efficient solution that meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n12",
            options: [
                { id: 0, text: "Use DynamoDB point-in-time recovery to back up the table continuously.", correct: false },
                { id: 1, text: "Use AWS Backup to create backup schedules and retention policies for the table.", correct: true },
                { id: 2, text: "Create an on-demand backup of the table by using the DynamoDB console.", correct: false },
                { id: 3, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing AWS Backup to create backup schedules and retention policies for the DynamoDB table is the most operationally efficient solution for long-term data retention. AWS Backup is a fully managed, centralized backup service that can automatically back up DynamoDB tables according to a schedule you define. It supports retention policies that can automatically delete backups after a specified retention period (up to the required 7 years), eliminating the need for manual backup management. AWS Backup provides a single interface for managing backups across multiple AWS services, simplifies compliance reporting, and handles all backup lifecycle management automatically. This solution requires minimal operational effort compared to manual backup processes.\n\n**Why option 0 is incorrect:**\nThe option that says use DynamoDB point-in-time recovery (PITR) to back up the table continuously is incorrect because PITR only retains backups for 35 days, which does not meet the 7-year retention requirement. PITR is designed for short-term recovery scenarios, not for long-term data retention. While PITR is useful for recovering from accidental deletions or corruption, it cannot be used to meet regulatory or compliance requirements for 7-year data retention.\n\n**Why option 2 is incorrect:**\nThe option that says create an on-demand backup using the DynamoDB console, store it in S3, and set an S3 Lifecycle configuration is incorrect because this approach requires manual intervention to create backups, which is not operationally efficient. On-demand backups must be created manually each time, and managing backup schedules, retention, and cleanup would require custom automation or manual processes. While this could work, it requires significantly more operational overhead than AWS Backup's automated approach.\n\n**Why option 3 is incorrect:**\nThe option that says create an EventBridge rule to invoke a Lambda function that backs up the table and stores it in S3, then set an S3 Lifecycle configuration is incorrect because this approach requires writing and maintaining custom Lambda code, managing EventBridge rules, and handling error scenarios manually. This adds significant operational overhead compared to AWS Backup, which handles all of this automatically. Additionally, you would need to implement your own retention logic in the Lambda function, whereas AWS Backup provides built-in retention policies.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company is planning to use an Amazon DynamoDB table for data storage. The company is \nconcerned about cost optimization. The table will not be used on most mornings. In the evenings, \nthe read and write traffic will often be unpredictable. When traffic spikes occur, they will happen \nvery quickly. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Create a DynamoDB table in on-demand capacity mode.", correct: true },
                { id: 1, text: "Create a DynamoDB table with a global secondary index.", correct: false },
                { id: 2, text: "Create a DynamoDB table with provisioned capacity and auto scaling.", correct: false },
                { id: 3, text: "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreating a DynamoDB table in on-demand capacity mode is the best recommendation for this scenario. On-demand mode automatically scales to accommodate your workload's traffic without requiring capacity planning or manual scaling configuration. It's ideal when you have unpredictable application traffic, as described in the scenario where traffic spikes happen very quickly. On-demand mode charges you only for the read and write requests you make, making it cost-effective for workloads with variable or unpredictable traffic patterns. Since the table is not used most mornings and has unpredictable evening traffic, on-demand mode eliminates the cost of paying for provisioned capacity that sits idle, while automatically handling rapid traffic spikes without any configuration changes.\n\n**Why option 1 is incorrect:**\nThe option that says create a DynamoDB table with a global secondary index (GSI) is incorrect because GSIs are used for querying data by different attributes, not for handling unpredictable traffic or cost optimization. GSIs don't address the capacity mode question or help with traffic spikes. The question is about choosing the right capacity mode for cost optimization and unpredictable traffic, not about indexing strategies.\n\n**Why option 2 is incorrect:**\nThe option that says create a DynamoDB table with provisioned capacity and auto scaling is incorrect because while auto scaling can adjust capacity based on traffic, it requires time to scale up (typically minutes), which may not be fast enough for traffic spikes that happen \"very quickly\" as described. Additionally, provisioned capacity with auto scaling requires capacity planning and may result in paying for minimum capacity even during idle periods (mornings). On-demand mode provides instant scaling without any delay.\n\n**Why option 3 is incorrect:**\nThe option that says create a DynamoDB table in provisioned capacity mode and configure it as a global table is incorrect because global tables are used for multi-region replication and high availability, not for handling unpredictable traffic or cost optimization. Provisioned capacity mode requires capacity planning and doesn't automatically scale instantly like on-demand mode. Global tables add complexity and cost without addressing the core requirements of unpredictable traffic and cost optimization.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for \nhelp with an application migration initiative. A solutions architect needs to share an Amazon \nMachine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The \nAMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses a customer managed \ncustomer master key (CMK) to encrypt EBS volume snapshots. \nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's \nAWS account?",
            options: [
                { id: 0, text: "Make the encrypted AMI and snapshots publicly available.", correct: false },
                { id: 1, text: "Modify the launchPermission property of the AMI.", correct: true },
                { id: 2, text: "Modify the launchPermission property of the AMI.", correct: false },
                { id: 3, text: "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nModifying the launchPermission property of the AMI to share it with the MSP Partner's AWS account only, and modifying the CMK's key policy to allow the MSP Partner's AWS account to use the key, is the most secure way to share an encrypted AMI. When an AMI is backed by EBS volumes encrypted with a customer-managed CMK, both the AMI and the underlying snapshots must be shared, and the CMK must be accessible to the target account. By modifying the AMI's launch permissions to share only with the specific MSP Partner account (not making it public), you maintain security through least-privilege access. Sharing the existing CMK with the MSP Partner account allows them to decrypt and use the AMI, as the snapshots are already encrypted with this key. This approach is more secure than creating new keys or making resources publicly available.\n\n**Why option 0 is incorrect:**\nThe option that says make the encrypted AMI and snapshots publicly available and modify the CMK's key policy to allow the MSP Partner's account to use the key is incorrect because making AMIs and snapshots publicly available exposes them to anyone on the internet, which is a significant security risk. Even though the data is encrypted, making resources public violates security best practices and the principle of least privilege. The AMI should only be shared with the specific MSP Partner account, not made publicly available.\n\n**Why option 2 is incorrect:**\nThe option that says modify the launchPermission property of the AMI to share it with the MSP Partner's account and modify the CMK's key policy to trust a new CMK owned by the MSP Partner is incorrect because you cannot simply \"trust a new CMK\" to decrypt data encrypted with a different CMK. The AMI snapshots are encrypted with the existing CMK, so the MSP Partner needs access to that same CMK to decrypt and use the AMI. Creating a new CMK would require re-encrypting the snapshots, which is not necessary and adds complexity.\n\n**Why option 3 is incorrect:**\nThe option that says export the AMI from the source account to an S3 bucket in the MSP Partner's account and encrypt the S3 bucket with a CMK owned by the MSP Partner is incorrect because exporting AMIs to S3 is a complex, time-consuming process that requires significant storage and transfer costs. Additionally, the exported AMI would still need to be imported and launched in the target account, and the original encryption key issue would still need to be resolved. The direct AMI sharing approach is simpler, faster, and more secure.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A solutions architect is designing the cloud architecture for a new application being deployed on \nAWS. The process should run in parallel while adding and removing application nodes as needed \nbased on the number of jobs to be processed. The processor application is stateless. The \nsolutions architect must ensure that the application is loosely coupled and the job items are \ndurably stored. \n \nWhich design should the solutions architect use?",
            options: [
                { id: 0, text: "Create an Amazon SNS topic to send the jobs that need to be processed.", correct: false },
                { id: 1, text: "Create an Amazon SQS queue to hold the jobs that need to be processed.", correct: false },
                { id: 2, text: "Create an Amazon SQS queue to hold the jobs that needs to be processed.", correct: true },
                { id: 3, text: "Create an Amazon SNS topic to send the jobs that need to be processed.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCreating an Amazon SQS queue to hold the jobs that need to be processed, creating an AMI with the processor application, creating a launch template using the AMI, creating an Auto Scaling group using the launch template, and setting the scaling policy to add and remove nodes based on the number of items in the SQS queue is the correct design. SQS provides durable storage for job items, ensuring they are not lost even if processing nodes fail. SQS decouples the job producers from the processors, creating a loosely coupled architecture. The Auto Scaling group can scale based on the ApproximateNumberOfMessagesVisible metric from SQS, automatically adding instances when the queue has many messages and removing them when the queue is empty. This ensures parallel processing capability while dynamically adjusting capacity based on workload. Launch templates are preferred over launch configurations as they support versioning and more features.\n\n**Why option 0 is incorrect:**\nThe option that says create an Amazon SNS topic to send jobs, create an AMI, launch configuration, Auto Scaling group, and scale based on CPU usage is incorrect because SNS is a pub/sub messaging service that delivers messages immediately to all subscribers. SNS does not provide durable storage or queuing - if a subscriber is not available, the message may be lost. Additionally, scaling based on CPU usage may not accurately reflect the number of jobs waiting to be processed, especially if jobs are CPU-intensive but arrive sporadically. SQS provides better durability and job queuing for this use case.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon SQS queue, AMI, launch configuration, Auto Scaling group, and scale based on network usage is incorrect because while SQS is correct for durable job storage, scaling based on network usage is not an appropriate metric for determining when to add or remove processing nodes. Network usage doesn't directly correlate with the number of jobs in the queue. The scaling policy should be based on the SQS queue depth (ApproximateNumberOfMessagesVisible) to accurately reflect workload and ensure nodes are added when jobs are waiting.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon SNS topic to send jobs, create an AMI, launch template, Auto Scaling group, and scale based on the number of messages published to the SNS topic is incorrect because SNS does not provide durable storage or queuing capabilities. SNS delivers messages immediately and doesn't maintain a queue of unprocessed jobs. Additionally, the number of messages published to SNS doesn't indicate how many jobs are waiting to be processed, as SNS messages are delivered immediately and don't accumulate like SQS messages do.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load \nBalancers to use certificate that are imported into AWS Certificate Manager (ACM). The \ncompany's security team must be notified 30 days before the expiration of each certificate.  \nWhat should a solutions architect recommend to meet the requirement?",
            options: [
                { id: 0, text: "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service", correct: false },
                { id: 1, text: "Create an AWS Config rule that checks for certificates that will expire within 30 days.", correct: true },
                { id: 2, text: "Use AWS trusted Advisor to check for certificates that will expire within to days.", correct: false },
                { id: 3, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCreating an AWS Config rule that checks for certificates that will expire within 30 days, and configuring Amazon EventBridge (CloudWatch Events) to invoke a custom alert via Amazon SNS when AWS Config reports a noncompliant resource, is the recommended approach for monitoring ACM certificate expiration. AWS Config can continuously monitor ACM certificates and evaluate them against rules you define. When a certificate is found to be expiring within 30 days, Config marks it as noncompliant. EventBridge can detect Config compliance changes and trigger SNS notifications to alert the security team. This solution provides automated, continuous monitoring without requiring manual checks or custom Lambda functions for certificate evaluation.\n\n**Why option 0 is incorrect:**\nThe option that says add a rule in ACM to publish a custom message to an SNS topic every day beginning 30 days before any certificate will expire is incorrect because ACM does not have a built-in feature to publish messages to SNS based on certificate expiration dates. ACM manages certificate provisioning and renewal for AWS-issued certificates, but it does not provide notification capabilities for imported certificates. You cannot configure ACM rules to send notifications.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Trusted Advisor to check for certificates expiring within 30 days and create a CloudWatch alarm based on Trusted Advisor metrics is incorrect because Trusted Advisor is a best practices checking service that provides recommendations, but it does not provide real-time monitoring or CloudWatch metrics for certificate expiration. Trusted Advisor checks are performed periodically and may not meet the requirement for timely notifications. Additionally, Trusted Advisor does not expose metrics that can be used in CloudWatch alarms for this purpose.\n\n**Why option 3 is incorrect:**\nThe option that says create an EventBridge rule to detect certificates expiring within 30 days, configure it to invoke a Lambda function, and configure the Lambda function to send an SNS alert is incorrect because EventBridge does not have built-in events for ACM certificate expiration. EventBridge rules are triggered by AWS service events, but ACM does not emit events for certificate expiration dates. You would need to write a Lambda function that periodically checks certificate expiration dates, which adds operational overhead compared to using AWS Config's built-in certificate monitoring capabilities.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "A company's dynamic website is hosted using on-premises servers in the United States. The \ncompany is launching its product in Europe, and it wants to optimize site loading times for new \nEuropean users. The site's backend must remain in the United States. The product is being \nlaunched in a few days, and an immediate solution is needed. \n \nWhat should the solutions architect recommend?",
            options: [
                { id: 0, text: "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.", correct: false },
                { id: 1, text: "Move the website to Amazon S3. Use cross-Region replication between Regions.", correct: false },
                { id: 2, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.", correct: true },
                { id: 3, text: "Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing Amazon CloudFront with a custom origin pointing to the on-premises servers is the best solution for optimizing site loading times for European users while keeping the backend in the United States. CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, including locations in Europe. When European users request content, CloudFront serves it from the nearest edge location, significantly reducing latency. For dynamic content, CloudFront can still route requests to the on-premises origin in the United States, but it can cache static assets at edge locations. CloudFront supports custom origins, allowing you to point to on-premises servers via the internet. This solution can be implemented quickly (within days) without requiring infrastructure migration, making it ideal for the immediate solution requirement.\n\n**Why option 0 is incorrect:**\nThe option that says launch an Amazon EC2 instance in us-east-1 and migrate the site to it is incorrect because this would still keep the site in the United States, not optimizing it for European users. Additionally, migrating a website to AWS would take significantly more time than \"a few days\" and would require application changes, data migration, and testing. This approach doesn't address the latency optimization requirement for European users.\n\n**Why option 1 is incorrect:**\nThe option that says move the website to Amazon S3 and use cross-Region replication between Regions is incorrect because S3 is designed for static website hosting, not dynamic websites. The scenario mentions a \"dynamic website,\" which typically requires server-side processing, databases, and application logic that S3 cannot provide. Additionally, cross-Region replication copies objects between S3 buckets in different regions but doesn't automatically route users to the nearest region - you would still need CloudFront or Route 53 for that.\n\n**Why option 3 is incorrect:**\nThe option that says use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers is incorrect because Route 53 DNS routing can direct users to different endpoints based on location, but it cannot reduce latency for users accessing on-premises servers in the United States from Europe. The physical distance and network latency would remain the same. Route 53 routing policies are useful when you have multiple endpoints in different locations, but here there's only one on-premises location in the United States. CloudFront's edge caching is what actually reduces latency by serving content from locations closer to users.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A company wants to reduce the cost of its existing three-tier web architecture. The web, \napplication, and database servers are running on Amazon EC2 instances for the development, \ntest, and production environments. The EC2 instances average 30% CPU utilization during peak \nhours and 10% CPU utilization during non-peak hours. \n \nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run \nfor at least 8 hours each day. The company plans to implement automation to stop the \ndevelopment and test EC2 instances when they are not in use. \n \nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-\neffectively?",
            options: [
                { id: 0, text: "Use Spot Instances for the production EC2 instances.", correct: false },
                { id: 1, text: "Use Reserved Instances for the production EC2 instances.", correct: true },
                { id: 2, text: "Use Spot blocks for the production EC2 instances.", correct: false },
                { id: 3, text: "Use On-Demand Instances for the production EC2 instances.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Reserved Instances for the production EC2 instances and On-Demand Instances for the development and test EC2 instances is the most cost-effective solution. Reserved Instances provide significant cost savings (up to 72% compared to On-Demand) for predictable workloads that run 24/7, which matches the production environment. Since production instances run continuously, committing to Reserved Instances makes financial sense. For development and test instances that run only 8 hours per day and can be stopped when not in use, On-Demand Instances are appropriate because you only pay when they're running. The automation to stop dev/test instances when not in use further optimizes costs. This combination maximizes savings on production while minimizing costs for non-production environments.\n\n**Why option 0 is incorrect:**\nThe option that says use Spot Instances for production EC2 instances and Reserved Instances for dev/test instances is incorrect because Spot Instances can be interrupted by AWS with only a 2-minute notice, making them unsuitable for production workloads that require 24/7 availability. Production environments need guaranteed capacity and cannot tolerate interruptions. Additionally, Spot Instances are not recommended for critical production workloads due to their unpredictable availability.\n\n**Why option 2 is incorrect:**\nThe option that says use Spot blocks for production EC2 instances and Reserved Instances for dev/test instances is incorrect because Spot blocks are no longer available (AWS discontinued Spot blocks). Even when they were available, Spot instances (including Spot blocks) are not suitable for production workloads that require continuous availability, as they can still be interrupted.\n\n**Why option 3 is incorrect:**\nThe option that says use On-Demand Instances for production and Spot blocks for dev/test instances is incorrect because On-Demand Instances are the most expensive option and don't provide cost savings for predictable 24/7 workloads. Reserved Instances would provide significant savings for production. Additionally, Spot blocks are no longer available, and Spot instances are not ideal for dev/test environments that need to run during specific hours, as availability is not guaranteed.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 25,
            text: "A company has a production web application in which users upload documents through a web \ninterlace or a mobile app. \n According to a new regulatory requirement, new documents cannot be modified or deleted after \nthey are stored. \nWhat should a solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock", correct: true },
                { id: 1, text: "Store the uploaded documents in an Amazon S3 bucket.", correct: false },
                { id: 2, text: "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled.", correct: false },
                { id: 3, text: "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nStoring the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled is the correct solution to meet the regulatory requirement that documents cannot be modified or deleted after they are stored. S3 Object Lock provides a write-once-read-many (WORM) model that prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock supports two modes: Governance mode (allows some users with special permissions to delete objects) and Compliance mode (prevents all users from deleting objects, even root users). S3 Versioning is required and automatically activated when Object Lock is enabled, ensuring that all versions of objects are preserved. This combination provides the strongest protection against modification or deletion, meeting strict regulatory requirements.\n\n**Why option 1 is incorrect:**\nThe option that says store the uploaded documents in an Amazon S3 bucket and configure an S3 Lifecycle policy to archive the documents periodically is incorrect because lifecycle policies can transition objects to different storage classes or delete them, but they do not prevent modification or deletion of objects. Lifecycle policies are used for cost optimization through storage class transitions, not for enforcing immutability. Objects can still be deleted or overwritten before the lifecycle policy acts on them.\n\n**Why option 2 is incorrect:**\nThe option that says store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled and configure an ACL to restrict all access to read-only is incorrect because ACLs control who can access objects, but they cannot prevent authorized users (including the bucket owner) from deleting objects. Versioning preserves previous versions when objects are overwritten, but it doesn't prevent deletion of current or previous versions. Object Lock is specifically designed to prevent deletion and overwriting, which ACLs cannot do.\n\n**Why option 3 is incorrect:**\nThe option that says store the uploaded documents on an Amazon Elastic File System (EFS) volume and access the data by mounting the volume in read-only mode is incorrect because mounting a volume in read-only mode only prevents modification from that specific mount point, but the volume itself can still be modified or deleted by users with appropriate permissions. EFS does not provide the same level of immutability protection as S3 Object Lock. Additionally, EFS is designed for shared file storage with multiple concurrent access, not for document storage with regulatory compliance requirements.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A company has several web servers that need to frequently access a common Amazon RDS \nMySQL Multi-AZ DB instance. The company wants a secure method for the web servers to \nconnect to the database while meeting a security requirement to rotate user credentials \nfrequently. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Store the database user credentials in AWS Secrets Manager.", correct: true },
                { id: 1, text: "Store the database user credentials in AWS Systems Manager OpsCenter.", correct: false },
                { id: 2, text: "Store the database user credentials in a secure Amazon S3 bucket.", correct: false },
                { id: 3, text: "Store the database user credentials in files encrypted with AWS Key Management Service", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nStoring the database user credentials in AWS Secrets Manager and granting the necessary IAM permissions to allow the web servers to access Secrets Manager is the best solution. Secrets Manager enables you to replace hardcoded credentials in your code with an API call to retrieve secrets programmatically, ensuring secrets aren't exposed in code. Most importantly, Secrets Manager supports automatic rotation of secrets according to a schedule you define, which meets the security requirement to rotate credentials frequently. Secrets Manager can automatically rotate RDS database credentials by creating new credentials, updating the database, and updating the secret. The web servers can retrieve the latest credentials from Secrets Manager whenever needed. This solution provides both secure credential storage and automated rotation with minimal operational overhead.\n\n**Why option 1 is incorrect:**\nThe option that says store the database user credentials in AWS Systems Manager OpsCenter and grant IAM permissions to access OpsCenter is incorrect because OpsCenter is part of AWS Systems Manager and is designed for operational insights and troubleshooting, not for secrets management. OpsCenter does not provide automatic credential rotation capabilities like Secrets Manager does. While Systems Manager Parameter Store can store encrypted parameters, it doesn't support automatic rotation for database credentials.\n\n**Why option 2 is incorrect:**\nThe option that says store the database user credentials in a secure Amazon S3 bucket and grant IAM permissions to retrieve credentials is incorrect because while S3 can store encrypted objects, it does not provide automatic credential rotation capabilities. You would need to manually rotate credentials and update the S3 object, which doesn't meet the requirement for frequent rotation. Additionally, managing credentials in S3 requires custom application logic to retrieve and use them, whereas Secrets Manager provides a dedicated API and automatic rotation.\n\n**Why option 3 is incorrect:**\nThe option that says store the database user credentials in files encrypted with AWS KMS on the web server file system is incorrect because storing credentials on the file system (even if encrypted) creates security risks if the server is compromised. Additionally, this approach requires manual credential rotation and updating files on each web server, which is operationally complex and doesn't scale well. Secrets Manager provides centralized secret management and automatic rotation, eliminating the need to manage credentials on individual servers.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company hosts an application on AWS Lambda functions mat are invoked by an Amazon API \nGateway API. The Lambda functions save customer data to an Amazon Aurora MySQL \ndatabase. Whenever the company upgrades the database, the Lambda functions fail to establish \ndatabase connections until the upgrade is complete. The result is that customer data Is not \nrecorded for some of the event. \nA solutions architect needs to design a solution that stores customer data that is created during \ndatabase upgrades. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Provision an Amazon RDS proxy to sit between the Lambda functions and the database.", correct: false },
                { id: 1, text: "Increase the run time of me Lambda functions to the maximum.", correct: false },
                { id: 2, text: "Persist the customer data to Lambda local storage.", correct: false },
                { id: 3, text: "Store the customer data in an Amazon Simple Queue Service (Amazon SOS) FIFO queue.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nStoring the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue and creating a new Lambda function that polls the queue and stores the customer data in the database is the correct solution. SQS provides durable message storage, ensuring that customer data is not lost even if the Lambda function fails or if the database is unavailable during upgrades. When the database is upgraded and connections fail, the Lambda functions can still successfully write customer data to the SQS queue. Once the database upgrade is complete and connections are restored, the separate Lambda function can process the queued messages and store them in the database. This decouples the data ingestion from database availability, ensuring no data loss during maintenance windows. FIFO queues ensure message ordering, which may be important for customer data.\n\n**Why option 0 is incorrect:**\nThe option that says provision an Amazon RDS Proxy to sit between the Lambda functions and the database and configure Lambda functions to connect to the RDS proxy is incorrect because RDS Proxy is designed for connection pooling and management, not for storing data during database unavailability. While RDS Proxy can help manage database connections and provide some resilience, it cannot store customer data when the database itself is unavailable during upgrades. If the database is down for maintenance, RDS Proxy cannot help store the data.\n\n**Why option 1 is incorrect:**\nThe option that says increase the run time of the Lambda functions to the maximum and create a retry mechanism in the code to store customer data in the database is incorrect because retry mechanisms will fail if the database is completely unavailable during upgrades. Increasing Lambda timeout doesn't solve the fundamental problem that the database cannot accept connections during maintenance. Retries will continue to fail until the database upgrade is complete, potentially causing Lambda functions to timeout and lose customer data.\n\n**Why option 2 is incorrect:**\nThe option that says persist the customer data to Lambda local storage and configure new Lambda functions to scan the local storage to save the data to the database is incorrect because Lambda local storage (/tmp) is ephemeral and is not shared between Lambda invocations or different Lambda functions. Each Lambda invocation gets a fresh /tmp directory, so data stored there would be lost when the function completes. Additionally, Lambda functions are stateless and cannot reliably share local storage between different function executions.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A survey company has gathered data for several years from areas m\\ the United States. The \ncompany hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company \nhas started to share the data with a European marketing firm that has S3 buckets. The company \nwants to ensure that its data transfer costs remain as low as possible. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the Requester Pays feature on the company's S3 bucket", correct: true },
                { id: 1, text: "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing", correct: false },
                { id: 2, text: "Configure cross-account access for the marketing firm so that the marketing firm has access to", correct: false },
                { id: 3, text: "Configure the company's S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nConfiguring the Requester Pays feature on the company's S3 bucket is the most cost-effective solution for sharing data with the European marketing firm. When Requester Pays is enabled, the requester (the marketing firm) pays for the data transfer costs and requests instead of the bucket owner. This is ideal for sharing large datasets (3 TB and growing) where the data owner wants to share data without incurring the costs of others accessing it. The marketing firm will pay for the data transfer costs when they download data from the US-based S3 bucket to their European S3 buckets, keeping the company's data transfer costs at zero. This is commonly used for sharing large datasets, reference data, or public data where the data consumer should bear the transfer costs.\n\n**Why option 1 is incorrect:**\nThe option that says configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets is incorrect because Cross-Region Replication incurs data transfer costs that are charged to the bucket owner (the company), not the requester. Additionally, Cross-Region Replication automatically replicates all objects, which may not be desired if the company only wants to share specific data. The company would incur ongoing replication costs for the 3 TB+ dataset, which doesn't meet the requirement to keep data transfer costs as low as possible.\n\n**Why option 2 is incorrect:**\nThe option that says configure cross-account access for the marketing firm so they have access to the company's S3 bucket is incorrect because while this allows the marketing firm to access the data, the company (bucket owner) would still incur all data transfer costs when the marketing firm downloads data. Cross-account access controls who can access the bucket, but doesn't change who pays for data transfer. The company would still pay for all data transfer costs.\n\n**Why option 3 is incorrect:**\nThe option that says configure the company's S3 bucket to use S3 Intelligent-Tiering and sync the S3 bucket to one of the marketing firm's S3 buckets is incorrect because S3 Intelligent-Tiering optimizes storage costs by automatically moving objects between access tiers, but it doesn't reduce data transfer costs. Additionally, \"syncing\" buckets would require data transfer, and the company would incur those costs. Intelligent-Tiering doesn't address the data transfer cost issue when sharing data with external parties.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 29,
            text: "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses \nbucket policies to restrict access to audit team IAM user credentials according to the principle of \nleast privilege. Company managers are worried about accidental deletion of documents in the S3 \nbucket and want a more secure solution. \n \nWhat should a solutions architect do to secure the audit documents?",
            options: [
                { id: 0, text: "Enable the versioning and MFA Delete features on the S3 bucket.", correct: true },
                { id: 1, text: "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM", correct: false },
                { id: 2, text: "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject", correct: false },
                { id: 3, text: "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nEnabling versioning and MFA Delete features on the S3 bucket provides comprehensive protection against accidental deletion of audit documents. S3 Versioning preserves all versions of objects, so if a document is deleted or overwritten, previous versions are retained and can be restored. MFA Delete adds an additional security layer by requiring multi-factor authentication (MFA) before any delete operation can be performed on versioned objects. This means that even if someone with delete permissions accidentally attempts to delete a document, they must provide an MFA code, significantly reducing the risk of accidental deletion. This combination provides both recovery capability (versioning) and prevention (MFA Delete), making it the most secure solution for protecting confidential audit documents.\n\n**Why option 1 is incorrect:**\nThe option that says enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account is incorrect because IAM MFA protects IAM user access to AWS services, but it doesn't prevent deletion of S3 objects once the user is authenticated. IAM MFA ensures that only authorized users can access AWS, but it doesn't add an extra layer of protection specifically for S3 delete operations like MFA Delete does. MFA Delete is a bucket-level feature that requires MFA for delete operations, which is more specific to the requirement.\n\n**Why option 2 is incorrect:**\nThe option that says add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates is incorrect because S3 Lifecycle policies are used for managing object transitions between storage classes and expiration, not for access control. You cannot use lifecycle policies to deny IAM actions. Access control for S3 operations is managed through bucket policies, IAM policies, or ACLs, not lifecycle policies. Additionally, restricting deletion only during audit dates doesn't provide comprehensive protection.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key is incorrect because encryption protects data confidentiality, not data deletion. If audit team members have s3:DeleteObject permissions, they can still delete encrypted objects even if they don't have KMS key access (the objects would just be deleted in encrypted form). Encryption doesn't prevent deletion - it only protects data at rest. To prevent deletion, you need versioning and MFA Delete, not just encryption.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Modify the DB instance to be a Multi-AZ deployment", correct: false },
                { id: 1, text: "Create a read replica of the database.", correct: true },
                { id: 2, text: "Instruct the development team to manually export the entries in the database at the end of each", correct: false },
                { id: 3, text: "Use Amazon ElastiCache to cache the common queries that the script runs against the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCreating a read replica of the database and configuring the script to query only the read replica is the best solution with the least operational overhead. Read replicas are asynchronously replicated copies of the primary database that can handle read traffic, offloading read queries from the primary instance. By directing the script's queries to the read replica, the development team's queries on the primary database won't be impacted by the script's workload. Read replicas are fully managed by RDS, require minimal configuration, and automatically replicate data from the primary instance. This solution isolates the reporting script's workload from the development workload without requiring code changes to the development application.\n\n**Why option 0 is incorrect:**\nThe option that says modify the DB instance to be a Multi-AZ deployment is incorrect because Multi-AZ deployments provide high availability and failover capabilities, but they don't improve performance or separate read workloads. In a Multi-AZ deployment, there's still only one primary instance handling all read and write traffic. The standby replica in Multi-AZ is only used for failover, not for serving read traffic. This wouldn't solve the performance issue caused by the script's queries competing with development queries.\n\n**Why option 2 is incorrect:**\nThe option that says instruct the development team to manually export the entries in the database at the end of each day is incorrect because this approach requires manual work, doesn't solve the performance issue during the day when the script runs, and doesn't meet the requirement that the script must report a final total during business hours. Manual exports add operational overhead and don't address the root cause of performance degradation.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon ElastiCache to cache the common queries that the script runs against the database is incorrect because ElastiCache is designed for caching frequently accessed, relatively static data. The script is looking for new movies added to the database, which means it's querying for data that changes frequently. Caching wouldn't be effective for this use case since the script needs current data, not cached results. Additionally, ElastiCache requires application code changes to implement caching logic, adding operational overhead.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications \nneeds to call the Amazon S3 API to store and read objects. According to the company's security \nregulations, no traffic from the applications is allowed to travel across the internet. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an S3 interface endpoint.", correct: false },
                { id: 1, text: "Configure an S3 gateway endpoint.", correct: true },
                { id: 2, text: "Create an S3 bucket in a private subnet.", correct: false },
                { id: 3, text: "Create an S3 bucket in the same Region as the EC2 instance.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nConfiguring an S3 gateway endpoint within the VPC is the correct solution for allowing EC2 instances to access S3 without internet connectivity. Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect. Traffic between your VPC and S3 stays within the AWS network and never traverses the public internet. Gateway endpoints are free to use (you only pay for data transfer) and are automatically scaled and highly available. They route traffic to S3 using prefix lists in your route tables, ensuring all S3 traffic stays on the AWS backbone network.\n\n**Why option 0 is incorrect:**\nThe option that says configure an S3 interface endpoint is incorrect because S3 does not support interface endpoints (PrivateLink). Interface endpoints are available for many AWS services, but S3 and DynamoDB use gateway endpoints instead. Interface endpoints use ENIs (Elastic Network Interfaces) in your subnets, while gateway endpoints are virtual devices that are added to your route tables.\n\n**Why option 2 is incorrect:**\nThe option that says create an S3 bucket in a private subnet is incorrect because S3 is a regional service that doesn't run in subnets. S3 buckets are not deployed in VPCs or subnets - they exist at the AWS account level in a specific region. You cannot place an S3 bucket in a subnet. S3 access is controlled through VPC endpoints, IAM policies, and bucket policies, not through subnet placement.\n\n**Why option 3 is incorrect:**\nThe option that says create an S3 bucket in the same Region as the EC2 instance is incorrect because while being in the same region reduces latency and data transfer costs, it doesn't prevent traffic from traversing the internet. Without a VPC endpoint, EC2 instances in private subnets would need a NAT gateway to reach S3, which routes traffic through the internet. The requirement is that no traffic should travel across the internet, which requires a VPC endpoint regardless of region.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A company is storing sensitive user information in an Amazon S3 bucket. The company wants to \nprovide secure access to this bucket from the application tier running on Amazon EC2 instances \ninside a VPC. \nWhich combination of steps should a solutions architect take to accomplish this? (Choose two.)",
            options: [
                { id: 0, text: "Configure a VPC gateway endpoint for Amazon S3 within the VPC", correct: true },
                { id: 1, text: "Create a bucket policy to make the objects to the S3 bucket public", correct: false },
                { id: 2, text: "Create a bucket policy that limits access to only the application tier running in the VPC", correct: false },
                { id: 3, text: "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance", correct: false },
                { id: 4, text: "Create a NAT instance and have the EC2 instances use the NAT instance to access the S3", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nConfiguring a VPC gateway endpoint for Amazon S3 within the VPC provides private connectivity between EC2 instances and S3 without requiring internet gateway or NAT devices. This ensures that traffic between the VPC and S3 stays within the AWS network and never traverses the public internet, meeting security requirements for sensitive data access.\n\n**Why option 2 is correct:**\nCreating a bucket policy that limits access to only the application tier running in the VPC provides fine-grained access control. The bucket policy can use conditions like `aws:SourceVpc` or `aws:SourceIp` to restrict access to requests originating from the VPC or specific VPC endpoints. This ensures that only the EC2 instances in the VPC can access the sensitive user information, providing an additional layer of security beyond IAM permissions.\n\n**Why option 1 is incorrect:**\nThe option that says create a bucket policy to make the objects in the S3 bucket public is incorrect because making objects public would expose sensitive user information to anyone on the internet, which violates security best practices and likely compliance requirements. Public access should never be used for sensitive data.\n\n**Why option 3 is incorrect:**\nThe option that says create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance is incorrect because storing IAM credentials on EC2 instances is a security anti-pattern. Credentials stored on instances can be compromised, and managing credentials across multiple instances is operationally complex. Instead, EC2 instances should use IAM roles, which provide temporary credentials automatically and don't require credential management.\n\n**Why option 4 is incorrect:**\nThe option that says create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket is incorrect because NAT instances route traffic through the internet, which violates the security requirement. Additionally, NAT instances add unnecessary complexity and cost. VPC gateway endpoints provide private connectivity without requiring NAT devices.",
            domain: "Design Secure Architectures",
        },
        {
            id: 33,
            text: "A company runs an on-premises application that is powered by a MySQL database. The \ncompany is migrating the application to AWS to Increase the application's elasticity and \navailability. The current architecture shows heavy read activity on the database during times of \nnormal operation. Every 4 hours the company's development team pulls a full export of the \nproduction database to populate a database in the staging environment. During this period, users \nexperience unacceptable application latency. The development team is unable to use the staging \nenvironment until the procedure completes. \nA solutions architect must recommend replacement architecture that alleviates the application \nlatency issue.  \nThe replacement architecture also must give the development team the ability to continue using \nthe staging environment without delay. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.", correct: false },
                { id: 1, text: "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.", correct: true },
                { id: 2, text: "Use Amazon RDS for MySQL with a Mufti AZ deployment and read replicas for production.", correct: false },
                { id: 3, text: "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production and using database cloning to create the staging database on-demand is the best solution. Aurora Replicas can handle read traffic, offloading reads from the primary instance and reducing latency during normal operation. Most importantly, Aurora's database cloning feature creates a new database cluster that shares the same storage volume as the source database, allowing near-instantaneous creation of staging databases without copying data. This eliminates the 4-hour export process that causes application latency. The development team can create a fresh staging database clone on-demand without waiting, and the clone doesn't impact production performance since it uses copy-on-write technology. This solution provides both elasticity/availability for production and instant staging database creation.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production and populate the staging database using mysqldump utility is incorrect because mysqldump still requires exporting the entire database, which takes 4 hours and causes the same application latency issues. While Aurora Replicas help with read traffic, the mysqldump process still impacts the primary database during the export, and the development team still has to wait for the export to complete before using staging.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon RDS for MySQL with Multi-AZ deployment and read replicas, and use the standby instance for the staging database is incorrect because in a Multi-AZ deployment, the standby instance is only used for failover and cannot be used for read traffic or staging purposes. The standby instance is not accessible for queries - it's only activated during failover events. This approach doesn't solve the latency issue or provide staging database access.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon RDS for MySQL with Multi-AZ deployment and read replicas, and populate staging using mysqldump utility is incorrect because this still requires the 4-hour export process that causes application latency. While read replicas can help with read traffic, mysqldump exports still impact the primary database and don't solve the staging environment delay issue. RDS doesn't have the database cloning feature that Aurora provides.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A company is preparing to store confidential data in Amazon S3. For compliance reasons the \ndata must be encrypted at rest Encryption key usage must be logged tor auditing purposes. Keys \nmust be rotated every year. \nWhich solution meets these requirements and the MOST operationally efferent?",
            options: [
                { id: 0, text: "Server-side encryption with customer-provided keys (SSE-C)", correct: false },
                { id: 1, text: "Server-side encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
                { id: 2, text: "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual", correct: false },
                { id: 3, text: "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUsing server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automatic rotation is the most operationally efficient solution that meets all requirements. SSE-KMS provides encryption at rest for S3 objects using keys managed by AWS KMS. KMS automatically logs all key usage to CloudTrail, providing the auditing capability required for compliance. Most importantly, KMS supports automatic key rotation for customer-managed CMKs, which rotates the cryptographic material every 365 days automatically without any manual intervention. This meets the requirement for annual key rotation with minimal operational overhead. KMS handles all key management, rotation, and auditing automatically.\n\n**Why option 0 is incorrect:**\nThe option that says use server-side encryption with customer-provided keys (SSE-C) is incorrect because SSE-C requires you to manage and provide encryption keys yourself. You would need to manually rotate keys every year, which adds operational overhead. Additionally, SSE-C doesn't provide automatic key usage logging to CloudTrail - you would need to implement your own logging solution. This approach requires significant operational effort for key management and rotation.\n\n**Why option 1 is incorrect:**\nThe option that says use server-side encryption with Amazon S3 managed keys (SSE-S3) is incorrect because SSE-S3 uses keys that are fully managed by S3, and you cannot access or control these keys. S3 managed keys don't provide key usage logging to CloudTrail for auditing purposes, which is a compliance requirement. Additionally, you cannot rotate S3-managed keys - AWS handles this internally without visibility or control. This doesn't meet the auditing and key rotation requirements.\n\n**Why option 2 is incorrect:**\nThe option that says use server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual rotation is incorrect because while SSE-KMS provides encryption and CloudTrail logging, manual key rotation requires operational overhead. You would need to create new keys, update applications to use new keys, and manage the rotation process manually every year. Automatic rotation eliminates this operational burden while still meeting all requirements.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A bicycle sharing company is developing a multi-tier architecture to track the location of its \nbicycles during peak operating hours. The company wants to use these data points in its existing \nanalytics platform. A solutions architect must determine the most viable multi-tier option to \nsupport this architecture. The data points must be accessible from the REST API.  \nWhich action meets these requirements for storing and retrieving location data?",
            options: [
                { id: 0, text: "Use Amazon Athena with Amazon S3", correct: false },
                { id: 1, text: "Use Amazon API Gateway with AWS Lambda", correct: true },
                { id: 2, text: "Use Amazon QuickSight with Amazon Redshift.", correct: false },
                { id: 3, text: "Use Amazon API Gateway with Amazon Kinesis Data Analytics", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon API Gateway with AWS Lambda provides a REST API for storing and retrieving location data points. API Gateway provides the REST API interface that can accept location data from bicycles during peak operating hours. Lambda functions can process the incoming data, store it in a durable storage solution like Amazon DynamoDB or Amazon S3, and retrieve it when requested through the API. Lambda can also integrate with analytics platforms to process the data. This serverless architecture scales automatically to handle peak traffic, provides low latency for real-time location tracking, and integrates seamlessly with existing analytics platforms. The REST API makes the data accessible as required.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Athena with Amazon S3 is incorrect because Athena is a serverless interactive query service for analyzing data in S3, not for storing and retrieving real-time location data through a REST API. Athena is designed for ad-hoc SQL queries on data already stored in S3, not for real-time data ingestion or API-based data access. It doesn't provide a REST API for storing location data points.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon QuickSight with Amazon Redshift is incorrect because QuickSight is a business intelligence and visualization tool, not a data storage or API service. Redshift is a data warehouse for analytics, but it's not optimized for real-time location tracking or REST API access. This combination doesn't provide the REST API interface required for storing and retrieving location data points.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon API Gateway with Amazon Kinesis Data Analytics is incorrect because Kinesis Data Analytics is designed for real-time stream processing and analytics, not for providing REST API access to stored data. While API Gateway can send data to Kinesis, Kinesis Data Analytics processes streaming data and outputs to Kinesis Data Streams, Firehose, or Lambda - it doesn't provide a REST API for retrieving stored location data. The data would need to be stored elsewhere (like S3 or DynamoDB) and accessed through a different mechanism.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company has an automobile sales website that stores its listings in a database on Amazon \nRDS. When an automobile is sold the listing needs to be removed from the website and the data \nmust be sent to multiple target systems. \nWhich design should a solutions architect recommend?",
            options: [
                { id: 0, text: "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to", correct: false },
                { id: 1, text: "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to", correct: false },
                { id: 2, text: "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon", correct: false },
                { id: 3, text: "Subscribe to an RDS event notification and send an Amazon Simple Notification Service", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nSubscribing to an RDS event notification and sending to an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues, then using AWS Lambda functions to update the targets, is the correct design. RDS event notifications use SNS to publish events when database changes occur. When an automobile is sold and the listing is removed from the RDS database, RDS can publish an event to an SNS topic. SNS can then fan out the message to multiple SQS queues (one for each target system), ensuring each target system receives the data. Lambda functions can then process messages from each SQS queue and update the respective target systems. This design decouples the database from target systems and provides reliable message delivery.\n\n**Why option 0 is incorrect:**\nThe option that says create an AWS Lambda function triggered when the RDS database is updated to send information to an Amazon SQS queue for targets to consume is incorrect because RDS doesn't natively trigger Lambda functions when database records are updated. RDS doesn't have built-in change data capture (CDC) capabilities like DynamoDB Streams. You would need to implement custom application logic or use Database Migration Service (DMS) for CDC, which adds complexity. RDS event notifications are the proper way to react to RDS events.\n\n**Why option 1 is incorrect:**\nThe option that says create an AWS Lambda function triggered when the RDS database is updated to send information to an Amazon SQS FIFO queue for targets to consume is incorrect for the same reason as option 0 - RDS doesn't trigger Lambda functions on data changes. Additionally, using a single FIFO queue for multiple target systems would require all targets to consume from the same queue, which doesn't provide proper fan-out to multiple systems. Each target system should have its own queue.\n\n**Why option 2 is incorrect:**\nThe option that says subscribe to an RDS event notification and send to an SQS queue fanned out to multiple SNS topics, then use Lambda functions to update targets is incorrect because the fan-out pattern is reversed. SNS should be used to fan out to multiple SQS queues (one per target), not the other way around. SQS queues are designed for point-to-point messaging, while SNS topics are designed for pub/sub fan-out to multiple subscribers. The correct pattern is SNS topic → multiple SQS queues → Lambda functions.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A company needs to store data in Amazon S3 and must prevent the data from being changed. \nThe company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a \nnonspecific amount of time until the company decides to modify the objects. Only specific users in \nthe company's AWS account can have the ability 10 delete the objects. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an S3 Glacier vault.", correct: false },
                { id: 1, text: "Create an S3 bucket with S3 Object Lock enabled.", correct: false },
                { id: 2, text: "Create an S3 bucket.", correct: false },
                { id: 3, text: "Create an S3 bucket with S3 Object Lock enabled.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCreating an S3 bucket with S3 Object Lock enabled, enabling versioning, adding a legal hold to the objects, and adding the s3:PutObjectLegalHold permission to IAM policies of users who need to delete objects is the correct solution. S3 Object Lock with legal hold prevents objects from being overwritten or deleted, and unlike retention periods, legal holds don't have a specific time duration - they remain in effect until explicitly removed. This meets the requirement for objects to remain unchangeable for a non-specific amount of time. Versioning is required for Object Lock and preserves all object versions. By granting s3:PutObjectLegalHold permission only to specific users, only those authorized users can remove the legal hold and delete objects, meeting the requirement that only specific users can delete objects.\n\n**Why option 0 is incorrect:**\nThe option that says create an S3 Glacier vault and apply a write-once, read-many (WORM) vault lock policy is incorrect because S3 Glacier is an archival storage service, not suitable for active data storage that needs to be accessible through S3 APIs. Glacier vaults are used for long-term archival, and accessing data from Glacier requires retrieval requests that can take minutes to hours. The scenario requires storing data in S3 (not Glacier) with Object Lock capabilities.\n\n**Why option 1 is incorrect:**\nThe option that says create an S3 bucket with Object Lock enabled, enable versioning, set a retention period of 100 years, and use governance mode is incorrect because setting a fixed retention period of 100 years doesn't meet the requirement for a \"non-specific amount of time until the company decides to modify the objects.\" Retention periods have fixed durations, whereas legal holds can be removed at any time by authorized users. Additionally, governance mode allows users with special permissions to bypass retention, but doesn't provide the same level of control as legal holds for non-specific time periods.\n\n**Why option 2 is incorrect:**\nThe option that says create an S3 bucket, use CloudTrail to track S3 API events, and restore modified objects from backups upon notification is incorrect because this is a reactive approach that doesn't prevent modification or deletion - it only detects and attempts to recover after the fact. CloudTrail logs events but doesn't prevent them. This approach doesn't meet the requirement to prevent data from being changed. Object Lock provides proactive protection that prevents changes, rather than reactive recovery.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A social media company allows users to upload images to its website. The website runs on \nAmazon EC2 instances.  \nDuring upload requests, the website resizes the images to a standard size and stores the resized \nimages in Amazon S3.  \nUsers are experiencing slow upload requests to the website. \n \nThe company needs to reduce coupling within the application and improve website performance.  \nA solutions architect must design the most operationally efficient process for image uploads. \n \nWhich combination of actions should the solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Configure the application to upload images to S3 Glacier.", correct: false },
                { id: 1, text: "Configure the web server to upload the original images to Amazon S3.", correct: true },
                { id: 2, text: "Configure the application to upload images directly from each user's browser to Amazon S3", correct: false },
                { id: 3, text: "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is", correct: false },
                { id: 4, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "**Why option 1 is correct:**\nConfiguring the web server to upload the original images to Amazon S3 decouples the image upload from the image processing. Instead of the web server handling both upload and resizing (which causes slow upload requests), the web server can quickly upload the original image to S3 and return a response to the user immediately. This improves website performance by reducing the time users wait for uploads. The web server no longer needs to process/resize images, reducing coupling between upload and processing functions.\n\n**Why option 3 is correct:**\nConfiguring S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded, and using the function to resize the image, provides an operationally efficient, serverless solution for image processing. When an image is uploaded to S3, S3 automatically triggers a Lambda function that resizes the image and stores it back in S3. This decouples image resizing from the upload process, allowing uploads to complete quickly while resizing happens asynchronously. Lambda automatically scales to handle varying workloads and requires no server management, providing the most operationally efficient solution.\n\n**Why option 0 is incorrect:**\nThe option that says configure the application to upload images to S3 Glacier is incorrect because Glacier is an archival storage service with retrieval delays (minutes to hours), making it completely unsuitable for web application image uploads. Users would experience extremely slow uploads and wouldn't be able to access images immediately. Glacier is designed for long-term archival, not active web application storage.\n\n**Why option 2 is incorrect:**\nThe option that says configure the application to upload images directly from each user's browser to Amazon S3 through presigned URLs is incorrect because while this can reduce server load, it doesn't address the image resizing requirement. Users would upload original images directly to S3, but there would be no mechanism to resize them. Additionally, managing presigned URLs for each user adds complexity. The web server still needs to generate presigned URLs, and image resizing would still need to be handled separately.\n\n**Why option 4 is incorrect:**\nThe option that says create an Amazon EventBridge rule that invokes a Lambda function on a schedule to resize uploaded images is incorrect because scheduled processing (polling) is less efficient than event-driven processing. S3 Event Notifications provide immediate, event-driven triggers when images are uploaded, whereas scheduled rules would require periodic polling, causing delays in image processing. Event-driven architecture is more operationally efficient and provides better user experience.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A company recently migrated a message processing system to AWS. The system receives \nmessages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are \nprocessed by a consumer application running on Amazon EC2. The consumer application \nprocesses the messages and writes results to a MySQL database running on Amazon EC2. The \ncompany wants this application to be highly available with low operational complexity. \nWhich architecture offers the HIGHEST availability?",
            options: [
                { id: 0, text: "Add a second ActiveMQ server to another Availably Zone.", correct: false },
                { id: 1, text: "Use Amazon MO with active/standby brokers configured across two Availability Zones.", correct: false },
                { id: 2, text: "Use Amazon MO with active/standby blotters configured across two Availability Zones.", correct: false },
                { id: 3, text: "Use Amazon MQ with active/standby brokers configured across two Availability Zones.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUsing Amazon MQ with active/standby brokers configured across two Availability Zones provides the highest availability with low operational complexity. Amazon MQ is a managed message broker service that supports ActiveMQ and RabbitMQ. The active/standby broker configuration provides automatic failover - if the active broker fails, the standby broker automatically takes over, ensuring message queue availability. Deploying brokers across two Availability Zones protects against AZ-level failures. Amazon MQ is fully managed, eliminating the operational complexity of managing ActiveMQ on EC2 instances. Combined with Auto Scaling groups for consumer EC2 instances across multiple AZs and RDS Multi-AZ for the database, this provides comprehensive high availability with minimal operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says add a second ActiveMQ server to another Availability Zone is incorrect because simply adding a second ActiveMQ server doesn't provide automatic failover or high availability. You would need to configure ActiveMQ for high availability (like network of brokers or master/slave configuration), which requires significant operational complexity and manual configuration. Managing ActiveMQ on EC2 instances requires patching, monitoring, and maintenance, which increases operational overhead compared to the managed Amazon MQ service.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon MQ with active/standby brokers configured across two Availability Zones appears to be a duplicate or typo of the correct answer. However, if this refers to a different configuration, it may not include all the necessary components (consumer Auto Scaling, RDS Multi-AZ) for complete high availability.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon MQ with active/standby brokers configured across two Availability Zones appears to be another duplicate or typo (\"blotters\" instead of \"brokers\"). The correct answer is option 3, which specifies the proper Amazon MQ configuration.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A company hosts a containerized web application on a fleet of on-premises servers that process \nincoming requests. The number of requests is growing quickly. The on-premises servers cannot \nhandle the increased number of requests. The company wants to move the application to AWS \nwith minimum code changes and minimum development effort. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized", correct: true },
                { id: 1, text: "Use two Amazon EC2 instances to host the containerized web application.", correct: false },
                { id: 2, text: "Use AWS Lambda with a new code that uses one of the supported languages.", correct: false },
                { id: 3, text: "Use a high performance computing (HPC) solution such as AWS ParallelClusterto establish an", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application provides the least operational overhead while meeting all requirements. Fargate is a serverless compute engine for containers that eliminates the need to manage EC2 instances, servers, or clusters. You simply define your container image and resource requirements, and Fargate handles the infrastructure. ECS can automatically scale the number of Fargate tasks based on demand, handling the growing number of requests. Since the application is already containerized, it can be moved to ECS/Fargate with minimal code changes - you just need to adapt the container configuration. An Application Load Balancer (ALB) can be added to distribute traffic across multiple Fargate tasks. This solution requires no server management, automatic scaling, and minimal code changes.\n\n**Why option 1 is incorrect:**\nThe option that says use two Amazon EC2 instances to host the containerized web application is incorrect because using a fixed number of EC2 instances doesn't scale to handle growing requests. You would need to manually add more instances as traffic grows, which requires operational overhead. Additionally, managing EC2 instances (patching, monitoring, scaling) adds significant operational complexity compared to Fargate's serverless approach. Two instances also don't provide the scalability needed for quickly growing request volumes.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Lambda with new code that uses one of the supported languages is incorrect because this would require rewriting the application code to fit Lambda's programming model, which violates the \"minimum code changes and minimum development effort\" requirement. Containerized applications are designed to run as long-running processes, while Lambda functions are event-driven and stateless with execution time limits. Converting a containerized web application to Lambda would require significant code refactoring.\n\n**Why option 3 is incorrect:**\nThe option that says use a high performance computing (HPC) solution such as AWS ParallelCluster is incorrect because ParallelCluster is designed for HPC workloads like scientific computing, simulations, and batch processing, not for web applications. HPC solutions are overkill for a web application and would require significant configuration and operational overhead. The containerized web application should run on container orchestration services like ECS, not HPC clusters.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A company uses 50 TB of data for reporting. The company wants to move this data from on \npremises to AWS A custom application in the company's data center runs a weekly data \ntransformation job. The company plans to pause the application until the data transfer is complete \nand needs to begin the transfer process as soon as possible. \nThe data center does not have any available network bandwidth for additional workloads.  \nA solutions architect must transfer the data and must configure the transformation job to continue \nto run in the AWS Cloud. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS DataSync to move the data.", correct: false },
                { id: 1, text: "Order an AWS Snowcone device to move the data.", correct: false },
                { id: 2, text: "Order an AWS Snowball Edge Storage Optimized device.", correct: true },
                { id: 3, text: "Order an AWS D. Snowball Edge Storage Optimized device that includes Amazon EC2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nOrdering an AWS Snowball Edge Storage Optimized device to move the 50 TB of data is the correct solution when network bandwidth is unavailable. Snowball Edge Storage Optimized devices can store up to 80 TB of data and are designed for large-scale data transfers when network transfer is impractical or too slow. The device is shipped to the data center, data is copied to it locally (which doesn't require network bandwidth), and then the device is shipped back to AWS where the data is imported into S3. After the data is in AWS, AWS Glue can be used to create and run the transformation job, replacing the on-premises custom application. This solution provides the fastest transfer method (physical shipment) when network bandwidth is unavailable and allows the transformation job to run in AWS with minimal operational overhead using the managed Glue service.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DataSync to move the data and create a custom transformation job using AWS Glue is incorrect because DataSync requires network connectivity to transfer data. The scenario explicitly states that the data center has no available network bandwidth for additional workloads. DataSync transfers data over the network, which would be slow or impossible given the bandwidth constraints. Additionally, transferring 50 TB over the network would take significantly longer than using a Snowball device.\n\n**Why option 1 is incorrect:**\nThe option that says order an AWS Snowcone device to move the data and deploy the transformation application to the device is incorrect because Snowcone devices have a storage capacity of only 8 TB, which is insufficient for the 50 TB dataset. You would need multiple Snowcone devices, which adds complexity and cost. Additionally, while Snowcone can run EC2 instances, it's not designed for running complex transformation applications - AWS Glue is the proper managed service for ETL jobs.\n\n**Why option 3 is incorrect:**\nThe option that says order an AWS Snowball Edge Storage Optimized device with EC2 compute, copy data to the device, and create a new EC2 instance on AWS to run the transformation application is incorrect because while the Snowball device can handle the data transfer, running the transformation application on a separate EC2 instance adds operational overhead. You would need to manage the EC2 instance, install dependencies, and maintain the application. Using AWS Glue (a managed ETL service) is more operationally efficient than managing EC2 instances for transformation jobs.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company has created an image analysis application in which users can upload photos and add \nphoto frames to their images. The users upload images and metadata to indicate which photo \nframes they want to add to their images. The application uses a single Amazon EC2 instance and \nAmazon DynamoDB to store the metadata. \n \nThe application is becoming more popular, and the number of users is increasing. The company \nexpects the number of concurrent users to vary significantly depending on the time of day and \nday of week. The company must ensure that the application can scale to meet the needs of the \ngrowing user base. \n \nWhich solution meats these requirements?",
            options: [
                { id: 0, text: "Use AWS Lambda to process the photos.", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and", correct: false },
                { id: 2, text: "Use AWS Lambda to process the photos.", correct: true },
                { id: 3, text: "Increase the number of EC2 instances to three.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing AWS Lambda to process the photos, storing the photos in Amazon S3, and retaining DynamoDB to store the metadata is the best solution for scaling. Lambda functions automatically scale to handle varying concurrent user loads without any configuration, meeting the requirement for significant variation in concurrent users. S3 provides virtually unlimited storage for photos and can handle any number of uploads concurrently. DynamoDB is ideal for storing metadata (which is small and structured) and can scale automatically to handle varying read/write loads. This serverless architecture eliminates the need to manage EC2 instances and provides automatic scaling, making it operationally efficient and cost-effective for variable workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Lambda to process photos and store both photos and metadata in DynamoDB is incorrect because storing photos (large binary files) in DynamoDB is not scalable or cost-effective. DynamoDB has a 400 KB item size limit, which would prevent storing full-size photos. Even if photos were small enough, DynamoDB charges for storage and read/write capacity, making it expensive for large binary objects. S3 is designed for object storage and is much more cost-effective for photos.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Kinesis Data Firehose to process photos and store photos and metadata is incorrect because Kinesis Data Firehose is designed for streaming data ingestion and delivery, not for processing images or handling user uploads. Firehose doesn't have image processing capabilities and is not suitable for an interactive application where users upload photos and expect processed results. Additionally, Firehose would require a separate system to handle the uploads and trigger the processing.\n\n**Why option 3 is incorrect:**\nThe option that says increase the number of EC2 instances to three and use Provisioned IOPS SSD (io2) EBS volumes to store photos and metadata is incorrect because a fixed number of instances (three) doesn't provide the automatic scaling needed for significantly varying concurrent user loads. You would need to manually scale instances up and down, which requires operational overhead. Additionally, storing photos on EBS volumes attached to EC2 instances doesn't scale well and creates a single point of failure. EBS volumes are also more expensive than S3 for large-scale object storage.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A medical records company is hosting an application on Amazon EC2 instances. The application \nprocesses customer data files that are stored on Amazon S3. The EC2 instances are hosted in \npublic subnets. The EC2 instances access Amazon S3 over the internet, but they do not require \nany other network access. \nA new requirement mandates that the network traffic for file transfers take a private route and not \nbe sent over the internet. \nWhich change to the network architecture should a solutions architect recommend to meet this \nrequirement?",
            options: [
                { id: 0, text: "Create a NAT gateway.", correct: false },
                { id: 1, text: "Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic", correct: false },
                { id: 2, text: "Move the EC2 instances to private subnets.", correct: true },
                { id: 3, text: "Remove the internet gateway from the VPC.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nMoving the EC2 instances to private subnets and creating a VPC gateway endpoint for Amazon S3, linking the endpoint to the route table for the private subnets, is the correct solution. VPC gateway endpoints provide private connectivity to S3 without requiring internet gateway, NAT device, or VPN connections. Traffic between the VPC and S3 stays within the AWS network and never traverses the public internet. By moving instances to private subnets and configuring the VPC endpoint in the route table, all S3 traffic will be routed through the private AWS network. This meets the requirement for private routing while maintaining the ability to access S3.\n\n**Why option 0 is incorrect:**\nThe option that says create a NAT gateway and configure the route table for public subnets to send traffic to S3 through the NAT gateway is incorrect because NAT gateways route traffic through the internet gateway, which means traffic still traverses the public internet. NAT gateways are used to allow resources in private subnets to access the internet, but they don't provide private connectivity to AWS services. Additionally, NAT gateways incur hourly charges and data processing fees, whereas VPC endpoints are free (you only pay for data transfer).\n\n**Why option 1 is incorrect:**\nThe option that says configure the security group for EC2 instances to restrict outbound traffic so that only traffic to the S3 prefix list is permitted is incorrect because security groups control access but don't change the routing path. Even with security group restrictions, if instances are in public subnets without a VPC endpoint, traffic to S3 would still go through the internet gateway and traverse the public internet. Security groups don't create private routes - they only filter traffic.\n\n**Why option 3 is incorrect:**\nThe option that says remove the internet gateway from the VPC and set up AWS Direct Connect to route traffic to S3 is incorrect because Direct Connect is designed for hybrid connectivity between on-premises and AWS, not for VPC-to-AWS-service connectivity. Additionally, removing the internet gateway would prevent any internet access if needed in the future, and Direct Connect requires physical connections and significant setup time and cost. VPC endpoints provide immediate, free private connectivity without requiring Direct Connect.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A company uses a popular content management system (CMS) for its corporate website. \nHowever, the required patching and maintenance are burdensome. The company is redesigning \nits website and wants anew solution. The website will be updated four times a year and does not \nneed to have any dynamic content available. The solution must provide high scalability and \nenhanced security. \n \nWhich combination of changes will meet these requirements with the LEAST operational \noverhead? (Choose two.)",
            options: [
                { id: 0, text: "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality", correct: true },
                { id: 1, text: "Create and deploy an AWS Lambda function to manage and serve the website content", correct: false },
                { id: 2, text: "Create the new website and an Amazon S3 bucket Deploy the website on the S3 bucket with", correct: false },
                { id: 3, text: "Create the new website. Deploy the website on an Amazon S3 bucket with static website hosting enabled. Use Amazon CloudFront to distribute the website content and require HTTPS.", correct: true },
            ],
            correctAnswers: [0, 3],
            explanation: "**Why option 0 is correct:**\nDeploying an AWS WAF web ACL in front of the website provides enhanced security by protecting against common web exploits like SQL injection and cross-site scripting (XSS) attacks. While WAF doesn't directly provide HTTPS functionality (that's handled by CloudFront with ACM certificates), WAF can be integrated with CloudFront to provide security at the edge. WAF rules can be configured to block malicious requests before they reach the origin, providing an additional security layer. This meets the enhanced security requirement with minimal operational overhead since WAF is a managed service.\n\n**Why option 3 is correct:**\nCreating the new website, deploying it on an Amazon S3 bucket with static website hosting enabled, and using Amazon CloudFront to distribute the website content provides high scalability and minimal operational overhead. Since the website has no dynamic content and is only updated four times a year, static website hosting on S3 is ideal. S3 provides virtually unlimited scalability and requires no server management, patching, or maintenance. CloudFront CDN distributes content globally, improving performance and scalability by caching content at edge locations. This solution eliminates the operational burden of managing CMS servers, patching, and maintenance.\n\n**Why option 1 is incorrect:**\nThe option that says create and deploy an AWS Lambda function to manage and serve the website content is incorrect because Lambda functions are designed for event-driven processing, not for serving static website content. While Lambda@Edge can be used with CloudFront for dynamic content, for a static website that doesn't need dynamic content, S3 static website hosting is simpler and more cost-effective. Lambda functions have execution time limits and are charged per invocation, making them less suitable for serving static content compared to S3.\n\n**Why option 2 is incorrect:**\nThe option that says create the new website and an Amazon S3 bucket and deploy the website on the S3 bucket with static website hosting enabled is partially correct but incomplete. While S3 static website hosting provides scalability, it doesn't include CloudFront for global distribution and enhanced security features. The complete solution should include CloudFront for CDN capabilities, HTTPS enforcement, and integration with WAF for enhanced security. Additionally, S3 static website hosting alone doesn't provide the same level of security features as CloudFront + WAF.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company stores its application logs in an Amazon CloudWatch Logs log group.  \nA new policy requires the company to store all application logs in Amazon OpenSearch Service \n(Amazon Elasticsearch Service) in near-real time. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service", correct: true },
                { id: 1, text: "Create an AWS Lambda function.", correct: false },
                { id: 2, text: "Create an Amazon Kinesis Data Firehose delivery stream.", correct: false },
                { id: 3, text: "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nConfiguring a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service is the solution with the least operational overhead. CloudWatch Logs has a native feature that automatically creates and manages a Lambda function with pre-populated code to stream logs to OpenSearch. When you enable this subscription, AWS handles all the infrastructure, code, and configuration automatically. The Lambda function is created, configured, and managed by AWS, requiring no manual coding or infrastructure management. This provides near-real-time log streaming with minimal operational effort, as you only need to configure the subscription in the CloudWatch Logs console.\n\n**Why option 1 is incorrect:**\nThe option that says create an AWS Lambda function and use the log group to invoke the function to write logs to OpenSearch is incorrect because while this approach can work, it requires you to write, deploy, and maintain the Lambda function code yourself. You would need to handle error handling, retries, batching, and OpenSearch API integration, which adds significant operational overhead compared to the managed CloudWatch Logs subscription feature. The native subscription feature eliminates all this manual work.\n\n**Why option 2 is incorrect:**\nThe option that says create an Amazon Kinesis Data Firehose delivery stream, configure the log group as the source, and configure OpenSearch as the destination is incorrect because CloudWatch Logs cannot be directly configured as a Firehose source. Firehose can receive data from Kinesis Data Streams, but CloudWatch Logs would need to send data to Kinesis Data Streams first, or you would need a Lambda function to read from CloudWatch Logs and write to Firehose. This adds complexity and operational overhead compared to the native CloudWatch Logs subscription.\n\n**Why option 3 is incorrect:**\nThe option that says install and configure Amazon Kinesis Agent on each application server to deliver logs to Kinesis Data Streams, then configure Kinesis Data Streams to deliver to OpenSearch is incorrect because this requires installing and managing agents on every application server, which adds significant operational overhead. You would need to install, configure, and maintain agents across all servers, handle agent failures, and manage the Kinesis Data Streams infrastructure. Additionally, Kinesis Data Streams doesn't directly deliver to OpenSearch - you would need additional components like Lambda or Kinesis Data Firehose, adding more complexity.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A company is building a web-based application running on Amazon EC2 instances in multiple \nAvailability Zones. The web application will provide access to a repository of text documents \ntotaling about 900 TB in size. The company anticipates that the web application will experience \nperiods of high demand. A solutions architect must ensure that the storage component for the text \ndocuments can scale to meet the demand of the application at all times. The company is \nconcerned about the overall cost of the solution. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon Elasticsearch Service (Amazon ES)", correct: false },
                { id: 3, text: "Amazon S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon S3 is the most cost-effective storage solution for storing 900 TB of text documents that need to scale to meet high demand. S3 provides virtually unlimited scalability, automatically handling any amount of storage and concurrent access requests without requiring capacity planning or manual scaling. S3 offers multiple storage classes (Standard, Standard-IA, One Zone-IA, Intelligent-Tiering, Glacier) that allow cost optimization based on access patterns. For a web application with periods of high demand, S3 Standard provides low-latency access, and you can use lifecycle policies to transition less frequently accessed documents to cheaper storage classes. S3's pay-as-you-go pricing model means you only pay for what you store and access, making it highly cost-effective for large-scale document storage compared to block or file storage solutions.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Elastic Block Store (Amazon EBS) is incorrect because EBS volumes have size limits (up to 64 TiB per volume) and are attached to EC2 instances, making them unsuitable for 900 TB of storage. You would need multiple EBS volumes and instances, which adds complexity and cost. EBS is designed for block storage attached to EC2 instances, not for large-scale object storage accessible by web applications. EBS also doesn't scale automatically and requires manual provisioning.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Elastic File System (Amazon EFS) is incorrect because while EFS can scale to petabyte scale, it is significantly more expensive than S3 for large-scale storage. EFS charges for storage and data transfer, and its pricing is higher than S3 for most use cases. EFS is designed for shared file storage with low-latency access, but for a repository of text documents accessed via a web application, S3 provides better cost-effectiveness and scalability. EFS also requires EC2 instances to mount the file system, adding infrastructure complexity.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon Elasticsearch Service (Amazon ES) is incorrect because Elasticsearch is a search and analytics engine, not a general-purpose storage solution. While Elasticsearch can store documents, it's designed for full-text search and analytics workloads, not for simple document storage and retrieval. Elasticsearch is significantly more expensive than S3 and requires cluster management, which adds operational overhead. For storing 900 TB of text documents that need to scale cost-effectively, S3 is the appropriate choice.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 47,
            text: "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in \nthe us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution \nto protect these API Gateway managed REST APIs across multiple accounts from SQL injection \nand cross-site scripting attacks. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
            options: [
                { id: 0, text: "Set up AWS WAF in both Regions.", correct: false },
                { id: 1, text: "Set up AWS Firewall Manager in both Regions.", correct: true },
                { id: 2, text: "Set up AWS Shield in bath Regions.", correct: false },
                { id: 3, text: "Set up AWS Shield in one of the Regions.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSetting up AWS Firewall Manager in both Regions and centrally configuring AWS WAF rules provides the least administrative effort for protecting API Gateway REST APIs across multiple accounts and regions. Firewall Manager is a security management service that allows you to centrally configure and manage WAF rules across multiple AWS accounts and resources. Instead of manually setting up WAF in each region and associating web ACLs with each API stage, Firewall Manager allows you to define security policies once and automatically apply them to API Gateway APIs across all accounts and regions. Firewall Manager integrates with AWS Organizations to manage WAF rules centrally, significantly reducing administrative overhead. WAF rules can protect against SQL injection and cross-site scripting (XSS) attacks by inspecting web requests and blocking malicious patterns.\n\n**Why option 0 is incorrect:**\nThe option that says set up AWS WAF in both Regions and associate Regional web ACLs with an API stage is incorrect because this approach requires manual setup and configuration in each region for each API Gateway stage. With APIs in two regions and potentially multiple accounts, you would need to configure WAF separately in each region and associate web ACLs with each API stage manually. This requires significant administrative effort compared to Firewall Manager's centralized approach.\n\n**Why option 2 is incorrect:**\nThe option that says set up AWS Shield in both Regions and associate Regional web ACLs with an API stage is incorrect because AWS Shield is a DDoS protection service, not a web application firewall. Shield protects against distributed denial-of-service attacks but doesn't provide protection against SQL injection or XSS attacks. While Shield can work alongside WAF, Shield alone doesn't meet the requirement for protecting against SQL injection and XSS. Additionally, this approach still requires manual configuration in each region.\n\n**Why option 3 is incorrect:**\nThe option that says set up AWS Shield in one of the Regions and associate Regional web ACLs with an API stage is incorrect for the same reasons as option 2 - Shield doesn't protect against SQL injection or XSS attacks. Additionally, setting up protection in only one region leaves the APIs in the other region unprotected, which doesn't meet the requirement to protect APIs across both regions.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A company has implemented a self-managed DNS solution on three Amazon EC2 instances \nbehind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are \nlocated in the United States and Europe. The company wants to improve the performance and \navailability of the solution. The company launches and configures three EC2 instances in the eu-\nwest-1 Region and adds the EC2 instances as targets for a new NLB. \n \nWhich solution can the company use to route traffic to all the EC2 instances?",
            options: [
                { id: 0, text: "Create an Amazon Route 53 geolocation routing policy to route requests to one of the two", correct: false },
                { id: 1, text: "Create a standard accelerator in AWS Global Accelerator.", correct: true },
                { id: 2, text: "Attach Elastic IP addresses to the six EC2 instances.", correct: false },
                { id: 3, text: "Replace the two NLBs with two Application Load Balancers (ALBs).", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCreating a standard accelerator in AWS Global Accelerator, creating endpoint groups in us-west-2 and eu-west-1, and adding the two NLBs as endpoints for the endpoint groups is the best solution for routing traffic to all EC2 instances across regions. Global Accelerator provides static IP addresses that serve as fixed entry points to your applications, eliminating the need to manage region-specific IPs. It automatically routes user traffic to the optimal endpoint based on performance metrics, user location, and application health. Global Accelerator uses AWS's global network infrastructure to route traffic efficiently, improving performance for users in both the United States and Europe. It provides automatic failover and health checks, ensuring high availability. This solution works seamlessly with NLBs and doesn't require replacing the existing load balancers.\n\n**Why option 0 is incorrect:**\nThe option that says create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs and create a CloudFront distribution using the Route 53 record as the origin is incorrect because CloudFront is designed for caching and distributing static content, not for routing to backend services like DNS servers. Additionally, using Route 53 geolocation routing with CloudFront adds unnecessary complexity. Global Accelerator is specifically designed for this use case of routing traffic to application endpoints across multiple regions.\n\n**Why option 2 is incorrect:**\nThe option that says attach Elastic IP addresses to the six EC2 instances, create a Route 53 geolocation routing policy to route to one of the six EC2 instances, and create a CloudFront distribution is incorrect because this approach bypasses the NLBs, eliminating their load balancing and health checking benefits. Directly routing to individual EC2 instances doesn't provide the same level of availability and performance as using load balancers. Additionally, CloudFront is not suitable for routing to backend application servers - it's designed for content delivery.\n\n**Why option 3 is incorrect:**\nThe option that says replace the two NLBs with two Application Load Balancers (ALBs), create a Route 53 latency routing policy, and create a CloudFront distribution is incorrect because replacing NLBs with ALBs may not be necessary and adds operational overhead. NLBs are appropriate for DNS workloads. Additionally, using CloudFront with Route 53 for routing to backend services is not the optimal architecture. Global Accelerator is specifically designed for this multi-region routing scenario and works with NLBs without requiring replacements.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A company is running an online transaction processing (OLTP) workload on AWS. This workload \nuses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database \nsnapshots are taken from this instance. \n \nWhat should a solutions architect do to ensure the database and snapshots are always encrypted \nmoving forward?",
            options: [
                { id: 0, text: "Encrypt a copy of the latest DB snapshot.", correct: true },
                { id: 1, text: "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the", correct: false },
                { id: 2, text: "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS).", correct: false },
                { id: 3, text: "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nEncrypting a copy of the latest DB snapshot and replacing the existing DB instance by restoring the encrypted snapshot is the correct approach to enable encryption on an existing RDS database. RDS encryption cannot be enabled on an existing database instance - it can only be enabled when creating a new instance. To encrypt an existing database, you must: 1) Create a snapshot of the unencrypted database, 2) Copy the snapshot and enable encryption during the copy process (specifying a KMS key), 3) Restore a new DB instance from the encrypted snapshot, and 4) Replace the old instance with the new encrypted instance. Once the new encrypted instance is running, all future snapshots will be encrypted automatically. This ensures both the database and all future snapshots are encrypted.\n\n**Why option 1 is incorrect:**\nThe option that says create a new encrypted EBS volume and copy the snapshots to it, then enable encryption on the DB instance is incorrect because RDS manages its own storage and you cannot directly attach EBS volumes to RDS instances or enable encryption on an existing RDS instance. RDS encryption must be enabled at instance creation time, not after. You cannot \"enable encryption\" on an existing RDS instance - you must create a new encrypted instance from an encrypted snapshot.\n\n**Why option 2 is incorrect:**\nThe option that says copy the snapshots and enable encryption using AWS KMS, then restore the encrypted snapshot to an existing DB instance is incorrect because you cannot restore a snapshot to an existing DB instance - you must create a new DB instance from the snapshot. The process requires creating a new encrypted instance from the encrypted snapshot, not restoring to an existing instance. Additionally, the existing unencrypted instance would need to be replaced, not updated.\n\n**Why option 3 is incorrect:**\nThe option that says copy the snapshots to an Amazon S3 bucket encrypted with SSE-KMS is incorrect because RDS snapshots are managed by AWS and stored in S3 automatically, but you cannot manually copy RDS snapshots to S3 buckets. RDS snapshots are stored in AWS-managed S3 buckets, not in customer buckets. To encrypt snapshots, you must copy the snapshot within RDS and enable encryption during the copy, then restore a new encrypted instance from that encrypted snapshot.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A company wants to build a scalable key management Infrastructure to support developers who need to encrypt data in their applications. What should a solutions architect do to reduce the operational burden?",
            options: [
                { id: 0, text: "Use multifactor authentication (MFA) to protect the encryption keys.", correct: false },
                { id: 1, text: "Use AWS Key Management Service (AWS KMS) to protect the encryption keys", correct: true },
                { id: 2, text: "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys", correct: false },
                { id: 3, text: "Use an IAM policy to limit the scope of users who have access permissions to protect the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing AWS Key Management Service (AWS KMS) to protect encryption keys provides a scalable, fully managed key management infrastructure that reduces operational burden. KMS is a managed service that handles key creation, storage, rotation, and access control automatically. Developers can use the AWS Encryption SDK with KMS to easily generate, use, and protect symmetric encryption keys in their applications without managing key infrastructure. KMS integrates seamlessly with AWS services and provides APIs for application-level encryption. KMS automatically handles key rotation, backup, and compliance requirements, eliminating the need for developers to manage these aspects. This centralized approach scales to support any number of developers and applications.\n\n**Why option 0 is incorrect:**\nThe option that says use multifactor authentication (MFA) to protect encryption keys is incorrect because MFA is an authentication mechanism, not a key management service. MFA adds an extra layer of security for accessing systems but doesn't provide key management infrastructure. Developers would still need a system to create, store, rotate, and manage encryption keys, which MFA doesn't provide. MFA protects access to keys but doesn't reduce the operational burden of key management itself.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) to create, store, and assign encryption keys is incorrect because ACM is designed for managing SSL/TLS certificates for use with AWS services like CloudFront, Elastic Load Balancing, and API Gateway. ACM doesn't provide general-purpose encryption key management for application data encryption. ACM certificates are used for securing network communications, not for encrypting application data. For application-level encryption, developers need a key management service like KMS.\n\n**Why option 3 is incorrect:**\nThe option that says use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys is incorrect because IAM policies control access to AWS resources and services, but they don't provide key management infrastructure. IAM can control who can access KMS keys, but you still need KMS (or another key management service) to actually create, store, and manage the keys. IAM policies alone don't provide the key management functionality that developers need to encrypt data in their applications.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company has a dynamic web application hosted on two Amazon EC2 instances. The company \nhas its own SSL certificate, which is on each instance to perform SSL termination. \n \nThere has been an increase in traffic recently, and the operations team determined that SSL \nencryption and decryption is causing the compute capacity of the web servers to reach their \nmaximum limit. \n \nWhat should a solutions architect do to increase the application's performance?",
            options: [
                { id: 0, text: "Create a new SSL certificate using AWS Certificate Manager (ACM) install the ACM certificate", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket.", correct: false },
                { id: 2, text: "Create another EC2 instance as a proxy server Migrate the SSL certificate to the new instance", correct: false },
                { id: 3, text: "Import the SSL certificate into AWS Certificate Manager (ACM).", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nImporting the SSL certificate into AWS Certificate Manager (ACM) and creating an Application Load Balancer (ALB) with an HTTPS listener that uses the SSL certificate from ACM is the correct solution. By moving SSL termination from the EC2 instances to the ALB, the load balancer handles all SSL encryption and decryption, offloading this CPU-intensive work from the web servers. This allows the EC2 instances to focus on processing application logic instead of SSL/TLS processing. The ALB can handle SSL termination at scale and distribute the load across multiple EC2 instances. This architecture improves performance by removing SSL processing overhead from the application servers and allows the application to scale more effectively.\n\n**Why option 0 is incorrect:**\nThe option that says create a new SSL certificate using ACM and install the ACM certificate on each instance is incorrect because this still requires SSL termination on the EC2 instances, which doesn't solve the performance problem. Installing certificates on instances means the instances still need to perform SSL encryption and decryption, consuming CPU resources. The issue is the location of SSL termination, not the certificate source.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon S3 bucket, migrate the SSL certificate to the S3 bucket, and configure EC2 instances to reference the bucket for SSL termination is incorrect because S3 is object storage and cannot perform SSL termination. SSL termination requires a service that can handle TLS handshakes and encrypt/decrypt traffic, which S3 doesn't provide. Additionally, storing certificates in S3 doesn't change where SSL termination occurs - it would still need to happen on the EC2 instances.\n\n**Why option 2 is incorrect:**\nThe option that says create another EC2 instance as a proxy server, migrate the SSL certificate to the new instance, and configure it to direct connections to existing EC2 instances is incorrect because this adds another EC2 instance that would also need to handle SSL termination, potentially creating a bottleneck. While this could offload SSL from the web servers, it doesn't provide the scalability, high availability, and managed service benefits of using an Application Load Balancer. An ALB is a managed service that automatically scales and provides better performance than a single proxy EC2 instance.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job. What should the solutions architect recommend?",
            options: [
                { id: 0, text: "Implement EC2 Spot Instances", correct: true },
                { id: 1, text: "Purchase EC2 Reserved Instances", correct: false },
                { id: 2, text: "Implement EC2 On-Demand Instances", correct: false },
                { id: 3, text: "Implement the processing on AWS Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nImplementing EC2 Spot Instances is the most cost-effective solution for this highly dynamic, stateless batch processing job. Spot Instances provide up to 90% savings compared to On-Demand Instances, making them ideal for cost optimization. Since the job is stateless and can be started and stopped at any time without negative impact, it can tolerate Spot Instance interruptions. The job takes 60 minutes to complete, which is well-suited for Spot Instances. Spot Instances automatically scale based on availability and can handle the highly dynamic nature of the workload. When Spot capacity is available, the job runs at a fraction of the cost of On-Demand Instances. If interrupted, the job can be restarted on other Spot Instances or On-Demand Instances as a fallback.\n\n**Why option 1 is incorrect:**\nThe option that says purchase EC2 Reserved Instances is incorrect because Reserved Instances require a 1-3 year commitment and are best for steady-state, predictable workloads. The scenario describes a \"highly dynamic\" batch processing job, which means the number of instances needed varies significantly. Reserved Instances don't provide the flexibility needed for dynamic workloads and may result in paying for capacity that isn't always used. Additionally, Reserved Instances don't automatically scale - you still need to manage instance provisioning.\n\n**Why option 2 is incorrect:**\nThe option that says implement EC2 On-Demand Instances is incorrect because while On-Demand Instances provide flexibility and scalability, they are significantly more expensive than Spot Instances (up to 90% more). For a stateless, interruptible batch job that can tolerate interruptions, Spot Instances provide the same functionality at a much lower cost. On-Demand Instances should be reserved for workloads that cannot tolerate interruptions.\n\n**Why option 3 is incorrect:**\nThe option that says implement the processing on AWS Lambda is incorrect because Lambda has a 15-minute execution time limit per invocation, and the batch job takes upwards of 60 minutes to complete. Lambda cannot handle long-running batch jobs that exceed 15 minutes. While you could potentially chain Lambda functions, this would add significant complexity and may not be cost-effective for a 60-minute batch processing job that requires many instances.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load \nbalancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS \nDB instance. The EC2 instances and the RDS DB instance should not be exposed to the public \ninternet. The EC2 instances require internet access to complete payment processing of orders \nthrough a third-party web service. The application must be highly available. \n \nWhich combination of configuration options will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use an Auto Scaling group to launch the EC2 instances in private subnets.", correct: true },
                { id: 1, text: "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones.", correct: false },
                { id: 2, text: "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability", correct: false },
                { id: 3, text: "Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two", correct: false },
                { id: 4, text: "Configure a VPC with two public subnets, two private subnets, and two NAT gateways across", correct: false },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nUsing an Auto Scaling group to launch the EC2 instances in private subnets and deploying an RDS Multi-AZ DB instance in private subnets ensures that neither the EC2 instances nor the RDS database are exposed to the public internet. Private subnets don't have direct internet gateway access, providing security isolation. The Auto Scaling group ensures high availability by automatically replacing failed instances and distributing them across Availability Zones. RDS Multi-AZ provides automatic failover and high availability for the database.\n\n**Why option 4 is correct:**\nConfiguring a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones, and deploying an Application Load Balancer in the public subnets provides the complete architecture. The public subnets host the ALB (which needs internet connectivity to receive traffic) and NAT gateways (for outbound internet access from private subnets). The private subnets host the EC2 instances and RDS database. Two NAT gateways (one per AZ) provide high availability for outbound internet access, allowing EC2 instances in private subnets to access the third-party payment processing service. The ALB in public subnets can route traffic to EC2 instances in private subnets. This architecture provides high availability across multiple AZs.\n\n**Why option 1 is incorrect:**\nThe option that says configure a VPC with two private subnets and two NAT gateways across two Availability Zones, and deploy an ALB in private subnets is incorrect because Application Load Balancers must be deployed in public subnets to receive traffic from the internet. ALBs need internet gateway access to accept incoming connections from users. You cannot deploy an ALB in private subnets and expect it to receive public internet traffic.\n\n**Why option 2 is incorrect:**\nThe option that says use an Auto Scaling group to launch EC2 instances in public subnets across two Availability Zones and deploy RDS Multi-AZ in private subnets is incorrect because placing EC2 instances in public subnets exposes them directly to the public internet, which violates the requirement that EC2 instances should not be exposed to the public internet. EC2 instances should be in private subnets for security.\n\n**Why option 3 is incorrect:**\nThe option that says configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones, and deploy an ALB in the public subnet is incorrect because you need at least one public subnet and one private subnet in each Availability Zone for high availability. Having only one public subnet and one private subnet total doesn't provide redundancy across AZs. If one AZ fails, the entire application would be unavailable.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.", correct: false },
                { id: 1, text: "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.", correct: true },
                { id: 2, text: "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3", correct: false },
                { id: 3, text: "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSetting up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years is the most cost-effective solution that meets all requirements. Data from the most recent 2 years remains in S3 Standard, providing high availability and immediate retrievability. After 2 years, objects automatically transition to Glacier Deep Archive, which is the lowest-cost storage class in S3 (designed for long-term archival with retrieval times of 12 hours). This approach optimizes costs by keeping frequently accessed recent data in Standard storage while archiving older data to the cheapest storage class. The data remains accessible (with retrieval time) and is stored for the required 25+ years at minimal cost.\n\n**Why option 0 is incorrect:**\nThe option that says set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately is incorrect because this would move all data (including recent data) to Glacier Deep Archive immediately, which doesn't meet the requirement that data from the most recent 2 years must be highly available and immediately retrievable. Glacier Deep Archive has a 12-hour retrieval time, which is not \"immediately retrievable.\" Recent data should stay in S3 Standard for immediate access.\n\n**Why option 2 is incorrect:**\nThe option that says use S3 Intelligent-Tiering and activate the archiving option to ensure data is archived in S3 Glacier Deep Archive is incorrect because while Intelligent-Tiering can automatically move objects between access tiers, it doesn't provide the same level of control as lifecycle policies for ensuring data stays in Standard for exactly 2 years before archiving. Additionally, Intelligent-Tiering monitors access patterns and may archive data sooner if it's not accessed, which could violate the requirement for 2 years of immediate availability. Lifecycle policies provide more predictable, rule-based transitions.\n\n**Why option 3 is incorrect:**\nThe option that says set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access immediately and to S3 Glacier Deep Archive after 2 years is incorrect because transitioning to One Zone-IA immediately would move all data (including recent data) to Infrequent Access storage, which may not provide the same level of availability as Standard. Additionally, One Zone-IA stores data in a single Availability Zone, providing less durability than Standard (which stores across multiple AZs). The requirement specifies that recent data must be \"highly available,\" which Standard provides better than One Zone-IA.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A company runs its ecommerce application on AWS. Every new order is published as a message \nin a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These \nmessages are processed by a different application that runs on a separate EC2 instance. This \napplication stores the details in a PostgreSQL database on another EC2 instance. All the EC2 \ninstances are in the same Availability Zone. \nThe company needs to redesign its architecture to provide the highest availability with the least \noperational overhead. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.", correct: false },
                { id: 1, text: "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.", correct: true },
                { id: 2, text: "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.", correct: false },
                { id: 3, text: "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nMigrating the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ, creating a Multi-AZ Auto Scaling group for EC2 instances that host the application, and migrating the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL provides the highest availability with the least operational overhead. Amazon MQ is a managed message broker service that provides active/standby broker deployment across multiple Availability Zones, eliminating the need to manage RabbitMQ on EC2 instances. RDS Multi-AZ provides automatic failover, backups, and patching for PostgreSQL, significantly reducing operational overhead compared to managing PostgreSQL on EC2. The Multi-AZ Auto Scaling group ensures the application layer is highly available. This solution leverages managed services to minimize operational complexity while maximizing availability.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the queue to Amazon MQ, create Multi-AZ Auto Scaling groups for the application, and create another Multi-AZ Auto Scaling group for EC2 instances hosting PostgreSQL is incorrect because managing PostgreSQL on EC2 instances requires significant operational overhead including database installation, configuration, backups, patching, replication setup, and monitoring. RDS provides all of this as a managed service, reducing operational burden. Creating an Auto Scaling group for database EC2 instances doesn't provide the same level of database-specific high availability features (like automatic failover, synchronous replication) that RDS Multi-AZ provides.\n\n**Why option 2 is incorrect:**\nThe option that says create a Multi-AZ Auto Scaling group for EC2 instances hosting RabbitMQ queue, create another for the application, and migrate the database to RDS Multi-AZ is incorrect because managing RabbitMQ on EC2 instances in an Auto Scaling group requires significant operational overhead including broker installation, configuration, clustering setup, monitoring, and failover management. Amazon MQ provides all of this as a managed service with active/standby deployment, eliminating the need to manage RabbitMQ infrastructure. Auto Scaling groups don't provide message broker-specific high availability features.\n\n**Why option 3 is incorrect:**\nThe option that says create Multi-AZ Auto Scaling groups for RabbitMQ queue EC2 instances, application EC2 instances, and PostgreSQL database EC2 instances is incorrect because this approach requires managing all three components (RabbitMQ, application, and PostgreSQL) on EC2 instances, which maximizes operational overhead. None of the components benefit from managed services. This solution requires the most operational effort for database management, message broker management, and application infrastructure management compared to using managed services like Amazon MQ and RDS.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 56,
            text: "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.", correct: false },
                { id: 1, text: "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.", correct: false },
                { id: 2, text: "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.", correct: false },
                { id: 3, text: "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nConfiguring S3 replication between the S3 buckets, configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events), configuring an ObjectCreated rule in EventBridge, and configuring Lambda and SageMaker Pipelines as targets for the rule provides the least operational overhead. S3 replication automatically copies files from the source bucket to the destination bucket without requiring Lambda functions to manage the copying process, which is more efficient for large files. EventBridge is more advanced than S3 event notifications and supports multiple targets including Lambda and SageMaker Pipelines. EventBridge also provides filtering and pattern matching capabilities, allowing you to route events based on file patterns or metadata. This solution eliminates the need to write and maintain Lambda code for file copying and provides a more scalable, event-driven architecture.\n\n**Why option 0 is incorrect:**\nThe option that says create a Lambda function to copy files to the analysis bucket, create an S3 event notification for the analysis bucket, and configure Lambda and SageMaker as destinations is incorrect because this approach requires a Lambda function to handle file copying, which adds operational overhead for managing Lambda code, error handling, and retries. Additionally, S3 event notifications have limitations - they may not directly support SageMaker Pipelines as a destination, and they don't provide the same filtering capabilities as EventBridge. For large files, Lambda functions have timeout and memory limits that may not be suitable.\n\n**Why option 1 is incorrect:**\nThe option that says create a Lambda function to copy files to the analysis bucket and configure EventBridge with Lambda and SageMaker as targets is incorrect because it still requires a Lambda function to handle file copying, which adds operational overhead. While EventBridge is better than S3 event notifications, using Lambda for file copying is less efficient than S3 replication, especially for large files. S3 replication is a native feature that handles copying automatically without requiring code.\n\n**Why option 2 is incorrect:**\nThe option that says configure S3 replication and create an S3 event notification for the analysis bucket with Lambda and SageMaker as destinations is incorrect because S3 event notifications have limitations - they may not directly support SageMaker Pipelines as a destination, and they don't provide the advanced filtering and routing capabilities that EventBridge offers. EventBridge provides better integration with multiple AWS services and more flexible event routing.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A solutions architect needs to help a company optimize the cost of running an application on \nAWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for \ncompute within the architecture. \n \nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic \nand unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The \napplication front end will run on Fargate, and Lambda will serve the API layer. The front-end \nutilization and API layer utilization will be predictable over the course of the next year. \n \nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting \nthis application? (Choose two.)",
            options: [
                { id: 0, text: "Use Spot Instances for the data ingestion layer", correct: true },
                { id: 1, text: "Use On-Demand Instances for the data ingestion layer", correct: false },
                { id: 2, text: "Purchase a 1-year Compute Savings Plan for the front end and API layer.", correct: false },
                { id: 3, text: "Purchase 1-year All Upfront Reserved instances for the data ingestion layer.", correct: false },
                { id: 4, text: "Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nUsing Spot Instances for the data ingestion layer is the most cost-effective choice because EC2 usage is sporadic, unpredictable, and workloads can be interrupted at any time. Spot Instances provide up to 90% savings compared to On-Demand Instances and are ideal for interruptible workloads. Since the data ingestion layer can tolerate interruptions, Spot Instances maximize cost savings while still providing the compute capacity needed when available.\n\n**Why option 2 is correct:**\nPurchasing a 1-year Compute Savings Plan for the front end (Fargate) and API layer (Lambda) is the most cost-effective option because Compute Savings Plans apply to Fargate and Lambda usage, providing up to 66% savings compared to On-Demand pricing. Since front-end and API layer utilization is predictable over the next year, committing to a Savings Plan makes financial sense. Compute Savings Plans provide flexibility by automatically applying to EC2, Fargate, and Lambda usage regardless of instance family, size, AZ, region, OS, or tenancy, making them ideal for mixed compute architectures.\n\n**Why option 1 is incorrect:**\nThe option that says use On-Demand Instances for the data ingestion layer is incorrect because On-Demand Instances are significantly more expensive than Spot Instances (up to 90% more). Since the workload can be interrupted and is sporadic/unpredictable, Spot Instances provide the same functionality at a much lower cost. On-Demand Instances should be reserved for workloads that cannot tolerate interruptions.\n\n**Why option 3 is incorrect:**\nThe option that says purchase 1-year All Upfront Reserved Instances for the data ingestion layer is incorrect because Reserved Instances require a commitment for predictable workloads, but the scenario states EC2 usage is sporadic and unpredictable. Reserved Instances don't provide the flexibility needed for dynamic workloads and may result in paying for capacity that isn't always used. Additionally, Reserved Instances don't apply to Fargate or Lambda, so they wouldn't help with the front-end and API layer costs.\n\n**Why option 4 is incorrect:**\nThe option that says purchase a 1-year EC2 instance Savings Plan for the front end and API layer is incorrect because EC2 instance Savings Plans only apply to EC2 usage, not to Fargate or Lambda. The front end runs on Fargate and the API layer runs on Lambda, so an EC2 instance Savings Plan would not provide any savings for these components. Compute Savings Plans are needed to cover Fargate and Lambda usage.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 58,
            text: "A company runs a web-based portal that provides users with global breaking news, local alerts, \nand weather updates. The portal delivers each user a personalized view by using mixture of static \nand dynamic content. Content is served over HTTPS through an API server running on an \nAmazon EC2 instance behind an Application Load Balancer (ALB). The company wants the \nportal to provide this content to its users across the world as quickly as possible. \n \nHow should a solutions architect design the application to ensure the LEAST amount of latency \nfor all users?",
            options: [
                { id: 0, text: "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.", correct: true },
                { id: 1, text: "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.", correct: false },
                { id: 2, text: "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB.", correct: false },
                { id: 3, text: "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDeploying the application stack in a single AWS Region and using Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin provides the least latency for global users. CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, bringing content closer to users regardless of their geographic location. CloudFront can cache static content at edge locations and can also accelerate dynamic content by routing requests over AWS's optimized network backbone. By using CloudFront with the ALB as the origin, users worldwide receive content from the nearest edge location, significantly reducing latency compared to accessing the ALB directly from a single region. This approach provides global low latency without requiring multi-region deployment.\n\n**Why option 1 is incorrect:**\nThe option that says deploy the application stack in two AWS Regions and use Route 53 latency routing to serve content from the closest ALB is incorrect because while Route 53 can route users to the closest region, users still access the ALB directly, which means they don't benefit from CloudFront's edge caching and optimized routing. Multi-region deployment adds complexity and cost without providing the same level of latency reduction as CloudFront's global edge network. Route 53 routing is based on DNS resolution, which doesn't provide the same performance benefits as CloudFront's edge caching.\n\n**Why option 2 is incorrect:**\nThe option that says deploy in a single region, use CloudFront for static content, and serve dynamic content directly from the ALB is incorrect because serving dynamic content directly from the ALB means users worldwide must connect to a single region, resulting in higher latency for users far from that region. CloudFront can accelerate dynamic content by using AWS's optimized network and connection pooling, providing better performance than direct ALB access. Using CloudFront for both static and dynamic content provides better overall latency reduction.\n\n**Why option 3 is incorrect:**\nThe option that says deploy in two regions and use Route 53 geolocation routing to serve content from the closest ALB is incorrect because geolocation routing is based on user location, not actual network latency. Users may be geographically close to a region but experience higher latency due to network conditions. Additionally, this approach doesn't leverage CloudFront's edge caching and optimized routing, which provides better performance than direct ALB access. Multi-region deployment also adds operational complexity and cost.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A gaming company is designing a highly available architecture. The application runs on a \nmodified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier \nto provide the best possible user experience. That tier must have low latency, route traffic to the \nnearest edge location, and provide static IP addresses for entry into the application endpoints. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Configure Amazon Route 53 to forward requests to an Application Load Balancer.", correct: false },
                { id: 1, text: "Configure Amazon CloudFront to forward requests to a Network Load Balancer.", correct: false },
                { id: 2, text: "Configure AWS Global Accelerator to forward requests to a Network Load Balancer.", correct: true },
                { id: 3, text: "Configure Amazon API Gateway to forward requests to an Application Load Balancer.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nConfiguring AWS Global Accelerator to forward requests to a Network Load Balancer is the correct solution for UDP-based gaming applications requiring low latency, routing to nearest edge locations, and static IP addresses. Global Accelerator uses AWS's global network infrastructure and edge locations to route traffic to the optimal endpoint based on performance metrics, user location, and application health. It provides static IP addresses that serve as fixed entry points, which is essential for gaming applications that may have firewall rules or client configurations tied to specific IPs. Global Accelerator supports both TCP and UDP protocols, making it suitable for gaming workloads. It automatically routes traffic to the nearest healthy endpoint, providing low latency and high availability. Network Load Balancers are appropriate for UDP traffic and gaming workloads.\n\n**Why option 0 is incorrect:**\nThe option that says configure Amazon Route 53 to forward requests to an Application Load Balancer is incorrect because Route 53 is a DNS service that routes based on DNS queries, not a traffic routing service for low-latency UDP connections. Additionally, Application Load Balancers operate at Layer 7 (HTTP/HTTPS) and are not designed for UDP traffic. Gaming applications using UDP require Layer 4 load balancing provided by Network Load Balancers, not ALBs.\n\n**Why option 1 is incorrect:**\nThe option that says configure Amazon CloudFront to forward requests to a Network Load Balancer is incorrect because CloudFront is designed for HTTP/HTTPS content delivery and caching, not for UDP-based gaming traffic. CloudFront operates at the application layer (HTTP) and doesn't support UDP protocols. While CloudFront can accelerate web content, it's not suitable for real-time gaming applications that require UDP support and low-latency packet routing.\n\n**Why option 3 is incorrect:**\nThe option that says configure Amazon API Gateway to forward requests to an Application Load Balancer is incorrect because API Gateway is designed for RESTful APIs over HTTP/HTTPS, not for UDP-based gaming traffic. API Gateway doesn't support UDP protocols and operates at the application layer. Additionally, Application Load Balancers don't support UDP traffic - they only handle HTTP/HTTPS. Gaming applications require UDP support and Layer 4 load balancing.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A company wants to migrate its existing on-premises monolithic application to AWS. The \ncompany wants to keep as much of the front-end code and the backend code as possible. \nHowever, the company wants to break the application into smaller applications. A different team \nwill manage each application. The company needs a highly scalable solution that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Host the application on AWS Lambda Integrate the application with Amazon API Gateway.", correct: false },
                { id: 1, text: "Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API", correct: false },
                { id: 2, text: "Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2", correct: false },
                { id: 3, text: "Host the application on Amazon Elastic Container Service (Amazon ECS) Set up an Application", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nHosting the application on Amazon Elastic Container Service (Amazon ECS) and setting up an Application Load Balancer with ECS as the target is the best solution for migrating a monolithic application to microservices while keeping existing code. ECS allows you to containerize the existing application code with minimal changes, breaking it into smaller containerized services that can be managed by different teams. Each microservice can run as a separate ECS service, allowing independent scaling, deployment, and management. ECS provides automatic scaling, load balancing, service discovery, and integration with other AWS services, minimizing operational overhead. The Application Load Balancer can route traffic to different ECS services based on path or host, enabling the microservices architecture. This approach allows the company to gradually break down the monolith while reusing existing code.\n\n**Why option 0 is incorrect:**\nThe option that says host the application on AWS Lambda and integrate with API Gateway is incorrect because Lambda functions require significant code refactoring to fit the serverless, event-driven model. The existing monolithic application code would need to be rewritten to work as Lambda functions, which violates the requirement to \"keep as much of the front-end code and the backend code as possible.\" Lambda functions are stateless and have execution time limits, which may not be suitable for all parts of a monolithic application without significant architectural changes.\n\n**Why option 1 is incorrect:**\nThe option that says host the application with AWS Amplify and connect to an API Gateway API integrated with Lambda is incorrect because Amplify is primarily designed for front-end web and mobile applications, not for hosting backend monolithic applications. Additionally, this approach still requires rewriting backend code to work with Lambda functions, which doesn't meet the requirement to keep existing code. Amplify is better suited for building new applications from scratch rather than migrating existing monolithic applications.\n\n**Why option 2 is incorrect:**\nThe option that says host the application on Amazon EC2 instances and set up an Application Load Balancer with EC2 instances in an Auto Scaling group is incorrect because while this allows keeping existing code, it doesn't provide the same level of operational efficiency and microservices support as ECS. Managing applications directly on EC2 requires more operational overhead for container orchestration, service discovery, and deployment automation. ECS provides better tooling and integration for breaking applications into microservices and managing them independently.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 61,
            text: "A company recently started using Amazon Aurora as the data store for its global ecommerce \napplication.  \nWhen large reports are run developers report that the ecommerce application is performing \npoorly After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the \nReadlOPS and CPUUtilization metrics are spiking when monthly reports run. \nWhat is the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Migrate the monthly reporting to Amazon Redshift.", correct: false },
                { id: 1, text: "Migrate the monthly reporting to an Aurora Replica", correct: true },
                { id: 2, text: "Migrate the Aurora database to a larger instance class", correct: false },
                { id: 3, text: "Increase the Provisioned IOPS on the Aurora instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nMigrating the monthly reporting to an Aurora Replica is the most cost-effective solution. Aurora Replicas are read-only copies of the primary database that can handle read traffic, offloading reporting queries from the primary instance. By directing reporting queries to a replica, the primary database's ReadIOPS and CPU utilization are not impacted, allowing the ecommerce application to perform normally. Aurora Replicas are automatically replicated from the primary instance and can be created with a single API call. This solution doesn't require migrating data to a different service or increasing instance size, making it the most cost-effective approach. The replica shares the same storage volume as the primary (using copy-on-write), so there's no additional storage cost.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the monthly reporting to Amazon Redshift is incorrect because migrating reporting to Redshift requires extracting, transforming, and loading (ETL) data from Aurora to Redshift, which adds significant complexity and operational overhead. Redshift is a data warehouse designed for analytics workloads, but setting up ETL pipelines, maintaining data synchronization, and managing a separate data warehouse adds cost and complexity. This is overkill for monthly reporting that can be handled by an Aurora Replica.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the Aurora database to a larger instance class is incorrect because this increases costs significantly (larger instances cost more) and doesn't address the root cause - reporting queries competing with production traffic. A larger instance would handle both workloads, but reporting queries would still impact production performance during report execution. This solution is more expensive than using a replica and doesn't provide the same isolation.\n\n**Why option 3 is incorrect:**\nThe option that says increase the Provisioned IOPS on the Aurora instance is incorrect because Aurora doesn't use Provisioned IOPS - it uses a distributed storage system that automatically scales IOPS based on database activity. You cannot manually provision IOPS for Aurora. Additionally, increasing IOPS capacity (if it were possible) would increase costs and still wouldn't isolate reporting queries from production traffic like a replica would.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 62,
            text: "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. \nThe analytics software is written in PHP and uses a MySQL database. The analytics software, the \nweb server that provides PHP, and the database server are all hosted on the EC2 instance. The \napplication is showing signs of performance degradation during busy times and is presenting 5xx \nerrors.  \nThe company needs to make the application scale seamlessly. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.", correct: false },
                { id: 1, text: "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.", correct: false },
                { id: 2, text: "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.", correct: false },
                { id: 3, text: "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nMigrating the database to an Amazon Aurora MySQL DB instance, creating an AMI of the web application, applying it to a launch template, creating an Auto Scaling group with the launch template configured to use a Spot Fleet, and attaching an Application Load Balancer provides the most cost-effective scalable solution. Aurora provides managed database services with automatic backups and scaling, reducing operational overhead. Spot Fleet allows you to use a combination of Spot and On-Demand Instances, providing significant cost savings (up to 90% with Spot) while ensuring capacity availability. Auto Scaling automatically adjusts the number of instances based on demand, providing seamless scaling. The Application Load Balancer distributes traffic across instances, ensuring high availability. This solution addresses performance degradation by scaling horizontally and provides cost optimization through Spot Fleet.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the database to RDS MySQL, create an AMI, launch a second EC2 On-Demand Instance, and use an ALB is incorrect because this creates a fixed two-instance setup that doesn't scale automatically. While it provides some redundancy, it doesn't address the \"scale seamlessly\" requirement. The application would still experience performance issues if two instances aren't enough, and you would need to manually add more instances. Additionally, using only On-Demand Instances is more expensive than using Spot Fleet.\n\n**Why option 1 is incorrect:**\nThe option that says migrate the database to RDS MySQL, create an AMI, launch a second EC2 On-Demand Instance, and use Route 53 weighted routing is incorrect because Route 53 weighted routing is a DNS-level load balancing solution that doesn't provide the same level of health checking, connection draining, and automatic failover as an Application Load Balancer. DNS-based routing has longer TTLs and doesn't react as quickly to instance failures. Additionally, this still creates a fixed two-instance setup without automatic scaling.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the database to Aurora, create a Lambda function to stop the EC2 instance and change the instance type, and use a CloudWatch alarm is incorrect because stopping an instance to change its type causes downtime, which doesn't meet the \"scale seamlessly\" requirement. This approach also doesn't provide horizontal scaling - it only changes the instance size vertically. Additionally, stopping and changing instance types takes time and causes service interruption, which is not seamless scaling.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 63,
            text: "A company runs a stateless web application in production on a group of Amazon EC2 On-\nDemand Instances behind an Application Load Balancer. The application experiences heavy \nusage during an 8-hour period each business day. Application usage is moderate and steady \novernight Application usage is low during weekends. \nThe company wants to minimize its EC2 costs without affecting the availability of the application. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Spot Instances for the entire workload.", correct: false },
                { id: 1, text: "Use Reserved instances for the baseline level of usage.", correct: true },
                { id: 2, text: "Use On-Demand Instances for the baseline level of usage.", correct: false },
                { id: 3, text: "Use Dedicated Instances for the baseline level of usage.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Reserved Instances for the baseline level of usage and Spot Instances for any additional capacity provides the most cost-effective solution while maintaining availability. Reserved Instances provide significant cost savings (up to 72% compared to On-Demand) for predictable baseline workloads, which covers the moderate overnight usage and low weekend usage. Spot Instances can handle the additional capacity needed during the 8-hour heavy usage period, providing up to 90% savings compared to On-Demand. Since the application is stateless and behind a load balancer, if Spot Instances are interrupted, the Reserved Instances continue serving traffic, maintaining availability. This combination optimizes costs for both predictable and variable workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use Spot Instances for the entire workload is incorrect because Spot Instances can be interrupted with only 2 minutes notice, which could impact availability if all instances are Spot. While the application is stateless and behind a load balancer, having all instances as Spot creates a risk of simultaneous interruptions that could affect availability. For production applications, it's best practice to have a baseline of guaranteed capacity (Reserved or On-Demand) with Spot for additional capacity.\n\n**Why option 2 is incorrect:**\nThe option that says use On-Demand Instances for the baseline and Spot Instances for additional capacity is incorrect because while this provides availability, On-Demand Instances are significantly more expensive than Reserved Instances for predictable baseline workloads. Since overnight and weekend usage is moderate and steady (predictable), committing to Reserved Instances for the baseline provides substantial cost savings (up to 72%) compared to On-Demand, without affecting availability.\n\n**Why option 3 is incorrect:**\nThe option that says use Dedicated Instances for the baseline and On-Demand Instances for additional capacity is incorrect because Dedicated Instances are the most expensive option, providing physical isolation but at a premium cost. Dedicated Instances are typically 10-20% more expensive than On-Demand Instances and don't provide the same cost savings as Reserved Instances. This approach maximizes costs rather than minimizing them, which doesn't meet the requirement.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A company needs to retain application logs files for a critical application for 10 years. The \napplication team regularly accesses logs from the past month for troubleshooting, but logs older \nthan 1 month are rarely accessed. The application generates more than 10 TB of logs per month. \nWhich storage option meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.", correct: false },
                { id: 1, text: "Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.", correct: true },
                { id: 2, text: "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.", correct: false },
                { id: 3, text: "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nStoring the logs in Amazon S3 and using S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive is the most cost-effective solution. S3 provides immediate access to recent logs (past month) for troubleshooting, meeting the requirement for regular access. S3 Lifecycle policies automatically transition objects to Glacier Deep Archive after 30 days, which is the lowest-cost storage class in S3 (designed for long-term archival with 12-hour retrieval times). For logs older than 1 month that are rarely accessed, Glacier Deep Archive provides the lowest storage cost while still maintaining accessibility. This approach optimizes costs by keeping frequently accessed recent logs in Standard storage and archiving older logs to the cheapest storage class. S3 can store virtually unlimited amounts of data and supports 10-year retention requirements.\n\n**Why option 0 is incorrect:**\nThe option that says store logs in S3 and use AWS Backup to move logs more than 1 month old to Glacier Deep Archive is incorrect because AWS Backup is designed for backing up AWS resources (like EBS volumes, RDS databases, DynamoDB tables), not for managing S3 object lifecycle transitions. S3 Lifecycle policies are the native, automated way to transition objects between storage classes. Using AWS Backup for this purpose adds unnecessary complexity and operational overhead compared to S3 Lifecycle policies, which are simpler and more cost-effective.\n\n**Why option 2 is incorrect:**\nThe option that says store logs in CloudWatch Logs and use AWS Backup to move logs more than 1 month old to Glacier Deep Archive is incorrect because CloudWatch Logs has limitations - it doesn't support direct lifecycle policies to archive to Glacier Deep Archive, and CloudWatch Logs is more expensive than S3 for large-scale log storage (10+ TB per month). CloudWatch Logs charges for ingestion, storage, and data transfer, making it cost-prohibitive for 10 TB+ per month over 10 years. Additionally, CloudWatch Logs doesn't provide the same level of lifecycle management as S3.\n\n**Why option 3 is incorrect:**\nThe option that says store logs in CloudWatch Logs and use S3 Lifecycle policies to move logs to Glacier Deep Archive is incorrect because you cannot apply S3 Lifecycle policies to CloudWatch Logs. CloudWatch Logs and S3 are separate services, and lifecycle policies are specific to S3 buckets. CloudWatch Logs would need to export logs to S3 first before lifecycle policies could be applied. Additionally, CloudWatch Logs is significantly more expensive than S3 for large-scale log storage, making it not cost-effective for 10 TB+ per month.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test10: [
        {
            id: 0,
            text: "A company has a data ingestion workflow that includes the following components: \n \n- An Amazon Simple Notation Service (Amazon SNS) topic that receives \nnotifications about new data deliveries. \n- An AWS Lambda function that processes and stores the data \n \nThe ingestion workflow occasionally fails because of network connectivity issues.  \nWhen tenure occurs the corresponding data is not ingested unless the company manually reruns \nthe job. \n \nWhat should a solutions architect do to ensure that all notifications are eventually processed?",
            options: [
                { id: 0, text: "Configure the Lambda function for deployment across multiple Availability Zones", correct: false },
                { id: 1, text: "Modify me Lambda functions configuration to increase the CPU and memory allocations tor the", correct: false },
                { id: 2, text: "Configure the SNS topic's retry strategy to increase both the number of retries and the wait time", correct: false },
                { id: 3, text: "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on failure", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nConfiguring an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination for the SNS topic and modifying the Lambda function to process messages in the queue ensures that all notifications are eventually processed. When the Lambda function fails due to network connectivity issues, SNS can route failed messages to the SQS dead-letter queue (DLQ). SQS provides durable message storage, ensuring messages are not lost even if processing fails. The Lambda function can then be configured to poll the SQS queue and retry processing messages until they succeed. This decouples the message delivery from processing, providing resilience against transient failures. SQS automatically retries message delivery and maintains messages until they are successfully processed and deleted.\n\n**Why option 0 is incorrect:**\nThe option that says configure the Lambda function for deployment across multiple Availability Zones is incorrect because Lambda functions are automatically deployed across multiple AZs by AWS - you cannot configure this manually. Additionally, deploying across multiple AZs doesn't solve the problem of network connectivity issues causing message processing failures. The issue is that when Lambda fails, messages are lost unless there's a mechanism to retry them.\n\n**Why option 1 is incorrect:**\nThe option that says modify the Lambda function configuration to increase CPU and memory allocations is incorrect because increasing CPU and memory doesn't address network connectivity issues. Network connectivity problems are external to the Lambda function's resources and won't be resolved by allocating more compute resources. The function may still fail due to network issues regardless of its CPU/memory configuration.\n\n**Why option 2 is incorrect:**\nThe option that says configure the SNS topic's retry strategy to increase both the number of retries and the wait time between retries is incorrect because while SNS can retry failed deliveries, if the Lambda function continues to fail due to persistent network issues, retries will eventually exhaust and messages may be lost. SNS retries have limits, and without a durable storage mechanism like SQS, messages can be lost if all retries fail. Additionally, SNS doesn't have configurable retry strategies in the way described - it has default retry policies.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 1,
            text: "A company has a service that produces event data. The company wants to use AWS to process \nthe event data as it is received.  \nThe data is written in a specific order that must be maintained throughout processing.  \nThe company wants to implement a solution that minimizes operational overhead. \nHow should a solutions architect accomplish this?",
            options: [
                { id: 0, text: "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages.", correct: true },
                { id: 1, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications", correct: false },
                { id: 2, text: "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages.", correct: false },
                { id: 3, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreating an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages and setting up an AWS Lambda function to process messages from the queue is the solution that minimizes operational overhead while maintaining message order. FIFO queues guarantee that messages are processed exactly once and in the exact order they are sent, which is essential when data must be written and processed in a specific order. FIFO queues are fully managed, requiring no infrastructure provisioning or management. Lambda functions can be configured as event sources for SQS queues, automatically triggering when messages arrive, which minimizes operational overhead. This serverless architecture scales automatically and requires minimal configuration.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon SNS topic to deliver notifications containing payloads and configure a Lambda function as a subscriber is incorrect because SNS is a pub/sub messaging service that delivers messages immediately to all subscribers, but it doesn't guarantee message ordering. SNS messages can arrive out of order, and if a subscriber is unavailable, messages may be lost. SNS doesn't provide the FIFO ordering guarantee required when data must be written and processed in a specific order.\n\n**Why option 2 is incorrect:**\nThe option that says create an Amazon SQS standard queue to hold messages and set up a Lambda function to process messages independently is incorrect because standard SQS queues provide at-least-once delivery but don't guarantee message ordering. Messages in a standard queue may be delivered out of order, which violates the requirement that data must be written and processed in a specific order. Standard queues are designed for high throughput but don't maintain strict ordering.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon SNS topic to deliver notifications and configure an SQS queue as a subscriber is incorrect because while this creates a queue, SNS doesn't guarantee message ordering. When SNS delivers messages to an SQS queue, the messages may arrive out of order. Additionally, if you need FIFO ordering, you would need a FIFO SQS queue, but SNS cannot deliver directly to FIFO queues in a way that maintains strict ordering. The direct SQS FIFO queue approach is simpler and more operationally efficient.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company is migrating an application from on-premises servers to Amazon EC2 instances. As \npart of the migration design requirements, a solutions architect must implement infrastructure \nmetric alarms. The company does not need to take action if CPU utilization increases to more \nthan 50% for a short burst of time. However, if the CPU utilization increases to more than 50% \nand read IOPS on the disk are high at the same time, the company needs to act as soon as \npossible. The solutions architect also must reduce false alarms. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create Amazon CloudWatch composite alarms where possible.", correct: true },
                { id: 1, text: "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.", correct: false },
                { id: 2, text: "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.", correct: false },
                { id: 3, text: "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreating Amazon CloudWatch composite alarms is the correct solution for monitoring multiple conditions simultaneously and reducing false alarms. Composite alarms evaluate the states of multiple underlying metric alarms and only trigger when all specified conditions are met. In this case, you can create two metric alarms: one for CPU utilization > 50% and another for high read IOPS. Then create a composite alarm that only goes into ALARM state when BOTH underlying alarms are in ALARM state simultaneously. This ensures that alerts are only sent when CPU is high AND disk IOPS are high at the same time, not when CPU spikes briefly alone. Composite alarms reduce false alarms by requiring multiple conditions to be true before alerting, which is exactly what's needed here.\n\n**Why option 1 is incorrect:**\nThe option that says create Amazon CloudWatch dashboards to visualize metrics and react to issues quickly is incorrect because dashboards are visualization tools that require manual monitoring and reaction. Dashboards don't automatically alert when conditions are met - they just display metrics. The requirement is to \"act as soon as possible\" when specific conditions occur, which requires automated alarms, not manual dashboard monitoring. Dashboards don't reduce false alarms or provide automated alerting.\n\n**Why option 2 is incorrect:**\nThe option that says create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm is incorrect because Synthetics canaries are designed for end-to-end monitoring of application availability and performance from a user's perspective, not for monitoring infrastructure metrics like CPU utilization and disk IOPS. Canaries run synthetic transactions to test application functionality, but they don't monitor EC2 instance-level metrics. This approach doesn't address the requirement to monitor CPU and IOPS metrics simultaneously.\n\n**Why option 3 is incorrect:**\nThe option that says create single Amazon CloudWatch metric alarms with multiple metric thresholds is incorrect because a single metric alarm can only monitor one metric at a time. You cannot create a single alarm that monitors both CPU utilization and read IOPS simultaneously. To monitor multiple metrics together, you need either multiple alarms with a composite alarm, or separate alarms that don't provide the \"both conditions must be true\" logic needed to reduce false alarms.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A company wants to migrate its on-premises data center to AWS. According to the company's \ncompliance requirements, the company can use only the ap-northeast-3 Region. Company \nadministrators are not permitted to connect VPCs to the internet. \n \nWhich solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Control Tower to implement data residency guardrails to deny internet access and", correct: true },
                { id: 1, text: "Use rules in AWS WAF to prevent internet access.", correct: false },
                { id: 2, text: "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from", correct: false },
                { id: 3, text: "Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0.", correct: false },
                { id: 4, text: "Use AWS Config to activate managed rules to detect and alert for internet gateways and to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3 is correct. AWS Control Tower provides guardrails that can enforce organizational policies at scale. The \"Region Deny\" guardrail can restrict resource creation to only the ap-northeast-3 Region, ensuring compliance with data residency requirements. Additionally, Control Tower can implement guardrails that prevent VPCs from having internet gateways attached, effectively denying internet access. Control Tower integrates with AWS Organizations and provides centralized governance, making it the most comprehensive solution for enforcing both the region restriction and the no-internet-access requirement across all accounts.\n\n**Why option 1 is incorrect:**\nThe option that says use rules in AWS WAF to prevent internet access is incorrect because AWS WAF (Web Application Firewall) is designed to protect web applications from common web exploits and attacks. WAF operates at the application layer (Layer 7) and filters HTTP/HTTPS traffic, but it cannot prevent VPCs from connecting to the internet. WAF doesn't control network-level access or prevent internet gateway attachments. Additionally, denying access to all AWS Regions except ap-northeast-3 in AWS account settings is not a standard feature - region restrictions require service control policies or Control Tower guardrails.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Organizations to configure service control policies (SCPs) that prevent VPCs from gaining internet access is partially correct but incomplete. While SCPs can restrict certain actions, SCPs cannot directly prevent VPCs from having internet gateways attached or prevent internet connectivity. SCPs work by denying IAM permissions, but internet gateway attachment and routing are network-level configurations that SCPs cannot directly control. However, SCPs can be used to deny access to regions other than ap-northeast-3, but this alone doesn't fully meet the requirement to prevent internet access.\n\n**Why option 3 is incorrect:**\nThe option that says create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0 is incorrect because network ACLs (NACLs) are stateless and operate at the subnet level, not the VPC level. Creating NACL rules to deny outbound traffic would prevent legitimate internal communication and would need to be configured for each subnet in each VPC, which is operationally complex. Additionally, NACLs don't prevent internet gateways from being attached to VPCs - they only filter traffic. The IAM policy approach to prevent region usage is also not the most effective method compared to SCPs or Control Tower guardrails.\n\n**Why option 4 is incorrect:**\nThe option that says use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3 is incorrect because AWS Config is a compliance and auditing service that detects and reports on configuration changes, but it does not prevent actions from occurring. Config can alert when internet gateways are attached or when resources are created in other regions, but it cannot prevent these actions. The requirement is to prevent VPCs from connecting to the internet, not just to detect and alert when they do. Config is reactive, not preventive.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company uses a three-tier web application to provide training to new employees. The \napplication is accessed for only 12 hours every day. The company is using an Amazon RDS for \nMySQL DB instance to store information and wants to minimize costs. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Configure an IAM policy for AWS Systems Manager Session Manager.", correct: false },
                { id: 1, text: "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the", correct: false },
                { id: 2, text: "Launch an Amazon EC2 instance.", correct: false },
                { id: 3, text: "Create AWS Lambda functions to start and stop the DB instance.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCreating AWS Lambda functions to start and stop the DB instance, along with Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions, is the most cost-effective solution. Since the application is only accessed for 12 hours per day, the RDS instance can be stopped during the 12 hours when it's not needed, eliminating compute costs. Lambda functions can be created to start the RDS instance before the 12-hour access window and stop it after. EventBridge scheduled rules can trigger these Lambda functions on a daily schedule (e.g., start at 8 AM, stop at 8 PM). This approach minimizes costs by only paying for RDS compute resources during the 12 hours of actual usage, while storage costs continue at a lower rate. Lambda functions are serverless and incur minimal costs, making this solution highly cost-effective.\n\n**Why option 0 is incorrect:**\nThe option that says configure an IAM policy for AWS Systems Manager Session Manager, create an IAM role for the policy, update the trust relationship of the role, and set up automatic start and stop for the DB instance is incorrect because Systems Manager Session Manager is used for secure shell access to EC2 instances, not for managing RDS instances. Session Manager doesn't have built-in capabilities to start and stop RDS instances on a schedule. While you could potentially use Systems Manager Automation documents, this approach is more complex and less efficient than using Lambda with EventBridge. The IAM policy and role configuration described doesn't directly enable scheduled start/stop functionality.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from the cache when the DB instance is stopped, and invalidate the cache after the DB instance is started is incorrect because ElastiCache doesn't solve the cost minimization requirement - it adds additional costs for the cache cluster. While caching can improve performance, the requirement is specifically to minimize costs for the RDS instance. Additionally, if the DB instance is stopped, users cannot access data from cache if the cache needs to be populated from the database. Cache invalidation after DB restart doesn't address the core requirement of minimizing RDS costs through scheduled start/stop.\n\n**Why option 2 is incorrect:**\nThe option that says launch an Amazon EC2 instance, create an IAM role that grants access to Amazon RDS, attach the role to the EC2 instance, and configure a cron job to start and stop the EC2 instance on the desired schedule is incorrect because this approach adds unnecessary infrastructure costs. Running an EC2 instance 24/7 just to manage RDS start/stop schedules defeats the purpose of cost minimization. The EC2 instance itself incurs costs even when idle. Additionally, the cron job would start and stop the EC2 instance, not the RDS instance. A serverless Lambda-based approach eliminates the need for a persistent EC2 instance and is more cost-effective.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company sells ringtones created from clips of popular songs. The files containing the ringtones \nare stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of \nfiles, but downloads are infrequent for ringtones older than 90 days. The company needs to save \nmoney on storage while keeping the most accessed files readily available for its users. \n \nWhich action should the company take to meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of", correct: false },
                { id: 1, text: "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive", correct: false },
                { id: 2, text: "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access", correct: false },
                { id: 3, text: "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nImplementing an S3 Lifecycle policy that moves objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days is the most cost-effective solution. S3 Lifecycle policies automatically transition objects between storage classes based on age, without requiring manual intervention or additional infrastructure. Since files older than 90 days have infrequent downloads, moving them to S3 Standard-IA reduces storage costs by approximately 50% while maintaining immediate access when needed. Files less than 90 days old remain in S3 Standard, keeping the most accessed files readily available with low latency. Lifecycle policies are free to configure and execute automatically, making this the most cost-effective approach that meets both the cost savings requirement and the need to keep frequently accessed files readily available.\n\n**Why option 0 is incorrect:**\nThe option that says configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects is incorrect because S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. If all files start in Standard-IA, the company will pay higher storage costs and retrieval fees for files that are frequently accessed (those less than 90 days old). Standard-IA has a minimum storage duration charge of 30 days and charges per-GB retrieval fees, which would be expensive for frequently accessed files. The requirement states that downloads are infrequent only for files older than 90 days, so newer files should remain in Standard storage.\n\n**Why option 1 is incorrect:**\nThe option that says move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days is incorrect because S3 Intelligent-Tiering automatically moves objects between access tiers based on access patterns, not based on age. Intelligent-Tiering monitors access patterns and moves objects that haven't been accessed for 30+ days to the Infrequent Access tier. However, you cannot configure Intelligent-Tiering to move objects after a specific number of days (like 90 days) - it works based on access patterns. Additionally, Intelligent-Tiering charges a small monitoring and automation fee per object, which could add up with millions of files, making it less cost-effective than a simple Lifecycle policy.\n\n**Why option 2 is incorrect:**\nThe option that says configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days is incorrect because S3 Inventory is a reporting and analytics tool that generates reports about objects and their metadata. Inventory reports are stored in S3 buckets and can be used for compliance, operational, and cost optimization purposes, but Inventory itself does not move objects between storage classes. You would need a separate process (like Lambda functions or manual scripts) to read the inventory reports and move objects, which adds complexity and operational overhead compared to the automated Lifecycle policy approach.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 6,
            text: "A company needs to save the results from a medical trial to an Amazon S3 repository. The \nrepository must allow a few scientists to add new files and must restrict all other users to read-\nonly access. No users can have the ability to modify or delete any files in the repository. The \ncompany must keep every file in the repository for a minimum of 1 year after its creation date. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n39",
            options: [
                { id: 0, text: "Use S3 Object Lock in governance mode with a legal hold of 1 year", correct: false },
                { id: 1, text: "Use S3 Object Lock in compliance mode with a retention period of 365 days.", correct: true },
                { id: 2, text: "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an", correct: false },
                { id: 3, text: "Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing S3 Object Lock in compliance mode with a retention period of 365 days is the correct solution. S3 Object Lock prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely. Compliance mode is the most restrictive mode - even the root user cannot delete or overwrite objects during the retention period, ensuring that files are kept for the minimum 1 year requirement. This provides immutable storage that meets compliance requirements for medical trial data. Combined with IAM policies that grant write access only to scientists and read-only access to others, this solution ensures that files cannot be modified or deleted by anyone during the retention period, while still allowing authorized scientists to add new files.\n\n**Why option 0 is incorrect:**\nThe option that says use S3 Object Lock in governance mode with a legal hold of 1 year is incorrect because governance mode allows users with special permissions (like `s3:BypassGovernanceMode`) to delete or overwrite objects during the retention period, which violates the requirement that no users can modify or delete files. Legal hold is a separate feature that prevents deletion until the legal hold is removed, but it doesn't have a fixed retention period - it must be manually removed. The requirement specifies a minimum 1 year retention period, which is better handled by compliance mode with a fixed retention period.\n\n**Why option 2 is incorrect:**\nThe option that says use an IAM role to restrict all users from deleting or changing objects in the S3 bucket is incorrect because IAM policies alone cannot prevent object deletion or modification if a user has the necessary permissions, and they don't provide immutable storage guarantees. Even with restrictive IAM policies, there's a risk that permissions could be changed, or that users with administrative access could delete objects. Additionally, IAM policies don't enforce time-based retention requirements - they can't guarantee that files are kept for a minimum of 1 year. S3 Object Lock provides a stronger, immutable storage mechanism that cannot be bypassed.\n\n**Why option 3 is incorrect:**\nThe option that says configure the S3 bucket to invoke an AWS Lambda function every time an object is added, and use the Lambda function to set a tag on the object with a timestamp is incorrect because S3 object tags are metadata that can be modified or deleted, and they don't prevent object deletion or modification. Tags are useful for organization and lifecycle policies, but they don't provide immutable storage or prevent users from deleting objects. Even if you use tags to track creation dates, there's no mechanism to enforce the 1-year retention requirement or prevent deletion based on tags alone.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A large media company hosts a web application on AWS. The company wants to start caching \nconfidential media files so that users around the world will have reliable access to the files. The \ncontent is stored in Amazon S3 buckets. The company must deliver the content quickly, \nregardless of where the requests originate geographically. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS DataSync to connect the S3 buckets to the web application.", correct: false },
                { id: 1, text: "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.", correct: false },
                { id: 2, text: "Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.", correct: true },
                { id: 3, text: "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nDeploying Amazon CloudFront to connect the S3 buckets to CloudFront edge servers is the correct solution for delivering confidential media files quickly to users worldwide. CloudFront is a Content Delivery Network (CDN) that caches content at edge locations around the world, reducing latency by serving content from locations closest to users. CloudFront can be configured with Origin Access Identity (OAI) or Origin Access Control (OAC) to securely access private S3 buckets, ensuring that confidential media files are protected. CloudFront caches files at edge locations, so subsequent requests for the same content are served from the cache, providing fast and reliable access regardless of geographic location. This solution meets both the caching requirement and the global delivery requirement.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DataSync to connect the S3 buckets to the web application is incorrect because AWS DataSync is a data transfer service designed for moving large amounts of data between on-premises storage and AWS, or between AWS services. DataSync is not designed for serving content to end users or providing global content delivery. It doesn't provide caching capabilities or reduce latency for user requests. DataSync is used for one-time or scheduled data migrations, not for real-time content delivery to users worldwide.\n\n**Why option 1 is incorrect:**\nThe option that says deploy AWS Global Accelerator to connect the S3 buckets to the web application is incorrect because AWS Global Accelerator is a networking service that improves application availability and performance by routing traffic over AWS's global network infrastructure to application endpoints. However, Global Accelerator doesn't cache content - it proxies requests and connects to the application every time for the response. For media files stored in S3, Global Accelerator would route requests to S3, but without caching, each request would still need to travel to the S3 bucket's region, which doesn't provide the fast, cached delivery that CloudFront offers. Global Accelerator is better suited for dynamic applications, not static media file delivery.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application is incorrect because SQS is a message queuing service designed for decoupling and scaling microservices, distributed systems, and serverless applications. SQS doesn't provide content delivery, caching, or CDN capabilities. It's used for asynchronous message processing, not for serving media files to end users. SQS cannot deliver cached content to users around the world or reduce latency for media file access.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company produces batch data that comes from different databases. The company also \nproduces live stream data from network sensors and application APIs. The company needs to \nconsolidate all the data into one place for business analytics. The company needs to process the \nincoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-\ntime queries and import the data into a business intelligence tool to show key performance \nindicators (KPIs). \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
            options: [
                { id: 0, text: "Use Amazon Athena foe one-time queries.", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Analytics for one-time queries.", correct: false },
                { id: 2, text: "Create custom AWS Lambda functions to move the individual records from me databases to an", correct: false },
                { id: 3, text: "Use an AWS Glue extract transform, and toad (ETL) job to convert the data into JSON format.", correct: false },
                { id: 4, text: "Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake.", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nUsing Amazon Athena for one-time queries is correct because Athena is a serverless interactive query service that allows you to analyze data directly in S3 using standard SQL. Since the data will be staged in different S3 buckets, Athena can query this data without requiring any infrastructure setup or data loading into a database. Athena charges only for the queries you run and the data scanned, making it ideal for one-time queries with minimal operational overhead. Teams can run ad-hoc queries on the consolidated data in S3 without managing servers or clusters.\n\n**Why option 4 is correct:**\nUsing blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake, and using AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format is correct. Lake Formation provides blueprints that help identify and catalog data sources, making it easier to set up a data lake. AWS Glue is a serverless ETL service that can crawl various data sources (databases, APIs, streams), extract and transform the data, and load it into S3 in optimized formats like Parquet. This approach consolidates batch data from databases and live stream data from sensors/APIs into S3 with minimal operational overhead, as Glue is fully managed and doesn't require infrastructure provisioning.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Kinesis Data Analytics for one-time queries is incorrect because Kinesis Data Analytics is designed for real-time stream processing and continuous queries on streaming data, not for one-time ad-hoc queries on batch data. Kinesis Data Analytics processes data in real-time as it flows through streams, which doesn't match the requirement for one-time queries on consolidated data. Additionally, Kinesis Data Analytics requires setting up and managing Kinesis streams, which adds operational overhead compared to serverless solutions like Athena.\n\n**Why option 2 is incorrect:**\nThe option that says create custom AWS Lambda functions to move individual records from the databases to an Amazon Redshift cluster is incorrect because this approach requires significant operational overhead. You would need to write, deploy, and maintain custom Lambda functions for each data source. Additionally, Redshift is a data warehouse that requires cluster provisioning, management, and scaling, which adds operational complexity. The requirement is to stage data in S3 buckets, not load it into Redshift, and the approach should minimize operational overhead.\n\n**Why option 3 is incorrect:**\nThe option that says use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format and load the data into multiple Amazon OpenSearch Service clusters is incorrect because while Glue can handle the ETL part, loading data into multiple OpenSearch clusters adds unnecessary operational overhead. OpenSearch is designed for search and analytics workloads, not for general business intelligence queries. The requirement is to stage data in S3 buckets for one-time queries, not to load it into search clusters. Additionally, managing multiple OpenSearch clusters increases operational complexity and costs.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A gaming company has a web application that displays scores. The application runs on Amazon \nEC2 instances behind an Application Load Balancer. The application stores data in an Amazon \nRDS for MySQL database. Users are starting to experience long delays and interruptions that are \ncaused by database read performance. The company wants to improve the user experience while \nminimizing changes to the application's architecture. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon ElastiCache in front of the database.", correct: true },
                { id: 1, text: "Use RDS Proxy between the application and the database.", correct: false },
                { id: 2, text: "Migrate the application from EC2 instances to AWS Lambda.", correct: false },
                { id: 3, text: "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing Amazon ElastiCache in front of the database is the correct solution to improve database read performance while minimizing changes to the application architecture. ElastiCache provides an in-memory caching layer (using Redis or Memcached) that stores frequently accessed data, allowing the application to retrieve data from the fast cache instead of querying the slower RDS MySQL database for every request. This significantly reduces database load and improves response times. The application only needs minor code changes to check the cache first before querying the database, and to update the cache when data changes. This approach minimizes architectural changes while dramatically improving read performance for frequently accessed data like game scores.\n\n**Why option 1 is incorrect:**\nThe option that says use RDS Proxy between the application and the database is incorrect because RDS Proxy is primarily designed for connection pooling and managing database connections, not for improving read performance through caching. RDS Proxy helps reduce the number of database connections and improves connection management, but it doesn't cache query results or reduce database load for read operations. While RDS Proxy can help with connection-related performance issues, it doesn't address the core problem of slow database read performance that requires caching frequently accessed data.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the application from EC2 instances to AWS Lambda is incorrect because migrating to Lambda would require significant architectural changes, which violates the requirement to minimize changes. Additionally, Lambda migration alone doesn't solve the database read performance issue - the application would still need to query the same RDS database, potentially experiencing the same delays. Lambda is better suited for event-driven workloads, and migrating a web application from EC2 to Lambda would require rewriting significant portions of the application code and changing how the application handles requests.\n\n**Why option 3 is incorrect:**\nThe option that says migrate the database from Amazon RDS for MySQL to Amazon DynamoDB is incorrect because this would require massive architectural changes, rewriting the application's data access layer, and potentially redesigning the data model to fit DynamoDB's NoSQL structure. This violates the requirement to minimize changes to the application's architecture. Additionally, DynamoDB migration is a complex process that could introduce compatibility issues, data migration challenges, and require extensive application code changes. The requirement is to improve read performance with minimal changes, not to migrate to a completely different database system.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A business's backup data totals 700 terabytes (TB) and is kept in network attached storage \n(NAS) at its data center. This backup data must be available in the event of occasional regulatory \ninquiries and preserved for a period of seven years. The organization has chosen to relocate its \nbackup data from its on-premises data center to Amazon Web Services (AWS). Within one \nmonth, the migration must be completed. The company's public internet connection provides 500 \nMbps of dedicated capacity for data transport. \n \nWhat should a solutions architect do to ensure that data is migrated and stored at the LOWEST \npossible cost?",
            options: [
                { id: 0, text: "Order AWS Snowball devices to transfer the data.", correct: true },
                { id: 1, text: "Deploy a VPN connection between the data center and Amazon VPC.", correct: false },
                { id: 2, text: "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3.", correct: false },
                { id: 3, text: "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nOrdering AWS Snowball devices to transfer the data and using a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive is the most cost-effective solution. With 700 TB of data and only 500 Mbps internet capacity, transferring over the internet would take approximately 130 days (700 TB / 500 Mbps), which exceeds the 1-month requirement. AWS Snowball devices can transfer up to 80 TB per device, so approximately 9 Snowball devices would be needed. Snowball provides fast, secure, offline data transfer that bypasses internet bandwidth limitations. After the data is transferred to S3, a lifecycle policy can automatically transition it to S3 Glacier Deep Archive, which is the lowest-cost storage class at $0.00099 per GB/month, perfect for long-term archival storage with occasional regulatory inquiries. This solution meets both the migration timeline and the lowest cost storage requirement.\n\n**Why option 1 is incorrect:**\nThe option that says deploy a VPN connection between the data center and Amazon VPC and use the AWS CLI to copy the data from on-premises to Amazon S3 Glacier is incorrect because a VPN connection doesn't increase bandwidth - it still uses the same 500 Mbps internet connection. Transferring 700 TB over 500 Mbps would take approximately 130 days, far exceeding the 1-month requirement. Additionally, VPN adds encryption overhead and doesn't improve transfer speeds. While S3 Glacier is cost-effective, the transfer method doesn't meet the timeline requirement.\n\n**Why option 2 is incorrect:**\nThe option that says provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3, then use a lifecycle policy to transition to S3 Glacier Deep Archive is incorrect because Direct Connect at 500 Mbps would still take approximately 130 days to transfer 700 TB, exceeding the 1-month timeline. Additionally, Direct Connect requires physical installation and setup time, which could delay the migration start. While Direct Connect provides dedicated network connectivity, it doesn't solve the bandwidth limitation problem for such a large dataset within the required timeframe.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS DataSync to transfer the data and deploy a DataSync agent on-premises, then use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier is incorrect because DataSync still relies on the available network bandwidth. With 500 Mbps capacity, DataSync would take approximately 130 days to transfer 700 TB, which exceeds the 1-month requirement. While DataSync can optimize transfers and handle network interruptions, it cannot overcome the fundamental bandwidth limitation. Additionally, DataSync cannot directly write to S3 Glacier - it writes to S3 Standard, and then a lifecycle policy would need to transition to Glacier, adding an extra step.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 11,
            text: "A company wants to direct its users to a backup static error page if the company's primary \nwebsite is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The \ndomain is pointing to an Application Load Balancer (ALB). The company needs a solution that \nminimizes changes and infrastructure overhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Update the Route 53 records to use a latency routing policy.", correct: false },
                { id: 1, text: "Set up a Route 53 active-passive failover configuration.", correct: true },
                { id: 2, text: "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that", correct: false },
                { id: 3, text: "Update the Route 53 records to use a multivalue answer routing policy.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSetting up a Route 53 active-passive failover configuration is the correct solution that minimizes changes and infrastructure overhead. Route 53 health checks can monitor the health of the Application Load Balancer (ALB). When the ALB is healthy, Route 53 routes traffic to it. When the health check detects that the ALB is unavailable, Route 53 automatically fails over to a secondary record that points to a static error page hosted in an S3 bucket (configured as a website endpoint). This solution requires minimal changes - you only need to create a health check for the ALB, create a secondary record pointing to the S3 bucket, and configure failover routing. No changes are needed to the ALB or application infrastructure. The static error page in S3 provides a cost-effective, highly available backup with minimal operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says update the Route 53 records to use a latency routing policy and add a static error page hosted in an S3 bucket to the records so that traffic is sent to the most responsive endpoints is incorrect because latency routing policy routes traffic based on the lowest latency to endpoints, not based on availability. If the primary ALB is unavailable, latency routing won't automatically fail over to the S3 bucket - it will still try to route to the ALB if it appears to have lower latency. Latency routing doesn't provide automatic failover based on health checks, which is what's needed to redirect users to a backup error page when the primary site is unavailable.\n\n**Why option 2 is incorrect:**\nThe option that says set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts the error page is incorrect because active-active configurations distribute traffic across multiple endpoints, which doesn't meet the requirement to direct users to a backup error page only when the primary site is unavailable. Additionally, running an EC2 instance to host a static error page adds unnecessary infrastructure overhead and costs compared to hosting the error page in S3. The requirement is to minimize infrastructure overhead, and S3 static website hosting is more cost-effective than an EC2 instance.\n\n**Why option 3 is incorrect:**\nThe option that says update the Route 53 records to use a multivalue answer routing policy is incorrect because multivalue answer routing returns multiple healthy IP addresses in response to DNS queries, allowing clients to choose which IP to connect to. However, multivalue routing doesn't provide automatic failover to a backup error page when the primary site is unavailable. It's designed for distributing traffic across multiple healthy endpoints, not for failover scenarios. Multivalue routing also doesn't integrate with health checks in a way that automatically redirects to a backup page when the primary endpoint fails.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A corporation has recruited a new cloud engineer who should not have access to the \nCompanyConfidential Amazon S3 bucket. The cloud engineer must have read and write \npermissions on an S3 bucket named AdminTools. \n \nWhich IAM policy will satisfy these criteria? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n42",
            options: [
                { id: 0, text: "B.", correct: true },
                { id: 2, text: "D.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe IAM policy that explicitly denies access to the CompanyConfidential S3 bucket and allows read and write permissions on the AdminTools bucket satisfies the requirements. IAM policies use explicit deny statements that take precedence over allow statements, ensuring that even if the user gains permissions through other means, they cannot access the CompanyConfidential bucket. The policy should include `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, and `s3:ListBucket` permissions for the AdminTools bucket to provide read and write access. The explicit deny for CompanyConfidential ensures compliance with the principle of least privilege by preventing access to sensitive data while allowing necessary access to AdminTools.\n\n**Why option 2 is incorrect:**\nThe other IAM policy option (D) likely doesn't include an explicit deny statement for the CompanyConfidential bucket, or it grants broader permissions than necessary. Without an explicit deny, there's a risk that the cloud engineer could gain access to CompanyConfidential through other IAM policies, groups, or roles. The requirement is clear that the engineer must not have access to CompanyConfidential, which requires an explicit deny statement in the IAM policy to ensure this restriction is enforced regardless of other permissions.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources.  \nA solutions architect wants the deployment engineer to perform job activities while following the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n44 \nprinciple of least privilege. \n \nWhich steps should the solutions architect do in conjunction to reach this goal? (Choose two.)",
            options: [
                { id: 0, text: "Have the deployment engineer use AWS account roof user credentials for performing AWS", correct: false },
                { id: 1, text: "Create a new IAM user for the deployment engineer and add the IAM user to a group that has", correct: false },
                { id: 2, text: "Create a new IAM user for the deployment engineer and add the IAM user to a group that has", correct: false },
                { id: 3, text: "Create a new IAM User for the deployment engineer and add the IAM user to a group that has", correct: true },
                { id: 4, text: "Create an IAM role for the deployment engineer to explicitly define the permissions specific to", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "**Why option 3 is correct:**\nCreating a new IAM user for the deployment engineer and adding the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only follows the principle of least privilege. This approach grants only the specific permissions needed for CloudFormation operations, preventing the engineer from performing actions beyond their job requirements. By using IAM groups and policies, permissions can be managed centrally and consistently. The policy should include permissions for CloudFormation actions (like `cloudformation:CreateStack`, `cloudformation:UpdateStack`, `cloudformation:DeleteStack`) and any necessary permissions for the resources that CloudFormation will create, but nothing more.\n\n**Why option 4 is correct:**\nCreating an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launching stacks using that IAM role is the best practice for least privilege. IAM roles are preferred over IAM users for programmatic access because they don't require long-term credentials and can be assumed when needed. The role should have permissions scoped specifically to the CloudFormation operations and the resources that will be created. When launching stacks, the engineer can specify this role using the `--role-arn` parameter, ensuring that CloudFormation uses the role's permissions rather than the engineer's user permissions. This provides better security and auditability.\n\n**Why option 0 is incorrect:**\nThe option that says have the deployment engineer use AWS account root user credentials for performing AWS CloudFormation stack operations is incorrect because using root user credentials violates the principle of least privilege entirely. The root user has full access to all AWS services and resources, which is far beyond what a deployment engineer needs. Root user credentials should never be shared or used for day-to-day operations. Using root credentials creates a significant security risk and makes it impossible to audit who performed which actions, as root actions cannot be attributed to specific individuals.\n\n**Why option 1 is incorrect:**\nThe option that says create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached is incorrect because the PowerUsers policy grants broad permissions to most AWS services, excluding IAM and billing management. This violates the principle of least privilege by granting far more permissions than necessary for CloudFormation operations. A deployment engineer only needs permissions to create, update, and delete CloudFormation stacks and the resources those stacks create, not broad access to all AWS services.\n\n**Why option 2 is incorrect:**\nThe option that says create a new IAM user for the deployment engineer and add the IAM user to a group that has the AdministratorAccess IAM policy attached is incorrect because AdministratorAccess grants full permissions to all AWS services and resources, which completely violates the principle of least privilege. A deployment engineer should only have permissions necessary for CloudFormation operations, not full administrative access. Granting AdministratorAccess defeats the purpose of following least privilege principles and creates significant security risks.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A company runs a high performance computing (HPC) workload on AWS. The workload required \nlow-latency network performance and high network throughput with tightly coupled node-to-node \ncommunication. The Amazon EC2 instances are properly sized for compute and storage \ncapacity, and are launched using default options. \n \nWhat should a solutions architect propose to improve the performance of the workload?",
            options: [
                { id: 0, text: "Choose a cluster placement group while launching Amazon EC2 instances.", correct: true },
                { id: 1, text: "Choose dedicated instance tenancy while launching Amazon EC2 instances.", correct: false },
                { id: 2, text: "Choose an Elastic Inference accelerator while launching Amazon EC2 instances.", correct: false },
                { id: 3, text: "Choose the required capacity reservation while launching Amazon EC2 instances.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nChoosing a cluster placement group while launching Amazon EC2 instances is the correct solution for HPC workloads requiring low-latency network performance and high network throughput with tightly coupled node-to-node communication. Cluster placement groups place instances within a single Availability Zone in close physical proximity, enabling instances to communicate over a high-bandwidth, low-latency network. This is ideal for applications that require tightly coupled node-to-node communication, such as HPC workloads, where network performance is critical. Cluster placement groups provide up to 10 Gbps network performance between instances, significantly better than the default network performance of EC2 instances launched without placement groups.\n\n**Why option 1 is incorrect:**\nThe option that says choose dedicated instance tenancy while launching Amazon EC2 instances is incorrect because dedicated tenancy ensures that instances run on hardware dedicated to a single AWS account, but it doesn't improve network performance or latency. Dedicated tenancy is used for compliance and isolation requirements, not for performance optimization. The network performance characteristics remain the same regardless of tenancy. For HPC workloads requiring low-latency, high-throughput communication, cluster placement groups are necessary, not dedicated tenancy.\n\n**Why option 2 is incorrect:**\nThe option that says choose an Elastic Inference accelerator while launching Amazon EC2 instances is incorrect because Elastic Inference accelerators are designed to attach GPU inference acceleration to EC2 instances for machine learning workloads, not for improving network performance. Elastic Inference doesn't affect network latency or throughput between instances. HPC workloads with tightly coupled node-to-node communication require optimized network placement, not GPU acceleration.\n\n**Why option 3 is incorrect:**\nThe option that says choose the required capacity reservation while launching Amazon EC2 instances is incorrect because capacity reservations guarantee EC2 capacity in a specific Availability Zone for a specified duration, but they don't improve network performance. Capacity reservations ensure capacity availability but don't affect network latency or throughput. For HPC workloads, the key requirement is low-latency, high-throughput network communication between instances, which is achieved through cluster placement groups, not capacity reservations.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A company wants to use the AWS Cloud to make an existing application highly available and \nresilient. The current version of the application resides in the company's data center. The \napplication recently experienced data loss after a database server crashed because of an \nunexpected power outage. The company needs a solution that avoids any single points of failure. \nThe solution must give the application the ability to scale to meet user demand. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across", correct: true },
                { id: 1, text: "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a", correct: false },
                { id: 2, text: "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across", correct: false },
                { id: 3, text: "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDeploying the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones and using an Amazon RDS DB instance in a Multi-AZ configuration is the correct solution. Auto Scaling groups across multiple Availability Zones ensure that if one AZ fails, instances in other AZs continue serving traffic, eliminating single points of failure at the application layer. Auto Scaling also provides the ability to scale out and in based on demand. RDS Multi-AZ configuration automatically replicates the database synchronously to a standby instance in a different Availability Zone. If the primary database fails due to an outage, RDS automatically fails over to the standby, preventing data loss and ensuring high availability. This addresses the previous data loss issue from the power outage by providing automatic failover and data replication.\n\n**Why option 1 is incorrect:**\nThe option that says deploy the application servers using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone, deploy the database on an EC2 instance, and enable EC2 Auto Recovery is incorrect because deploying everything in a single Availability Zone creates a single point of failure. If that AZ experiences an outage (like the power outage mentioned), both the application and database would be unavailable. EC2 Auto Recovery restarts failed instances but doesn't prevent data loss or provide automatic failover. Additionally, managing a database on EC2 requires significant operational overhead compared to managed RDS, and EC2 Auto Recovery doesn't provide the same level of data protection as RDS Multi-AZ.\n\n**Why option 2 is incorrect:**\nThe option that says deploy the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones, use an Amazon RDS DB instance with a read replica in a single Availability Zone, and promote the read replica to replace the primary DB instance if the primary fails is incorrect because this approach requires manual intervention to promote the read replica, which doesn't meet the requirement for automatic failover and high availability. Read replicas are asynchronous and may have replication lag, potentially causing data loss. Additionally, having the read replica in the same AZ as the primary doesn't protect against AZ-level failures. RDS Multi-AZ provides automatic, synchronous replication and failover without manual intervention.\n\n**Why option 3 is incorrect:**\nThe option that says deploy the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones, deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones, and use Amazon EBS Multi-Attach to create shared storage between the instances is incorrect because EBS Multi-Attach is designed for specific use cases like clustered applications that require shared block storage, but it doesn't provide automatic failover or data replication. Managing database replication and failover on EC2 requires significant operational overhead and custom configuration, which is error-prone and doesn't provide the same level of reliability as RDS Multi-AZ. This approach also doesn't address the data loss concern as effectively as RDS Multi-AZ's synchronous replication.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto \nScaling group in the AWS Cloud. The application will transmit data by using UDP packets. The \ncompany wants to ensure that the application can scale out and in as traffic increases and \ndecreases. What should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Attach a Network Load Balancer to the Auto Scaling group", correct: true },
                { id: 1, text: "Attach an Application Load Balancer to the Auto Scaling group.", correct: false },
                { id: 2, text: "Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately", correct: false },
                { id: 3, text: "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAttaching a Network Load Balancer to the Auto Scaling group is the correct solution. Network Load Balancers (NLBs) operate at Layer 4 and support both TCP and UDP protocols, making them ideal for gaming applications that transmit data using UDP packets. NLBs provide ultra-low latency and can handle millions of requests per second, which is essential for gaming workloads. When attached to an Auto Scaling group, the NLB automatically distributes traffic across healthy instances as the group scales out and in based on traffic patterns. This ensures that the gaming application can handle variable traffic loads while maintaining low latency for UDP packet transmission. NLBs also provide health checks to ensure traffic is only routed to healthy instances.\n\n**Why option 1 is incorrect:**\nThe option that says attach an Application Load Balancer to the Auto Scaling group is incorrect because Application Load Balancers (ALBs) operate at Layer 7 (HTTP/HTTPS) and are designed for HTTP/HTTPS traffic, not UDP packets. Gaming applications that use UDP require Layer 4 (transport layer) load balancing. ALBs cannot handle UDP traffic effectively and are not suitable for gaming applications that rely on UDP for real-time communication. ALBs are optimized for HTTP/HTTPS workloads and provide features like content-based routing that aren't relevant for UDP gaming traffic.\n\n**Why option 2 is incorrect:**\nThe option that says deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately is incorrect because Route 53 is a DNS service that routes traffic at the DNS level, not a load balancer. Weighted routing distributes traffic based on weights assigned to multiple resources, but it doesn't provide the same level of health checking, automatic failover, and connection-level load balancing that a Network Load Balancer provides. Route 53 weighted routing is better suited for blue/green deployments or gradual traffic shifting, not for real-time load balancing of gaming traffic. DNS-based routing also introduces additional latency and doesn't provide the same level of connection persistence needed for gaming applications.\n\n**Why option 3 is incorrect:**\nThe option that says deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group is incorrect because NAT instances are designed for outbound internet connectivity for private subnet instances, not for load balancing incoming traffic. NAT instances are single points of failure, don't scale automatically, and require manual configuration and management. They don't provide health checking or automatic traffic distribution across multiple instances. This approach doesn't meet the requirement for the application to scale out and in automatically, and NAT instances become a bottleneck for high-throughput gaming traffic.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A solutions architect is designing a customer-facing application for a company. The application's \ndatabase will have a clearly defined access pattern throughout the year and will have a variable \nnumber of reads and writes that depend on the time of year. The company must retain audit \nrecords for the database for 7 days. The recovery point objective (RPO) must be less than 5 \nhours. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB with auto scaling.", correct: false },
                { id: 1, text: "Use Amazon Redshift. Configure concurrency scaling.", correct: true },
                { id: 2, text: "Use Amazon RDS with Provisioned IOPS.", correct: false },
                { id: 3, text: "Use Amazon Aurora MySQL with auto scaling.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon Redshift, configuring concurrency scaling, activating audit logging, and performing database snapshots every 4 hours meets all the requirements. Redshift is a data warehouse designed for analytics workloads with clearly defined access patterns. Concurrency scaling automatically adds additional cluster capacity to handle variable read and write workloads based on demand, which addresses the requirement for variable reads and writes depending on the time of year. Redshift audit logging can be configured to retain audit records for 7 days. Performing database snapshots every 4 hours ensures that the Recovery Point Objective (RPO) is less than 5 hours, as snapshots capture the point-in-time state of the database. Redshift snapshots are incremental and stored in S3, providing durable backup capabilities.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon DynamoDB with auto scaling and use on-demand backups and Amazon DynamoDB Streams is incorrect because DynamoDB is a NoSQL database designed for high-performance, low-latency applications, not for analytics workloads with clearly defined access patterns. While DynamoDB auto scaling can handle variable workloads, DynamoDB Streams are designed for real-time data processing and change data capture, not for meeting RPO requirements. DynamoDB on-demand backups don't provide the same level of point-in-time recovery granularity needed for a 5-hour RPO requirement. Additionally, DynamoDB audit logging capabilities are more limited compared to Redshift.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon RDS with Provisioned IOPS, activate the database auditing parameter, and perform database snapshots every 5 hours is incorrect because performing snapshots every 5 hours doesn't meet the RPO requirement of less than 5 hours. If a failure occurs just before a snapshot, you could lose up to 5 hours of data, which exceeds the RPO requirement. Additionally, RDS Provisioned IOPS is designed for consistent I/O performance but doesn't automatically scale to handle variable workloads like Redshift concurrency scaling does. RDS is better suited for transactional workloads, not analytics workloads with clearly defined access patterns.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Aurora MySQL with auto scaling and activate the database auditing parameter is incorrect because Aurora auto scaling adjusts the number of Aurora Replicas based on workload, but it doesn't address the RPO requirement. Aurora provides continuous backup to S3, but without explicit snapshot scheduling, you cannot guarantee that the RPO will be less than 5 hours. Aurora is designed for transactional database workloads, not analytics workloads. While Aurora has audit logging capabilities, the solution doesn't specify how to meet the 7-day audit retention requirement or the RPO requirement of less than 5 hours.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The \napplication's demand varies based on the time of day. The load is minimal after work hours and \non weekends. The EC2 instances run in an EC2 Auto Scaling group that is configured with a \nminimum of two instances and a maximum of five instances. The application must be available at \nall times, but the company is concerned about overall cost. \n \nWhich solution meets the availability requirement MOST cost-effectively?",
            options: [
                { id: 0, text: "Use all EC2 Spot Instances.", correct: false },
                { id: 1, text: "Purchase EC2 Instance Savings Plans to cover five EC2 instances.", correct: false },
                { id: 2, text: "Purchase two EC2 Reserved Instances.", correct: false },
                { id: 3, text: "Purchase EC2 Instance Savings Plans to cover two EC2 instances.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nPurchasing EC2 Instance Savings Plans to cover two EC2 instances is the most cost-effective solution that meets the availability requirement. Savings Plans provide significant discounts (up to 72% compared to On-Demand pricing) for a 1- or 3-year commitment. By purchasing Savings Plans for the minimum capacity (2 instances), the company ensures that the base load is covered at a discounted rate. When demand increases and Auto Scaling adds more instances (up to 5), those additional instances will be charged at On-Demand rates, but only when needed. Since the load is minimal after work hours and on weekends, most of the time the application will run with just 2 instances at the discounted Savings Plan rate. This approach balances cost optimization with availability, as the application remains available at all times while minimizing costs during low-demand periods.\n\n**Why option 0 is incorrect:**\nThe option that says use all EC2 Spot Instances is incorrect because Spot Instances can be interrupted by AWS with a 2-minute notice when AWS needs the capacity back. While Spot Instances offer significant cost savings (up to 90% discount), they don't meet the availability requirement of being available at all times. Spot Instances are suitable for fault-tolerant, flexible workloads that can handle interruptions, but not for applications that must be available at all times. The requirement explicitly states the application must be available at all times, which Spot Instances cannot guarantee.\n\n**Why option 1 is incorrect:**\nThe option that says purchase EC2 Instance Savings Plans to cover five EC2 instances is incorrect because this approach purchases Savings Plans for the maximum capacity, which means paying for 5 instances even when only 2 are running. Since the load is minimal after work hours and on weekends, most of the time the application will only need 2 instances. Purchasing Savings Plans for 5 instances means paying for unused capacity during low-demand periods, which is not cost-effective. The requirement is to meet availability MOST cost-effectively, and purchasing for maximum capacity doesn't optimize costs.\n\n**Why option 2 is incorrect:**\nThe option that says purchase two EC2 Reserved Instances is incorrect because Reserved Instances provide discounts but are less flexible than Savings Plans. Reserved Instances are tied to specific instance types, sizes, and Availability Zones, which can limit flexibility. Savings Plans provide the same discount level but offer more flexibility - they apply to any instance type, size, or AZ within the instance family. Additionally, Reserved Instances require more upfront planning and commitment to specific configurations, while Savings Plans provide more operational flexibility for Auto Scaling groups that may use different instance types or sizes.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 19,
            text: "A company has an ecommerce checkout workflow that writes an order to a database and calls a \nservice to process the payment. Users are experiencing timeouts during the checkout process.  \nWhen users resubmit the checkout form, multiple unique orders are created for the same desired \ntransaction. \n \nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?",
            options: [
                { id: 0, text: "Configure the web application to send an order message to Amazon Kinesis Data Firehose.", correct: false },
                { id: 1, text: "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged", correct: false },
                { id: 2, text: "Store the order in the database.", correct: false },
                { id: 3, text: "Store the order in the database.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nStoring the order in the database, sending a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue, setting the payment service to retrieve the message and process the order, and deleting the message from the queue is the correct solution. This approach decouples order creation from payment processing, making the workflow more resilient to timeouts. When a user submits a checkout form, the order is immediately stored in the database with a unique order number. The order number is then sent to an SQS FIFO queue. FIFO queues guarantee exactly-once processing and prevent duplicate messages, which prevents multiple orders from being created for the same transaction. If the payment service times out or fails, the message remains in the queue and can be retried without creating duplicate orders. The message is only deleted after successful processing, ensuring idempotency.\n\n**Why option 0 is incorrect:**\nThe option that says configure the web application to send an order message to Amazon Kinesis Data Firehose and set the payment service to retrieve the message from Kinesis Data Firehose and process the order is incorrect because Kinesis Data Firehose is designed for streaming data to destinations like S3, Redshift, or Elasticsearch for analytics purposes, not for transactional message processing. Firehose doesn't provide message queuing capabilities, retry mechanisms, or exactly-once processing guarantees that are needed to prevent duplicate orders. Firehose is optimized for high-throughput data ingestion, not for ensuring transactional integrity in ecommerce workflows.\n\n**Why option 1 is incorrect:**\nThe option that says create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request, and use Lambda to query the database, call the payment service, and pass in the order information is incorrect because CloudTrail is an auditing and logging service that records API calls, not a real-time event processing system. CloudTrail logs are typically delivered with delays (5-15 minutes), which doesn't meet the requirement for immediate order processing. Additionally, this approach doesn't prevent duplicate orders - if a user resubmits the form, CloudTrail would log multiple requests, potentially creating multiple orders. CloudTrail is not designed for transactional workflows.\n\n**Why option 2 is incorrect:**\nThe option that says store the order in the database, send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS), set the payment service to poll Amazon SNS, retrieve the message, and process the order is incorrect because SNS is a pub/sub messaging service that delivers messages to multiple subscribers immediately, but it doesn't provide message queuing, exactly-once processing, or retry capabilities. SNS doesn't guarantee that messages are processed exactly once - if the payment service fails to process a message, it's lost. Additionally, SNS doesn't support polling - subscribers receive messages via HTTP/HTTPS endpoints or SQS queues. SNS is designed for fan-out messaging to multiple subscribers, not for ensuring transactional integrity in ecommerce workflows.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 20,
            text: "A company is planning to build a high performance computing (HPC) workload as a service \nsolution that Is hosted on AWS.  \nA group of 16 AmazonEC2Ltnux Instances requires the lowest possible latency for node-to-node \ncommunication.  \nThe instances also need a shared block device volume for high-performing storage. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use a cluster placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.", correct: true },
                { id: 1, text: "Use a cluster placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).", correct: false },
                { id: 2, text: "Use a partition placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).", correct: false },
                { id: 3, text: "Use a spread placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing a cluster placement group and attaching a single Provisioned IOPS SSD EBS volume to all instances using Amazon EBS Multi-Attach is the correct solution. Cluster placement groups place instances within a single Availability Zone in close physical proximity, providing the lowest possible latency for node-to-node communication (up to 10 Gbps network performance). EBS Multi-Attach allows a single Provisioned IOPS SSD (io1/io2) volume to be attached to multiple EC2 instances simultaneously, providing shared block storage with high performance. Provisioned IOPS volumes can deliver up to 64,000 IOPS and 1,000 MB/s throughput, making them ideal for high-performance HPC workloads that require low-latency shared storage. This combination provides both the network performance and shared storage requirements.\n\n**Why option 1 is incorrect:**\nThe option that says use a cluster placement group and create shared file systems across the instances using Amazon Elastic File System (Amazon EFS) is incorrect because while cluster placement groups provide low-latency networking, EFS is a network file system that introduces network latency for file operations. EFS is designed for shared file storage across multiple AZs and is optimized for throughput, not low-latency block-level access. HPC workloads that require high-performing shared block device volumes need block-level storage with low latency, not network file systems. EFS also has performance limitations compared to Provisioned IOPS EBS volumes for high-performance workloads.\n\n**Why option 2 is incorrect:**\nThe option that says use a partition placement group and create shared file systems across the instances using Amazon EFS is incorrect because partition placement groups are designed for large distributed workloads like Hadoop or HDFS, where instances are placed into logical partitions. Partition placement groups don't provide the same level of low-latency, high-throughput networking as cluster placement groups. Additionally, EFS introduces network latency and doesn't provide the high-performance block-level storage that HPC workloads require. Partition placement groups are optimized for fault isolation, not for low-latency node-to-node communication.\n\n**Why option 3 is incorrect:**\nThe option that says use a spread placement group and attach a single Provisioned IOPS SSD EBS volume using EBS Multi-Attach is incorrect because spread placement groups place instances on distinct underlying hardware to minimize the risk of simultaneous failures. While this provides high availability, spread placement groups don't provide the low-latency, high-throughput networking that cluster placement groups offer. Spread placement groups are designed for applications that need high availability, not for HPC workloads requiring the lowest possible latency for node-to-node communication. The network performance in spread placement groups is the same as default EC2 networking, not the enhanced performance of cluster placement groups.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A company has an event-driven application that invokes AWS Lambda functions up to 800 times \neach minute with varying runtimes.  \nThe Lambda functions access data that is stored in an Amazon Aurora MySQL OB cluster.  \nThe company is noticing connection timeouts as user activity increases The database shows no \nsigns of being overloaded. CPU, memory, and disk access metrics are all low.  \nWhich solution will resolve this issue with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Adjust the size of the Aurora MySQL nodes to handle more connections.", correct: false },
                { id: 1, text: "Set up Amazon ElastiCache tor Redls to cache commonly read items from the database.", correct: false },
                { id: 2, text: "Add an Aurora Replica as a reader node.", correct: false },
                { id: 3, text: "Use Amazon ROS Proxy to create a proxy.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUsing Amazon RDS Proxy to create a proxy, setting the DB cluster as the target database, and configuring the Lambda functions to connect to the proxy rather than directly to the DB cluster is the correct solution with the least operational overhead. RDS Proxy is a fully managed database proxy service that pools and shares database connections, reducing connection overhead and improving scalability. When Lambda functions connect directly to Aurora, each function invocation can create a new database connection, leading to connection exhaustion and timeouts. RDS Proxy maintains a pool of database connections and reuses them across Lambda invocations, dramatically reducing the number of connections to the database. Since the database shows no signs of being overloaded (low CPU, memory, disk), the issue is connection management, not database capacity. RDS Proxy requires minimal configuration and no code changes to the Lambda functions - just update the connection string.\n\n**Why option 0 is incorrect:**\nThe option that says adjust the size of the Aurora MySQL nodes to handle more connections and configure retry logic in the Lambda functions is incorrect because scaling up the database doesn't address the root cause - connection exhaustion from too many concurrent connections. Larger instances can handle more connections, but with 800 Lambda invocations per minute, the connection pool will still be exhausted. Additionally, retry logic doesn't solve connection timeout issues - it just retries failed connections, which will continue to fail if connections are exhausted. This approach adds operational overhead through manual scaling and code changes, and doesn't solve the underlying connection pooling problem.\n\n**Why option 1 is incorrect:**\nThe option that says set up Amazon ElastiCache for Redis to cache commonly read items and configure Lambda functions to connect to ElastiCache for reads is incorrect because caching read data doesn't solve connection timeout issues. The problem is that Lambda functions are creating too many database connections, not that they're reading too much data. Even if reads are cached, Lambda functions still need to connect to the database for writes and cache misses, so connection exhaustion will persist. Additionally, this approach requires significant code changes to implement caching logic and doesn't address the connection management issue.\n\n**Why option 2 is incorrect:**\nThe option that says add an Aurora Replica as a reader node and configure Lambda functions to connect to the reader endpoint is incorrect because adding a read replica doesn't solve connection timeout issues - it just distributes read traffic. Lambda functions still need to connect to the primary database for writes, and connection exhaustion will occur on both the primary and replica. Additionally, this approach requires code changes to implement read/write splitting logic and adds operational overhead through managing an additional database instance. The database metrics show it's not overloaded, so adding capacity doesn't address the connection pooling problem.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A company is building a containerized application on premises and decides to move the \napplication to AWS.  \nThe application will have thousands of users soon after li is deployed.  \nThe company Is unsure how to manage the deployment of containers at scale. The company \nneeds to deploy the containerized application in a highly available architecture that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Store container images In an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.", correct: true },
                { id: 1, text: "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.", correct: false },
                { id: 2, text: "Store container images in a repository that runs on an Amazon EC2 instance.", correct: false },
                { id: 3, text: "Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nStoring container images in an Amazon Elastic Container Registry (Amazon ECR) repository, using an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type, and using target tracking to scale automatically based on demand is the correct solution. ECR provides a fully managed container registry for storing and managing Docker images. Fargate is a serverless compute engine for containers that eliminates the need to provision, configure, or manage EC2 instances. With Fargate, you simply define your container requirements (CPU and memory), and AWS handles the infrastructure. Fargate automatically distributes containers across multiple Availability Zones for high availability. Target tracking scaling policies automatically adjust the number of tasks based on CloudWatch metrics, ensuring the application scales to meet demand with minimal operational overhead. This solution minimizes operational complexity while providing high availability.\n\n**Why option 1 is incorrect:**\nThe option that says store container images in ECR and use ECS with the EC2 launch type with target tracking is incorrect because the EC2 launch type requires you to provision, configure, and manage EC2 instances, which increases operational overhead. With EC2 launch type, you need to choose instance types, manage cluster capacity, handle instance scaling, and optimize cluster packing. This doesn't minimize operational overhead compared to Fargate, which is fully serverless. While ECS with EC2 can be highly available, it requires more management than Fargate.\n\n**Why option 2 is incorrect:**\nThe option that says store container images in a repository on an EC2 instance, run containers on EC2 instances across multiple AZs, monitor CPU utilization in CloudWatch, and launch new instances as needed is incorrect because this approach requires significant operational overhead. You need to manage the container registry server, configure EC2 instances, set up monitoring, and manually or semi-automatically scale instances. This doesn't leverage managed AWS services like ECR and ECS, which provide better operational efficiency. Running a container registry on EC2 adds unnecessary complexity and management overhead.\n\n**Why option 3 is incorrect:**\nThe option that says create an AMI containing the container image, launch EC2 instances in an Auto Scaling group across multiple AZs, and use CloudWatch alarms to scale is incorrect because this approach treats containers as static AMIs, which defeats the purpose of containerization. Containers should be deployed dynamically from container images, not baked into AMIs. This approach requires rebuilding AMIs for every container update, which is inefficient and increases operational overhead. Additionally, you lose the benefits of container orchestration platforms like ECS, which provide better container management, service discovery, and load balancing.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A company's application Is having performance issues. The application staleful and needs to \ncomplete m-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation \nto deploy infrastructure and used the M5 EC2 Instance family. As traffic increased, the application \nperformance degraded. Users are reporting delays when the users attempt to access the \napplication.  \nWhich solution will resolve these issues in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Replace the EC2 Instances with T3 EC2 instances that run in an Auto Scaling group.", correct: false },
                { id: 1, text: "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group.", correct: false },
                { id: 2, text: "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.", correct: false },
                { id: 3, text: "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nModifying the CloudFormation templates to replace M5 instances with R5 instances and deploying the Amazon CloudWatch agent on EC2 instances to generate custom application latency metrics is the most operationally efficient solution. The application is stateful and needs to complete in-memory tasks, indicating it's memory-intensive. R5 instances are memory-optimized (up to 768 GB RAM) compared to M5 general-purpose instances, which will improve performance for memory-intensive workloads. EC2 instances don't provide memory metrics to CloudWatch by default - the CloudWatch agent must be installed to collect memory and custom application metrics. This allows for proper monitoring and capacity planning. Since the infrastructure is managed via CloudFormation, modifying templates is the most operationally efficient approach, ensuring infrastructure-as-code practices are maintained.\n\n**Why option 0 is incorrect:**\nThe option that says replace EC2 instances with T3 instances that run in an Auto Scaling group is incorrect because T3 instances are burstable performance instances designed for workloads with variable CPU usage, not memory-intensive stateful applications. T3 instances have limited memory compared to M5 or R5 instances and use CPU credits that can be exhausted under sustained load, causing performance degradation. The application is stateful and memory-intensive, so it needs memory-optimized instances, not burstable instances. Additionally, this approach doesn't address the need for proper monitoring.\n\n**Why option 1 is incorrect:**\nThe option that says modify CloudFormation templates to run EC2 instances in an Auto Scaling group and manually increase capacity when necessary is incorrect because while Auto Scaling helps with availability, it doesn't solve the performance issue. The problem is that M5 instances don't have enough memory for the memory-intensive workload. Simply adding more M5 instances doesn't address the root cause - each instance still has insufficient memory. Additionally, manual capacity management increases operational overhead and doesn't scale efficiently. The application needs memory-optimized instances, not just more instances.\n\n**Why option 2 is incorrect:**\nThe option that says modify CloudFormation templates to replace instances with R5 instances and use CloudWatch built-in EC2 memory metrics is incorrect because EC2 instances don't provide memory metrics to CloudWatch by default. Built-in EC2 metrics only include CPU utilization, network I/O, disk I/O, and status checks - not memory utilization. To monitor memory usage for a memory-intensive application, you must install the CloudWatch agent on the instances. Without proper memory monitoring, you cannot track application performance or plan capacity effectively.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "An ecommerce company has an order-processing application that uses Amazon API Gateway \nand an AWS Lambda function.  \nThe application stores data in an Amazon Aurora PostgreSQL database.  \nDuring a recent sales event, a sudden surge in customer orders occurred.  \nSome customers experienced timeouts and the application did not process the orders of those \ncustomers.  \nA solutions architect determined that the CPU utilization and memory utilization were high on the \ndatabase because of a large number of open connections.  \nThe solutions architect needs to prevent the timeout errors while making the least possible \nchanges to the application. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure provisioned concurrency for the Lambda function.", correct: false },
                { id: 1, text: "Use Amazon RDS Proxy to create a proxy for the database.", correct: true },
                { id: 2, text: "Create a read replica for the database in a different AWS Region.", correct: false },
                { id: 3, text: "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon RDS Proxy to create a proxy for the database is the correct solution that requires the least application changes. RDS Proxy is a fully managed database proxy service that pools and shares database connections, dramatically reducing the number of connections to the Aurora database. When Lambda functions connect directly to Aurora, each invocation can create a new connection, leading to connection exhaustion and high CPU/memory utilization. RDS Proxy maintains a pool of database connections and reuses them across Lambda invocations, reducing connection overhead. The solution requires minimal changes - just update the Lambda function's connection string to point to the RDS Proxy endpoint instead of the database endpoint. RDS Proxy handles connection pooling, failover, and query routing automatically, improving database efficiency and preventing timeout errors.\n\n**Why option 0 is incorrect:**\nThe option that says configure provisioned concurrency for the Lambda function is incorrect because provisioned concurrency keeps Lambda functions warm and ready to respond, but it doesn't solve the database connection exhaustion problem. Provisioned concurrency reduces cold starts but actually increases the number of concurrent Lambda executions, which could worsen the connection problem if each execution creates a new database connection. The root cause is too many database connections, not Lambda cold starts. This approach doesn't address the high CPU and memory utilization on the database caused by connection exhaustion.\n\n**Why option 2 is incorrect:**\nThe option that says create a read replica for the database in a different AWS Region is incorrect because read replicas are designed to offload read traffic, not to solve connection exhaustion issues. The problem is that too many connections are being opened to the database, causing high CPU and memory utilization. Creating a read replica in another region doesn't reduce connections to the primary database - it just provides another endpoint for reads. Additionally, cross-region replication adds latency and complexity. The requirement is to prevent timeout errors with the least changes, and read replicas require significant application changes to implement read/write splitting.\n\n**Why option 3 is incorrect:**\nThe option that says migrate the data from Aurora PostgreSQL to Amazon DynamoDB using AWS Database Migration Service is incorrect because migrating to a completely different database system requires extensive application changes, which violates the requirement to make the least possible changes. DynamoDB is a NoSQL database with a different data model and API than PostgreSQL, requiring significant code rewrites. Additionally, DynamoDB migration is a complex, time-consuming process that doesn't address the immediate timeout issue. The problem is connection management, not the database technology itself.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A company runs a global web application on Amazon EC2 instances behind an Application Load \nBalancer. \nThe application stores data in Amazon Aurora.  \nThe company needs to create a disaster recovery solution and can tolerate up to 30 minutes of \ndowntime and potential data loss.  \nThe solution does not need to handle the load when the primary infrastructure is healthy. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n50 \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Deploy the application with the required infrastructure elements in place.", correct: true },
                { id: 1, text: "Host a scaled-down deployment of the application in a second AWS Region.", correct: false },
                { id: 2, text: "Replicate the primary infrastructure in a second AWS Region.", correct: false },
                { id: 3, text: "Back up data with AWS Backup.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDeploying the application with required infrastructure elements in place, using Amazon Route 53 to configure active-passive failover, and creating an Aurora Replica in a second AWS Region is the correct solution for a disaster recovery scenario with 30-minute downtime tolerance. Active-passive failover uses Route 53 health checks to monitor the primary infrastructure. When the primary fails, Route 53 automatically routes traffic to the standby infrastructure in the second region. Since the solution doesn't need to handle load when primary is healthy, a minimal standby deployment is cost-effective. The Aurora Replica in the second region provides database availability, and since the company can tolerate potential data loss, asynchronous replication is acceptable. This approach minimizes costs while meeting the RTO (30 minutes) and RPO (some data loss acceptable) requirements.\n\n**Why option 1 is incorrect:**\nThe option that says host a scaled-down deployment in a second region, use Route 53 active-active failover, and create an Aurora Replica is incorrect because active-active failover distributes traffic across multiple endpoints simultaneously, which doesn't match the requirement that the solution doesn't need to handle load when primary is healthy. Active-active is designed for load distribution and high availability, not disaster recovery. Additionally, active-active requires both endpoints to be fully operational, which increases costs compared to a passive standby deployment.\n\n**Why option 2 is incorrect:**\nThe option that says replicate the primary infrastructure in a second region, use Route 53 active-active failover, and create an Aurora database restored from the latest snapshot is incorrect because this approach is more expensive and complex than necessary. Replicating the full primary infrastructure increases costs significantly, and restoring from snapshots adds recovery time, potentially exceeding the 30-minute tolerance. Active-active failover doesn't match the requirement for a passive DR solution. Aurora Replicas provide better RPO than snapshots since they continuously replicate data.\n\n**Why option 3 is incorrect:**\nThe option that says back up data with AWS Backup, use the backup to create infrastructure in a second region, use Route 53 active-passive failover, and create an Aurora second primary instance is incorrect because restoring from backups takes time and may exceed the 30-minute downtime tolerance. AWS Backup restores require time to provision infrastructure and restore data, which doesn't meet the RTO requirement efficiently. Additionally, creating a second primary instance (Aurora Global Database) is more complex and expensive than using a read replica, which can be promoted to primary when needed. Aurora Replicas provide faster failover than backup restores.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A company wants to measure the effectiveness of its recent marketing campaigns.  \nThe company performs batch processing on csv files of sales data and stores the results in an \nAmazon S3 bucket once every hour.  \nThe S3 bipetabytes of objects. The company runs one-time queries in Amazon Athena to \ndetermine which products are most popular on a particular date for a particular region Queries \nsometimes fail or take longer than expected to finish.  \nWhich actions should a solutions architect take to improve the query performance and reliability? \n(Choose two.)",
            options: [
                { id: 0, text: "Reduce the S3 object sizes to less than 126 MB", correct: false },
                { id: 1, text: "Partition the data by date and region in Amazon S3", correct: false },
                { id: 2, text: "Store the files as large, single objects in Amazon S3.", correct: true },
                { id: 3, text: "Use Amazon Kinosis Data Analytics to run the Queries as pan of the batch processing operation", correct: false },
                { id: 4, text: "Use an AWS Glue extract, transform, and load (ETL) process to convert the csv files into Apache Parquet format.", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "**Why option 2 is correct:**\nStoring files as large, single objects in Amazon S3 improves Athena query performance. Athena charges based on the amount of data scanned, and smaller objects increase metadata overhead and the number of API calls needed. Large objects (ideally 128 MB to 1 GB) reduce the number of files Athena needs to process, improving query performance and reducing the likelihood of query failures. When objects are too small, Athena spends more time on metadata operations and list operations, which can cause queries to fail or timeout. Large objects also reduce the number of S3 requests, improving reliability.\n\n**Why option 4 is correct:**\nUsing an AWS Glue ETL process to convert CSV files into Apache Parquet format significantly improves Athena query performance and reliability. Parquet is a columnar storage format that is compressed and optimized for analytics queries. Parquet files are typically 10-100x smaller than CSV files, which reduces the amount of data Athena needs to scan, lowering costs and improving query speed. Parquet also supports predicate pushdown, allowing Athena to skip reading irrelevant data. Since queries filter by date and region, Parquet's columnar structure and partitioning capabilities make these queries much faster and more reliable than scanning CSV files.\n\n**Why option 0 is incorrect:**\nThe option that says reduce S3 object sizes to less than 126 MB is incorrect because smaller objects actually degrade Athena performance. Small objects increase metadata overhead, require more S3 API calls for listing and reading, and can cause queries to fail or timeout due to the large number of files to process. Athena performs best with objects between 128 MB and 1 GB. Reducing object sizes below 126 MB would worsen the performance and reliability issues the company is experiencing.\n\n**Why option 1 is incorrect:**\nThe option that says partition the data by date and region in Amazon S3 is incorrect because while partitioning can improve query performance, the question asks for actions to improve performance and reliability. Partitioning alone doesn't address the fundamental issues with CSV format and small object sizes. However, partitioning combined with Parquet format (option 4) would be optimal. But partitioning CSV files without converting to an optimized format like Parquet provides limited benefit and doesn't solve the reliability issues.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Kinesis Data Analytics to run queries as part of the batch processing operation is incorrect because Kinesis Data Analytics is designed for real-time stream processing, not for one-time ad-hoc queries on historical data. The requirement is to run one-time queries to determine product popularity for specific dates and regions, which is better suited for Athena's interactive query capabilities. Kinesis Data Analytics would require restructuring the entire data pipeline and doesn't address the performance issues with existing Athena queries on S3 data.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 27,
            text: "A company is running several business applications in three separate VPCs within the us-east-1 \nRegion. The applications must be able to communicate between VPCs. The applications also \nmust be able to consistently send hundreds of gigabytes of data each day to a latency-sensitive \napplication that runs in a single on- premises data center. \nA solutions architect needs to design a network connectivity solution that maximizes cost-\neffectiveness. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Configure three AWS Site-to-Site VPN connections from the data center to AWS.", correct: false },
                { id: 1, text: "Launch a third-party virtual network appliance in each VPC.", correct: false },
                { id: 2, text: "Set up three AWS Direct Connect connections from the data center to a Direct Connect", correct: false },
                { id: 3, text: "Set up one AWS Direct Connect connection from the data center to AWS.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nSetting up one AWS Direct Connect connection from the data center to AWS, creating a Transit Gateway, attaching each VPC to the Transit Gateway, and establishing connectivity between the Direct Connect connection and the Transit Gateway is the most cost-effective solution. Direct Connect provides dedicated network connectivity with consistent, low-latency performance ideal for transferring hundreds of gigabytes daily to a latency-sensitive application. A single Direct Connect connection can be shared across multiple VPCs through Transit Gateway, eliminating the need for multiple connections. Transit Gateway acts as a central hub, enabling VPC-to-VPC communication and connecting all VPCs to the on-premises data center through a single Direct Connect connection. This approach minimizes costs by using one connection instead of three, while meeting all connectivity requirements.\n\n**Why option 0 is incorrect:**\nThe option that says configure three AWS Site-to-Site VPN connections from the data center to AWS, with one VPN connection for each VPC is incorrect because VPN connections have bandwidth limitations (typically up to 1.25 Gbps per tunnel) and higher latency compared to Direct Connect. For transferring hundreds of gigabytes daily to a latency-sensitive application, VPN may not provide sufficient bandwidth or consistent performance. Additionally, three VPN connections cost more than a single Direct Connect connection shared across VPCs. VPN connections also have higher data transfer costs and less predictable performance than Direct Connect.\n\n**Why option 1 is incorrect:**\nThe option that says launch a third-party virtual network appliance in each VPC and establish an IPsec VPN tunnel between the data center and each appliance is incorrect because this approach requires managing three separate network appliances, which increases operational overhead and costs. Third-party appliances incur EC2 instance costs, licensing fees, and management overhead. Additionally, VPN tunnels through appliances don't provide the same level of performance, reliability, or cost-effectiveness as Direct Connect. This solution doesn't maximize cost-effectiveness as required.\n\n**Why option 2 is incorrect:**\nThe option that says set up three AWS Direct Connect connections from the data center to a Direct Connect gateway, with each VPC using one connection is incorrect because this approach is unnecessarily expensive. A single Direct Connect connection can handle the bandwidth requirements (hundreds of GB per day) and can be shared across multiple VPCs through Transit Gateway or a Direct Connect Gateway. Setting up three separate connections triples the costs without providing additional benefits. The requirement is to maximize cost-effectiveness, and using one shared connection is the optimal approach.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 28,
            text: "An online photo application lets users upload photos and perform image editing operations. The \napplication offers two classes of service free and paid Photos submitted by paid users are \nprocessed before those submitted by free users Photos are uploaded to Amazon S3 and the job \ninformation is sent to Amazon SQS. \nWhich configuration should a solutions architect recommend?",
            options: [
                { id: 0, text: "Use one SQS FIFO queue.", correct: false },
                { id: 1, text: "Use two SQS FIFO queues: one for paid and one for free.", correct: false },
                { id: 2, text: "Use two SQS standard queues one for paid and one for free.", correct: true },
                { id: 3, text: "Use one SQS standard queue.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing two SQS standard queues (one for paid and one for free) and configuring Amazon EC2 instances to prioritize polling for the paid queue over the free queue is the correct solution. Separate queues allow the application to implement priority processing by having workers poll the paid queue more frequently than the free queue. This ensures paid users' photos are processed before free users' photos. Standard queues are appropriate here because strict ordering (FIFO) isn't required - the requirement is just that paid photos are processed before free photos. The application can control priority by adjusting polling frequency and the number of workers dedicated to each queue.\n\n**Why option 0 is incorrect:**\nThe option that says use one SQS FIFO queue and assign a higher priority to paid photos is incorrect because SQS FIFO queues process messages in strict first-in-first-out order based on message groups, not priority levels. FIFO queues don't support priority-based message ordering - they ensure messages are processed exactly once and in order within a message group. You cannot assign priorities to messages in a FIFO queue to make paid photos process before free photos that arrived earlier.\n\n**Why option 1 is incorrect:**\nThe option that says use two SQS FIFO queues (one for paid and one for free) and set the free queue to use short polling and the paid queue to use long polling is incorrect because polling type (short vs long) doesn't determine processing priority. Short polling returns immediately even if no messages are available, while long polling waits up to 20 seconds for messages. However, if workers are polling both queues equally, there's no guarantee paid photos will be processed first. The solution needs explicit prioritization through polling frequency or dedicated workers, not just polling type.\n\n**Why option 3 is incorrect:**\nThe option that says use one SQS standard queue, set the visibility timeout of paid photos to zero, and configure EC2 instances to prioritize visibility settings is incorrect because visibility timeout determines how long a message is hidden after being received, not processing priority. Setting visibility timeout to zero would make messages immediately visible again after processing, potentially causing duplicate processing. Additionally, SQS doesn't support message-level priority settings in standard queues. You cannot assign different visibility timeouts to different messages based on user type in a single queue - visibility timeout is a queue-level setting, not a message-level setting.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "A company hosts its product information webpages on AWS. The existing solution uses multiple \nAmazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. The \nwebsite also uses a custom DNS name and communicates with HTTPS only using a dedicated \nSSL certificate. The company is planning a new product launch and wants to be sure that users \nfrom around the world have the best possible experience on the new website. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Redesign the application to use Amazon CloudFront", correct: true },
                { id: 1, text: "Redesign the application to use AWS Elastic Beanstalk", correct: false },
                { id: 2, text: "Redesign the application to use a Network Load Balancer.", correct: false },
                { id: 3, text: "Redesign the application to use Amazon S3 static website hosting", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nRedesigning the application to use Amazon CloudFront is the correct solution to provide the best possible experience for users around the world. CloudFront is a Content Delivery Network (CDN) that caches content at edge locations globally, reducing latency by serving content from locations closest to users. CloudFront integrates seamlessly with Application Load Balancers, allowing the existing ALB infrastructure to remain as the origin. CloudFront supports custom DNS names through Route 53 and can use custom SSL certificates, maintaining the existing HTTPS-only requirement. This solution improves global performance without requiring a complete application redesign - CloudFront can be added as a layer in front of the existing infrastructure.\n\n**Why option 1 is incorrect:**\nThe option that says redesign the application to use AWS Elastic Beanstalk is incorrect because Elastic Beanstalk is a platform-as-a-service that simplifies application deployment and management, but it doesn't improve global performance for users worldwide. Elastic Beanstalk deploys applications to a single region and doesn't provide CDN capabilities. While Elastic Beanstalk can help with deployment automation, it doesn't address the requirement to provide the best possible experience for global users, which requires content distribution through a CDN like CloudFront.\n\n**Why option 2 is incorrect:**\nThe option that says redesign the application to use a Network Load Balancer is incorrect because Network Load Balancers operate at Layer 4 and provide load balancing within a region, but they don't improve performance for global users. NLB distributes traffic across instances in the same region, reducing latency within that region, but users in other regions still experience high latency. The requirement is to provide the best experience for users \"from around the world,\" which requires global content distribution through a CDN, not just regional load balancing.\n\n**Why option 3 is incorrect:**\nThe option that says redesign the application to use Amazon S3 static website hosting is incorrect because S3 static website hosting is designed for static websites only. The existing solution uses EC2 instances behind an ALB, indicating a dynamic application. Migrating to S3 static hosting would require completely rewriting the application to be static, which may not be feasible. Additionally, S3 static hosting alone doesn't provide global content distribution - you would still need CloudFront in front of S3 to serve global users effectively.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company has 150 TB of archived image data stored on-premises that needs to be moved to the \nAWS Cloud within the next month. \nThe company's current network connection allows up to 100 Mbps uploads for this purpose \nduring the night only. \nWhat is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
            options: [
                { id: 0, text: "Use AWS Snowmobile to ship the data to AWS.", correct: false },
                { id: 1, text: "Order multiple AWS Snowball devices to ship the data to AWS.", correct: true },
                { id: 2, text: "Enable Amazon S3 Transfer Acceleration and securely upload the data.", correct: false },
                { id: 3, text: "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nOrdering multiple AWS Snowball devices to ship the data to AWS is the most cost-effective solution that meets the migration deadline. With only 100 Mbps upload capacity available during nights (approximately 6 hours), the maximum data transfer per night is approximately 2.1 TB (100 Mbps × 6 hours × 3600 seconds = 2.16 TB). Transferring 150 TB over the internet would take approximately 71 nights, far exceeding the 1-month deadline. AWS Snowball Edge Storage Optimized devices can store up to 80 TB each, so two devices (160 TB total) would be sufficient for 150 TB of data. Snowball devices provide fast, secure, offline data transfer that bypasses internet bandwidth limitations. The cost of Snowball devices and shipping is significantly less than the time and potential business impact of a 71-day migration, making it the most cost-effective solution.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Snowmobile to ship the data to AWS is incorrect because Snowmobile is designed for extremely large datasets of 10 PB or more in a single location. For 150 TB of data, Snowmobile is overkill and unnecessarily expensive. Snowmobile requires a high-speed network backbone and is designed for petabyte-scale migrations. Snowball devices are more appropriate and cost-effective for datasets in the hundreds of terabytes range.\n\n**Why option 2 is incorrect:**\nThe option that says enable Amazon S3 Transfer Acceleration and securely upload the data is incorrect because Transfer Acceleration optimizes data transfer to S3 using CloudFront edge locations, but it still relies on the available internet bandwidth. With only 100 Mbps available during nights, Transfer Acceleration cannot overcome the fundamental bandwidth limitation. Transferring 150 TB at 100 Mbps would take approximately 71 nights (150 TB / 2.1 TB per night), which exceeds the 1-month deadline. Transfer Acceleration improves transfer speeds but doesn't solve the bandwidth constraint problem.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon S3 VPC endpoint and establish a VPN to upload the data is incorrect because VPC endpoints are for private connectivity between VPC resources and AWS services, not for on-premises to AWS data transfer. Additionally, a VPN connection still uses the same 100 Mbps internet bandwidth limitation. VPN connections don't increase available bandwidth - they provide encrypted connectivity over the existing internet connection. This approach would still take approximately 71 nights to transfer 150 TB, exceeding the deadline.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "A company hosts its web application on AWS using seven Amazon EC2 instances. The company \nrequires that the IP addresses of all healthy EC2 instances be returned in response to DNS \nqueries. Which policy should be used to meet this requirement?",
            options: [
                { id: 0, text: "Simple routing policy", correct: false },
                { id: 1, text: "Latency routing policy", correct: false },
                { id: 2, text: "Multivalue routing policy", correct: true },
                { id: 3, text: "Geolocation routing policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing a multivalue answer routing policy is the correct solution. Multivalue answer routing returns multiple healthy IP addresses (up to 8) in response to DNS queries, allowing clients to choose which IP to connect to. Route 53 performs health checks on each resource and only returns healthy endpoints. Since the company requires that IP addresses of all healthy EC2 instances be returned, multivalue answer routing is ideal because it can return multiple healthy IPs simultaneously. Each record set can be associated with a health check, ensuring only healthy instances are included in DNS responses. This provides basic load balancing and fault tolerance at the DNS level.\n\n**Why option 0 is incorrect:**\nThe option that says use a simple routing policy is incorrect because simple routing policy returns a single IP address or value in response to DNS queries. It doesn't support returning multiple IP addresses for all healthy EC2 instances. Simple routing is used for a single resource or when you want Route 53 to respond to DNS queries with only one value. It doesn't meet the requirement to return IP addresses of all healthy instances.\n\n**Why option 1 is incorrect:**\nThe option that says use a latency routing policy is incorrect because latency routing returns the resource with the lowest latency based on the user's location, not all healthy instances. Latency routing is designed to route users to the endpoint that provides the best performance based on network latency measurements. It returns a single IP address per query based on latency, not multiple IPs for all healthy instances. This doesn't meet the requirement to return all healthy instance IPs.\n\n**Why option 3 is incorrect:**\nThe option that says use a geolocation routing policy is incorrect because geolocation routing returns resources based on the geographic location of the user making the DNS query, not all healthy instances. Geolocation routing allows you to route traffic based on the user's location (country/continent), returning different resources for different geographic regions. It returns a single IP address per query based on location, not multiple IPs for all healthy instances. This doesn't meet the requirement to return all healthy instance IPs.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 32,
            text: "A company wants to use AWS Systems Manager to manage a fleet of Amazon EC2 instances. \nAccording to the company's security requirements, no EC2 instances can have internet access. A \nsolutions architect needs to design network connectivity from the EC2 instances to Systems \nManager while fulfilling this security obligation. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the EC2 instances into a private subnet with no route to the internet.", correct: false },
                { id: 1, text: "Configure an interface VPC endpoint for Systems Manager.", correct: true },
                { id: 2, text: "Deploy a NAT gateway into a public subnet.", correct: false },
                { id: 3, text: "Deploy an internet gateway.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nConfiguring an interface VPC endpoint for Systems Manager is the correct solution. VPC endpoints provide private connectivity between VPC resources and AWS services without requiring internet access. Interface endpoints use AWS PrivateLink to create a private connection to Systems Manager APIs, allowing EC2 instances in private subnets to communicate with Systems Manager without traversing the internet. This meets the security requirement that no EC2 instances can have internet access while still enabling Systems Manager functionality. Interface endpoints are deployed as ENIs in your VPC subnets and use private IP addresses, ensuring all traffic stays within the AWS network.\n\n**Why option 0 is incorrect:**\nThe option that says deploy EC2 instances into a private subnet with no route to the internet is incorrect because this only addresses part of the requirement. While private subnets don't have direct internet routes, EC2 instances still need a way to communicate with Systems Manager. Without a VPC endpoint or NAT gateway, instances in private subnets cannot reach AWS services like Systems Manager, which require API calls over the internet. Simply placing instances in private subnets doesn't solve the connectivity requirement.\n\n**Why option 2 is incorrect:**\nThe option that says deploy a NAT gateway into a public subnet is incorrect because NAT gateways allow outbound internet access for resources in private subnets, which violates the security requirement that no EC2 instances can have internet access. While NAT gateways enable connectivity to AWS services, they provide internet connectivity, which the security requirements explicitly prohibit. NAT gateways route traffic through the internet gateway, exposing traffic to the public internet.\n\n**Why option 3 is incorrect:**\nThe option that says deploy an internet gateway is incorrect because internet gateways provide direct internet access to resources in public subnets, which directly violates the security requirement that no EC2 instances can have internet access. Internet gateways enable bidirectional internet connectivity, allowing both inbound and outbound internet traffic. This approach doesn't meet the security requirement and doesn't provide the private connectivity needed for Systems Manager.",
            domain: "Design Secure Architectures",
        },
        {
            id: 33,
            text: "A company needs to build a reporting solution on AWS. The solution must support SQL queries \nthat data analysts run on the data. \nThe data analysts will run lower than 10 total queries each day. The company generates 3 GB of \nnew data daily in an on-premises relational database. This data needs to be transferred to AWS \nto perform reporting tasks. \nWhat should a solutions architect recommend to meet these requirements at the LOWEST cost?",
            options: [
                { id: 0, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises", correct: false },
                { id: 1, text: "Use an Amazon Kinesis Data Firehose delivery stream to deliver the data into an Amazon", correct: false },
                { id: 2, text: "Export a daily copy of the data from the on-premises database.", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUsing AWS Database Migration Service (AWS DMS) to replicate data from the on-premises database to Amazon Redshift and using Amazon Redshift to run SQL queries is the lowest cost solution. DMS can perform continuous replication or one-time migration from the on-premises database to Redshift. With only 3 GB of new data daily and fewer than 10 queries per day, Redshift provides cost-effective analytics capabilities. Redshift's pay-as-you-go pricing means you only pay for the cluster when it's running, and with low query volume, you can use a small instance or even pause the cluster between queries. DMS handles the data transfer automatically, and Redshift provides SQL query capabilities that data analysts are familiar with, minimizing operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DMS to replicate data to Amazon S3 and use Amazon Athena for queries is incorrect because while this approach can work, it's not the lowest cost option for this use case. DMS to S3 requires additional steps, and Athena charges per query and data scanned. For 3 GB daily with fewer than 10 queries, Redshift can be more cost-effective, especially if the cluster can be paused when not in use. Additionally, the requirement specifies SQL queries that data analysts run, and while Athena supports SQL, Redshift provides better performance for repeated queries on structured data.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Kinesis Data Firehose to deliver data into Amazon Kinesis Data Analytics is incorrect because Kinesis Data Analytics is designed for real-time stream processing and continuous queries, not for on-demand SQL queries by data analysts. Kinesis services are optimized for streaming data and real-time analytics, which doesn't match the requirement for simple, on-demand queries. Additionally, Kinesis services have higher costs for low-volume use cases compared to Redshift. The requirement is for reporting with SQL queries, not real-time stream processing.\n\n**Why option 2 is incorrect:**\nThe option that says export a daily copy of the data from the on-premises database, upload it to S3, and use Amazon Athena for queries is incorrect because this manual approach increases operational overhead. Daily manual exports require scripting, monitoring, and error handling. While Athena can query S3 data, the manual export process adds complexity and potential points of failure. DMS provides automated, reliable replication with change data capture capabilities, reducing operational overhead. Additionally, for structured relational data, Redshift provides better query performance and cost-effectiveness than Athena for this volume.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A company wants to monitor its AWS costs for financial review. The cloud operations team is \ndesigning an architecture in the AWS Organizations management account to query AWS Cost \nand Usage Reports for all member accounts. \nThe team must run this query once a month and provide a detailed analysis of the bill. \nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
            options: [
                { id: 0, text: "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.", correct: false },
                { id: 1, text: "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3. Use Amazon Athena for analysis.", correct: false },
                { id: 2, text: "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3. Use Amazon Redshift for analysis.", correct: true },
                { id: 3, text: "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight for analysis.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nEnabling Cost and Usage Reports (CUR) for member accounts, delivering reports to Amazon S3, and using Amazon Redshift for analysis is the most scalable and cost-effective solution. When CUR is enabled for each member account, each account generates its own detailed billing reports, which scale automatically as new accounts are added. Reports are delivered to S3, providing durable, cost-effective storage. Redshift is a data warehouse optimized for analytics queries on large datasets, making it ideal for analyzing cost and usage data across all member accounts. Redshift can query data directly from S3 using Redshift Spectrum, or load data into Redshift for faster query performance. This approach scales to handle large volumes of cost data and provides powerful analytics capabilities.\n\n**Why option 0 is incorrect:**\nThe option that says enable CUR in the management account, deliver to Kinesis, and use EMR for analysis is incorrect because the management account's CUR only includes costs for the management account itself, not detailed usage data for all member accounts. Additionally, Kinesis is designed for real-time streaming data, not for batch cost reports that are generated daily. EMR is a big data processing framework that requires cluster management, which adds operational overhead and costs. This approach doesn't scale well and is more complex than necessary for monthly cost analysis.\n\n**Why option 1 is incorrect:**\nThe option that says enable CUR in the management account, deliver to S3, and use Athena for analysis is incorrect because the management account's CUR doesn't include detailed usage data for member accounts - it only includes aggregated billing information. To get detailed cost and usage analysis for all member accounts, you need CUR enabled for each member account. While Athena can query S3 data effectively, the limitation is that the management account's CUR doesn't provide the detailed data needed for comprehensive analysis across all accounts.\n\n**Why option 3 is incorrect:**\nThe option that says enable CUR for member accounts, deliver to Kinesis, and use QuickSight for analysis is incorrect because Kinesis is designed for real-time streaming data, not for batch cost reports. CUR reports are generated daily and delivered to S3, not streamed to Kinesis. QuickSight is a business intelligence tool that can visualize data, but it needs data in a queryable format (like S3 with Athena or Redshift). Delivering to Kinesis doesn't provide the storage and query capabilities needed for cost analysis. The correct approach is S3 for storage and Redshift or Athena for querying.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A company collects data for temperature, humidity, and atmospheric pressure in cities across \nmultiple continents. The average volume of data that the company collects from each site daily is \n500 GB. Each site has a high-speed Internet connection. \nThe company wants to aggregate the data from all these global sites as quickly as possible in a \nsingle Amazon S3 bucket. The solution must minimize operational complexity. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Turn on S3 Transfer Acceleration on the destination S3 bucket.", correct: true },
                { id: 1, text: "Upload the data from each site to an S3 bucket in the closest Region.", correct: false },
                { id: 2, text: "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each", correct: false },
                { id: 3, text: "Upload the data from each site to an Amazon EC2 instance in the closest Region.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTurning on S3 Transfer Acceleration on the destination S3 bucket is the correct solution. Transfer Acceleration uses CloudFront edge locations to optimize data transfer paths, routing uploads through the AWS edge network instead of the public internet. This significantly improves transfer speeds for long-distance transfers (50-500% faster) and is ideal when uploading to a centralized bucket from multiple global locations. With sites across multiple continents uploading 500 GB daily each, Transfer Acceleration optimizes the network path and reduces latency. It requires minimal operational complexity - just enable the feature on the bucket and use the transfer acceleration endpoint. This solution aggregates all data into a single S3 bucket as required while maximizing transfer speeds.\n\n**Why option 1 is incorrect:**\nThe option that says upload data from each site to an S3 bucket in the closest region is incorrect because this approach distributes data across multiple regions instead of aggregating it in a single bucket as required. The requirement explicitly states data must be aggregated in a single S3 bucket. While uploading to the closest region might improve individual site upload speeds, it doesn't meet the aggregation requirement. Additionally, this approach requires managing multiple buckets and potentially replicating data, which increases operational complexity.\n\n**Why option 2 is incorrect:**\nThe option that says schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site is incorrect because Snowball devices are designed for large-scale, one-time migrations or periodic bulk transfers, not for daily 500 GB transfers from multiple global sites. Snowball requires physical device shipping, which adds significant operational complexity and doesn't meet the requirement to minimize operational complexity. Additionally, with high-speed internet connections available at each site, Snowball is unnecessary and more expensive than Transfer Acceleration for regular daily transfers.\n\n**Why option 3 is incorrect:**\nThe option that says upload data from each site to an EC2 instance in the closest region is incorrect because this approach requires managing EC2 instances, which increases operational complexity. You would need to provision, configure, and manage EC2 instances in multiple regions, set up data transfer mechanisms, and then transfer data from EC2 to S3. This adds unnecessary infrastructure and operational overhead. Transfer Acceleration directly uploads to S3 without requiring intermediate infrastructure, minimizing operational complexity as required.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "A company needs the ability to analyze the log files of its proprietary application. The logs are \nstored in JSON format in an Amazon S3 bucket Queries will be simple and will run on-demand. \nA solutions architect needs to perform the analysis with minimal changes to the existing \narchitecture. \nWhat should the solutions architect do to meet these requirements with the LEAST amount of \noperational overhead?",
            options: [
                { id: 0, text: "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed", correct: false },
                { id: 1, text: "Use Amazon CloudWatch Logs to store the logs", correct: false },
                { id: 2, text: "Use Amazon Athena directly with Amazon S3 to run the queries as needed", correct: true },
                { id: 3, text: "Use AWS Glue to catalog the logs", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing Amazon Athena directly with Amazon S3 to run queries as needed is the solution with the least operational overhead. Athena is a serverless interactive query service that allows you to analyze data directly in S3 using standard SQL without provisioning infrastructure. Since the logs are already stored in JSON format in S3, Athena can query them directly without data movement or transformation. Athena requires minimal setup - just create a table schema pointing to the S3 bucket location, and you can start running SQL queries immediately. This approach requires minimal changes to the existing architecture (logs already in S3) and has no infrastructure to manage, making it operationally efficient.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Redshift to load all content into one place and run SQL queries is incorrect because Redshift requires provisioning and managing a data warehouse cluster, which increases operational overhead significantly. You need to size the cluster, manage scaling, handle backups, and pay for the cluster even when not running queries. Additionally, loading data from S3 into Redshift requires ETL processes, which adds complexity. For simple, on-demand queries on JSON logs in S3, Athena is more operationally efficient.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon CloudWatch Logs to store the logs is incorrect because this would require changing the existing architecture to send logs to CloudWatch Logs instead of S3. The requirement is to perform analysis with minimal changes to the existing architecture. CloudWatch Logs Insights can query logs, but migrating logs from S3 to CloudWatch Logs requires significant changes to the logging infrastructure. Additionally, CloudWatch Logs has retention limits and costs more than S3 for long-term log storage.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Glue to catalog the logs is incorrect because while Glue can create a data catalog, it doesn't provide query capabilities by itself. You would still need Athena or another query engine to actually run queries. Glue adds an extra step (cataloging) that may not be necessary if Athena can infer the schema from JSON files. For simple JSON logs, Athena can often query them directly without a Glue catalog, reducing operational overhead.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A company uses AWS Organizations to manage multiple AWS accounts for different \ndepartments. The management account has an Amazon S3 bucket that contains project reports. \nThe company wants to limit access to this S3 bucket to only users of accounts within the \norganization in AWS Organizations. \nWhich solution meets these requirements with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the", correct: true },
                { id: 1, text: "Create an organizational unit (OU) for each department.", correct: false },
                { id: 2, text: "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization,", correct: false },
                { id: 3, text: "Tag each user that needs access to the S3 bucket.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAdding the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy is the solution with the least operational overhead. The aws:PrincipalOrgID condition key allows you to restrict access to resources based on whether the principal belongs to a specific AWS Organization. By adding this condition to the bucket policy, you can allow access only to principals from accounts within your organization, without needing to list individual account IDs. This approach automatically includes new accounts added to the organization without requiring policy updates. It's a single policy change that scales automatically as the organization grows, minimizing operational overhead.\n\n**Why option 1 is incorrect:**\nThe option that says create an organizational unit (OU) for each department is incorrect because OUs are organizational structures for grouping accounts, not for controlling S3 bucket access. OUs don't provide access control mechanisms - they're used for applying service control policies (SCPs) and organizing accounts hierarchically. To restrict S3 access based on organization membership, you need to use IAM condition keys in bucket policies, not OUs. This approach doesn't directly address the S3 access requirement.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS CloudTrail to monitor account creation events is incorrect because CloudTrail is an auditing and logging service, not an access control mechanism. CloudTrail can log when accounts are created or invited to the organization, but it cannot prevent or allow access to S3 buckets. CloudTrail provides visibility into API calls but doesn't enforce access policies. This approach doesn't meet the requirement to limit access to the S3 bucket.\n\n**Why option 3 is incorrect:**\nThe option that says tag each user that needs access to the S3 bucket is incorrect because IAM user tags don't provide a reliable way to restrict access based on organization membership. Tags are metadata that can be used in IAM policies with condition keys, but there's no built-in tag that indicates organization membership. Additionally, manually tagging users increases operational overhead and doesn't automatically include new users or accounts added to the organization. The aws:PrincipalOrgID condition key is the proper way to restrict access based on organization membership.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that \nare stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without \nconnectivity to the internet. \nWhich solution will provide private network connectivity to Amazon S3?",
            options: [
                { id: 0, text: "Create a gateway VPC endpoint to the S3 bucket.", correct: true },
                { id: 1, text: "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.", correct: false },
                { id: 2, text: "Create an instance profile on Amazon EC2 to allow S3 access.", correct: false },
                { id: 3, text: "Create an Amazon API Gateway API with a private link to access the S3 endpoint.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreating a gateway VPC endpoint for S3 is the correct solution to provide private network connectivity. Gateway VPC endpoints are free and provide private connectivity between your VPC and S3 without requiring internet access, NAT gateway, or VPN connections. After creating the gateway endpoint, you add a route in your VPC route table that routes S3 traffic (using the S3 prefix list) to the gateway endpoint. All traffic between your VPC and S3 stays within the AWS network, never traversing the public internet. This meets the requirement for private connectivity without internet access. Gateway endpoints are highly available and automatically scale, requiring no management.\n\n**Why option 1 is incorrect:**\nThe option that says stream logs to Amazon CloudWatch Logs and export to S3 is incorrect because this approach changes the architecture unnecessarily and doesn't provide private connectivity. CloudWatch Logs requires the EC2 instance to have connectivity to CloudWatch, which would still need internet access or VPC endpoints. Additionally, this adds complexity by introducing CloudWatch Logs as an intermediate step. The requirement is for the EC2 instance to access S3 directly without internet connectivity, which is best achieved with a VPC endpoint.\n\n**Why option 2 is incorrect:**\nThe option that says create an instance profile on EC2 to allow S3 access is incorrect because instance profiles provide IAM permissions for accessing S3, but they don't provide network connectivity. An instance profile grants the EC2 instance permission to make S3 API calls, but the instance still needs network connectivity to reach S3. Without a VPC endpoint or internet access, the instance cannot reach S3 even with proper IAM permissions. Instance profiles solve the authentication/authorization problem, not the network connectivity problem.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon API Gateway API with a private link to access the S3 endpoint is incorrect because API Gateway doesn't provide direct private connectivity to S3. API Gateway is designed for building REST APIs, not for providing VPC-level connectivity to AWS services. Additionally, API Gateway would add unnecessary complexity, latency, and cost. VPC endpoints are the proper AWS service for providing private connectivity between VPCs and AWS services like S3.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores \nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the \ncompany duplicated the architecture and created a second EC2 instance and EBS volume in \nanother Availability Zone, placing both behind an Application Load Balancer. After completing this \nchange, users reported that, each time they refreshed the website, they could see one subset of \ntheir documents or the other, but never all of the documents at the same time. \nWhat should a solutions architect propose to ensure users see all of their documents at once?",
            options: [
                { id: 0, text: "Copy the data so both EBS volumes contain all the documents.", correct: false },
                { id: 1, text: "Configure the Application Load Balancer to direct a user to the server with the documents", correct: false },
                { id: 2, text: "Copy the data from both EBS volumes to Amazon EFS.", correct: true },
                { id: 3, text: "Configure the Application Load Balancer to send the request to both servers.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCopying the data from both EBS volumes to Amazon EFS and mounting the EFS file system on both EC2 instances is the correct solution. Amazon EFS is a fully managed network file system that provides shared file storage accessible from multiple EC2 instances across multiple Availability Zones. When both EC2 instances mount the same EFS file system, they share access to all documents, ensuring users see all their documents regardless of which instance handles their request. EFS automatically replicates data across multiple AZs, providing high availability. This solves the problem of data fragmentation across separate EBS volumes by providing a single, shared storage layer that both instances can access simultaneously.\n\n**Why option 0 is incorrect:**\nThe option that says copy the data so both EBS volumes contain all the documents is incorrect because EBS volumes are instance-specific storage that cannot be shared between instances. Even if you copy all documents to both EBS volumes, you'll have synchronization issues - when a user uploads a new document, it will only be stored on one EBS volume (the one attached to the instance handling that request). The other instance won't see the new document until you manually sync, which is operationally complex and error-prone. EBS volumes don't provide shared storage capabilities.\n\n**Why option 1 is incorrect:**\nThe option that says configure the Application Load Balancer to direct a user to the server with the documents is incorrect because ALBs don't have visibility into which documents are stored on which server. ALBs route traffic based on health checks and load balancing algorithms, not based on data location. Additionally, this approach doesn't solve the fundamental problem - documents are still fragmented across two separate EBS volumes, and users won't see all their documents in a single view. Sticky sessions might route a user to the same instance, but this doesn't ensure all documents are accessible.\n\n**Why option 3 is incorrect:**\nThe option that says configure the Application Load Balancer to send the request to both servers is incorrect because ALBs cannot send a single request to multiple targets simultaneously. ALBs distribute requests across healthy targets, but each request goes to only one target. Sending requests to both servers would require the application to handle duplicate requests, which doesn't solve the data fragmentation problem. The issue is that documents are stored on separate EBS volumes, not that requests aren't being distributed properly.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A company uses NFS to store large video files in on-premises network attached storage. Each \nvideo file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer \ngrowing. The company decides to migrate the video files to Amazon S3. The company must \nmigrate the video files as soon as possible while using the least possible network bandwidth. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an S3 bucket.", correct: false },
                { id: 1, text: "Create an AWS Snowball Edge job.", correct: true },
                { id: 2, text: "Deploy an S3 File Gateway on premises.", correct: false },
                { id: 3, text: "Set up an AWS Direct Connect connection between the on-premises network and AWS.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCreating an AWS Snowball Edge job is the correct solution for migrating 70 TB of data as soon as possible while using the least possible network bandwidth. Snowball Edge devices can transfer data at speeds up to 100 Gbps locally, allowing you to copy 70 TB in approximately 1.5-2 hours. The device is shipped to your location, you copy data to it locally (using no internet bandwidth), then ship it back to AWS. AWS then imports the data into S3. While the shipping process takes 6-9 business days total, this approach uses zero network bandwidth, which meets the requirement to use the least possible bandwidth. For large datasets like 70 TB, Snowball is the most bandwidth-efficient migration method.\n\n**Why option 0 is incorrect:**\nThe option that says create an S3 bucket and upload files directly is incorrect because uploading 70 TB over the internet would consume significant network bandwidth, which violates the requirement to use the least possible bandwidth. Even with high-speed internet, transferring 70 TB would take days or weeks depending on available bandwidth, and it would consume all available network capacity. This approach doesn't minimize bandwidth usage as required.\n\n**Why option 2 is incorrect:**\nThe option that says deploy an S3 File Gateway on-premises is incorrect because S3 File Gateway provides a file interface to S3 but still transfers data over the internet. When files are written to the File Gateway, they are uploaded to S3 over your network connection, consuming bandwidth. File Gateway is designed for ongoing file access, not for one-time bulk migration. It doesn't minimize bandwidth usage for a 70 TB migration - it still requires transferring all data over the internet.\n\n**Why option 3 is incorrect:**\nThe option that says set up an AWS Direct Connect connection between the on-premises network and AWS is incorrect because Direct Connect provides dedicated network connectivity but still uses network bandwidth. While Direct Connect offers consistent, low-latency connectivity, it doesn't eliminate bandwidth usage - you're still transferring 70 TB over the network connection. Direct Connect is better suited for ongoing connectivity needs, not for one-time migrations where bandwidth minimization is critical. Additionally, Direct Connect setup takes time and may not be the fastest migration method.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A company has an application that ingests incoming messages. Dozens of other applications and \nmicroservices then quickly consume these messages. The number of messages varies drastically \nand sometimes increases suddenly to 100,000 each second. The company wants to decouple the \nsolution and increase scalability. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Persist the messages to Amazon Kinesis Data Analytics.", correct: false },
                { id: 1, text: "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale", correct: false },
                { id: 2, text: "Write the messages to Amazon Kinesis Data Streams with a single shard.", correct: false },
                { id: 3, text: "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nPublishing messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) queues as subscribers is the correct solution. SNS provides pub/sub messaging that decouples the message producer from consumers. When messages are published to an SNS topic, they are automatically delivered to all subscribed SQS queues. This allows dozens of applications and microservices to consume messages independently, each reading from their own SQS queue. SNS supports nearly unlimited throughput (can handle 100,000+ messages per second), and SQS queues can scale independently based on consumption patterns. This architecture provides decoupling, scalability, and allows each consumer to process messages at its own rate.\n\n**Why option 0 is incorrect:**\nThe option that says persist messages to Amazon Kinesis Data Analytics is incorrect because Kinesis Data Analytics is designed for real-time stream processing and analytics, not for pub/sub message distribution to multiple consumers. Kinesis Data Analytics processes streaming data and runs SQL queries or applications on the stream, but it doesn't provide the same decoupling and fan-out capabilities as SNS. Additionally, Kinesis requires managing shards and has different scaling characteristics than SNS/SQS for this use case.\n\n**Why option 1 is incorrect:**\nThe option that says deploy the ingestion application on EC2 instances in an Auto Scaling group to scale based on message volume is incorrect because this only addresses scaling the ingestion layer, not decoupling and distributing messages to multiple consumers. Auto Scaling helps handle incoming message volume, but it doesn't solve the problem of distributing messages to dozens of applications and microservices. The requirement is to decouple the solution and allow multiple consumers to process messages independently, which requires a messaging service like SNS/SQS.\n\n**Why option 2 is incorrect:**\nThe option that says write messages to Amazon Kinesis Data Streams with a single shard is incorrect because a single shard has throughput limitations (1 MB/second or 1,000 records/second for writes, 2 MB/second for reads). With message volumes that can spike to 100,000 messages per second, a single shard would be a bottleneck and couldn't handle the load. Additionally, Kinesis requires consumers to manage shard processing and doesn't provide the same level of decoupling as SNS/SQS for multiple independent consumers.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company is migrating a distributed application to AWS. The application serves variable \nworkloads. The legacy platform consists of a primary server that coordinates jobs across multiple \ncompute nodes. The company wants to modernize the application with a solution that maximizes \nresiliency and scalability. \nHow should a solutions architect design the architecture to meet these requirements?",
            options: [
                { id: 0, text: "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the", correct: false },
                { id: 1, text: "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the", correct: true },
                { id: 2, text: "Implement the primary server and the compute nodes with Amazon EC2 instances that are", correct: false },
                { id: 3, text: "implement the primary server and the compute nodes with Amazon EC2 instances that are", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nConfiguring an Amazon Simple Queue Service (Amazon SQS) queue as a destination for jobs and configuring EC2 Auto Scaling to scale compute nodes based on the SQS queue size is the correct solution. SQS decouples the primary server from compute nodes, allowing the primary server to submit jobs to the queue without needing to know about or directly communicate with compute nodes. This increases resiliency because if compute nodes fail, jobs remain in the queue and can be processed when nodes recover. Auto Scaling based on SQS queue size ensures compute nodes scale automatically to handle variable workloads - when the queue grows, more instances are added; when the queue shrinks, instances are terminated. This maximizes both resiliency and scalability.\n\n**Why option 0 is incorrect:**\nThe option that says configure an SQS queue as a destination but doesn't include Auto Scaling configuration is incomplete. While SQS provides decoupling, without Auto Scaling the compute nodes won't automatically scale to handle variable workloads. The requirement is to maximize resiliency and scalability, which requires both SQS for decoupling and Auto Scaling for dynamic scaling based on workload.\n\n**Why option 2 is incorrect:**\nThe option that says implement the primary server and compute nodes with EC2 instances that are manually scaled is incorrect because manual scaling doesn't maximize scalability or resiliency. Manual scaling requires human intervention to add or remove instances, which doesn't respond quickly to variable workloads. Additionally, if this approach doesn't use SQS for decoupling, the primary server remains tightly coupled to compute nodes, reducing resiliency. If the primary server fails or compute nodes fail, the system may lose jobs or become unavailable.\n\n**Why option 3 is incorrect:**\nThe option that says implement the primary server and compute nodes with EC2 instances (duplicate/incomplete) is incorrect for similar reasons as option 2. Without SQS for decoupling and Auto Scaling for dynamic scaling, the solution doesn't maximize resiliency and scalability. The legacy architecture with a single primary server coordinating jobs is a single point of failure and doesn't scale well for variable workloads.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company is running an SMB file server in its data center. The file server stores large files that \nare accessed frequently for the first few days after the files are created. After 7 days the files are \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n60 \nrarely accessed. \n \nThe total data size is increasing and is close to the company's total storage capacity. A solutions \narchitect must increase the company's available storage space without losing low-latency access \nto the most recently accessed files. The solutions architect must also provide file lifecycle \nmanagement to avoid future storage issues. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.", correct: false },
                { id: 1, text: "Create an Amazon S3 File Gateway to extend the company's storage space.", correct: true },
                { id: 2, text: "Create an Amazon FSx for Windows File Server file system to extend the company's storage", correct: false },
                { id: 3, text: "Install a utility on each user's computer to access Amazon S3.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCreating an Amazon S3 File Gateway to extend the company's storage space is the correct solution. S3 File Gateway provides an SMB or NFS file interface to S3, allowing the existing SMB file server infrastructure to seamlessly extend storage into S3. File Gateway caches frequently accessed files locally, providing low-latency access to recently created files (those accessed frequently in the first few days). Files are automatically stored in S3, which provides virtually unlimited storage capacity. S3 Lifecycle policies can be configured to transition files older than 7 days to cheaper storage classes (like S3 Glacier), providing file lifecycle management to avoid future storage issues. This solution increases available storage without losing low-latency access to recent files.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DataSync to copy data older than 7 days from the SMB file server to AWS is incorrect because DataSync is a one-time or scheduled data transfer service, not a file gateway. DataSync would copy files to S3, but users would lose access to those files from the SMB server. The requirement is to extend storage space while maintaining access, not to archive old files. Additionally, DataSync doesn't provide the low-latency caching that File Gateway offers for frequently accessed files.\n\n**Why option 2 is incorrect:**\nThe option that says create an Amazon FSx for Windows File Server file system to extend storage is incorrect because FSx for Windows File Server is a fully managed Windows file server in AWS, not a gateway to extend on-premises storage. FSx would require migrating the entire file server to AWS, which doesn't meet the requirement to extend existing storage. Additionally, FSx doesn't provide the same cost-effective lifecycle management as S3 for files that are rarely accessed after 7 days.\n\n**Why option 3 is incorrect:**\nThe option that says install a utility on each user's computer to access S3 is incorrect because this approach requires changes on every user's computer, increases operational overhead, and doesn't provide the seamless integration that File Gateway offers. Users would need to learn new tools and workflows, and the solution doesn't integrate with the existing SMB file server infrastructure. File Gateway provides transparent access through the existing SMB protocol without requiring client-side changes.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A company is building an ecommerce web application on AWS. The application sends \ninformation about new orders to an Amazon API Gateway REST API to process. The company \nwants to ensure that orders are processed in the order that they are received. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an API Gateway integration to publish a message to an Amazon Simple Notification", correct: false },
                { id: 1, text: "Use an API Gateway integration to send a message to an Amazon Simple Queue Service", correct: true },
                { id: 2, text: "Use an API Gateway authorizer to block any requests while the application processes an order.", correct: false },
                { id: 3, text: "Use an API Gateway integration to send a message to an Amazon Simple Queue Service", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue is the correct solution. SQS FIFO queues guarantee that messages are processed exactly once and in the exact order they are received. When orders are sent to the API Gateway, they are forwarded to the FIFO queue, which maintains strict first-in-first-out ordering. This ensures orders are processed in the order they are received, meeting the requirement. FIFO queues are designed for scenarios where message ordering is critical, such as ecommerce order processing.\n\n**Why option 0 is incorrect:**\nThe option that says use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic is incorrect because SNS is a pub/sub messaging service that delivers messages to multiple subscribers, but it doesn't guarantee message ordering. SNS messages can arrive out of order, especially when there are multiple subscribers or when messages are retried. SNS is designed for fan-out messaging, not for maintaining strict order like FIFO queues.\n\n**Why option 2 is incorrect:**\nThe option that says use an API Gateway authorizer to block requests while the application processes an order is incorrect because API Gateway authorizers are used for authentication and authorization, not for controlling message processing order. Authorizers validate requests before they reach the backend, but they cannot ensure orders are processed in sequence. This approach would block all requests during processing, which doesn't scale and doesn't guarantee order - it just serializes all requests, which is inefficient.\n\n**Why option 3 is incorrect:**\nThe option that says use an API Gateway integration to send a message to an SQS standard queue is incorrect because SQS standard queues provide high throughput and at-least-once delivery, but they don't guarantee message ordering. Messages in standard queues may be delivered out of order, especially under high load. The requirement explicitly states orders must be processed in the order received, which requires a FIFO queue, not a standard queue.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 45,
            text: "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora \ndatabase. The EC2 instances connect to the database by using user names and passwords that \nare stored locally in a file. The company wants to minimize the operational overhead of credential \nmanagement. \nWhat should a solutions architect do to accomplish this goal? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n61",
            options: [
                { id: 0, text: "Use AWS Secrets Manager.", correct: true },
                { id: 1, text: "Use AWS Systems Manager Parameter Store.", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket lo store objects that are encrypted with an AWS Key.", correct: false },
                { id: 3, text: "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume or each EC2 instance.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing AWS Secrets Manager is the correct solution to minimize operational overhead of credential management. Secrets Manager provides a centralized service for storing, retrieving, and rotating database credentials. Instead of storing usernames and passwords in local files on EC2 instances (which requires manual updates and is insecure), Secrets Manager allows EC2 instances to retrieve credentials programmatically using IAM roles. Secrets Manager can automatically rotate RDS database credentials on a schedule, reducing the operational burden of manual credential rotation. The service integrates with RDS to update database passwords automatically, and applications can retrieve the latest credentials without code changes.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS Systems Manager Parameter Store is incorrect because while Parameter Store can store encrypted parameters (including SecureString parameters), it doesn't provide automatic credential rotation capabilities. Secrets Manager is specifically designed for secrets management with built-in rotation support for RDS databases. Parameter Store is better suited for configuration data and application parameters, not for credentials that need regular rotation. Using Parameter Store would still require manual rotation processes, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nThe option that says create an S3 bucket to store encrypted objects with AWS KMS is incorrect because storing credentials in S3 adds unnecessary complexity and doesn't provide the same level of integration and automation as Secrets Manager. You would need to build custom logic to retrieve and decrypt credentials from S3, manage access permissions, and implement rotation manually. This approach increases operational overhead rather than minimizing it. Secrets Manager is purpose-built for this use case.\n\n**Why option 3 is incorrect:**\nThe option that says create an encrypted EBS volume for each EC2 instance to store credentials is incorrect because this approach still requires storing credentials on the instances themselves, which doesn't solve the operational overhead problem. You would still need to manually update credentials on each EBS volume when they change, and there's no centralized management or automatic rotation. Additionally, EBS volumes are instance-specific, making credential management across multiple instances complex and error-prone.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A global company hosts its web application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The web application has static data and dynamic data. The company \nstores its static data in an Amazon S3 bucket. The company wants to improve performance and \nreduce latency for the static data and dynamic data. The company is using its own domain name \nregistered with Amazon Route 53. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins.", correct: true },
                { id: 1, text: "Create an Amazon CloudFront distribution that has the ALB as an origin.", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution that has the S3 bucket as an origin.", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution that has the ALB as an origin.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreating an Amazon CloudFront distribution that has both the S3 bucket and the ALB as origins is the correct solution. CloudFront is a Content Delivery Network (CDN) that caches content at edge locations globally, improving performance and reducing latency for users worldwide. By configuring CloudFront with multiple origins, you can route static content requests to the S3 bucket origin and dynamic content requests to the ALB origin. CloudFront caches static content at edge locations, serving it directly to users without hitting S3 or the ALB, dramatically reducing latency. Dynamic content is routed to the ALB, which forwards to EC2 instances. CloudFront integrates seamlessly with Route 53 for custom domain names and supports custom SSL certificates, meeting all requirements.\n\n**Why option 1 is incorrect:**\nThe option that says create a CloudFront distribution with only the ALB as an origin is incorrect because this approach doesn't optimize static content delivery. Static content stored in S3 should be served directly from S3 through CloudFront for best performance, not routed through the ALB and EC2 instances. Routing static content through the ALB adds unnecessary latency and load on the application servers. The requirement is to improve performance for both static and dynamic data, which requires both origins.\n\n**Why option 2 is incorrect:**\nThe option that says create a CloudFront distribution with only the S3 bucket as an origin is incorrect because this approach doesn't handle dynamic content. Dynamic content from the EC2 instances behind the ALB cannot be served from S3. Users requesting dynamic content would either get errors or stale cached content. The requirement is to improve performance for both static and dynamic data, which requires CloudFront to route to both S3 (for static) and ALB (for dynamic) based on the request path or behavior.\n\n**Why option 3 is incorrect:**\nThe option that says create a CloudFront distribution with the ALB as an origin (duplicate of option 1) is incorrect for the same reasons as option 1. Without S3 as an origin, static content performance isn't optimized, and the solution doesn't fully meet the requirement to improve performance for both static and dynamic data.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company performs monthly maintenance on its AWS infrastructure. During these maintenance \nactivities, the company needs to rotate the credentials tor its Amazon ROS tor MySQL databases \nacross multiple AWS Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the credentials as secrets in AWS Secrets Manager.", correct: true },
                { id: 1, text: "Store the credentials as secrets in AWS Systems Manager by creating a secure string", correct: false },
                { id: 2, text: "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled.", correct: false },
                { id: 3, text: "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nStoring credentials as secrets in AWS Secrets Manager is the solution with the least operational overhead. Secrets Manager is specifically designed for storing and rotating secrets, with built-in support for automatic credential rotation for Amazon RDS databases (including MySQL). Secrets Manager can automatically rotate RDS credentials on a schedule using Lambda functions, eliminating the need for manual rotation processes. The service integrates directly with RDS to update database passwords automatically. For multi-region deployments, Secrets Manager supports replicating secrets across regions, allowing centralized management of credentials across multiple AWS Regions. This minimizes operational overhead by automating the rotation process.\n\n**Why option 1 is incorrect:**\nThe option that says store credentials as secrets in AWS Systems Manager Parameter Store by creating a secure string is incorrect because Parameter Store doesn't provide automatic credential rotation capabilities. While Parameter Store can store encrypted SecureString parameters, you would need to build custom automation (Lambda functions, scripts) to rotate credentials manually. Secrets Manager has built-in rotation support for RDS, which significantly reduces operational overhead compared to building custom rotation logic for Parameter Store.\n\n**Why option 2 is incorrect:**\nThe option that says store credentials in an S3 bucket with server-side encryption enabled is incorrect because S3 is not designed for secrets management. You would need to build custom logic to retrieve, decrypt, and rotate credentials, which increases operational overhead. Additionally, managing access permissions, implementing rotation schedules, and updating RDS passwords would all require custom development and ongoing maintenance. Secrets Manager provides these capabilities out of the box.\n\n**Why option 3 is incorrect:**\nThe option that says encrypt credentials using AWS KMS multi-region keys is incorrect because KMS is a key management service, not a secrets management service. While KMS can encrypt data, it doesn't provide storage, retrieval, or rotation capabilities for secrets. You would still need to build a complete secrets management solution on top of KMS, including storage, access control, rotation logic, and RDS integration, which significantly increases operational overhead.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A company is planning to run a group of Amazon EC2 instances that connect to an Amazon \nAurora database. The company has built an AWS CloudFormation template to deploy the EC2 \ninstances and the Aurora DB cluster. The company wants to allow the instances to authenticate \nto the database in a secure way. The company does not want to maintain static database \ncredentials. \nWhich solution meets these requirements with the LEAST operational effort?",
            options: [
                { id: 0, text: "Create a database user with a user name and password. Add parameters for the database user name and password to the CloudFormation template. Pass the parameters to the EC2 instances when the instances are launched.", correct: false },
                { id: 1, text: "Create a database user with a user name and password. Store the user name and password in AWS Systems Manager Parameter Store. Configure the EC2 instances to retrieve the database credentials from Parameter Store.", correct: false },
                { id: 2, text: "Configure the DB cluster to use IAM database authentication. Create a database user to use with IAM authentication. Associate a role with the EC2 instances to allow applications on the instances to access the database.", correct: true },
                { id: 3, text: "Configure the DB cluster to use IAM database authentication with an IAM user. Create a database user that has a name that matches the IAM user. Associate the IAM user with the EC2 instances to allow applications on the instances to access the database.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nConfiguring the DB cluster to use IAM database authentication, creating a database user for IAM authentication, and associating an IAM role with the EC2 instances is the solution with the least operational effort that meets the requirement to avoid static credentials. IAM database authentication eliminates the need for static usernames and passwords. EC2 instances use their IAM role to generate authentication tokens, which are temporary and automatically rotated. This eliminates credential management overhead - no passwords to store, rotate, or manage. The CloudFormation template can create the IAM role and associate it with EC2 instances, and configure Aurora to enable IAM authentication, all in a single deployment.\n\n**Why option 0 is incorrect:**\nThe option that says create a database user with username and password, add parameters to CloudFormation template, and pass to EC2 instances is incorrect because this approach still uses static credentials (username and password). Even though credentials are passed via CloudFormation parameters, they are still static and need to be managed, rotated, and updated manually. This violates the requirement to avoid maintaining static database credentials. Additionally, storing credentials in CloudFormation parameters or passing them to EC2 instances increases security risk and operational overhead.\n\n**Why option 1 is incorrect:**\nThe option that says create a database user with username and password, store in Parameter Store, and configure EC2 instances to retrieve from Parameter Store is incorrect because this approach still uses static credentials. Parameter Store can securely store credentials, but they are still static username/password combinations that need manual rotation and management. This doesn't eliminate static credential maintenance as required. While Parameter Store is better than local files, it doesn't meet the requirement to avoid static credentials.\n\n**Why option 3 is incorrect:**\nThe option that says configure IAM database authentication with an IAM user (not a role) and associate the IAM user with EC2 instances is incorrect because IAM users are for human access or programmatic access with long-term credentials, not for EC2 instances. EC2 instances should use IAM roles, not IAM users. Roles provide temporary credentials that are automatically rotated, while IAM users require managing access keys. Using IAM users with EC2 instances is an anti-pattern and increases operational overhead compared to using roles.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A solutions architect is designing a shared storage solution for a web application that is deployed \nacross multiple Availability Zones. The web application runs on \nAmazon EC2 instances that are in an Auto Scaling group. The company plans to make frequent \nchanges to the content. The solution must have strong consistency in returning the new content \nas soon as the changes occur. \nWhich solutions meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block storage that is mounted to the individual EC2 instances.", correct: true },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the individual EC2 instances.", correct: true },
                { id: 2, text: "Create a shared Amazon Elastic Block Store (Amazon EBS) volume.", correct: false },
                { id: 3, text: "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the", correct: false },
                { id: 4, text: "Create an Amazon S3 bucket to store the web content.", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "**Why option 0 is correct:**\nUsing AWS Storage Gateway Volume Gateway with iSCSI block storage mounted to individual EC2 instances is correct. Volume Gateway provides iSCSI block storage that can be mounted by multiple EC2 instances across Availability Zones. When content is written to the volume, all instances see the changes immediately due to strong consistency. Volume Gateway caches frequently accessed data locally while storing data durably in S3, providing low-latency access with strong consistency guarantees. This solution supports frequent content changes with immediate visibility across all instances.\n\n**Why option 1 is correct:**\nCreating an Amazon Elastic File System (Amazon EFS) file system and mounting it on individual EC2 instances is correct. EFS is a fully managed network file system that provides shared storage accessible from multiple EC2 instances across multiple Availability Zones. EFS provides strong read-after-write consistency, meaning that when content is written, subsequent reads immediately see the new content. EFS automatically scales storage capacity and performance, making it ideal for applications with frequent content changes. All EC2 instances in the Auto Scaling group can mount the same EFS file system and see changes immediately.\n\n**Why option 2 is incorrect:**\nThe option that says create a shared Amazon EBS volume and mount it on individual EC2 instances is incorrect because EBS volumes cannot be shared across multiple EC2 instances simultaneously in a read-write fashion. EBS Multi-Attach allows a volume to be attached to multiple instances, but it's designed for specific use cases like clustered applications and doesn't provide the same level of consistency and performance as EFS or Volume Gateway for general file sharing. EBS volumes are instance-specific storage, not shared file systems.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS DataSync to perform continuous synchronization of data between EC2 hosts is incorrect because DataSync is a data transfer service for one-time or scheduled migrations, not for real-time shared storage. DataSync would require continuous synchronization jobs running on a schedule, which introduces latency between when content changes and when it's visible on other instances. This doesn't provide strong consistency \"as soon as changes occur\" - there's a delay based on the sync schedule. Additionally, this approach adds operational complexity and doesn't scale well with Auto Scaling groups.\n\n**Why option 4 is incorrect:**\nThe option that says create an S3 bucket to store web content, set Cache-Control to no-cache, and use CloudFront to deliver content is incorrect because S3 is object storage, not a file system. While S3 can store web content, it doesn't provide the same file system semantics and strong consistency guarantees as EFS or Volume Gateway. Additionally, even with no-cache headers, CloudFront and S3 eventual consistency models may introduce slight delays. For applications that need immediate visibility of changes across all instances, a shared file system like EFS or Volume Gateway is more appropriate.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A company that operates a web application on premises is preparing to launch a newer version of \nthe application on AWS. The company needs to route requests to either the AWS-hosted or the \non-premises-hosted application based on the URL query string. The on-premises application is \nnot available from the internet, and a VPN connection is established between Amazon VPC and \nthe company's data center. The company wants to use an Application Load Balancer (ALB) for \nthis launch. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use two ALBs: one for on-premises and one for the AWS resource.", correct: false },
                { id: 1, text: "Use two ALBs: one for on-premises and one for the AWS resource.", correct: false },
                { id: 2, text: "Use one ALB with two target groups: one for the AWS resource and one for on premises.", correct: true },
                { id: 3, text: "Use one ALB with two AWS Auto Scaling groups: one for the AWS resource and one for on", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing one ALB with two target groups (one for AWS resources and one for on-premises) is the correct solution. Application Load Balancers support advanced request routing based on query string parameters. You can create listener rules that examine the query string and route traffic to different target groups accordingly. The on-premises target group can use IP addresses as targets, pointing to the on-premises application accessible through the VPN connection. ALB supports routing based on query string, path, host header, HTTP headers, and source IP, making it ideal for this use case. This approach uses a single ALB, simplifying the architecture while meeting the routing requirement.\n\n**Why option 0 is incorrect:**\nThe option that says use two ALBs (one for on-premises and one for AWS resource) is incorrect because ALBs cannot be directly connected to on-premises resources - they route to targets within AWS (EC2 instances, IP addresses, Lambda functions, etc.). While you could potentially use an ALB with IP targets pointing to on-premises resources through VPN, having two separate ALBs doesn't provide query string-based routing between them. You would need an additional service (like Route 53) to route between ALBs based on query strings, which adds complexity. A single ALB with query string routing rules is simpler and more efficient.\n\n**Why option 1 is incorrect:**\nThe option that says use two ALBs (duplicate of option 0) is incorrect for the same reasons as option 0. Two ALBs don't provide integrated query string-based routing, and the architecture is more complex than necessary.\n\n**Why option 3 is incorrect:**\nThe option that says use one ALB with two AWS Auto Scaling groups (one for AWS resource and one for on-premises) is incorrect because Auto Scaling groups are AWS resources that manage EC2 instances, not on-premises resources. You cannot create an Auto Scaling group for on-premises infrastructure. Additionally, Auto Scaling groups are not target groups - ALBs route to target groups, which can contain Auto Scaling groups, EC2 instances, IP addresses, or Lambda functions. For on-premises resources, you would use an IP target group, not an Auto Scaling group.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 51,
            text: "A company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Select TWO )",
            options: [
                { id: 0, text: "Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.", correct: true },
                { id: 1, text: "Set up an Amazon Cognito identity pool. Configure AWS Single Sign-On to accept Amazon Cognito authentication.", correct: false },
                { id: 2, text: "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS Single Sign-On to AWS Directory Service.", correct: false },
                { id: 3, text: "Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.", correct: false },
                { id: 4, text: "Set up AWS Single Sign-On (AWS SSO) in the organization. Configure AWS SSO and integrate it with the company's corporate directory service.", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nCreating a new organization in AWS Organizations with all features turned on and creating the new AWS accounts in the organization is the first step. AWS Organizations provides centralized management of multiple AWS accounts, allowing you to consolidate billing, apply policies, and manage accounts as a single unit. Creating accounts within the organization enables centralized governance and is a prerequisite for using AWS SSO (now IAM Identity Center) for centralized authentication.\n\n**Why option 4 is correct:**\nSetting up AWS Single Sign-On (AWS SSO, now IAM Identity Center) in the organization and configuring it to integrate with the company's corporate directory service is the correct solution for centralized authentication. AWS SSO integrates with corporate directory services like Active Directory, Okta, Azure AD, and other SAML 2.0 identity providers. This allows users to authenticate using their corporate credentials and access AWS accounts and applications through a single sign-on experience. AWS SSO eliminates the need to manage IAM users in each account separately.\n\n**Why option 1 is incorrect:**\nThe option that says set up an Amazon Cognito identity pool and configure AWS SSO to accept Cognito authentication is incorrect because Cognito is designed for end-user authentication in web and mobile applications, not for corporate directory integration in multi-account AWS environments. Cognito provides user pools and identity pools for application users, but it doesn't integrate with corporate directory services like Active Directory. AWS SSO integrates directly with corporate directories, not through Cognito.\n\n**Why option 2 is incorrect:**\nThe option that says configure a service control policy (SCP) to manage AWS accounts and add AWS SSO to AWS Directory Service is incorrect because SCPs are used for permission management and governance, not for authentication. While SCPs are important for managing accounts in an organization, they don't provide authentication capabilities. Additionally, \"adding AWS SSO to AWS Directory Service\" is not the correct integration approach - AWS SSO integrates with existing directory services, it doesn't add to AWS Directory Service.\n\n**Why option 3 is incorrect:**\nThe option that says create a new organization and configure the organization's authentication mechanism to use AWS Directory Service directly is incorrect because AWS Organizations doesn't have a built-in authentication mechanism that directly uses AWS Directory Service. AWS Directory Service provides managed Active Directory, but authentication for AWS accounts is handled through AWS SSO (IAM Identity Center), which integrates with directory services. You cannot directly configure Organizations to use Directory Service for authentication without AWS SSO.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "An entertainment company is using Amazon DynamoDB to store media metadata. \nThe application is read intensive and experiencing delays. \nThe company does not have staff to handle additional operational overhead and needs to \nimprove the performance efficiency of DynamoDB without reconfiguring the application. \nWhat should a solutions architect recommend to meet this requirement?",
            options: [
                { id: 0, text: "Use Amazon ElastiCache for Redis", correct: false },
                { id: 1, text: "Use Amazon DynamoDB Accelerate (DAX)", correct: true },
                { id: 2, text: "Replicate data by using DynamoDB global tables", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Memcached with Auto Discovery enabled", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon DynamoDB Accelerator (DAX) is the correct solution. DAX is a fully managed, in-memory caching service for DynamoDB that provides microsecond latency for read-heavy workloads. DAX is a drop-in replacement for DynamoDB - applications can use the same DynamoDB API calls, and DAX automatically caches frequently accessed items. This requires no application reconfiguration, meeting the requirement to improve performance without reconfiguring the application. DAX handles cache management, scaling, and failover automatically, requiring no operational overhead from staff. It's specifically designed for read-intensive DynamoDB workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon ElastiCache for Redis is incorrect because ElastiCache requires application changes to implement caching logic. You would need to modify the application to check ElastiCache first, then fall back to DynamoDB if there's a cache miss, and update the cache when data changes. This requires significant application reconfiguration, which violates the requirement. Additionally, ElastiCache doesn't integrate seamlessly with DynamoDB like DAX does - it's a separate service that requires custom integration code.\n\n**Why option 2 is incorrect:**\nThe option that says replicate data using DynamoDB global tables is incorrect because global tables provide multi-region replication for high availability and disaster recovery, but they don't improve read performance or reduce latency. Global tables replicate data across regions but don't cache data or reduce read latency. The requirement is to improve performance efficiency for a read-intensive workload experiencing delays, which requires caching, not replication. Global tables also add operational overhead for managing replication.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon ElastiCache for Memcached with Auto Discovery enabled is incorrect because, like Redis, Memcached requires application changes to implement caching logic. You would need to modify the application to use Memcached as a cache layer, which requires reconfiguration. Additionally, Memcached doesn't integrate seamlessly with DynamoDB - it's a separate caching service that requires custom application logic to manage cache hits, misses, and invalidation. DAX provides transparent caching without application changes.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "A company has an application that provides marketing services to stores. The services are based \non previous purchases by store customers. The stores upload transaction data to the company \nthrough SFTP, and the data is processed and analyzed to generate new marketing offers. Some \nof the files can exceed 200 GB in size. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n66 \nRecently, the company discovered that some of the stores have uploaded files that contain \npersonally identifiable information (PII) that should not have been included. The company wants \nadministrators to be alerted if PII is shared again. The company also wants to automate \nremediation. \n \nWhat should a solutions architect do to meet these requirements with the LEAST development \neffort?",
            options: [
                { id: 0, text: "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.", correct: false },
                { id: 1, text: "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.", correct: true },
                { id: 2, text: "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.", correct: false },
                { id: 3, text: "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the objects that contain PII.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing an Amazon S3 bucket as a secure transfer point, Amazon Macie to scan objects, and Amazon SNS to notify administrators is the solution with the least development effort. Macie is a fully managed data security service that automatically discovers, classifies, and protects sensitive data in S3. Macie uses machine learning to identify PII and other sensitive data without requiring custom scanning algorithms. When Macie detects PII, it publishes findings to Amazon EventBridge, which can trigger SNS notifications to administrators. This approach requires minimal development - just configure Macie, set up EventBridge rules, and configure SNS topics. Macie handles the scanning automatically, eliminating the need to write custom PII detection code.\n\n**Why option 0 is incorrect:**\nThe option that says use S3 as a transfer point, Amazon Inspector to scan objects, and S3 Lifecycle policy to remove objects containing PII is incorrect because Amazon Inspector is designed for security assessment of EC2 instances and container images, not for scanning S3 objects for PII. Inspector assesses applications for vulnerabilities and deviations from security best practices, but it doesn't detect PII in data files. Additionally, automatically deleting files via Lifecycle policy based on Inspector findings isn't a standard workflow and would require custom integration.\n\n**Why option 2 is incorrect:**\nThe option that says implement custom scanning algorithms in a Lambda function, trigger on object upload, and use SNS for notifications is incorrect because this approach requires significant development effort. You would need to write custom PII detection algorithms, handle large files (200+ GB), manage Lambda timeouts and memory limits, and implement proper scanning logic. This violates the requirement for least development effort. Macie provides pre-built PII detection capabilities that don't require custom code.\n\n**Why option 3 is incorrect:**\nThe option that says implement custom scanning algorithms in Lambda, use SES for notifications, and trigger Lifecycle policy is incorrect for the same reasons as option 2 - it requires significant development effort to build custom PII detection. Additionally, using SES for notifications and Lifecycle policies for automated deletion adds complexity. Macie with EventBridge and SNS provides a more integrated, managed solution that requires less development effort.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a \nspecific AWS Region for an upcoming event that will last 1 week. \n \nWhat should the company do to guarantee the EC2 capacity?",
            options: [
                { id: 0, text: "Purchase Reserved instances that specify the Region needed", correct: false },
                { id: 1, text: "Create an On Demand Capacity Reservation that specifies the Region needed", correct: false },
                { id: 2, text: "Purchase Reserved instances that specify the Region and three Availability Zones needed", correct: false },
                { id: 3, text: "Create an On-Demand Capacity Reservation that specifies the Region and three Availability", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCreating an On-Demand Capacity Reservation that specifies the Region and three Availability Zones is the correct solution. Capacity Reservations guarantee EC2 capacity in specific Availability Zones for a specified duration. You can create separate Capacity Reservations for each of the three required Availability Zones, ensuring capacity is available in all three zones. Capacity Reservations are ideal for short-term, predictable workloads like a 1-week event. They guarantee capacity without requiring a 1- or 3-year commitment like Reserved Instances. You pay only for the capacity reserved, and you can cancel the reservation when the event ends.\n\n**Why option 0 is incorrect:**\nThe option that says purchase Reserved Instances that specify only the Region is incorrect because Reserved Instances don't guarantee capacity in specific Availability Zones - they provide discounts but capacity is subject to availability. Additionally, Reserved Instances require 1- or 3-year commitments, which is not cost-effective for a 1-week event. Reserved Instances are designed for long-term, steady-state workloads, not short-term events.\n\n**Why option 1 is incorrect:**\nThe option that says create an On-Demand Capacity Reservation that specifies only the Region is incorrect because Capacity Reservations must be created for specific Availability Zones, not just Regions. You cannot create a single Capacity Reservation that covers multiple Availability Zones - you need separate reservations for each AZ. The requirement explicitly states capacity is needed in three specific Availability Zones, so you must create reservations for each zone.\n\n**Why option 2 is incorrect:**\nThe option that says purchase Reserved Instances that specify the Region and three Availability Zones is incorrect because Reserved Instances don't guarantee capacity - they provide billing discounts but capacity is still subject to availability. Additionally, Reserved Instances require long-term commitments (1-3 years) and are not suitable for a 1-week event. You would be paying for Reserved Instances for years when you only need capacity for one week, which is not cost-effective.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company's website uses an Amazon EC2 instance store for its catalog of items. The company \nwants to make sure that the catalog is highly available and that the catalog is stored in a durable \nlocation. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n67 \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Move the catalog to Amazon ElastiCache for Redis.", correct: false },
                { id: 1, text: "Deploy a larger EC2 instance with a larger instance store.", correct: false },
                { id: 2, text: "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.", correct: false },
                { id: 3, text: "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nMoving the catalog to an Amazon Elastic File System (Amazon EFS) file system is the correct solution. EFS provides a fully managed, highly available network file system that can be accessed by multiple EC2 instances simultaneously. EFS automatically replicates data across multiple Availability Zones, providing high availability and durability. Unlike instance store, which is ephemeral and lost when the instance stops or fails, EFS provides persistent, durable storage. EFS is designed for shared file storage that needs to be accessible from multiple instances, making it ideal for a website catalog that may need to be accessed by multiple web servers.\n\n**Why option 0 is incorrect:**\nThe option that says move the catalog to Amazon ElastiCache for Redis is incorrect because ElastiCache is an in-memory caching service, not durable storage. ElastiCache data is stored in memory and can be lost if the cache cluster fails or is restarted. While ElastiCache provides high performance, it doesn't meet the durability requirement. Additionally, ElastiCache is designed for caching frequently accessed data, not as primary storage for a catalog. The requirement is for durable storage, not caching.\n\n**Why option 1 is incorrect:**\nThe option that says deploy a larger EC2 instance with a larger instance store is incorrect because instance store is ephemeral storage that is lost when the instance stops, terminates, or fails. Instance store provides high-performance local storage but is not durable or highly available. If the instance fails, all data on the instance store is lost. This doesn't meet the requirement for durable storage or high availability. Larger instance store doesn't solve the durability problem.\n\n**Why option 2 is incorrect:**\nThe option that says move the catalog from instance store to Amazon S3 Glacier Deep Archive is incorrect because S3 Glacier Deep Archive is designed for long-term archival storage with retrieval times of 12 hours, not for active website catalogs that need immediate access. Glacier Deep Archive is the lowest-cost storage class but has the slowest retrieval times, making it unsuitable for a website catalog that users need to access in real-time. Additionally, Glacier doesn't provide file system semantics - it's object storage, not a file system.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 \nyear of the call, but users access the files infrequently after 1 year. The company wants to \noptimize its solution by giving users the ability to query and retrieve files that are less than 1-year-\nold as quickly as possible. A delay in retrieving older files is acceptable. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Store individual files with tags in Amazon S3 Glacier Instant Retrieval.", correct: false },
                { id: 1, text: "Store individual files in Amazon S3 Intelligent-Tiering.", correct: true },
                { id: 2, text: "Store individual files with tags in Amazon S3 Standard storage.", correct: false },
                { id: 3, text: "Store individual files in Amazon S3 Standard storage.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nStoring individual files in Amazon S3 Intelligent-Tiering is the most cost-effective solution. Intelligent-Tiering automatically moves objects between access tiers (Frequent Access, Infrequent Access, Archive Instant Access, Archive Access, Deep Archive Access) based on access patterns. Files accessed within 1 year will remain in the Frequent Access tier, providing fast retrieval. Files not accessed for extended periods will automatically move to cheaper tiers, reducing costs. Intelligent-Tiering monitors access patterns and optimizes storage costs automatically without requiring manual lifecycle policies. This solution provides fast access to recent files while cost-optimizing older files, meeting both the performance requirement for files less than 1 year old and the cost optimization requirement.\n\n**Why option 0 is incorrect:**\nThe option that says store files with tags in Amazon S3 Glacier Instant Retrieval is incorrect because Glacier Instant Retrieval is designed for archive data that is accessed once or twice per quarter, not for files that are accessed randomly within 1 year. While Glacier Instant Retrieval provides millisecond access times, it has higher storage costs than Standard storage and is optimized for long-term archival, not for active access patterns. Additionally, using tags to query files doesn't provide the same query capabilities as S3's native features.\n\n**Why option 2 is incorrect:**\nThe option that says store files with tags in Amazon S3 Standard storage is incorrect because storing all files in Standard storage, regardless of age, is not cost-effective. Files older than 1 year that are infrequently accessed should be moved to cheaper storage classes. Standard storage is the most expensive storage class and should only be used for frequently accessed data. While tags can help organize files, they don't automatically optimize storage costs based on access patterns like Intelligent-Tiering does.\n\n**Why option 3 is incorrect:**\nThe option that says store files in Amazon S3 Standard storage (without tags) is incorrect for the same reasons as option 2. Storing all files in Standard storage regardless of access patterns is not cost-effective. Files older than 1 year that are rarely accessed should be in cheaper storage tiers. Standard storage doesn't automatically optimize costs based on access patterns - you would need to implement lifecycle policies manually, which is less efficient than Intelligent-Tiering's automatic optimization.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 57,
            text: "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The \nworkload is powered by third-party software. The company needs to patch the third-party \nsoftware on all EC2 instances as quickly as possible to remediate a critical security vulnerability. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n68",
            options: [
                { id: 0, text: "Create an AWS Lambda function to apply the patch to all EC2 instances.", correct: false },
                { id: 1, text: "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.", correct: false },
                { id: 2, text: "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2", correct: false },
                { id: 3, text: "Use AWS Systems Manager Run Command to run a custom command that applies the patch to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUsing AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances is the correct solution for patching third-party software quickly. Run Command allows you to execute commands or scripts on multiple EC2 instances simultaneously, making it ideal for applying patches to 1,000 instances quickly. Since the software is third-party (not OS-level), Patch Manager may not have the patches available in its repository. Run Command provides the flexibility to execute custom patch installation commands or scripts across all instances. You can target instances by tags, instance IDs, or all instances, and Run Command executes the commands in parallel, significantly reducing the time to patch all instances.\n\n**Why option 0 is incorrect:**\nThe option that says create an AWS Lambda function to apply the patch to all EC2 instances is incorrect because Lambda functions don't have direct access to EC2 instances to execute commands. Lambda would need to use Systems Manager Run Command or another service to actually apply patches, adding an unnecessary layer. Additionally, Lambda has execution time limits and concurrency limits that may not be suitable for patching 1,000 instances quickly. Run Command is the direct, efficient solution for executing commands on EC2 instances.\n\n**Why option 1 is incorrect:**\nThe option that says configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances is incorrect because Patch Manager is designed for operating system patches and patches from supported software repositories (like Windows Update, Amazon Linux, Ubuntu, etc.). Third-party software patches may not be available in Patch Manager's repositories, and Patch Manager may not support the specific third-party software. For custom or third-party software patches, Run Command provides the flexibility to execute custom installation commands.\n\n**Why option 2 is incorrect:**\nThe option that says schedule an AWS Systems Manager maintenance window to apply the patch is incorrect because maintenance windows are designed for scheduled maintenance activities, not for urgent security patches that need to be applied \"as quickly as possible.\" Maintenance windows run on a schedule and may have delays before execution. Additionally, maintenance windows typically use Patch Manager or Run Command as the execution method, so you would still need to choose the appropriate patching mechanism. For urgent patches, Run Command can be executed immediately without waiting for a maintenance window schedule.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A company is developing an application that provides order shipping statistics for retrieval by a \nREST API. The company wants to extract the shipping statistics, organize the data into an easy-\nto-read HTML format, and send the report to several email addresses at the same time every \nmorning. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Configure the application to send the data to Amazon Kinesis Data Firehose.", correct: false },
                { id: 1, text: "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.", correct: true },
                { id: 2, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.", correct: false },
                { id: 3, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.", correct: true },
                { id: 4, text: "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "**Why option 1 is correct:**\nUsing Amazon Simple Email Service (Amazon SES) to format the data and send the report by email is correct. SES supports sending HTML-formatted emails, which meets the requirement to organize data into an easy-to-read HTML format. SES can send emails to multiple recipients simultaneously, meeting the requirement to send to several email addresses. SES provides a simple API for sending formatted emails programmatically.\n\n**Why option 3 is correct:**\nCreating an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data is correct. EventBridge supports scheduled rules (cron expressions or rate expressions) that can trigger Lambda functions at specific times, such as every morning. The Lambda function can query the REST API to extract shipping statistics, format the data into HTML, and then use SES to send the email. This combination provides the automation needed to run the process every morning.\n\n**Why option 0 is incorrect:**\nThe option that says configure the application to send data to Amazon Kinesis Data Firehose is incorrect because Kinesis Data Firehose is designed for streaming data to destinations like S3, Redshift, or Elasticsearch for analytics, not for scheduled email reporting. Firehose processes data streams continuously, not on a schedule. Additionally, Firehose doesn't provide email sending capabilities or HTML formatting - you would still need additional services to meet the email requirement.\n\n**Why option 2 is incorrect:**\nThe option that says create an EventBridge scheduled event that invokes an AWS Glue job to query the API is incorrect because AWS Glue is designed for ETL (extract, transform, load) jobs on large datasets stored in data stores like S3, databases, etc. Glue is not designed to query REST APIs or send emails. Glue jobs are better suited for batch processing of data already in storage, not for querying APIs and sending formatted emails. Lambda is more appropriate for this use case.\n\n**Why option 4 is incorrect:**\nThe option that says store application data in S3, create an SNS topic as an S3 event destination, and send the report by email is incorrect because S3 event notifications trigger when objects are created/modified, not on a schedule. The requirement is to send reports every morning, which requires scheduled execution, not event-driven execution. Additionally, SNS can send notifications but doesn't provide HTML email formatting capabilities like SES does. SNS is better for simple notifications, while SES provides full email capabilities including HTML formatting.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A company wants to migrate its on-premises application to AWS. The application produces output \nfiles that vary in size from tens of gigabytes to hundreds of terabytes The application data must \nbe stored in a standard file system structure. The company wants a solution that scales \nautomatically, is highly available, and requires minimum operational overhead. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.", correct: false },
                { id: 1, text: "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon EFS for storage.", correct: false },
                { id: 2, text: "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.", correct: true },
                { id: 3, text: "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon S3 for storage.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nMigrating the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group and using Amazon EFS for storage is the correct solution. EFS provides a standard file system structure (NFS) that the application requires, and it scales automatically to petabytes without provisioning. EFS is highly available, automatically replicating data across multiple Availability Zones. EC2 instances in a Multi-AZ Auto Scaling group provide high availability for the application, and EFS can be mounted on multiple instances simultaneously. This solution requires minimal operational overhead - EFS is fully managed, and Auto Scaling handles instance management automatically.\n\n**Why option 0 is incorrect:**\nThe option that says migrate to ECS containers and use Amazon S3 for storage is incorrect because S3 is object storage, not a standard file system. Applications that require a file system structure cannot directly use S3 as a file system without significant modifications. S3 uses REST APIs and object keys, not file paths and directory structures. While S3 can store large files, it doesn't provide the POSIX-compliant file system interface that many applications require.\n\n**Why option 1 is incorrect:**\nThe option that says migrate to EKS containers and use Amazon EFS for storage is partially correct regarding EFS, but EKS adds unnecessary complexity for this use case. EKS requires managing Kubernetes clusters, nodes, and container orchestration, which increases operational overhead. If the application doesn't already use containers, migrating to EKS requires containerization efforts. EC2 with EFS provides a simpler migration path with less operational overhead while still meeting all requirements.\n\n**Why option 3 is incorrect:**\nThe option that says migrate to EC2 instances in Multi-AZ Auto Scaling group and use Amazon S3 for storage is incorrect because S3 is object storage, not a file system. The requirement explicitly states \"the application data must be stored in a standard file system structure,\" which S3 cannot provide. S3 uses object keys and REST APIs, not file paths and directory structures. Applications that require file system semantics need EFS or another file system solution.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 60,
            text: "A company needs to store its accounting records in Amazon S3. The records must be \nimmediately accessible for 1 year and then must be archived for an additional 9 years. No one at \nthe company, including administrative users and root users, can be able to delete the records \nduring the entire 10-year period. The records must be stored with maximum resiliency. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Store the records in S3 Glacier for the entire 10-year period.", correct: false },
                { id: 1, text: "Store the records by using S3 Intelligent-Tiering.", correct: false },
                { id: 2, text: "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep", correct: true },
                { id: 3, text: "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing an S3 Lifecycle policy to transition records from S3 Standard to S3 Glacier Deep Archive after 1 year, combined with S3 Object Lock in compliance mode, is the correct solution. S3 Standard provides immediate accessibility for the first year. After 1 year, the Lifecycle policy automatically transitions objects to S3 Glacier Deep Archive for cost-effective long-term archival (years 2-10). S3 Object Lock in compliance mode prevents anyone, including root users and administrators, from deleting or modifying objects during the retention period. Compliance mode provides the strongest protection - even root users cannot override the retention settings. S3 Glacier Deep Archive provides maximum resiliency with 99.999999999% (11 9's) durability and stores data across multiple Availability Zones.\n\n**Why option 0 is incorrect:**\nThe option that says store records in S3 Glacier for the entire 10-year period is incorrect because S3 Glacier has retrieval times (minutes to hours depending on retrieval tier), which doesn't meet the requirement for immediate accessibility during the first year. The requirement states records must be \"immediately accessible for 1 year,\" which requires S3 Standard or Intelligent-Tiering, not Glacier. Additionally, this approach doesn't address the requirement to prevent deletion - you would still need Object Lock.\n\n**Why option 1 is incorrect:**\nThe option that says store records using S3 Intelligent-Tiering is incorrect because Intelligent-Tiering doesn't prevent deletion by administrators or root users. While Intelligent-Tiering optimizes costs by moving objects between tiers based on access patterns, it doesn't provide the immutability protection required. To prevent deletion during the 10-year period, you need S3 Object Lock, which is not mentioned in this option. Intelligent-Tiering alone doesn't meet the requirement that \"no one at the company, including administrative users and root users, can be able to delete the records.\"\n\n**Why option 3 is incorrect:**\nThe option that says use an S3 Lifecycle policy to transition records from S3 Standard to S3 One Zone-IA after 1 year is incorrect because S3 One Zone-IA stores data in a single Availability Zone, which doesn't provide maximum resiliency. The requirement explicitly states records must be stored with \"maximum resiliency,\" which requires multi-AZ storage. One Zone-IA has lower durability (99.5%) compared to Standard (99.999999999%) and doesn't provide the same level of protection against AZ failures. Additionally, this option doesn't address the requirement to prevent deletion.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A company runs multiple Windows workloads on AWS. The company's employees use Windows \nfile shares that are hosted on two Amazon EC2 instances. The file shares synchronize data \nbetween themselves and maintain duplicate copies. The company wants a highly available and \ndurable storage solution that preserves how users currently access the files. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Migrate all the data to Amazon S3.", correct: false },
                { id: 1, text: "Set up an Amazon S3 File Gateway.", correct: false },
                { id: 2, text: "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ", correct: true },
                { id: 3, text: "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nExtending the file share environment to Amazon FSx for Windows File Server with a Multi-AZ deployment is the correct solution. FSx for Windows File Server provides a fully managed Windows file server that supports the SMB protocol, preserving how users currently access files. Multi-AZ deployment provides high availability by automatically replicating data synchronously to a standby file server in another Availability Zone. If the primary file server fails, FSx automatically fails over to the standby with minimal downtime. FSx provides durable, highly available storage that integrates seamlessly with existing Windows environments and Active Directory, requiring no changes to how users access files.\n\n**Why option 0 is incorrect:**\nThe option that says migrate all data to Amazon S3 is incorrect because S3 is object storage, not a file system. Users currently access files through Windows file shares (SMB protocol), which requires a file system interface. S3 uses REST APIs and object keys, not SMB file shares. Migrating to S3 would require significant changes to how users access files, violating the requirement to preserve current access methods. Additionally, S3 doesn't provide the same file system semantics and permissions model as Windows file shares.\n\n**Why option 1 is incorrect:**\nThe option that says set up an Amazon S3 File Gateway is incorrect because S3 File Gateway provides a file interface to S3 but doesn't provide the same level of high availability and durability as FSx Multi-AZ. File Gateway runs on-premises or on EC2 instances, which still requires managing the gateway infrastructure. Additionally, File Gateway doesn't provide the same Windows file server features, Active Directory integration, and SMB protocol support that FSx provides. FSx is purpose-built for Windows file shares with native Windows features.\n\n**Why option 3 is incorrect:**\nThe option that says extend to Amazon Elastic File System (Amazon EFS) with Multi-AZ is incorrect because EFS uses the NFS protocol, not SMB. Windows file shares require the SMB protocol, which EFS doesn't support. Users accessing Windows file shares expect SMB protocol support, Active Directory integration, and Windows file permissions, which EFS (being Linux-based) doesn't provide. FSx for Windows File Server is specifically designed for Windows file shares with SMB protocol support.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A solutions architect is developing a multiple-subnet VPC architecture. The solution will consist of \nsix subnets in two Availability Zones. The subnets are defined as public, private and dedicated for \ndatabases. Only the Amazon EC2 instances running in the private subnets should be able to \naccess a database. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Create a now route table that excludes the route to the public subnets' CIDR blocks.", correct: false },
                { id: 1, text: "Create a security group that denies ingress from the security group used by instances in the", correct: false },
                { id: 2, text: "Create a security group that allows ingress from the security group used by instances in the", correct: true },
                { id: 3, text: "Create a new peering connection between the public subnets and the private subnets.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCreating a security group that allows ingress from the security group used by instances in the private subnets is the correct solution. Security groups are stateful firewalls that control traffic at the instance level. By creating a security group for the database that only allows inbound traffic from the security group associated with private subnet instances, you ensure that only EC2 instances in private subnets can access the database. Security groups use a default-deny model - all inbound traffic is blocked by default, and you explicitly allow traffic from specific sources. This approach provides fine-grained access control and is the standard way to restrict database access in AWS.\n\n**Why option 0 is incorrect:**\nThe option that says create a new route table that excludes the route to the public subnets' CIDR blocks is incorrect because route tables control routing between subnets and networks, not access to specific resources like databases. Route tables determine where network traffic is directed, but they don't control which instances can access a database. Even if you modify routing, instances in public subnets could still access the database if security groups allow it. Security groups are the proper mechanism for controlling database access, not route tables.\n\n**Why option 1 is incorrect:**\nThe option that says create a security group that denies ingress from the security group used by instances in public subnets is incorrect because security groups don't support deny rules - they only support allow rules. Security groups use a whitelist model where you specify what is allowed, and everything else is denied by default. You cannot create explicit deny rules in security groups. To restrict access, you create allow rules only for the sources you want to permit (private subnet security groups), and deny is implicit for all other sources.\n\n**Why option 3 is incorrect:**\nThe option that says create a new peering connection between public subnets and private subnets is incorrect because VPC peering is used to connect different VPCs, not to control access within the same VPC. Since all subnets are in the same VPC, peering is not applicable. Additionally, peering connections enable connectivity but don't restrict access - you still need security groups to control which instances can access the database. Peering would actually enable more connectivity, which is the opposite of what's needed.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company has registered its domain name with Amazon Route 53. The company uses Amazon \nAPI Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. \nThird-party services consume the APIs securely. The company wants to design its API Gateway \nURL with the company's domain name and corresponding certificate so that the third-party \nservices can use HTTPS. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).", correct: false },
                { id: 1, text: "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.", correct: false },
                { id: 2, text: "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.", correct: true },
                { id: 3, text: "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCreating a Regional API Gateway endpoint, associating it with the company's domain name, importing the SSL/TLS certificate into ACM in the same Region (ca-central-1), attaching the certificate to the API Gateway endpoint, and configuring Route 53 to route traffic to the endpoint is the correct solution. Regional API Gateway endpoints require certificates to be in the same Region as the API. Since the API is in ca-central-1, the certificate must be imported into ACM in ca-central-1, not us-east-1. Route 53 can then create DNS records (A or AAAA) that point to the custom domain name, enabling third-party services to access the API using HTTPS with the company's domain name.\n\n**Why option 0 is incorrect:**\nThe option that says create stage variables in API Gateway to overwrite the default URL and import certificate into ACM is incorrect because stage variables are used to pass configuration data to backend integrations, not to configure custom domain names. Stage variables cannot change the API Gateway endpoint URL or domain name. To use a custom domain name, you must create a custom domain name configuration in API Gateway, not use stage variables.\n\n**Why option 1 is incorrect:**\nThe option that says create Route 53 DNS records pointing to the Regional API Gateway endpoint and import certificate into ACM in us-east-1 is incorrect because Regional API Gateway endpoints require certificates to be in the same Region as the API. Since the API is in ca-central-1, the certificate must be imported into ACM in ca-central-1, not us-east-1. Edge-optimized endpoints require certificates in us-east-1, but Regional endpoints require certificates in the same Region as the API.\n\n**Why option 3 is incorrect:**\nThe option that says create Regional API Gateway endpoint, import certificate into ACM in us-east-1, attach to APIs, and create Route 53 A record pointing to domain name is incorrect because Regional endpoints require certificates in the same Region as the API (ca-central-1), not us-east-1. Additionally, you attach certificates to the custom domain name configuration, not directly to APIs. The Route 53 configuration should point to the API Gateway custom domain name, not create a circular reference pointing to itself.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company is running a popular social media website. The website gives users the ability to \nupload images to share with other users. The company wants to make sure that the images do \nnot contain inappropriate content. The company needs a solution that minimizes development \neffort. What should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Comprehend to detect inappropriate content.", correct: false },
                { id: 1, text: "Use Amazon Rekognition to detect inappropriate content.", correct: true },
                { id: 2, text: "Use Amazon SageMaker to detect inappropriate content.", correct: false },
                { id: 3, text: "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon Rekognition to detect inappropriate content is the correct solution that minimizes development effort. Amazon Rekognition Content Moderation is a fully managed service that uses machine learning to detect inappropriate, unwanted, or offensive content in images and videos. Rekognition can detect adult content, violent content, and other inappropriate material without requiring you to build custom machine learning models. The service provides a simple API that can be integrated into the image upload workflow - when users upload images, the application can call Rekognition's moderation API to check for inappropriate content before allowing the image to be shared. This requires minimal development effort compared to building custom detection algorithms.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Comprehend to detect inappropriate content is incorrect because Amazon Comprehend is a natural language processing (NLP) service designed for analyzing text, not images. Comprehend can detect sentiment, entities, and key phrases in text, but it cannot analyze image content to detect inappropriate visual content. For image moderation, you need a computer vision service like Rekognition, not a text analysis service.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon SageMaker to detect inappropriate content is incorrect because SageMaker is a machine learning platform that requires you to build, train, and deploy custom models. This approach requires significant development effort - you would need to collect training data, train a model, deploy it, and manage the infrastructure. This violates the requirement to minimize development effort. Rekognition provides pre-trained models that work out of the box without any model development.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Fargate to deploy a custom machine learning model to detect inappropriate content is incorrect because this approach requires building a custom machine learning model, which involves significant development effort. You would need to develop the model, create container images, deploy to Fargate, and manage the infrastructure. This is much more complex than using Rekognition's pre-built content moderation capabilities. Fargate is for running containers, not for providing ML services - you would still need to build the ML model yourself.",
            domain: "Design Resilient Architectures",
        },
    ],
    test11: [
        {
            id: 0,
            text: "A company wants to run its critical applications in containers to meet requirements tor scalability \nand availability The company prefers to focus on maintenance of the critical applications. The \ncompany does not want to be responsible for provisioning and managing the underlying \ninfrastructure that runs the containerized workload. \nWhat should a solutions architect do to meet those requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 Instances, and Install Docker on the Instances", correct: false },
                { id: 1, text: "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes", correct: false },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate", correct: true },
                { id: 3, text: "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without having to manage servers. AWS Fargate is compatible with Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). https://aws.amazon.com/fr/fargate/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 1,
            text: "A company hosts more than 300 global websites and applications. The company requires a \nplatform to analyze more than 30 TB of clickstream data each day. What should a solutions \narchitect do to transmit and process the clickstream data?",
            options: [
                { id: 0, text: "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon", correct: false },
                { id: 1, text: "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an", correct: false },
                { id: 2, text: "Cache the data to Amazon CloudFron.", correct: false },
                { id: 3, text: "Collect the data from Amazon Kinesis Data Streams.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/es/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming- ingestion/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company is running a multi-tier ecommerce web application in the AWS Cloud. \nThe web application is running on Amazon EC2 instances. \nThe database tier Is on a provisioned Amazon Aurora MySQL DB cluster with a writer and a \nreader in a Multi-AZ environment. \nThe new requirement for the database tier is to serve the application to achieve continuous write \navailability through an Instance failover. \nWhat should a solutions architect do to meet this new requirement?",
            options: [
                { id: 0, text: "Add a new AWS Region to the DB cluster for multiple writes", correct: false },
                { id: 1, text: "Add a new reader In the same Availability Zone as the writer.", correct: false },
                { id: 2, text: "Migrate the database tier to an Aurora multi-master cluster.", correct: true },
                { id: 3, text: "Migrate the database tier to an Aurora DB cluster with parallel query enabled.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBring-your-own-shard (BYOS) A situation where you already have a database schema and associated applications that use sharding. You can transfer such deployments relatively easily to Aurora multi-master clusters. In this case, you can devote your effort to investigating the Aurora benefits such as server consolidation and high availability. You don't need to create new application logic to handle multiple connections for write requests. Global read-after-write (GRAW) A setting that introduces synchronization so that any read operations always see the most current Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 \nhour before the desired Amazon EC2 capacity is reached. The peak capacity is the ‘same every \nnight and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-\neffective solution that will allow for the desired EC2 capacity to be reached quickly and allow the \nAuto Scaling group to scale down after the batch jobs are complete. \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Increase the minimum capacity for the Auto Scaling group.", correct: false },
                { id: 1, text: "Increase the maximum capacity for the Auto Scaling group.", correct: false },
                { id: 2, text: "Configure scheduled scaling to scale up to the desired compute level.", correct: true },
                { id: 3, text: "Change the scaling policy to add more EC2 instances during each scaling operation.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy configuring scheduled scaling, the solutions architect can set the Auto Scaling group to automatically scale up to the desired compute level at a specific time (IAM) when the batch job starts and then automatically scale down after the job is complete. This will allow the desired EC2 capacity to be reached quickly and also help in reducing the cost.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A company runs an application in the AWS Cloud and uses Amazon DynamoDB as the database. \nThe company deploys Amazon EC2 instances to a private network to process data from the \ndatabase. \nThe company uses two NAT instances to provide connectivity to DynamoDB. \nThe company wants to retire the NAT instances. \nA solutions architect must implement a solution that provides connectivity to DynamoDB and that \ndoes not require ongoing management. \nWhat is the MOST cost-effective solution that meets these requirements?",
            options: [
                { id: 0, text: "Create a gateway VPC endpoint to provide connectivity to DynamoDB", correct: true },
                { id: 1, text: "Configure a managed NAT gateway to provide connectivity to DynamoDB", correct: false },
                { id: 2, text: "Establish an AWS Direct Connect connection between the private network and DynamoDB", correct: false },
                { id: 3, text: "Deploy an AWS PrivateLink endpoint service between the private network and DynamoDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS recommends changing from NAT Gateway to VPC endpoints to access S3 or DynamoDB. \"Determine whether the majority of your NAT gateway charges are from traffic to Amazon Simple Storage Service or Amazon DynamoDB in the same Region. If they are, set up a gateway VPC endpoint. Route traffic to and from the AWS resource through the gateway VPC endpoint, rather than through the NAT gateway. There's no data processing or hourly charges for using gateway VPC endpoints.\" https://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer- Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 5,
            text: "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low-latency connection to the \napplication servers. \nA new company policy states all application-generated files must be copied to AWS. \nThere is already a VPN connection to AWS. \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
            options: [
                { id: 0, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 1, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 2, text: "AWS Snowball", correct: false },
                { id: 3, text: "AWS Storage Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe files will be on the storgare gateway with low latency and copied to AWS as a second copy. FSx in AWS will not provide low latency for the on prem apps over a vpn to the FSx file system.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A company has an automobile sales website that stores its listings in an database on Amazon \nRDS. \nWhen an automobile is sold, the listing needs to be removed from the website and the data must \nbe sent to multiple target systems. \nWhich design should a solutions architect recommend?",
            options: [
                { id: 0, text: "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to", correct: true },
                { id: 1, text: "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to", correct: false },
                { id: 2, text: "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon", correct: false },
                { id: 3, text: "Subscribe to an RDS event notification and send an Amazon Simple Notification Service", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can use AWS Lambda to process event notifications from an Amazon Relational Database Service (Amazon RDS) database. Amazon RDS sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function. Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function. https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures- Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A company is developing a video conversion application hosted on AWS. \nThe application will be available in two tiers: a free tier and a paid tier. \nUsers in the paid tier will have their videos converted first and then the tree tier users will have \ntheir videos converted. \nWhich solution meets these requirements and is MOST cost-effective?",
            options: [
                { id: 0, text: "One FIFO queue for the paid tier and one standard queue for the free tier", correct: false },
                { id: 1, text: "A single FIFO Amazon Simple Queue Service (Amazon SQS) queue for all file types", correct: false },
                { id: 2, text: "A single standard Amazon Simple Queue Service (Amazon SQS) queue for all file types", correct: false },
                { id: 3, text: "Two standard Amazon Simple Queue Service (Amazon SQS) queues with one for the paid tier", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nIn AWS, the queue service is the Simple Queue Service (SQS). Multiple SQS queues may be prepared to prepare queues for individual priority levels (with a priority queue and a secondary queue). Moreover, you may also use the message Delayed Send function to delay process execution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 8,
            text: "A company runs an ecommerce application on Amazon EC2 instances behind an Application \nLoad Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple \nAvailability Zones. The Auto Scaling group scales based on CPU utilization metrics. The \necommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a \nlarge EC2 instance. \n \nThe database's performance degrades quickly as application load increases. The application \nhandles more read requests than write transactions. The company wants a solution that will \nautomatically scale the database to meet the demand of unpredictable read workloads while \nmaintaining high availability. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Redshift with a single node for leader and compute functionality.", correct: false },
                { id: 1, text: "Use Amazon RDS with a Single-AZ deployment.", correct: false },
                { id: 2, text: "Use Amazon Aurora with a Multi-AZ deployment.", correct: true },
                { id: 3, text: "Use Amazon ElastiCache for Memcached with EC2 Spot Instances.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAURORA is 5x performance improvement over MySQL on RDS and handles more read requests than write, maintaining high availability = Multi-AZ deployment\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "A company recently migrated to AWS and wants to implement a solution to protect the traffic that \nflows in and out of the production VPC. The company had an inspection server in its on-premises \ndata center. The inspection server performed specific operations such as traffic flow inspection \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n76 \nand traffic filtering. The company wants to have the same functionalities in the AWS Cloud. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC", correct: false },
                { id: 1, text: "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.", correct: false },
                { id: 2, text: "Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering", correct: true },
                { id: 3, text: "Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Network Firewall is a stateful, managed network firewall and intrusion detection and prevention service for your virtual private cloud (VPC) that you created in Amazon Virtual Private Cloud (Amazon VPC). With Network Firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon \nRDS for PostgreSQL. The company needs a reporting solution that provides data visualization \nand includes all the data sources within the data lake. Only the company's management team \nshould have full access to all the visualizations. The rest of the company should have only limited \naccess. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an analysis in Amazon QuickSight.", correct: false },
                { id: 1, text: "Create an analysis in Amazon OuickSighl.", correct: true },
                { id: 2, text: "Create an AWS Glue table and crawler for the data in Amazon S3.", correct: false },
                { id: 3, text: "Create an AWS Glue table and crawler for the data in Amazon S3.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/quicksight/latest/user/sharing-a-dashboard.html https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A company is implementing a new business application. The application runs on two Amazon \nEC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs \nto ensure that the EC2 instances can access the S3 bucket. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n77 \n \nWhat should the solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role that grants access to the S3 bucket.", correct: true },
                { id: 1, text: "Create an IAM policy that grants access to the S3 bucket.", correct: false },
                { id: 2, text: "Create an IAM group that grants access to the S3 bucket.", correct: false },
                { id: 3, text: "Create an IAM user that grants access to the S3 bucket.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAlways remember that you should associate IAM roles to EC2 instances. https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "An application development team is designing a microservice that will convert large images to \nsmaller, compressed images. When a user uploads an image through the web interface, the \nmicroservice should store the image in an Amazon S3 bucket, process and compress the image \nwith an AWS Lambda function, and store the image in its compressed form in a different S3 \nbucket. \n \nA solutions architect needs to design a solution that uses durable, stateless components to \nprocess the images automatically. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue.", correct: true },
                { id: 1, text: "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 2, text: "Configure the Lambda function to monitor the S3 bucket for new uploads.", correct: false },
                { id: 3, text: "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company has a three-tier web application that is deployed on AWS. The web servers are \ndeployed in a public subnet in a VPC. The application servers and database servers are deployed \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n78 \nin private subnets in the same VPC. The company has deployed a third-party virtual firewall \nappliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP \ninterface that can accept IP packets. \nA solutions architect needs to Integrate the web application with the appliance to inspect all traffic \nto the application before the traffic teaches the web server. \n \nWhich solution will moot these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a Network Load Balancer the public subnet of the application's VPC to route the traffic to", correct: false },
                { id: 1, text: "Create an Application Load Balancer in the public subnet of the application's VPC to route the", correct: false },
                { id: 2, text: "Deploy a transit gateway in the inspection VPC.", correct: false },
                { id: 3, text: "Deploy a Gateway Load Balancer in the inspection VPC.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nGateway Load Balancer is a new type of load balancer that operates at layer 3 of the OSI model and is built on Hyperplane, which is capable of handling several thousands of connections per second. Gateway Load Balancer endpoints are configured in spoke VPCs originating or receiving traffic from the Internet. This architecture allows you to perform inline inspection of traffic from multiple spoke VPCs in a simplified and scalable fashion while still centralizing your virtual appliances. https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection- using-aws-gateway-load-balancer/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A company wants to improve its ability to clone large amounts of production data into a test \nenvironment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon \nElastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the \nproduction environment. The software that accesses this data requires consistently high I/O \nperformance. \n \nA solutions architect needs to minimize the time that is required to clone the production data into \nthe test environment. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Take EBS snapshots of the production EBS volumes.", correct: false },
                { id: 1, text: "Configure the production EBS volumes to use the EBS Multi-Attach feature.", correct: false },
                { id: 2, text: "Take EBS snapshots of the production EBS volumes.", correct: false },
                { id: 3, text: "Take EBS snapshots of the production EBS volumes.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html https://aws.amazon.com/cn/about-aws/whats-new/2020/11/amazon-ebs-fast-snapshot-restore- now-available-us-govcloud-regions/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will \nfeature exactly one product on sale for a period of 24 hours. The company wants to be able to \nhandle millions of requests each hour with millisecond latency during peak hours. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon S3 to host the full website in different S3 buckets.", correct: false },
                { id: 1, text: "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across", correct: false },
                { id: 2, text: "Migrate the full application to run in containers.", correct: false },
                { id: 3, text: "Use an Amazon S3 bucket to host the website's static content.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAll of the components are infinitely scalable dynamoDB, API Gateway, Lambda, and of course s3+cloudfront.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media \napplication. The media files must be resilient to the loss of an Availability Zone Some files are \naccessed frequently while other files are rarely accessed in an unpredictable pattern. The \nsolutions architect must minimize the costs of storing and retrieving the media files. \nWhich storage option meets these requirements?",
            options: [
                { id: 0, text: "S3 Standard", correct: false },
                { id: 1, text: "S3 Intelligent-Tiering", correct: true },
                { id: 2, text: "S3 Standard-Infrequent Access {S3 Standard-IA)", correct: false },
                { id: 3, text: "S3 One Zone-Infrequent Access (S3 One Zone-IA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 Intelligent-Tiering -Perfect use case when you don't know the frequency of access or irregular patterns of usage. Amazon S3 offers a range of storage classes designed for different use cases. These include S3 Standard for general-purpose storage of frequently accessed data; S3 Intelligent-Tiering for data with unknown or changing access patterns; S3 Standard-Infrequent Access (S3 Standard-IA) and S3 One Zone-Infrequent Access (S3 One Zone-IA) for long-lived, but less frequently accessed data; and Amazon S3 Glacier (S3 Glacier) and Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation. If you have data residency requirements that can't be met by an existing AWS Region, you can use the S3 Outposts storage class to store your S3 data on-premises. Amazon S3 also offers capabilities to manage your data throughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application. https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A company has an on-premises MySQL database used by the global sales team with infrequent \naccess patterns. \nThe sales team requires the database to have minimal downtime. \nA database administrator wants to migrate this database to AWS without selecting a particular \ninstance type in anticipation of more users in the future. \nWhich service should a solution architect recommend?",
            options: [
                { id: 0, text: "Amazon Aurora MySQL", correct: false },
                { id: 1, text: "Amazon Aurora Serverless for MySQL", correct: true },
                { id: 2, text: "Amazon Redshift Spectrum", correct: false },
                { id: 3, text: "Amazon RDS for MySQL", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future\" Serverless sounds right, and it's compatible with MySQL and PostgreSQL. https://aws.amazon.com/rds/aurora/serverless/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company is building an application on Amazon EC2 instances that generates temporary \ntransactional data.  \nThe application requires access to data storage that can provide configurable and consistent \nIOPS. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Provision an EC2 instance with a Throughput Optimized HDD (st1) root volume and a Cold", correct: false },
                { id: 1, text: "Provision an EC2 instance with a Throughput Optimized HDD (st1) volume that will serve as the", correct: false },
                { id: 2, text: "Provision an EC2 instance with a General Purpose SSD (gp2) root volume and Provisioned", correct: true },
                { id: 3, text: "Provision an EC2 instance with a General Purpose SSD (gp2) root volume.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nOnly gp3, io1, or io2 Volumes have configurable IOPS. You cannot add HDD in root volume. SSD needs to be selected as root volume and HDD as Data Volume. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company is hosting 60 TB of production-level data in an Amazon S3 bucket. A solution \narchitect needs to bring that data on premises for quarterly audit requirements. This export of \ndata must be encrypted while in transit. The company has low network bandwidth in place \nbetween AWS and its on-premises data center. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Deploy AWS Migration Hub with 90-day replication windows for data transfer.", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway volume gateway on AWS.", correct: false },
                { id: 2, text: "Deploy Amazon Elastic File System (Amazon EFS), with lifecycle policies enabled, on AWS.", correct: false },
                { id: 3, text: "Deploy an AWS Snowball device in the on-premises data center after completing an export job", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Snowball with the Snowball device has the following features: 80 TB and 50 TB models are available in US Regions; 50 TB model available in all other AWS Regions. https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A solutions architect is designing the cloud architecture for a company that needs to host \nhundreds of machine learning models for its users. During startup, the models need to load up to \n10 GB of data from Amazon S3 into memory, but they do not need disk access. Most of the \nmodels are used sporadically, but the users expect all of them to be highly available and \naccessible with low latency. \n \nWhich solution meets the requirements and is MOST cost-effective?",
            options: [
                { id: 0, text: "Deploy models as AWS Lambda functions behind an Amazon API Gateway for each model.", correct: false },
                { id: 1, text: "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind an", correct: false },
                { id: 2, text: "Deploy models as AWS Lambda functions behind a single Amazon API Gateway with path-", correct: true },
                { id: 3, text: "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind a single", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "A company is developing an ecommerce application that will consist of a load-balanced front end, \na container-based application, and a relational database. A solutions architect needs to create a \nhighly available solution that operates with as little manual intervention as possible. \n \nWhich solutions meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Create an Amazon RDS DB instance in Multi-AZ mode.", correct: true },
                { id: 1, text: "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.", correct: false },
                { id: 2, text: "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.", correct: false },
                { id: 3, text: "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type", correct: false },
                { id: 4, text: "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html 1. Relational database: RDS 2. Container-based applications: ECS \"Amazon ECS enables you to launch and stop your container-based applications by using simple API calls. You can also retrieve the state of your cluster from a centralized service and have access to many familiar Amazon EC2 features.\" 3. Little manual intervention: Fargate You can run your tasks and services on a serverless infrastructure that is managed by AWS Fargate. Alternatively, for more control over your infrastructure, you can run your tasks and services on a cluster of Amazon EC2 instances that you manage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A company has an ecommerce application that stores data in an on-premises SQL database. The \ncompany has decided to migrate this database to AWS. However, as part of the migration, the \ncompany wants to find a way to attain sub-millisecond responses to common read requests. \n \nA solutions architect knows that the increase in speed is paramount and that a small percentage \nof stale data returned in the database reads is acceptable. \n \nWhat should the solutions architect recommend?",
            options: [
                { id: 0, text: "Build Amazon RDS read replicas.", correct: false },
                { id: 1, text: "Build the database as a larger instance type.", correct: false },
                { id: 2, text: "Build a database cache using Amazon ElastiCache.", correct: true },
                { id: 3, text: "Build a database cache using Amazon Elasticsearch Service (Amazon ES).", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A company is designing an application where users upload small files into Amazon S3.  \nAfter a user uploads a file, the file requires one-time simple processing to transform the data and \nsave the data in JSON format for later analysis. \n \nEach file must be processed as quickly as possible after it is uploaded. Demand will vary.  \nOn some days, users will upload a high number of files. On other days, users will upload a few \nfiles or no files. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon EMR to read text files from Amazon S3.", correct: false },
                { id: 1, text: "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service", correct: false },
                { id: 2, text: "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service", correct: true },
                { id: 3, text: "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon S3 sends event notifications about S3 buckets (for example, object created, object removed, or object restored) to an SNS topic in the same Region. The SNS topic publishes the event to an SQS queue in the central Region. The SQS queue is configured as the event source for your Lambda function and buffers the event messages for the Lambda function. The Lambda function polls the SQS queue for messages and processes the Amazon S3 event notifications according to your application's requirements. https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/subscribe-a-lambda-function- to-event-notifications-from-s3-buckets-in-different-aws-regions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "An application allows users at a company's headquarters to access product data. The product \ndata is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an \napplication performance slowdown and wants to separate read traffic from write traffic. \nA solutions architect needs to optimize the application's performance quickly. \nWhat should the solutions architect recommend?",
            options: [
                { id: 0, text: "Change the existing database to a Multi-AZ deployment.", correct: false },
                { id: 1, text: "Change the existing database to a Multi-AZ deployment.", correct: false },
                { id: 2, text: "Create read replicas for the database.", correct: false },
                { id: 3, text: "Create read replicas for the database.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nFor replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source DB instance. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadRe plicas.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "An Amazon EC2 administrator created the following policy associated with an IAM group \ncontaining several users. \n \n \n \n \nWhat is the effect of this policy?",
            options: [
                { id: 0, text: "Users can terminate an EC2 instance in any AWS Region except us-east-1.", correct: false },
                { id: 1, text: "Users can terminate an EC2 instance with the IP address 10 100 100 1 in the us-east-1 Region", correct: false },
                { id: 2, text: "Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is", correct: true },
                { id: 3, text: "Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAs the policy prevents anyone from doing any EC2 action on any region except us-east-1 and allows only users with source ip 10.100.100.0/24 to terminate instances. So user with source ip 10.100.100.254 can terminate instances in us-east-1 region.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A company has a large Microsoft SharePoint deployment running on-premises that requires \nMicrosoft Windows shared file storage. The company wants to migrate this workload to the AWS \nCloud and is considering various storage options. The storage solution must be highly available \nand integrated with Active Directory for access control. \n \nWhich solution will satisfy these requirements?",
            options: [
                { id: 0, text: "Configure Amazon EFS storage and set the Active Directory domain for authentication", correct: false },
                { id: 1, text: "Create an SMB Me share on an AWS Storage Gateway tile gateway in two Availability Zones", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume", correct: false },
                { id: 3, text: "Create an Amazon FSx for Windows File Server file system on AWS and set the Active", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon FSx for Windows File Server is a fully managed file storage service that is designed to be used with Microsoft Windows workloads. It is integrated with Active Directory for access control and is highly available, as it stores data across multiple availability zones. Additionally, FSx can be used to migrate data from on-premises Microsoft Windows file servers to the AWS Cloud. This makes it a good fit for the requirements described in the\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "An image-processing company has a web application that users use to upload images. The \napplication uploads the images into an Amazon S3 bucket. The company has set up S3 event \nnotifications to publish the object creation events to an Amazon Simple Queue Service (Amazon \nSQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function \nthat processes the images and sends the results to users through email. \n \nUsers report that they are receiving multiple email messages for every uploaded image. A \nsolutions architect determines that SQS messages are invoking the Lambda function more than \nonce, resulting in multiple email messages. \n \nWhat should the solutions architect do to resolve this issue with the LEAST operational \noverhead?",
            options: [
                { id: 0, text: "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30", correct: false },
                { id: 1, text: "Change the SQS standard queue to an SQS FIFO queue.", correct: false },
                { id: 2, text: "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the", correct: true },
                { id: 3, text: "Modify the Lambda function to delete each message from the SQS queue immediately after the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nImmediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs- visibility-timeout.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company is implementing a shared storage solution for a media application that is hosted in the \nAWS Cloud.  \nThe company needs the ability to use SMB clients to access data. The solution must he fully \nmanaged. \nWhich AWS solution meets these requirements?",
            options: [
                { id: 0, text: "Create an AWS Storage Gateway volume gateway.", correct: false },
                { id: 1, text: "Create an AWS Storage Gateway tape gateway.", correct: false },
                { id: 2, text: "Create an Amazon EC2 Windows instance.", correct: false },
                { id: 3, text: "Create an Amazon FSx for Windows File Server tile system.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon FSx for Lustre is a fully managed file system that is designed for high-performance workloads, such as gaming applications. It provides a high-performance, scalable, and fully managed file system that is optimized for Lustre clients, and it is fully integrated with Amazon EC2. It is the only option that meets the requirements of being fully managed and able to support Lustre clients. https://aws.amazon.com/fsx/lustre/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A company's containerized application runs on an Amazon EC2 instance. The application needs \nto download security certificates before it can communicate with other business applications. The \ncompany wants a highly secure solution to encrypt and decrypt the certificates in near real time. \nThe solution also needs to store data in highly available storage after the data is encrypted. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create AWS Secrets Manager secrets for encrypted certificates.", correct: false },
                { id: 1, text: "Create an AWS Lambda function that uses the Python cryptography library to receive and", correct: false },
                { id: 2, text: "Create an AWS Key Management Service (AWS KMS) customer managed key.", correct: true },
                { id: 3, text: "Create an AWS Key Management Service (AWS KMS) customer managed key.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nS3 is highly available with LEAST operational overhead. Amazon S3 provides durability by redundantly storing the data across multiple Availability Zones whereas EBS provides durability by redundantly storing the data in a single Availability Zone. Both S3 and EBS gives the availability of 99.99%, but the only difference that occurs is that S3 is accessed via the internet using API’s and EBS is accessed by the single instance attached to EBS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A solutions architect is designing a VPC with public and private subnets. The VPC and subnets \nuse IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three \nAvailability Zones (AZs) for high availability. An internet gateway is used to provide internet \naccess for the public subnets. The private subnets require access to the internet to allow Amazon \nEC2 instances to download software updates. \n \nWhat should the solutions architect do to enable Internet access for the private subnets?",
            options: [
                { id: 0, text: "Create three NAT gateways, one for each public subnet in each AZ.", correct: true },
                { id: 1, text: "Create three NAT instances, one for each private subnet in each AZ.", correct: false },
                { id: 2, text: "Create a second internet gateway on one of the private subnets.", correct: false },
                { id: 3, text: "Create an egress-only internet gateway on one of the public subnets.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo enable Internet access for the private subnets, the solutions architect should create three NAT gateways, one for each public subnet in each Availability Zone (AZ). NAT gateways allow private instances to initiate outbound traffic to the Internet but do not allow inbound traffic from the Internet to reach the private instances. The solutions architect should then create a private route table for each AZ that forwards non- VPC traffic to the NAT gateway in its AZ. This will allow instances in the private subnets to access the Internet through the NAT gateways in the public subnets. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP \nserver that stores its data on an NFS-based file system. The server holds 200 GB of data that \nneeds to be transferred. The server must be hosted on an Amazon EC2 instance that uses an \nAmazon Elastic File System (Amazon EFS) file system. \nWhich combination of steps should a solutions architect take to automate this task? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Launch the EC2 instance into the same Availability Zone as the EFS file system.", correct: true },
                { id: 1, text: "Install an AWS DataSync agent in the on-premises data center.", correct: false },
                { id: 2, text: "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance tor", correct: false },
                { id: 3, text: "Manually use an operating system copy command to push the data to the EC2 instance.", correct: false },
                { id: 4, text: "Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA - Makes sense to have the instance in the same AZ the EFS storage is. B - The DataSync with move the data to the EFS, which already uses the EC2 instance (see the info provided).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 32,
            text: "A company has an AWS Glue extract. transform, and load (ETL) job that runs every day at the \nsame time. The job processes XML data that is in an Amazon S3 bucket. New data is added to \nthe S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data \nduring each run. \nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data?",
            options: [
                { id: 0, text: "Edit the job to use job bookmarks.", correct: true },
                { id: 1, text: "Edit the job to delete data after the data is processed", correct: false },
                { id: 2, text: "Edit the job by setting the NumberOfWorkers field to 1.", correct: false },
                { id: 3, text: "Use a FindMatches machine learning (ML) transform.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Glue tracks data that has already been processed during a previous run of an ETL job by persisting state information from the job run. This persisted state information is called a job bookmark. Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data. https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A solutions architect must design a highly available infrastructure for a website. The website is \npowered by Windows web servers that run on Amazon EC2 instances. The solutions architect \nmust implement a solution that can mitigate a large-scale DDoS attack that originates from \nthousands of IP addresses. Downtime is not acceptable for the website. \nWhich actions should the solutions architect take to protect the website from such an attack? \n(Choose two.)",
            options: [
                { id: 0, text: "Use AWS Shield Advanced to stop the DDoS attack.", correct: true },
                { id: 1, text: "Configure Amazon GuardDuty to automatically block the attackers.", correct: false },
                { id: 2, text: "Configure the website to use Amazon CloudFront for both static and dynamic content.", correct: false },
                { id: 3, text: "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network", correct: false },
                { id: 4, text: "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Shield can handle the DDoS attacks. Amazon CloudFront supports DDoS protection, integration with Shield, AWS Web Application Firewall.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "A company is preparing to deploy a new serverless workload.  \nA solutions architect must use the principle of least privilege to configure permissions that will be \nused to run an AWS Lambda function.  \nAn Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Add an execution role to the function with lambda:InvokeFunction as the action and * as the", correct: false },
                { id: 1, text: "Add an execution role to the function with lambda:InvokeFunction as the action and", correct: false },
                { id: 2, text: "Add a resource-based policy to the function with lambda:'* as the action and", correct: false },
                { id: 3, text: "Add a resource-based policy to the function with lambda:InvokeFunction as the action and", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe principle of least privilege requires that permissions are granted only to the minimum necessary to perform a task. In this case, the Lambda function needs to be able to be invoked by Amazon EventBridge (Amazon CloudWatch Events). To meet these requirements, you can add a resource-based policy to the function that allows the InvokeFunction action to be performed by the Service: events.amazonaws.com principal. This will allow Amazon EventBridge to invoke the function, but will not grant any additional permissions to the function. https://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies- eventbridge.html#lambda-permissions\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A company has an image processing workload running on Amazon Elastic Container Service \n(Amazon ECS) in two private subnets. Each private subnet uses a NAT instance for internet \naccess. All images are stored in Amazon S3 buckets. \nThe company is concerned about the data transfer costs between Amazon ECS and Amazon S3. \n \nWhat should a solutions architect do to reduce costs?",
            options: [
                { id: 0, text: "Configure a NAT gateway to replace the NAT instances.", correct: false },
                { id: 1, text: "Configure a gateway endpoint for traffic destined to Amazon S3.", correct: true },
                { id: 2, text: "Configure an interface endpoint for traffic destined to Amazon S3.", correct: false },
                { id: 3, text: "Configure Amazon CloudFront for the S3 bucket storing the images.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 and Dynamo DB does not support interface endpoints. Both S3 and DynamoDB are routed via Gateway endpoint. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html Interface Endpoint only supports services which are integrated with PrivateLink. https://docs.aws.amazon.com/vpc/latest/userguide/integrated-services-vpce-list.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company is moving Its on-premises Oracle database to Amazon Aurora PostgreSQL.  \nThe database has several applications that write to the same tables. \nThe applications need to be migrated one by one with a month in between each migration \nManagement has expressed concerns that the database has a high number of reads and writes. \nThe data must be kept in sync across both databases throughout tie migration. \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Use AWS DataSync tor the initial migration.", correct: false },
                { id: 1, text: "UseAVVS DataSync for the initial migration.", correct: false },
                { id: 2, text: "Use the AWS Schema Conversion led with AWS DataBase Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAs you can see, we have three important memory buffers in this architecture for CDC in AWS DMS. If any of these buffers experience memory pressure, the migration can have performance issues that can potentially cause failures. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.Types.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A company wants to migrate a high performance computing (HPC) application and data from on-\npremises to the AWS Cloud. The company uses tiered storage on premises with hot high-\nperformance parallel storage to support the application during periodic runs of the application, \nand more economical cold storage to hold the data when the application is not actively running. \nWhich combination of solutions should a solutions architect recommend to support the storage \nneeds of the application? (Choose two.)",
            options: [
                { id: 0, text: "Amazon S3 for cold data storage", correct: true },
                { id: 1, text: "Amazon EFS for cold data storage", correct: false },
                { id: 2, text: "Amazon S3 for high-performance parallel storage", correct: false },
                { id: 3, text: "Amazon FSx for clustre tor high-performance parallel storage", correct: false },
                { id: 4, text: "Amazon FSx for Windows for high-performance parallel storage", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/fsx/lustre/ Amazon FSx for Lustre makes it easy and cost effective to launch and run the world's most popular high-performance file system. Use it for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "A company is experiencing growth as demand for its product has increased. The company's \nexisting purchasing application is slow when traffic spikes. The application is a monolithic three \ntier application that uses synchronous transactions and sometimes sees bottlenecks in the \napplication tier. A solutions architect needs to design a solution that can meet required application \nresponse times while accounting for traffic volume spikes. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Vertically scale the application instance using a larger Amazon EC2 instance size.", correct: false },
                { id: 1, text: "Scale the application's persistence layer horizontally by introducing Oracle RAC on AWS", correct: false },
                { id: 2, text: "Scale the web and application tiers horizontally using Auto Scaling groups and an Application", correct: true },
                { id: 3, text: "Decouple the application and data tiers using Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe Application uses synchronous transactions each operation is dependent on the previous one. Using asynchronous lambda calls may not work here.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A solutions architect needs to ensure that all Amazon Elastic Block Store (Amazon EBS) volumes \nrestored from unencrypted EBS snapshots are encrypted. \n \nWhat should the solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Enable EBS encryption by default for the AWS Region", correct: true },
                { id: 1, text: "Enable EBS encryption by default for the specific volumes", correct: false },
                { id: 2, text: "Create a new volume and specify the symmetric customer master key (CMK) to use for", correct: false },
                { id: 3, text: "Create a new volume and specify the asymmetric customer master key (CMK) to use for", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nQuestion asked is to ensure that all volumes restored are encrypted. So have to be \"Enable encryption by default\". https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by- default\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed \nfrequently for 1 month. However, the files are not accessed after 1 month. The company must \nkeep the files indefinitely. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n92 \n \nWhich storage solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure S3 Intelligent-Tiering to automatically migrate objects.", correct: false },
                { id: 1, text: "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep", correct: true },
                { id: 2, text: "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-", correct: false },
                { id: 3, text: "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTransition to Glacier deep archive for cost efficiency.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team \nnotices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions \narchitect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-\ndepth analysis to identify the root cause of the vertical scaling. \nHow should the solutions architect generate the information with the LEAST operational \noverhead?",
            options: [
                { id: 0, text: "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types", correct: false },
                { id: 1, text: "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs", correct: true },
                { id: 2, text: "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs", correct: false },
                { id: 3, text: "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You can explore your usage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI reports. You can view data for up to the last 12 months, forecast how much you're likely to spend for the next 12 months, and get recommendations for what Reserved Instances to purchase. You can use Cost Explorer to identify areas that need further inquiry and see trends that you can use to understand your costs. https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 42,
            text: "A company is designing an application. The application uses an AWS Lambda function to receive \ninformation through Amazon API Gateway and to store the information in an Amazon Aurora \nPostgreSQL database. \nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly \nto handle the high volumes of data that the company needs to load into the database. A solutions \narchitect must recommend a new design to improve scalability and minimize the configuration \neffort. \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n93",
            options: [
                { id: 0, text: "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2", correct: false },
                { id: 1, text: "Change the platform from Aurora to Amazon DynamoDB.", correct: false },
                { id: 2, text: "Set up two Lambda functions.", correct: false },
                { id: 3, text: "Set up two Lambda functions. Configure one function to receive the information.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nOption D uses SQS, so the 2nd lambda function can go to the queue when responsive to keep with the DB load process. Usually the app decoupling helps with the performance improvement by distributing load.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 43,
            text: "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do \nnot have unauthorized configuration changes. \n \nWhat should a solutions architect do to accomplish this goal?",
            options: [
                { id: 0, text: "Turn on AWS Config with the appropriate rules.", correct: true },
                { id: 1, text: "Turn on AWS Trusted Advisor with the appropriate checks.", correct: false },
                { id: 2, text: "Turn on Amazon Inspector with the appropriate assessment template.", correct: false },
                { id: 3, text: "Turn on Amazon S3 server access logging.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A company is launching a new application and will display application metrics on an Amazon \nCloudWatch dashboard. The company's product manager needs to access this dashboard \nperiodically. The product manager does not have an AWS account. A solution architect must \nprovide access to the product manager by following the principle of least privilege. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Share the dashboard from the CloudWatch console.", correct: true },
                { id: 1, text: "Create an IAM user specifically for the product manager.", correct: false },
                { id: 2, text: "Create an IAM user for the company's employees.", correct: false },
                { id: 3, text: "Deploy a bastion server in a public subnet.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nShare a single dashboard and designate specific email addresses of the people who can view the dashboard. Each of these users creates their own password that they must enter to view the dashboard. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard- sharing.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company is migrating applications to AWS. The applications are deployed in different accounts. \nThe company manages the accounts centrally by using AWS Organizations. The company's \nsecurity team needs a single sign-on (SSO) solution across all the company's accounts.  \nThe company must continue managing the users and groups in its on-premises self-managed \nMicrosoft Active Directory. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.", correct: false },
                { id: 1, text: "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.", correct: true },
                { id: 2, text: "Use AWS Directory Service.", correct: false },
                { id: 3, text: "Deploy an identity provider (IdP) on premises.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nYou can configure one and two-way external and forest trust relationships between your AWS Directory Service for Microsoft Active Directory and self-managed (on-premises) directories, as well as between multiple AWS Managed Microsoft AD directories in the AWS cloud. AWS Managed Microsoft AD supports all three trust relationship directions: Incoming, Outgoing and Two-way (Bi-directional). https://aws.amazon.com/blogs/security/everything-you-wanted-to-know-about-trusts-with-aws- managed-microsoft-ad/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. \nThe service consists of Amazon EC2 instances that run in an Auto Scaling group. The company \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n95 \nhas deployments across multiple AWS Regions. \n \nThe company needs to route users to the Region with the lowest latency. The company also \nneeds automated failover between Regions. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy a Network Load Balancer (NLB) and an associated target group.", correct: true },
                { id: 1, text: "Deploy an Application Load Balancer (ALB) and an associated target group.", correct: false },
                { id: 2, text: "Deploy a Network Load Balancer (NLB) and an associated target group.", correct: false },
                { id: 3, text: "Deploy an Application Load Balancer (ALB) and an associated target group.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nGlobal Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. https://aws.amazon.com/global-accelerator/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS \nfor MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a \nmonth and is the only process that uses the database. The team wants to reduce the cost of \nrunning the tests without reducing the compute and memory attributes of the DB instance. \n \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Stop the DB instance when tests are completed.", correct: false },
                { id: 1, text: "Use an Auto Scaling policy with the DB instance to automatically scale when tests are", correct: false },
                { id: 2, text: "Create a snapshot when tests are completed.", correct: true },
                { id: 3, text: "Modify the DB instance to a low-capacity instance when tests are completed.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nIt's a DB instance, not an EC2 instance. If the DB instance is stopped, you are still paying for the storage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 48,
            text: "A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n96 \nAmazon RDS DB instances and Amazon Redshift clusters are configured with tags. The \ncompany wants to minimize the effort of configuring and operating this check. \n \nWhat should a solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Use AWS Config rules to define and detect resources that are not properly tagged.", correct: true },
                { id: 1, text: "Use Cost Explorer to display resources that are not properly tagged.", correct: false },
                { id: 2, text: "Write API calls to check all resources for proper tag allocation.", correct: false },
                { id: 3, text: "Write API calls to check all resources for proper tag allocation.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/config/latest/developerguide/tagging.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A development team needs to host a website that will be accessed by other teams. The website \ncontents consist of HTML, CSS, client-side JavaScript, and images. \nWhich method is the MOST cost-effective for hosting the website?",
            options: [
                { id: 0, text: "Containerize the website and host it in AWS Fargate.", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket and host the website there", correct: true },
                { id: 2, text: "Deploy a web server on an Amazon EC2 instance to host the website.", correct: false },
                { id: 3, text: "Configure an Application Loa d Balancer with an AWS Lambda target that uses the Express js", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIn Static Websites, Web pages are returned by the server which are prebuilt. They use simple languages such as HTML, CSS, or JavaScript. There is no processing of content on the server (according to the user) in Static Websites. Web pages are returned by the server with no change therefore, static Websites are fast. There is no interaction with databases. Also, they are less costly as the host does not need to support server-side processing with different languages. ============ In Dynamic Websites, Web pages are returned by the server which are processed during runtime means they are not prebuilt web pages but they are built during runtime according to the user's demand. These use server-side scripting languages such as PHP, Node.js, ASP.NET and many more supported by the server. So, they are slower than static websites but updates and interaction with databases are possible.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 50,
            text: "A company runs an online marketplace web application on AWS. The application serves \nhundreds of thousands of users during peak hours. The company needs a scalable, near-real-\ntime solution to share the details of millions of financial transactions with several other internal \napplications Transactions also need to be processed to remove sensitive data before being \nstored in a document database for low-latency retrieval. \n \nWhat should a solutions architect recommend to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n97",
            options: [
                { id: 0, text: "Store the transactions data into Amazon DynamoDB.", correct: false },
                { id: 1, text: "Stream the transactions data into Amazon Kinesis Data.", correct: false },
                { id: 2, text: "Stream the transactions data into Amazon Kinesis Data Streams.", correct: true },
                { id: 3, text: "Store the batched transactions data in Amazon S3 as files.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe destination of your Kinesis Data Firehose delivery stream. Kinesis Data Firehose can send data records to various destinations, including Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, and any HTTP endpoint that is owned by you or any of your third-party service providers. The following are the supported destinations: * Amazon OpenSearch Service * Amazon S3 * Datadog * Dynatrace * Honeycomb * HTTP Endpoint * Logic Monitor * MongoDB Cloud * New Relic * Splunk * Sumo Logic https://docs.aws.amazon.com/firehose/latest/dev/create-name.html https://aws.amazon.com/kinesis/data-streams/ Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and \nsecurity, the company must track configuration changes on its AWS resources and record a \nhistory of API calls made to these resources. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls", correct: false },
                { id: 1, text: "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls", correct: true },
                { id: 2, text: "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls", correct: false },
                { id: 3, text: "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCloudTrail - Track user activity and API call history. Config - Assess, audits, and evaluates the configuration and relationships of tag resources.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A company is preparing to launch a public-facing web application in the AWS Cloud. The \narchitecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer \n(ELB). A third-party service is used for the DNS. The company's solutions architect must \nrecommend a solution to detect and protect against large-scale DDoS attacks. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Enable Amazon GuardDuty on the account.", correct: false },
                { id: 1, text: "Enable Amazon Inspector on the EC2 instances.", correct: false },
                { id: 2, text: "Enable AWS Shield and assign Amazon Route 53 to it.", correct: false },
                { id: 3, text: "Enable AWS Shield Advanced and assign the ELB to it.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator standard accelerators. https://aws.amazon.com/shield/faqs/ https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load- balancing-bp6.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A company is building an application in the AWS Cloud. The application will store data in Amazon \nS3 buckets in two AWS Regions. The company must use an AWS Key Management Service \n(AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data \nin both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the \nkey must be stored in each of the two Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an S3 bucket in each Region.", correct: false },
                { id: 1, text: "Create a customer managed multi-Region KMS key.", correct: true },
                { id: 2, text: "Create a customer managed KMS key and an S3 bucket in each Region.", correct: false },
                { id: 3, text: "Create a customer managed KMS key and an S3 bucket in each Region.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nKMS Multi-region keys are required. https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data \nsources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 \ninstance is rebooted, the data in-flight is lost. \n \nThe company's data science team wants to query ingested data near-real time. \n \nWhich solution provides near-real-time data querying that is scalable with minimal data loss?",
            options: [
                { id: 0, text: "Publish data to Amazon Kinesis Data Streams.", correct: false },
                { id: 1, text: "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination.", correct: true },
                { id: 2, text: "Store ingested data in an EC2 instance store.", correct: false },
                { id: 3, text: "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nKinesis data streams consists of shards. The more througput is needed, the more shards you add, the less throughput, the more shards you remove, so it's scalable. Each shard can handle up to 1MB/s of writes. However Kinesis data streams stores ingested data for only 1 to 7 days so there is a chance of data loss. Additionally, Kinesis data analytics and kinesis data streams are both for real-time ingestion and analytics. Firehouse on the other hand is also scalable and processes data in near real time as per the requirement. It also transfers data into Redshift which is a data warehouse so data won't be lost. Redshift also has a SQL interface for performing queries for data analytics.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company is developing a mobile game that streams score updates to a backend processor and \nthen posts results on a leaderboard. \nA solutions architect needs to design a solution that can handle large traffic spikes, process the \nmobile game updates in order of receipt, and store the processed updates in a highly available \ndatabase. The company also wants to minimize the management overhead required to maintain \nthe solution. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to Amazon Kinesis Data Streams.", correct: true },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams.", correct: false },
                { id: 2, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic.", correct: false },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nKeywords to focus on would be highly available database - DynamoDB would be a better choice for leaderboard.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 56,
            text: "An ecommerce website is deploying its web application as Amazon Elastic Container Service \n(Amazon ECS) container instance behind an Application Load Balancer (ALB). During periods of \nhigh activity, the website slows down and availability is reduced. A solutions architect uses \nAmazon CloudWatch alarms to receive notifications whenever there is an availability issues so \nthey can scale out resource Company management wants a solution that automatically responds \nto such events. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Set up AWS Auto Scaling to scale out the ECS service when there are timeouts on the ALB.", correct: false },
                { id: 1, text: "Set up AWS Auto Scaling to scale out the ECS service when the ALB CPU utilization is too", correct: false },
                { id: 2, text: "Set up AWS Auto Scaling to scale out the ECS service when the service's CPU utilization is too", correct: true },
                { id: 3, text: "Set up AWS Auto Scaling to scale out the ECS service when the ALB target group CPU", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nMatch deployed capacity to the incoming application load, using scaling policies for both the ECS service and the Auto Scaling group in which the ECS cluster runs. Scaling up cluster instances and service tasks when needed and safely scaling them down when demand subsides, keeps you out of the capacity guessing game. This provides you high availability with lowered costs in the long run. https://aws.amazon.com/blogs/compute/automatic-scaling-with-amazon-ecs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 57,
            text: "A company has no existing file share services. A new project requires access to file storage that \nis mountable as a drive for on-premises desktops. The file server must authenticate users to an \nActive Directory domain before they are able to access the storage. \nWhich service will allow Active Directory users to mount storage as a drive on their desktops? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n101",
            options: [
                { id: 0, text: "AWS S3 Glacier", correct: false },
                { id: 1, text: "AWS DataSync", correct: false },
                { id: 2, text: "AWS Snowball Edge", correct: false },
                { id: 3, text: "AWS Storage Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nBefore you create an SMB file share, make sure that you configure SMB security settings for your file gateway. You also configure either Microsoft Active Directory (AD) or guest access for authentication. https://docs.aws.amazon.com/storagegateway/latest/userguide/CreatingAnSMBFileShare.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "Management has decided to deploy all AWS VPCs with IPv6 enabled. After sometime, a \nsolutions architect tries to launch a new instance and receives an error stating that there is no \nenough IP address space available in the subnet. \n \nWhat should the solutions architect do to fix this?",
            options: [
                { id: 0, text: "Check to make sure that only IPv6 was used during the VPC creation", correct: false },
                { id: 1, text: "Create a new IPv4 subnet with a larger range, and then launch the instance", correct: true },
                { id: 2, text: "Create a new IPv6-only subnet with a larger range, and then launch the instance", correct: false },
                { id: 3, text: "Disable the IPv4 subnet and migrate all instances to IPv6 only.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://cloudonaut.io/getting-started-with-ipv6-on-aws/ First of all, there is no IPv6-only VPC on AWS. A VPC is always IPv4 enabled, but you can optionally enable IPv6 (dual-stack).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A company operates an ecommerce website on Amazon EC2 instances behind an Application \nLoad Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues \nrelated to a high request rate from illegitimate external systems with changing IP addresses. The \nsecurity team is worried about potential DDoS attacks against the website. The company must \nblock the illegitimate incoming requests in a way that has a minimal impact on legitimate users. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Deploy Amazon Inspector and associate it with the ALB.", correct: false },
                { id: 1, text: "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.", correct: true },
                { id: 2, text: "Deploy rules to the network ACLs associated with the ALB to block the incoming traffic.", correct: false },
                { id: 3, text: "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nRate limit For a rate-based rule, enter the maximum number of requests to allow in any five-minute period from an IP address that matches the rule's conditions. The rate limit must be at least 100. You can specify a rate limit alone, or a rate limit and conditions. If you specify only a rate limit, Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A media company is evaluating the possibility of moving its systems to the AWS Cloud. The \ncompany needs at least 10 TB of storage with the maximum possible I/O performance for video \nprocessing, 300 TB of very durable storage for storing media content, and 900 TB of storage to \nmeet requirements for archival media that is not in use anymore. \nWhich set of services should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Amazon EBS for maximum performance.", correct: false },
                { id: 1, text: "Amazon EBS for maximum performance.", correct: false },
                { id: 2, text: "Amazon EC2 instance store for maximum performance.", correct: false },
                { id: 3, text: "Amazon EC2 Instance store for maximum performance.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nMax instance store possible at this time is 30TB for NVMe which has the higher I/O compared to EBS. is4gen.8xlarge 4 x 7,500 GB (30 TB) NVMe SSD https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store- volumes\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A company wants to run applications in containers in the AWS Cloud. These applications are \nstateless and can tolerate disruptions within the underlying infrastructure. The company needs a \nsolution that minimizes cost and operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.", correct: false },
                { id: 1, text: "Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node", correct: true },
                { id: 2, text: "Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application", correct: false },
                { id: 3, text: "Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nRunning your Kubernetes and containerized workloads on Amazon EC2 Spot Instances is a great way to save costs. ... AWS makes it easy to run Kubernetes with Amazon Elastic Kubernetes Service (EKS) a managed Kubernetes service to run production-grade workloads on AWS. To cost optimize these workloads, run them on Spot Instances. https://aws.amazon.com/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 62,
            text: "A company is running a multi-tier web application on premises. The web application is \ncontainerized and runs on a number of Linux hosts connected to a PostgreSQL database that \ncontains user records. The operational overhead of maintaining the infrastructure and capacity \nplanning is limiting the company's growth. A solutions architect must improve the application's \ninfrastructure. \n \nWhich combination of actions should the solutions architect take to accomplish this? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Migrate the PostgreSQL database to Amazon Aurora", correct: true },
                { id: 1, text: "Migrate the web application to be hosted on Amazon EC2 instances.", correct: false },
                { id: 2, text: "Set up an Amazon CloudFront distribution for the web application content.", correct: false },
                { id: 3, text: "Set up Amazon ElastiCache between the web application and the PostgreSQL database.", correct: false },
                { id: 4, text: "Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA - Aurora supports PostgreSQL. E - The existing WebApp already run in containers On-Prem and is logical to migrate to a cloud container svc like serverless Fargate on ECS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "An application runs on Amazon EC2 instances across multiple Availability Zones. The instances \nrun in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application \nperforms best when the CPU utilization of the EC2 instances is at or near 40%.  \nWhat should a solutions architect do to maintain the desired performance across all instances in \nthe group?",
            options: [
                { id: 0, text: "Use a simple scaling policy to dynamically scale the Auto Scaling group", correct: false },
                { id: 1, text: "Use a target tracking policy to dynamically scale the Auto Scaling group", correct: true },
                { id: 2, text: "Use an AWS Lambda function to update the desired Auto Scaling group capacity.", correct: false },
                { id: 3, text: "Use scheduled scaling actions to scale up and scale down the Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWith a target tracking scaling policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over- provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern. https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target- tracking.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 64,
            text: "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. \nThe company wants to serve all the files through an Amazon CloudFront distribution. The \ncompany does not want the files to be accessible through direct navigation to the S3 URL. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n104 \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Write individual policies for each S3 bucket to grant read permission for only CloudFront access.", correct: false },
                { id: 1, text: "Create an IAM user. Grant the user read permission to objects in the S3 bucket.", correct: false },
                { id: 2, text: "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and", correct: false },
                { id: 3, text: "Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCreate a CloudFront origin access identity (OAI) https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test12: [
        {
            id: 0,
            text: "A company's website provides users with downloadable historical performance reports. The \nwebsite needs a solution that will scale to meet the company's website demands globally. The \nsolution should be cost-effective, limit the provisioning of infrastructure resources, and provide the \nfastest possible response time. \n \nWhich combination should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Amazon CloudFront and Amazon S3", correct: true },
                { id: 1, text: "AWS Lambda and Amazon DynamoDB", correct: false },
                { id: 2, text: "Application Load Balancer with Amazon EC2 Auto Scaling", correct: false },
                { id: 3, text: "Amazon Route 53 with internal Application Load Balancers", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 1,
            text: "A company runs an Oracle database on premises. As part of the company's migration to AWS, \nthe company wants to upgrade the database to the most recent available version. The company \nalso wants to set up disaster recovery (DR) for the database. The company needs to minimize \nthe operational overhead for normal operations and DR setup. The company also needs to \nmaintain access to the database's underlying operating system. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the Oracle database to an Amazon EC2 instance.", correct: false },
                { id: 1, text: "Migrate the Oracle database to Amazon RDS for Oracle.", correct: false },
                { id: 2, text: "Migrate the Oracle database to Amazon RDS Custom for Oracle.", correct: true },
                { id: 3, text: "Migrate the Oracle database to Amazon RDS for Oracle.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company wants to move its application to a serverless solution. The serverless solution needs \nto analyze existing and new data by using SQL. The company stores the data in an Amazon S3 \nbucket. The data requires encryption and must be replicated to a different AWS Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a new S3 bucket. Load the data into the new S3 bucket.", correct: true },
                { id: 1, text: "Create a new S3 bucket. Load the data into the new S3 bucket.", correct: false },
                { id: 2, text: "Load the data into the existing S3 bucket.", correct: false },
                { id: 3, text: "Load the data into the existing S3 bucket.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon S3 Bucket Keys reduce the cost of Amazon S3 server-side encryption using AWS Key Management Service (SSE-KMS). This new bucket-level key for SSE can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS. With a few clicks in the AWS Management Console, and without any changes to your client applications, you can configure your bucket to use an S3 Bucket Key for AWS KMS-based encryption on new objects. The Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A company runs workloads on AWS. The company needs to connect to a service from an \nexternal provider. The service is hosted in the provider's VPC. According to the company's \nsecurity team, the connectivity must be private and must be restricted to the target service. The \nconnection must be initiated only from the company's VPC. \n \nWhich solution will mast these requirements?",
            options: [
                { id: 0, text: "Create a VPC peering connection between the company's VPC and the provider's VPC.", correct: false },
                { id: 1, text: "Ask the provider to create a virtual private gateway in its VPC.", correct: false },
                { id: 2, text: "Create a NAT gateway in a public subnet of the company's VPC.", correct: false },
                { id: 3, text: "Ask the provider to create a VPC endpoint for the target service.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS PrivateLink provides private connectivity between VPCs, AWS services, and your on- premises networks, without exposing your traffic to the public internet. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify your network architecture. Interface **VPC endpoints**, powered by AWS PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. https://aws.amazon.com/privatelink/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. \nThe on-premises database must remain online and accessible during the migration. The Aurora \ndatabase must remain synchronized with the on-premises database. \n \nWhich combination of actions must a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Create an ongoing replication task.", correct: true },
                { id: 1, text: "Create a database backup of the on-premises database", correct: false },
                { id: 2, text: "Create an AWS Database Migration Service (AWS DMS) replication server", correct: false },
                { id: 3, text: "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).", correct: false },
                { id: 4, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Database Migration Service (AWS DMS) helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can also continuously replicate data with low latency from any supported source to any supported target. https://aws.amazon.com/dms/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to \nmanage each business unit's account independently upon request. The root email recipient \nmissed a notification that was sent to the root user email address of one account. The company \nwants to ensure that all future notifications are not missed. Future notifications must be limited to \naccount administrators. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the company's email server to forward notification email messages that are sent to", correct: false },
                { id: 1, text: "Configure all AWS account root user email addresses as distribution lists that go to a few", correct: true },
                { id: 2, text: "Configure all AWS account root user email messages to be sent to one administrator who is", correct: false },
                { id: 3, text: "Configure all existing AWS accounts and all newly created accounts to use the same root user", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUse a group email address for the management account's root user https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt- acct.html#best-practices_mgmt-acct_email-address\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS \naccount. The company needs to create a strategy to access and administer the instances \nremotely and securely. The company needs to implement a repeatable process that works with \nnative AWS services and follows the AWS Well-Architected Framework.  \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use the EC2 serial console to directly access the terminal interface of each instance for", correct: false },
                { id: 1, text: "Attach the appropriate IAM role to each existing instance and new instance.", correct: true },
                { id: 2, text: "Create an administrative SSH key pair.", correct: false },
                { id: 3, text: "Establish an AWS Site-to-Site VPN connection.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/setup-launch-managed- instance.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. \nThe website is experiencing increased demand from around the world. The company must \ndecrease latency for users who access the website. \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Replicate the S3 bucket that contains the website to all AWS Regions.", correct: false },
                { id: 1, text: "Provision accelerators in AWS Global Accelerator.", correct: false },
                { id: 2, text: "Add an Amazon CloudFront distribution in front of the S3 bucket.", correct: true },
                { id: 3, text: "Enable S3 Transfer Acceleration on the bucket.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 8,
            text: "A company maintains a searchable repository of items on its website. The data is stored in an \nAmazon RDS for MySQL database table that contains more than 10 million rows. The database \nhas 2 TB of General Purpose SSD storage. There are millions of updates against this data every \nday through the company's website. \nThe company has noticed that some insert operations are taking 10 seconds or longer. \nThe company has determined that the database storage performance is the problem. \nWhich solution addresses this performance issue?",
            options: [
                { id: 0, text: "Change the storage type to Provisioned IOPS SSD", correct: true },
                { id: 1, text: "Change the DB instance to a memory optimized instance class", correct: false },
                { id: 2, text: "Change the DB instance to a burstable performance instance class", correct: false },
                { id: 3, text: "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nProvisioned IOPS volumes are backed by solid-state drives (SSDs) and are the highest performance EBS volumes designed for your critical, I/O intensive database applications. These volumes are ideal for both IOPS-intensive and throughput-intensive workloads that require extremely low latency. https://aws.amazon.com/ebs/features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A company has thousands of edge devices that collectively generate 1 TB of status alerts each \nday. Each alert is approximately 2 KB in size.  \nA solutions architect needs to implement a solution to ingest and store the alerts for future \nanalysis. \nThe company wants a highly available solution. However, the company needs to minimize costs \nand does not want to manage additional infrastructure. Additionally, the company wants to keep \n14 days of data available for immediate analysis and archive any data older than 14 days. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
            options: [
                { id: 0, text: "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts.", correct: true },
                { id: 1, text: "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic", correct: false },
                { id: 2, text: "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts.", correct: false },
                { id: 3, text: "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDefinitely A, it's the most operationally efficient compared to D, which requires a lot of code and infrastructure to maintain. A is mostly managed (firehose is fully managed and S3 lifecycles are also managed).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A company's application integrates with multiple software-as-a-service (SaaS) sources for data \ncollection. The company runs Amazon EC2 instances to receive the data and to upload the data \nto an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data \nalso sends a notification to the user when an upload is complete. The company has noticed slow \napplication performance and wants to improve the performance as much as possible. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Auto Scaling group so that EC2 instances can scale out.", correct: false },
                { id: 1, text: "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3", correct: true },
                { id: 2, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to", correct: false },
                { id: 3, text: "Create a Docker container to use instead of an EC2 instance. Host the containerized application", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. https://aws.amazon.com/appflow/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A company runs a highly available image-processing application on Amazon EC2 instances in a \nsingle VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The \nEC2 instances do not communicate with each other. However, the EC2 instances download \nimages from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The \ncompany is concerned about data transfer charges. \nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n110",
            options: [
                { id: 0, text: "Launch the NAT gateway in each Availability Zone", correct: false },
                { id: 1, text: "Replace the NAT gateway with a NAT instance", correct: false },
                { id: 2, text: "Deploy a gateway VPC endpoint for Amazon S3", correct: true },
                { id: 3, text: "Provision an EC2 Dedicated Host to run the EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nVPC gateway endpoints allow communication to Amazon S3 and Amazon DynamoDB without incurring data transfer charges within the same Region. On the other hand NAT gateway incurs additional data processing charges. https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common- architectures/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 12,
            text: "A company has an on-premises application that generates a large amount of time-sensitive data \nthat is backed up to Amazon S3. The application has grown and there are user complaints about \ninternet bandwidth limitations. A solutions architect needs to design a long-term solution that \nallows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for \ninternal users. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint", correct: false },
                { id: 1, text: "Establish a new AWS Direct Connect connection and direct backup traffic through this new", correct: true },
                { id: 2, text: "Order daily AWS Snowball devices Load the data onto the Snowball devices and return the", correct: false },
                { id: 3, text: "Submit a support ticket through the AWS Management Console. Request the removal of S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDirect connect is a dedicated connection between on-prem and AWS, this is the way to ensure stable network connectivity that will not wax and wane like internet connectivity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company has an Amazon S3 bucket that contains critical data. The company must protect the \ndata from accidental deletion. \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Enable versioning on the S3 bucket.", correct: true },
                { id: 1, text: "Enable MFA Delete on the S3 bucket.", correct: false },
                { id: 2, text: "Create a bucket policy on the S3 bucket.", correct: false },
                { id: 3, text: "Enable default encryption on the S3 bucket.", correct: false },
                { id: 4, text: "Create a lifecycle policy for the objects in the S3 bucket.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo prevent or mitigate future accidental deletions, consider the following features: - Enable versioning to keep historical versions of an object. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company has a data ingestion workflow that consists the following: \n \n- An Amazon Simple Notification Service (Amazon SNS) topic for \nnotifications about new data deliveries. \n- An AWS Lambda function to process the data and record metadata \n \nThe company observes that the ingestion workflow fails occasionally because of network \nconnectivity issues. When such a failure occurs, the Lambda function does not ingest the \ncorresponding data unless the company manually reruns the job.  \nWhich combination of actions should a solutions architect take to ensure that the Lambda \nfunction ingests all data in the future? (Choose two.)",
            options: [
                { id: 0, text: "Configure the Lambda function In multiple Availability Zones.", correct: false },
                { id: 1, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe It to me SNS", correct: true },
                { id: 2, text: "Increase the CPU and memory that are allocated to the Lambda function.", correct: false },
                { id: 3, text: "Increase provisioned throughput for the Lambda function.", correct: false },
                { id: 4, text: "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA, C, D options are wrong, since Lambda is fully managed service which provides high availability and scalability by its own.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company's web application is running on Amazon EC2 instances behind an Application Load \nBalancer. The company recently changed its policy, which now requires the application to be \naccessed from one specific country only. \nWhich configuration will meet this requirement?",
            options: [
                { id: 0, text: "Configure the security group for the EC2 instances.", correct: false },
                { id: 1, text: "Configure the security group on the Application Load Balancer.", correct: false },
                { id: 2, text: "Configure AWS WAF on the Application Load Balancer in a VPC.", correct: true },
                { id: 3, text: "Configure the network ACL for the subnet that contains the EC2 instances.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nGeographic (Geo) Match Conditions in AWS WAF. This new condition type allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access. https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic- match/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n112 \nA company has a multi-tier application that runs six front-end web servers in an Amazon EC2 \nAuto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A \nsolutions architect needs to modify the infrastructure to be highly available without modifying the \napplication. \n \nWhich architecture should the solutions architect choose that provides high availability?",
            options: [
                { id: 0, text: "Create an Auto Scaling group that uses three instances across each of two Regions.", correct: false },
                { id: 1, text: "Modify the Auto Scaling group to use three instances across each of two Availability Zones.", correct: true },
                { id: 2, text: "Create an Auto Scaling template that can be used to quickly create more instances in another", correct: false },
                { id: 3, text: "Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nHigh availability can be enabled for this architecture quite simply by modifying the existing Auto Scaling group to use multiple availability zones. The ASG will automatically balance the load so you don't actually need to specify the instances per AZ.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "Organizers for a global event want to put daily reports online as static HTML pages. The pages \nare expected to generate millions of views from users around the world. The files are stored In an \nAmazon S3 bucket. A solutions architect has been asked to design an efficient and effective \nsolution. Which action should the solutions architect take to accomplish this?",
            options: [
                { id: 0, text: "Generate presigned URLs for the files.", correct: false },
                { id: 1, text: "Use cross-Region replication to all Regions.", correct: false },
                { id: 2, text: "Use the geoproximity feature of Amazon Route 53.", correct: false },
                { id: 3, text: "Use Amazon CloudFront with the S3 bucket as its origin.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCloudFront is a content delivery network (CDN) offered by Amazon Web Services (AWS). It functions as a reverse proxy service that caches web content across AWS's global data centers, improving loading speeds and reducing the strain on origin servers. CloudFront can be used to efficiently deliver large amounts of static or dynamic content anywhere in the world.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A company runs an application using Amazon ECS. The application creates resized versions of \nan original image and then makes Amazon S3 API calls to store the resized images in Amazon \nS3. \nHow can a solutions architect ensure that the application has permission to access Amazon S3?",
            options: [
                { id: 0, text: "Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch", correct: false },
                { id: 1, text: "Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task", correct: true },
                { id: 2, text: "Create a security group that allows access from Amazon ECS to Amazon S3, and update the", correct: false },
                { id: 3, text: "Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThe short name or full Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that grants containers in the task permission to call AWS APIs on your behalf.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A solutions architect needs to securely store a database user name and password that an \napplication uses to access an Amazon RDS DB instance. The application that accesses the \ndatabase runs on an Amazon EC2 instance. The solutions architect wants to create a secure \nparameter in AWS Systems Manager Parameter Store. \nWhat should the solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role that has read access to the Parameter Store parameter.", correct: true },
                { id: 1, text: "Create an IAM policy that allows read access to the Parameter Store parameter.", correct: false },
                { id: 2, text: "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance.", correct: false },
                { id: 3, text: "Create an IAM trust relationship between the DB instance and the EC2 instance.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThere should be the Decrypt access to KMS. \"If you choose the SecureString parameter type when you create your parameter, Systems Manager uses AWS KMS to encrypt the parameter value.\" https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter- store.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A company is running a batch application on Amazon EC2 instances.  \nThe application consists of a backend with multiple Amazon RDS databases.  \nThe application is causing a high number of leads on the databases.  \nA solutions architect must reduce the number of database reads while ensuring high availability. \nWhat should the solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Add Amazon RDS read replicas.", correct: false },
                { id: 1, text: "Use Amazon ElasbCache for Redls.", correct: true },
                { id: 2, text: "Use Amazon Route 53 DNS caching.", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Memcached.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUse ElastiCache to reduce reading and choose redis to ensure high availability.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n114 \nA security team wants to limit access to specific services or actions in all of the team's AWS \naccounts. All accounts belong to a large organization in AWS Organizations. The solution must \nbe scalable and there must be a single point where permissions can be maintained.  \nWhat should a solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Create an ACL to provide access to the services or actions.", correct: false },
                { id: 1, text: "Create a security group to allow accounts and attach it to user groups.", correct: false },
                { id: 2, text: "Create cross-account roles in each account to deny access to the services or actions.", correct: false },
                { id: 3, text: "Create a service control policy in the root organizational unit to deny access to the services or", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nService control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization's access control guidelines. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A company is concerned about the security of its public web application due to recent web \nattacks. The application uses an Application Load Balancer (ALB). A solutions architect must \nreduce the risk of DDoS attacks against the application. \nWhat should the solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Add an Amazon Inspector agent to the ALB.", correct: false },
                { id: 1, text: "Configure Amazon Macie to prevent attacks.", correct: false },
                { id: 2, text: "Enable AWS Shield Advanced to prevent attacks.", correct: true },
                { id: 3, text: "Configure Amazon GuardDuty to monitor the ALB.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Shield is a managed Distributed Denial of Service (DDoS) protection service that helps protect web applications running on AWS from DDoS attacks. AWS Shield Advanced is an additional layer of protection that provides enhanced DDoS protection capabilities, including proactive monitoring and automatic inline mitigations, to help protect against even the largest and most sophisticated DDoS attacks. By enabling AWS Shield Advanced, the solutions architect can help protect the application from DDoS attacks and reduce the risk of disruption to the application.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "A company runs a production application on a fleet of Amazon EC2 instances. The application \nreads the data from an Amazon SQS queue and processes the messages in parallel. The \nmessage volume is unpredictable and often has intermittent traffic. This application should \ncontinually process messages without any downtime. \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Spot Instances exclusively to handle the maximum capacity required.", correct: false },
                { id: 1, text: "Use Reserved Instances exclusively to handle the maximum capacity required.", correct: false },
                { id: 2, text: "Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional", correct: false },
                { id: 3, text: "Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nWe recommend that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 \norigin to store a static website. The company’s security policy requires that all website traffic be \ninspected by AWS WAF. \nHow should the solutions architect comply with these requirements?",
            options: [
                { id: 0, text: "Configure an S3 bucket policy lo accept requests coming from the AWS WAF Amazon Resource", correct: false },
                { id: 1, text: "Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting", correct: false },
                { id: 2, text: "Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3", correct: false },
                { id: 3, text: "Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUse an OAI to lockdown CloudFront to S3 origin & enable WAF on CF distribution. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content- restricting-access-to-s3.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web- awswaf.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "A company wants to manage Amazon Machine Images (AMIs). The company currently copies \nAMIs to the same AWS Region where the AMIs were created. The company needs to design an \napplication that captures AWS API calls and sends alerts whenever the Amazon EC2 \nCreatelmage API operation is called within the company's account. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a", correct: false },
                { id: 1, text: "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 2, text: "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call.", correct: true },
                { id: 3, text: "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nTo create an EventBridge rule to send a notification when an AMI is created and in the available state. https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-events.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "An online retail company has more than 50 million active customers and receives more than \n25,000 orders each day. The company collects purchase data for customers and stores this data \nin Amazon S3. Additional customer data is stored in Amazon RDS. The company wants to make \nall the data available to various teams so that the teams can perform analytics. The solution must \nprovide the ability to manage fine-grained permissions for the data and must minimize operational \noverhead. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the purchase data to write directly to Amazon RDS.", correct: false },
                { id: 1, text: "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3.", correct: false },
                { id: 2, text: "Create a data lake by using AWS Lake Formation.", correct: true },
                { id: 3, text: "Create an Amazon Redshift cluster.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nManage fine-grained access control using AWS Lake Formation. https://aws.amazon.com/blogs/big-data/manage-fine-grained-access-control-using-aws-lake- formation/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company owns an asynchronous API that is used to ingest user requests and, based on the \nrequest type, dispatch requests to the appropriate microservice for processing. The company is \nusing Amazon API Gateway to deploy the API front end, and an AWS Lambda function that \ninvokes Amazon DynamoDB to store user requests before dispatching them to the processing \nmicroservices. The company provisioned as much DynamoDB throughput as its budget allows, \nbut the company is still experiencing availability issues and is losing user requests. \nWhat should a solutions architect do to address this issue without impacting existing users?",
            options: [
                { id: 0, text: "Add throttling on the API Gateway with server-side throttling limits.", correct: false },
                { id: 1, text: "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.", correct: false },
                { id: 2, text: "Create a secondary index in DynamoDB for the table with the user requests.", correct: false },
                { id: 3, text: "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nbecause all other options put some more charges to DynamoDB. But the company supplied as much as they can for DynamoDB. And it is async request and we need to have retry mechanism not to lose the customer data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 28,
            text: "A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The \ncompany must ensure that no API calls and no data are routed through public internet routes. \nOnly the EC2 instance can have access to upload data to the S3 bucket. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is", correct: true },
                { id: 1, text: "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance", correct: false },
                { id: 2, text: "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3", correct: false },
                { id: 3, text: "Use the AWS provided, publicly available ip-ranges.json tile to obtain the private IP address of the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A gaming company hosts a browser-based application on AWS. The users of the application \nconsume a large number of videos and images that are stored in Amazon S3. This content is the \nsame for all users. \nThe application has increased in popularity, and millions of users worldwide accessing these \nmedia files. The company wants to provide the files to the users while reducing the load on the \norigin. \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Deploy an AWS Global Accelerator accelerator in front of the web servers.", correct: false },
                { id: 1, text: "Deploy an Amazon CloudFront web distribution in front of the S3 bucket.", correct: true },
                { id: 2, text: "Deploy an Amazon ElastiCache for Redis instance in front of the web servers.", correct: false },
                { id: 3, text: "Deploy an Amazon ElastiCache for Memcached instance in front of the web servers.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCloud front is best for content delivery. Global Accelerator is best for non-HTTP (TCP/UDP) cases and supports HTTP cases as well but with static IP (elastic IP) or anycast IP address only.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 30,
            text: "A company has two applications: a sender application that sends messages with payloads to be \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n118 \nprocessed and a processing application intended to receive the messages with payloads. The \ncompany wants to implement an AWS service to handle messages between the two applications. \nThe sender application can send about 1.000 messages each hour. The messages may take up \nto 2 days to be processed. If the messages fail to process, they must be retained so that they do \nnot impact the processing of any remaining messages. \nWhich solution meets these requirements and is the MOST operationally efficient?",
            options: [
                { id: 0, text: "Set up an Amazon EC2 instance running a Redis database.", correct: false },
                { id: 1, text: "Use an Amazon Kinesis data stream to receive the messages from the sender application.", correct: false },
                { id: 2, text: "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon", correct: true },
                { id: 3, text: "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead- letter-queues.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company has an AWS account used for software engineering. The AWS account has access to \nthe company's on-premises data center through a pair of AWS Direct Connect connections. All \nnon-VPC traffic routes to the virtual private gateway. \nA development team recently created an AWS Lambda function through the console. \nThe development team needs to allow the function to access a database that runs in a private \nsubnet in the company's data center. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the Lambda function to run in the VPC with the appropriate security group.", correct: false },
                { id: 1, text: "Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda", correct: false },
                { id: 2, text: "Update the route tables in the VPC to allow the Lambda function to access the on-premises data", correct: true },
                { id: 3, text: "Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nTo connect to another AWS service, you can use VPC endpoints for private communications between your VPC and supported AWS services. An alternative approach is to use a NAT gateway to route outbound traffic to another AWS service. To give your function access to the internet, route outbound traffic to a NAT gateway in a public subnet. The NAT gateway has a public IP address and can connect to the internet through the VPC's internet gateway. https://docs.aws.amazon.com/lambda/latest/dg/foundation-networking.html#foundation-nw- connecting Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is \nprocessed sequentially, but the order of results does not matter. The application uses a \nmonolithic architecture. The only way that the company can scale the application to meet \nincreased demand is to increase the size of the instances. \nThe company's developers have decided to rewrite the application to use a microservices \narchitecture on Amazon Elastic Container Service (Amazon ECS). \nWhat should a solutions architect recommend for communication between the microservices?",
            options: [
                { id: 0, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue.", correct: true },
                { id: 1, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic.", correct: false },
                { id: 2, text: "Create an AWS Lambda function to pass messages.", correct: false },
                { id: 3, text: "Create an Amazon DynamoDB table.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nQueue has Limited throughput (300 msg/s without batching, 3000 msg/s with batching whereby up-to 10 msg per batch operation; Msg duplicates not allowed in the queue (exactly-once delivery); Msg order is preserved (FIFO); Queue name must end with .fifo\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A hospital wants to create digital copies for its large collection of historical written records. The \nhospital will continue to add hundreds of new documents each day. The hospital's data team will \nscan the documents and will upload the documents to the AWS Cloud. A solutions architect must \nimplement a solution to analyze the documents, extract the medical information, and store the \ndocuments so that an application can run SQL queries on the data. The solution must maximize \nscalability and operational efficiency. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Write the document information to an Amazon EC2 instance that runs a MySQL database.", correct: false },
                { id: 1, text: "Write the document information to an Amazon S3 bucket.", correct: true },
                { id: 2, text: "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that", correct: false },
                { id: 3, text: "Create an AWS Lambda function that runs when new documents are uploaded.", correct: false },
                { id: 4, text: "Create an AWS Lambda function that runs when new documents are uploaded.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A solutions architect is optimizing a website for an upcoming musical event. Videos of the \nperformances will be streamed in real time and then will be available on demand. The event is \nexpected to attract a global online audience. \nWhich service will improve the performance of both the real-lime and on-demand streaming?",
            options: [
                { id: 0, text: "Amazon CloudFront", correct: true },
                { id: 1, text: "AWS Global Accelerator", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon S3 Transfer Acceleration", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin. One way you can set up video workflows in the cloud is by using CloudFront together with AWS Media Services. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming- video.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A company wants to migrate its MySQL database from on premises to AWS. The company \nrecently experienced a database outage that significantly impacted the business. To ensure this \ndoes not happen again, the company wants a reliable database solution on AWS that minimizes \ndata loss and stores every transaction on at least two nodes. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Create an Amazon RDS DB instance with synchronous replication to three nodes in three", correct: false },
                { id: 1, text: "Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously", correct: true },
                { id: 2, text: "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS", correct: false },
                { id: 3, text: "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nQ: What does Amazon RDS manage on my behalf? Amazon RDS manages the work involved in setting up a relational database: from provisioning the infrastructure capacity you request to installing the database software. Once your database is up and running, Amazon RDS automates common administrative tasks such as performing backups and patching the software that powers your database. With optional Multi-AZ deployments, Amazon RDS also manages synchronous data replication across Availability Zones with automatic failover. https://aws.amazon.com/rds/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 36,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n121 \nAn ecommerce company hosts its analytics application in the AWS Cloud. The application \ngenerates about 300 MB of data each month. The data is stored in JSON format. The company is \nevaluating a disaster recovery solution to back up the data. The data must be accessible in \nmilliseconds if it is needed, and the data must be kept for 30 days. \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Amazon OpenSearch Service (Amazon Elasticsearch Service)", correct: false },
                { id: 1, text: "Amazon S3 Glacier", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: true },
                { id: 3, text: "Amazon RDS for PostgreSQL", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has a Windows-based application that must be migrated to AWS. The application \nrequires the use of a shared Windows file system attached to multiple Amazon EC2 Windows \ninstances that are deployed across multiple Availability Zones. \nWhat should a solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Configure AWS Storage Gateway in volume gateway mode.", correct: false },
                { id: 1, text: "Configure Amazon FSx for Windows File Server.", correct: true },
                { id: 2, text: "Configure a file system by using Amazon Elastic File System (Amazon EFS).", correct: false },
                { id: 3, text: "Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nMicrosoft Windows-based application = shared Windows file system = Amazon FSX for Windows https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of \nthe information submitted by users is sensitive. The application uses HTTPS but needs another \nlayer of security. The sensitive information should be protected throughout the entire application \nstack, and access to the information should be restricted to certain applications. \nWhich action should the solutions architect take?",
            options: [
                { id: 0, text: "Configure a CloudFront signed URL.", correct: false },
                { id: 1, text: "Configure a CloudFront signed cookie.", correct: false },
                { id: 2, text: "Configure a CloudFront field-level encryption profile.", correct: true },
                { id: 3, text: "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level- encryption.html \"With Amazon CloudFront, you can enforce secure end-to-end connections to origin servers by Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted \nwhen it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated \nevery year. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Move the data to the S3 bucket.", correct: false },
                { id: 1, text: "Create an AWS Key Management Service (AWS KMS) customer managed key.", correct: true },
                { id: 2, text: "Create an AWS Key Management Service (AWS KMS) customer managed key.", correct: false },
                { id: 3, text: "Encrypt the data with customer key material before moving the data to the S3 bucket.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html Customer managed keys Automatic key rotation is disabled by default on customer managed keys but authorized users can enable and disable it. When you enable (or re-enable) automatic key rotation, AWS KMS automatically rotates the KMS key one year (approximately 365 days) after the enable date and every year thereafter.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "An application runs on Amazon EC2 instances in private subnets. The application needs to \naccess an Amazon DynamoDB table. What is the MOST secure way to access the table while \nensuring that the traffic does not leave the AWS network?",
            options: [
                { id: 0, text: "Use a VPC endpoint for DynamoDB.", correct: true },
                { id: 1, text: "Use a NAT gateway in a public subnet.", correct: false },
                { id: 2, text: "Use a NAT instance in a private subnet.", correct: false },
                { id: 3, text: "Use the internet gateway attached to the VPC.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints- dynamodb.html A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet. Your EC2 instances do not require public IP addresses, and you don't need an internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company provides an API to its users that automates inquiries for tax computations based on \nitem prices. The company experiences a larger number of inquiries during the holiday season \nonly that cause slower response times. A solutions architect needs to design a solution that is \nscalable and elastic. \nWhat should the solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Provide an API hosted on an Amazon EC2 instance.", correct: false },
                { id: 1, text: "Design a REST API using Amazon API Gateway that accepts the item names.", correct: true },
                { id: 2, text: "Create an Application Load Balancer that has two Amazon EC2 instances behind it.", correct: false },
                { id: 3, text: "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nLambda server-less is scalable and elastic than EC2 api gateway solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 42,
            text: "A company wants to use high performance computing (HPC) infrastructure on AWS for financial \nrisk modeling. The company's HPC workloads run on Linux. Each HPC workflow runs on \nhundreds of AmazonEC2 Spot Instances, is short-lived, and generates thousands of output files \nthat are ultimately stored in persistent storage for analytics and long-term future use. \nThe company seeks a cloud storage solution that permits the copying of on premises data to \nlong-term persistent storage to make data available for processing by all EC2 instances. The \nsolution should also be a high performance file system that is integrated with persistent storage to \nread and write datasets and output files. \nWhich combination of AWS services meets these requirements?",
            options: [
                { id: 0, text: "Amazon FSx for Lustre integrated with Amazon S3", correct: true },
                { id: 1, text: "Amazon FSx for Windows File Server integrated with Amazon S3", correct: false },
                { id: 2, text: "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 3, text: "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/fsx/lustre/ Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A company is running a publicly accessible serverless application that uses Amazon API \nGateway and AWS Lambda.  \nThe application's traffic recently spiked due to fraudulent requests from botnets. \nWhich steps should a solutions architect take to block requests from unauthorized users? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n124",
            options: [
                { id: 0, text: "Create a usage plan with an API key that is shared with genuine users only.", correct: true },
                { id: 1, text: "Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.", correct: false },
                { id: 2, text: "Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.", correct: false },
                { id: 3, text: "Convert the existing public API to a private API.", correct: false },
                { id: 4, text: "Create an IAM role for each user attempting to access the API.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage- plans.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A solutions architect is designing the architecture of a new application being deployed to the AWS \nCloud. The application will run on Amazon EC2 On-Demand Instances and will automatically \nscale across multiple Availability Zones. The EC2 instances will scale up and down frequently \nthroughout the day. An Application Load Balancer (ALB) will handle the load distribution. The \narchitecture needs to support distributed session data management. The company is willing to \nmake changes to code if needed. \nWhat should the solutions architect do to ensure that the architecture supports distributed session \ndata management?",
            options: [
                { id: 0, text: "Use Amazon ElastiCache to manage and store session data.", correct: true },
                { id: 1, text: "Use session affinity (sticky sessions) of the ALB to manage session data.", correct: false },
                { id: 2, text: "Use Session Manager from AWS Systems Manager to manage the session.", correct: false },
                { id: 3, text: "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/vi/caching/session-management/ In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached. ElastiCache offerings for In-Memory key/value stores include ElastiCache for Redis, which can support replication, and ElastiCache for Memcached which does not support replication.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 45,
            text: "A company hosts a marketing website in an on-premises data center. The website consists of \nstatic documents and runs on a single server. An administrator updates the website content \ninfrequently and uses an SFTP client to upload new documents. \nThe company decides to host its website on AWS and to use Amazon CloudFront. The \ncompany's solutions architect creates a CloudFront distribution. The solutions architect must \ndesign the most cost-effective and resilient architecture for website hosting to serve as the \nCloudFront origin. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a virtual server by using Amazon Lightsail.", correct: false },
                { id: 1, text: "Create an AWS Auto Scaling group for Amazon EC2 instances.", correct: false },
                { id: 2, text: "Create a private Amazon S3 bucket.", correct: true },
                { id: 3, text: "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS transfer is a cost and doesn't mention using CloudFront. https://aws.amazon.com/aws-transfer-family/pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "A company is designing a cloud communications platform that is driven by APIs. The application \nis hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses \nAmazon API Gateway to provide external users with access to the application through APIs. The \ncompany wants to protect the platform against web exploits like SQL injection and also wants to \ndetect and mitigate large, sophisticated DDoS attacks. \nWhich combination of solutions provides the MOST protection? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS WAF to protect the NLB.", correct: false },
                { id: 1, text: "Use AWS Shield Advanced with the NLB.", correct: true },
                { id: 2, text: "Use AWS WAF to protect Amazon API Gateway.", correct: false },
                { id: 3, text: "Use Amazon GuardDuty with AWS Shield Standard.", correct: false },
                { id: 4, text: "Use AWS Shield Standard with Amazon API Gateway.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Shield Advanced - DDos attacks AWS WAF to protect Amazon API Gateway, because WAF sits before the API Gateway and then comes NLB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A company has a web application that is based on Java and PHP. The company plans to move \nthe application from on premises to AWS. The company needs the ability to test new site features \nfrequently. The company also needs a highly available and managed solution that requires \nminimum operational overhead. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket.", correct: false },
                { id: 1, text: "Deploy the web application to an AWS Elastic Beanstalk environment.", correct: true },
                { id: 2, text: "Deploy the web application lo Amazon EC2 instances that are configured with Java and PHP.", correct: false },
                { id: 3, text: "Containerize the web application.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nElastic Beanstalk is a fully managed service that makes it easy to deploy and run applications in the AWS; To enable frequent testing of new site features, you can use URL swapping to switch between multiple Elastic Beanstalk environments. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A company has a Microsoft .NET application that runs on an on-premises Windows Server. The \napplication stores data by using an Oracle Database Standard Edition server. The company is \nplanning a migration to AWS and wants to minimize development changes while moving the \napplication. The AWS application environment should be highly available. \nWhich combination of actions should the company take to meet these requirements? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Refactor the application as serverless with AWS Lambda functions running .NET Core.", correct: false },
                { id: 1, text: "Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.", correct: true },
                { id: 2, text: "Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to", correct: false },
                { id: 4, text: "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCompany wants to minimize development modifications throughout the process. Option A & C i.e. refactoring or re-platforming options get eliminated. As for option D, oracle to dynamo DB is not possible.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 49,
            text: "A rapidly growing ecommerce company is running its workloads in a single AWS Region. A \nsolutions architect must create a disaster recovery (DR) strategy that includes a different AWS \nRegion. The company wants its database to be up to date in the DR Region with the least \npossible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity \nand must be able to scale up if necessary. \nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)?",
            options: [
                { id: 0, text: "Use an Amazon Aurora global database with a pilot light deployment", correct: false },
                { id: 1, text: "Use an Amazon Aurora global database with a warm standby deployment", correct: true },
                { id: 2, text: "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment", correct: false },
                { id: 3, text: "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIn case of disaster, both pilot light and warm standby offer the capability to limit data loss (RPO). Both offer sufficient RTO performance that enables you to limit downtime. Between these two strategies, you have a choice of optimizing for RTO or for cost. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A company's order system sends requests from clients to Amazon EC2 instances. The EC2 \ninstances process the orders and then store the orders in a database on Amazon RDS. Users \nreport that they must reprocess orders when the system fails. The company wants a resilient \nsolution that can process orders automatically if a system outage occurs. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Move the EC2 instances into an Auto Scaling group.", correct: false },
                { id: 1, text: "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB).", correct: false },
                { id: 2, text: "Move the EC2 instances into an Auto Scaling group.", correct: true },
                { id: 3, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "A company runs an application on a large fleet of Amazon EC2 instances. The application reads \nand write entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously \ngrows, but the application needs only data from the last 30 days. The company needs a solution \nthat minimizes cost and development effort. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use an AWS CloudFormation template to deploy the complete solution.", correct: false },
                { id: 1, text: "Use an EC2 instance that runs a monitoring application from AWS Marketplace.", correct: false },
                { id: 2, text: "Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is", correct: false },
                { id: 3, text: "Extend the application to add an attribute that has a value of the current timestamp plus 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 52,
            text: "A company runs a containerized application on a Kubernetes cluster in an on-premises data \ncenter. The company is using a MongoDB database for data storage. \nThe company wants to migrate some of these environments to AWS, but no code changes or \ndeployment method changes are possible at this time. The company needs a solution that \nminimizes operational overhead. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for", correct: false },
                { id: 1, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and", correct: false },
                { id: 2, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for", correct: false },
                { id: 3, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB- compatible databases in the cloud. With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB. https://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A company selves a dynamic website from a fleet of Amazon EC2 instances behind an \nApplication Load Balancer (ALB). The website needs to support multiple languages to serve \ncustomers around the world. The website's architecture is running in the us-west-1 Region and is \nexhibiting high request latency tor users that are located in other parts of the world. The website \nneeds to serve requests quickly and efficiently regardless of a user's location. However the \ncompany does not want to recreate the existing architecture across multiple Regions.  \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Replace the existing architecture with a website that is served from an Amazon S3 bucket.", correct: false },
                { id: 1, text: "Configure an Amazon CloudFront distribution with the ALB as the origin.", correct: true },
                { id: 2, text: "Create an Amazon API Gateway API that is integrated with the ALB.", correct: false },
                { id: 3, text: "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nConfiguring caching based on the language of the viewer. If you want CloudFront to cache different versions of your objects based on the language specified in the request, configure CloudFront to forward the Accept-Language header to your origin. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A telemarketing company is designing its customer call center functionality on AWS. The \ncompany needs a solution to provides multiples ipsafcar recognition and generates transcript \nfiles. The company wants to query the transcript files to analyze the business patterns. The \ntranscript files must be stored for 7 years for auditing policies. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Rekognition for multiple speaker recognition.", correct: false },
                { id: 1, text: "Use Amazon Transcribe for multiple speaker recognition.", correct: true },
                { id: 2, text: "Use Amazon Translate for multiple speaker recognition.", correct: false },
                { id: 3, text: "Use Amazon Rekognition for multiple speaker recognition.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon Transcribe now supports speaker labeling for streaming transcription. Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy for you to convert speech-to-text. In live audio transcription, each stream of audio may contain multiple speakers. Now you can conveniently turn on the ability to label speakers, thus helping to identify who is saying what in the output transcript. https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-transcribe-supports-speaker- labeling-streaming-transcription/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store \nall the data for 5 years and must delete all the data after 5 years. The company also must \nindefinitely keep audit logs of actions that are performed within the database. Currently, the \ncompany has automated backups configured for Aurora. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n130",
            options: [
                { id: 0, text: "Take a manual snapshot of the DB cluster.", correct: false },
                { id: 1, text: "Create a lifecycle policy for the automated backups.", correct: false },
                { id: 2, text: "Configure automated backup retention for 5 years.", correct: false },
                { id: 3, text: "Configure an Amazon CloudWatch Logs export for the DB cluster.", correct: true },
                { id: 4, text: "Use AWS Backup to take the backups and to keep the backups for 5 years.", correct: false },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Backup adds Amazon Aurora database cluster snapshots as its latest protected resource. Starting today, you can use AWS Backup to manage Amazon Aurora database cluster snapshots. AWS Backup can centrally configure backup policies, monitor backup activity, copy a snapshot within and across AWS regions, except for China regions, where snapshots can only be copied from one China region to another.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A company has a small Python application that processes JSON documents and outputs the \nresults to an on-premises SQL database. The application runs thousands of times each day. The \ncompany wants to move the application to the AWS Cloud. The company needs a highly \navailable solution that maximizes scalability and minimizes operational overhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Place the JSON documents in an Amazon S3 bucket.", correct: false },
                { id: 1, text: "Place the JSON documents in an Amazon S3 bucket.", correct: true },
                { id: 2, text: "Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume.", correct: false },
                { id: 3, text: "Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy placing the JSON documents in an S3 bucket, the documents will be stored in a highly durable and scalable object storage service. The use of AWS Lambda allows the company to run their Python code to process the documents as they arrive in the S3 bucket without having to worry about the underlying infrastructure. This also allows for horizontal scalability, as AWS Lambda will automatically scale the number of instances of the function based on the incoming rate of requests. The results can be stored in an Amazon Aurora DB cluster, which is a fully-managed, high-performance database service that is compatible with MySQL and PostgreSQL. This will provide the necessary durability and scalability for the results of the processing. https://aws.amazon.com/rds/aurora/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A company's infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance \nin a single AWS Region. The company wants to back up its data in a separate Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.", correct: true },
                { id: 1, text: "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to", correct: false },
                { id: 2, text: "Create Amazon Machine Images (AMIs) of the EC2 instances.", correct: false },
                { id: 3, text: "Create Amazon Elastic Block Store (Amazon EBS) snapshots.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCross-Region backup Using AWS Backup, you can copy backups to multiple different AWS Regions on demand or automatically as part of a scheduled backup plan. Cross-Region backup is particularly valuable if you have business continuity or compliance requirements to store backups a minimum distance away from your production data. https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A company is building a new dynamic ordering website. The company wants to minimize server \nmaintenance and patching. The website must be highly available and must scale read and write \ncapacity as quickly as possible to meet changes in user demand. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Host static content in Amazon S3.", correct: true },
                { id: 1, text: "Host static content in Amazon S3.", correct: false },
                { id: 2, text: "Host all the website content on Amazon EC2 instances.", correct: false },
                { id: 3, text: "Host all the website content on Amazon EC2 instances.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nOn-demand mode is a good option if any of the following are true: Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A company uses Amazon S3 as its data lake. The company has a new partner that must use \nSFTP to upload data files. A solutions architect needs to implement a highly available SFTP \nsolution that minimizes operational overhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible", correct: true },
                { id: 1, text: "Use Amazon S3 File Gateway as an SFTP server.", correct: false },
                { id: 2, text: "Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload", correct: false },
                { id: 3, text: "Launch Amazon EC2 instances in a private subnet in a VPC.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Transfer for SFTP, a fully-managed, highly-available SFTP service. You simply create a server, set up user accounts, and associate the server with one or more Amazon Simple Storage Service (Amazon S3) buckets.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 60,
            text: "A company needs to store contract documents. A contract lasts for 5 years. During the 5-year \nperiod, the company must ensure that the documents cannot be overwritten or deleted. The \ncompany needs to encrypt the documents at rest and rotate the encryption keys automatically \nevery year. \n \nWhich combination of steps should a solutions architect take to meet these requirements with the \nLEAST operational overhead? (Choose two.)",
            options: [
                { id: 0, text: "Store the documents in Amazon S3.", correct: false },
                { id: 1, text: "Store the documents in Amazon S3.", correct: true },
                { id: 2, text: "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).", correct: false },
                { id: 3, text: "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed", correct: false },
                { id: 4, text: "Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "You have been given a scope to deploy some AWS infrastructure for a large organisation. The \nrequirements are that you will have a lot of EC2 instances but may need to add more when the \naverage utilization of your Amazon EC2 fleet is high and conversely remove them when CPU \nutilization is low. Which AWS services would be best to use to accomplish this?",
            options: [
                { id: 0, text: "Auto Scaling, Amazon CloudWatch and AWS Elastic Beanstalk", correct: false },
                { id: 1, text: "Auto Scaling, Amazon CloudWatch and Elastic Load Balancing.", correct: true },
                { id: 2, text: "Amazon CloudFront, Amazon CloudWatch and Elastic Load Balancing.", correct: false },
                { id: 3, text: "AWS Elastic Beanstalk , Amazon CloudWatch and Elastic Load Balancing.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAuto Scaling enables you to follow the demand curve for your applications closely, reducing the need to manually provision Amazon EC2 capacity in advance. For example, you can set a condition to add new Amazon EC2 instances in increments to the Auto Scaling group when the average utilization of your Amazon EC2 fleet is high; and similarly, you can set a condition to remove instances in the same increments when CPU utilization is low. If you have predictable load changes, you can set a schedule through Auto Scaling to plan your scaling activities. You can use Amazon CloudWatch to send alarms to trigger scaling activities and Elastic Load Balancing to help distribute traffic to your instances within Auto Scaling groups. Auto Scaling enables you to run your Amazon EC2 fleet at optimal utilization. Reference: http://aws.amazon.com/autoscaling/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "of the below mentioned options is not available when an instance is launched by Auto \nScaling with EC2 Classic?",
            options: [
                { id: 0, text: "Public IP", correct: false },
                { id: 1, text: "Elastic IP", correct: true },
                { id: 2, text: "Private DNS", correct: false },
                { id: 3, text: "Private IP", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAuto Scaling supports both EC2 classic and EC2-VPC. When an instance is launched as a part of EC2 classic, it will have the public IP and DNS as well as the private IP and DNS. Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/GettingStartedTutorial.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 63,
            text: "A company's application is running on Amazon EC2 instances in a single Region in the event of a \ndisaster a solutions architect needs to ensure that the resources can also be deployed to a \nsecond Region. \nWhich combination of actions should the solutions architect take to accomplish this? (Select \nTWO) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n134",
            options: [
                { id: 0, text: "Detach a volume on an EC2 instance and copy it to Amazon S3", correct: false },
                { id: 1, text: "Launch a new EC2 instance from an Amazon Machine image (AMI) in a new Region", correct: true },
                { id: 2, text: "Launch a new EC2 instance in a new Region and copy a volume from Amazon S3 to the new", correct: false },
                { id: 3, text: "Copy an Amazon Machine Image (AMI) of an EC2 instance and specify a different Region for the", correct: false },
                { id: 4, text: "Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy default, when you create an AMI from an instance, snapshots are taken of each EBS volume attached to the instance. AMIs can launch with multiple EBS volumes attached, allowing you to replicate both an instance's configuration and the state of all the EBS volumes that are attached to that instance. https://aws.amazon.com/premiumsupport/knowledge-center/create-ami-ebs-backed/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A recently acquired company is required to buikl its own infrastructure on AWS and migrate \nmultiple applications to the cloud within a month. \nEach application has approximately 50 TB of data to be transferred. \nAfter the migration is complete this company and its parent company will both require secure \nnetwork connectivity with consistent throughput from their data centers to the applications. \nA solutions architect must ensure one-time data migration and ongoing network connectivity. \nWhich solution will meet these requirements''",
            options: [
                { id: 0, text: "AWS Direct Connect for both the initial transfer and ongoing connectivity", correct: false },
                { id: 1, text: "AWS Site-to-Site VPN for both the initial transfer and ongoing connectivity", correct: false },
                { id: 2, text: "AWS Snowball for the initial transfer and AWS Direct Connect for ongoing connectivity", correct: true },
                { id: 3, text: "AWS Snowball for the initial transfer and AWS Site-to-Site VPN for ongoing connectivity", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n\"Each application has approximately 50 TB of data to be transferred\" = AWS Snowball; \"secure network connectivity with consistent throughput from their data centers to the applications\" What are the benefits of using AWS Direct Connect and private network connections? In many circumstances, private network connections can reduce costs, increase bandwidth, and provide a more consistent network experience than Internet-based connections. \"more consistent network experience\", hence AWS Direct Connect. Direct Connect is better than VPN; reduced cost+increased bandwith+(remain connection or consistent network) = direct connect\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test13: [
        {
            id: 0,
            text: "Much of your company's data does not need to be accessed often, and can take several hours for \nretrieval time, so it's stored on Amazon Glacier. However someone within your organization has \nexpressed concerns that his data is more sensitive than the other data, and is wondering whether \nthe high level of encryption that he knows is on S3 is also used on the much cheaper Glacier \nservice. Which of the following statements would be most applicable in regards to this concern?",
            options: [
                { id: 0, text: "There is no encryption on Amazon Glacier, that's why it is cheaper.", correct: false },
                { id: 1, text: "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than", correct: false },
                { id: 2, text: "Amazon Glacier automatically encrypts the data using AES-256, the same as Amazon S3.", correct: true },
                { id: 3, text: "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nLike Amazon S3, the Amazon Glacier service provides low-cost, secure, and durable storage. But where S3 is designed for rapid retrieval, Glacier is meant to be used as an archival service for data that is not accessed often, and for which retrieval times of several hours are suitable. Amazon Glacier automatically encrypts the data using AES-256 and stores it durably in an immutable form. Amazon Glacier is designed to provide average annual durability of 99.999999999% for an archive. It stores each archive in multiple facilities and multiple devices. Unlike traditional systems which can require laborious data verification and manual repair, Glacier performs regular, systematic data integrity checks, and is built to be automatically self-healing. Reference: http://d0.awsstatic.com/whitepapers/Security/AWS%20Security%20Whitepaper.pdf\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "Your EBS volumes do not seem to be performing as expected and your team leader has \nrequested you look into improving their performance. Which of the following is not a true \nstatement relating to the performance of your EBS volumes?",
            options: [
                { id: 0, text: "Frequent snapshots provide a higher level of data durability and they will not degrade the", correct: true },
                { id: 1, text: "General Purpose (SSD) and Provisioned IOPS (SSD) volumes have a throughput limit of 128", correct: false },
                { id: 2, text: "There is a relationship between the maximum performance of your EBS volumes, the amount of", correct: false },
                { id: 3, text: "There is a 5 to 50 percent reduction in IOPS when you first access each block of data on a newly", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSeveral factors can affect the performance of Amazon EBS volumes, such as instance configuration, I/O characteristics, workload demand, and storage configuration. Frequent snapshots provide a higher level of data durability, but they may slightly degrade the performance of your application while the snapshot is in progress. This trade off becomes critical when you have data that changes rapidly. Whenever possible, plan for snapshots to occur during off-peak times in order to minimize workload impact. Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "You are building infrastructure for a data warehousing solution and an extra request has come \nthrough that there will be a lot of business reporting queries running all the time and you are not \nsure if your current DB instance will be able to handle it.  \nWhat would be the best solution for this?",
            options: [
                { id: 0, text: "DB Parameter Groups", correct: false },
                { id: 1, text: "Read Replicas", correct: true },
                { id: 2, text: "Multi-AZ DB Instance deployment", correct: false },
                { id: 3, text: "Database Snapshots", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nRead Replicas make it easy to take advantage of MySQL's built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. There are a variety of scenarios where deploying one or more Read Replicas for a given source DB Instance may make sense. Common reasons for deploying a Read Replica include: Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas. Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read Replica(s). For this use case, keep in mind that the data on the Read Replica may be \"stale\" since the source DB Instance is unavailable. Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance. Reference: https://aws.amazon.com/rds/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "You've created your first load balancer and have registered your EC2 instances with the load \nbalancer. Elastic Load Balancing routinely performs health checks on all the registered EC2 \ninstances and automatically distributes all incoming requests to the DNS name of your load \nbalancer across your registered, healthy EC2 instances. By default, the load balancer uses the \n___ protocol for checking the health of your instances.",
            options: [
                { id: 0, text: "HTTPS", correct: false },
                { id: 1, text: "HTTP", correct: true },
                { id: 2, text: "ICMP", correct: false },
                { id: 3, text: "IPv6", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIn Elastic Load Balancing a health configuration uses information such as protocol, ping port, ping path (URL), response timeout period, and health check interval to determine the health state of the instances registered with the load balancer. Currently, HTTP on port 80 is the default health check. Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/TerminologyandKeyCo ncepts.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 4,
            text: "A major finance organisation has engaged your company to set up a large data mining \napplication. Using AWS you decide the best service for this is Amazon Elastic MapReduce(EMR) \nwhich you know uses Hadoop. Which of the following statements best describes Hadoop?",
            options: [
                { id: 0, text: "Hadoop is 3rd Party software which can be installed using AMI", correct: false },
                { id: 1, text: "Hadoop is an open source python web framework", correct: false },
                { id: 2, text: "Hadoop is an open source Java software framework", correct: true },
                { id: 3, text: "Hadoop is an open source javascript framework", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon EMR uses Apache Hadoop as its distributed data processing engine. Hadoop is an open source, Java software framework that supports data-intensive distributed applications running on Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A company wants to host a scalable web application on AWS. \nThe application will be accessed by users from different geographic regions of the world. \nApplication users will be able to download and upload unique data up to gigabytes in size. \nThe development team wants a cost-effective solution to minimize upload and download latency \nand maximize performance. \nWhat should a solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Use Amazon S3 with Transfer Acceleration to host the application.", correct: true },
                { id: 1, text: "Use Amazon S3 with CacheControl headers to host the application.", correct: false },
                { id: 2, text: "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.", correct: false },
                { id: 3, text: "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe maximum size of a single file that can be delivered through Amazon CloudFront is 20 GB. This limit applies to all Amazon CloudFront distributions.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 6,
            text: "A company captures clickstream data from multiple websites and analyzes it using batch \nprocessing. \nThe data is loaded nightly into Amazon Redshift and is consumed by business analysts. \nThe company wants to move towards near-real-time data processing for timely insights. \nThe solution should process the streaming data with minimal effort and operational overhead. \nWhich combination of AWS services are MOST cost-effective for this solution? (Choose two.)",
            options: [
                { id: 0, text: "Amazon EC2", correct: false },
                { id: 1, text: "AWS Lambda", correct: false },
                { id: 2, text: "Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon Kinesis Data Firehose", correct: true },
                { id: 4, text: "Amazon Kinesis Data Analytics", correct: false },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://d0.awsstatic.com/whitepapers/whitepaper-streaming-data-solutions-on-aws-with- amazonkinesis.pdf (9) https://aws.amazon.com/kinesis/#Evolve_from_batch_to_real-time_analytics\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company is migrating a three-tier application to AWS. \nThe application requires a MySQL database. In the past, the application users reported poor \napplication performance when creating new entries. \nThese performance issues were caused by users generating different real-time reports from the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n138 \napplication duringworking hours. \nWhich solution will improve the performance of the application when it is moved to AWS?",
            options: [
                { id: 0, text: "Import the data into an Amazon DynamoDB table with provisioned capacity.", correct: false },
                { id: 1, text: "Create the database on a compute optimized Amazon EC2 instance.", correct: false },
                { id: 2, text: "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas.", correct: true },
                { id: 3, text: "Create an Amazon Aurora MySQL Multi-AZ DB cluster.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe MySQL-compatible edition of Aurora delivers up to 5X the throughput of standard MySQL running on the same hardware, and enables existing MySQL applications and tools to run without requiring modification. https://aws.amazon.com/rds/aurora/mysql-features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A start-up company has a web application based in the us-east-1 Region with multiple Amazon \nEC2 instances running behind an Application Load Balancer across multiple Availability Zones. \nAs the company's user base grows in the us-west-1 Region, it needs a solution with low latency \nand high availability. \nWhat should a solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Provision EC2 instances in us-west-1.", correct: false },
                { id: 1, text: "Provision EC2 instances and an Application Load Balancer in us-west-1.", correct: false },
                { id: 2, text: "Provision EC2 instances and configure an Application Load Balancer in us-west-1.", correct: true },
                { id: 3, text: "Provision EC2 instances and configure an Application Load Balancer in us-west-1.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n\"ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions [...] AWS Global Accelerator complements ELB by extending these capabilities beyond a single AWS Region, allowing you to provision a global interface for your applications in any number of Regions. If you have workloads that cater to a global client base, we recommend that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.\" https://aws.amazon.com/global-accelerator/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "A company must generate sales reports at the beginning of every month. \nThe reporting process launches 20 Amazon EC2 instances on the first of the month. \nThe process runs for 7 days and cannot be interrupted. The company wants to minimize costs. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n139 \nWhich pricing model should the company choose?",
            options: [
                { id: 0, text: "Reserved Instances", correct: false },
                { id: 1, text: "Spot Block Instances", correct: false },
                { id: 2, text: "On-Demand Instances", correct: false },
                { id: 3, text: "Scheduled Reserved Instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nScheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them. Scheduled Instances are a good choice for workloads that do not run continuously, but do run on a regular schedule. For example, you can use Scheduled Instances for an application that runs during business hours or for batch processing that runs at the end of the week. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company's web application uses an Amazon RDS PostgreSQL DB instance to store its \napplication data. \nDuring the financial closing period at the start of every month. Accountants run large queries that \nimpact the database's performance due to high usage. \nThe company wants to minimize the impact that the reporting activity has on the web application. \nWhat should a solutions architect do to reduce the impact on the database with the LEAST \namount of effort?",
            options: [
                { id: 0, text: "Create a read replica and direct reporting traffic to the replica.", correct: true },
                { id: 1, text: "Create a Multi-AZ database and direct reporting traffic to the standby.", correct: false },
                { id: 2, text: "Create a cross-Region read replica and direct reporting traffic to the replica.", correct: false },
                { id: 3, text: "Create an Amazon Redshift database and direct reporting traffic to the Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon RDS uses the MariaDB, MySQL, Oracle, PostgreSQL, and Microsoft SQL Server DB engines' built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A company has application running on Amazon EC2 instances in a VPC. \nOne of the applications needs to call an Amazon S3 API to store and read objects. \nThe company's security policies restrict any internet-bound traffic from the applications. \nWhich action will fulfill these requirements and maintain security?",
            options: [
                { id: 0, text: "Configure an S3 interface endpoint.", correct: false },
                { id: 1, text: "Configure an S3 gateway endpoint.", correct: true },
                { id: 2, text: "Create an S3 bucket in a private subnet.", correct: false },
                { id: 3, text: "Create an S3 bucket in the same Region as the EC2 instance.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nGateway Endpoint for S3 and DynamoDB https://medium.com/tensult/aws-vpc-endpoints-introduction-ef2bf85c4422 https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A website runs a web application that receives a burst of traffic each day at noon. The users \nupload new pictures and content daily, but have been complaining of timeouts. The architecture \nuses Amazon EC2 Auto Scaling groups, and the custom application consistently takes 1 minute \nto initiate upon boot up before responding to user requests. \nHow should a solutions architect redesign the architecture to better respond to changing traffic?",
            options: [
                { id: 0, text: "Configure a Network Load Balancer with a slow start configuration.", correct: false },
                { id: 1, text: "Configure AWS ElastiCache for Redis to offload direct requests to the servers.", correct: false },
                { id: 2, text: "Configure an Auto Scaling step scaling policy with an instance warmup condition.", correct: true },
                { id: 3, text: "Configure Amazon CloudFront to use an Application Load Balancer as the origin.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nIf you are creating a step policy, you can specify the number of seconds that it takes for a newly launched instance to warm up. Until its specified warm-up time has expired, an instance is not counted toward the aggregated metrics of the Auto Scaling group. https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html#as-step- scaling-warmup\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company hosts its website on Amazon S3. The website serves petabytes of outbound traffic \nmonthly, which accounts for most of the company's AWS costs. \nWhat should a solutions architect do to reduce costs?",
            options: [
                { id: 0, text: "Configure Amazon CloudFront with the existing website as the origin.", correct: true },
                { id: 1, text: "Move the website to Amazon EC2 with Amazon EBS volumes for storage.", correct: false },
                { id: 2, text: "Use AWS Global Accelerator and specify the existing website as the endpoint.", correct: false },
                { id: 3, text: "Rearchitect the website to run on a combination of Amazon API Gateway and AWS Lambda.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA textbook case for CloudFront. The data transfer cost in CloudFront is lower than in S3. With heavy read operations of static content, it's more economical to add CloudFront in front of you S3 bucket.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 14,
            text: "A company currently stores symmetric encryption keys in a hardware security module (HSM). A \nsolution architect must design a solution to migrate key management to AWS. The solution \nshould allow for key rotation and support the use of customer provided keys. Where should the \nkey material be stored to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n141",
            options: [
                { id: 0, text: "Amazon S3", correct: false },
                { id: 1, text: "AWS Secrets Manager", correct: true },
                { id: 2, text: "AWS Systems Manager Parameter store", correct: false },
                { id: 3, text: "AWS Key Management Service (AWS KMS)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. https://aws.amazon.com/secrets-manager/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company needs to implement a relational database with a multi-Region disaster recovery \nRecovery Point Objective (RPO) of 1 second and an Recovery Time Objective (RTO) of 1 minute. \nWhich AWS solution can achieve this?",
            options: [
                { id: 0, text: "Amazon Aurora Global Database", correct: true },
                { id: 1, text: "Amazon DynamoDB global tables.", correct: false },
                { id: 2, text: "Amazon RDS for MySQL with Multi-AZ enabled.", correct: false },
                { id: 3, text: "Amazon RDS for MySQL with a cross-Region snapshot copy.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCross-Region Disaster Recovery If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A company is designing a new service that will run on Amazon EC2 instance behind an Elastic \nLoad Balancer. However, many of the web service clients can only reach IP addresses \nwhitelisted on their firewalls. \nWhat should a solution architect recommend to meet the clients' needs?",
            options: [
                { id: 0, text: "A Network Load Balancer with an associated Elastic IP address.", correct: false },
                { id: 1, text: "An Application Load Balancer with an a associated Elastic IP address", correct: false },
                { id: 2, text: "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address", correct: true },
                { id: 3, text: "An EC2 instance with a public IP address running as a proxy in front of the load balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nRoute 53 routes end users to Internet applications so the correct answer is C. Map one of the whitelisted IP addresses using an A record to the Elastic IP address.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A company's packaged application dynamically creates and returns single-use text files in \nresponse to user requests. The company is using Amazon CloudFront for distribution, but wants \nto future reduce data transfer costs. The company modify the application's source code. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n142 \nWhat should a solution architect do to reduce costs?",
            options: [
                { id: 0, text: "Use Lambda@Edge to compress the files as they are sent to users.", correct: true },
                { id: 1, text: "Enable Amazon S3 Transfer Acceleration to reduce the response times.", correct: false },
                { id: 2, text: "Enable caching on the CloudFront distribution to store generated files at the edge.", correct: false },
                { id: 3, text: "Use Amazon S3 multipart uploads to move the files to Amazon S3 before returning them to users.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nB seems more expensive; C does not seem right because they are single use files and will not be needed again from the cache; D multipart mainly for large files and will not reduce data and cost; A seems the best: change the application code to compress the files and reduce the amount of data transferred to save costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "An application running on an Amazon EC2 instance in VPC-A needs to access files in another \nEC2 instance in VPC-B. Both are in separate AWS accounts. \nThe network administrator needs to design a solution to enable secure access to EC2 instance in \nVPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth \nconcerns. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up a VPC peering connection between VPC-A and VPC-B.", correct: true },
                { id: 1, text: "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.", correct: false },
                { id: 2, text: "Attach a virtual private gateway to VPC-B and enable routing from VPC-A.", correct: false },
                { id: 3, text: "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The traffic remains in the private IP space. All inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck. https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company stores user data in AWS. The data is used continuously with peak usage during \nbusiness hours. Access patterns vary, with some data not being used for months at a time. A \nsolutions architect must choose a cost-effective solution that maintains the highest level of \ndurability while maintaining high availability. \n \nWhich storage solution meets these requirements?",
            options: [
                { id: 0, text: "Amazon S3", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering", correct: true },
                { id: 2, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 3, text: "Amazon S3 One Zone-Infequent Access (S3 One Zone-IA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIntelligent tearing moves data between storage classes based on its current degree of usage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 20,
            text: "A solutions architect is creating an application that will handle batch processing of large amounts \nof data. The input data will be held in Amazon S3 and the output data will be stored in a different \nS3 bucket. For processing, the application will transfer the data over the network between \nmultiple Amazon EC2 instances. \n \nWhat should the solutions architect do to reduce the overall data transfer costs?",
            options: [
                { id: 0, text: "Place all the EC2 instances in an Auto Scaling group.", correct: false },
                { id: 1, text: "Place all the EC2 instances in the same AWS Region.", correct: false },
                { id: 2, text: "Place all the EC2 instances in the same Availability Zone.", correct: true },
                { id: 3, text: "Place all the EC2 instances in private subnets in multiple Availability Zones.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe transfer is between EC2 instances and not just between S3 and EC2. Also, be aware of inter-Availability Zones data transfer charges between Amazon EC2 instances, even within the same region. If possible, the instances in a development or test environment that need to communicate with each other should be co-located within the same Availability Zone to avoid data transfer charges. (This doesn't apply to production workloads which will most likely need to span multiple Availability Zones for high availability.) https://aws.amazon.com/blogs/mt/using-aws-cost-explorer-to-analyze-data-transfer-costs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "A company has recently updated its internal security standards. \nThe company must now ensure all Amazon S3 buckets and Amazon Elastic Block Store (Amazon \nEBS) volumes are encrypted with keys created and periodically rotated by internal security \nspecialists. \nThe company is looking for a native, software-based AWS service to accomplish this goal. \nWhat should a solutions architect recommend as a solution?",
            options: [
                { id: 0, text: "Use AWS Secrets Manager with customer master keys (CMKs) to store master key material and", correct: true },
                { id: 1, text: "Use AWS Key Management Service (AWS KMS) with customer master keys (CMKs) to store", correct: false },
                { id: 2, text: "Use an AWS CloudHSM cluster with customer master keys (CMKs) to store master key material", correct: false },
                { id: 3, text: "Use AWS Systems Manager Parameter Store with customer master keys (CMKs) keys to store", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Secrets Manager provides full lifecycle management for secrets within your environment. In this post, Maitreya and I will show you how to use Secrets Manager to store, deliver, and rotate SSH keypairs used for communication within compute clusters. Rotation of these keypairs is a security best practice, and sometimes a regulatory requirement. Traditionally, these keypairs have been associated with a number of tough challenges. For example, synchronizing key Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "An application is running on Amazon EC2 instances Sensitive information required for the \napplication is stored in an Amazon S3 bucket. \nThe bucket needs to be protected from internet access while only allowing services within the \nVPC access to the bucket. \nWhich combination of actions should a solutions archived take to accomplish this? (Choose two.)",
            options: [
                { id: 0, text: "Create a VPC endpoint for Amazon S3.", correct: true },
                { id: 1, text: "Enable server access logging on the bucket", correct: false },
                { id: 2, text: "Apply a bucket policy to restrict access to the S3 endpoint.", correct: false },
                { id: 3, text: "Add an S3 ACL to the bucket that has sensitive information", correct: false },
                { id: 4, text: "Restrict users using the IAM policy to use the specific bucket", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nACL is a property at object level not at bucket level .Also by just adding ACL you can't let the services in VPC allow access to the bucket .\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "A company relies on an application that needs at least 4 Amazon EC2 instances during regular \ntraffic and must scale up to 12 EC2 instances during peak loads. \nThe application is critical to the business and must be highly available. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the EC2 instances in an Auto Scaling group.", correct: false },
                { id: 1, text: "Deploy the EC2 instances in an Auto Scaling group.", correct: false },
                { id: 2, text: "Deploy the EC2 instances in an Auto Scaling group.", correct: true },
                { id: 3, text: "Deploy the EC2 instances in an Auto Scaling group.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nIt requires HA and if one AZ is down then at least 4 instances will be active in another AZ which is key for this\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "A company recently deployed a two-tier application in two Availability Zones in the us-east-1 \nRegion. The databases are deployed in a private subnet while the web servers are deployed in a \npublic subnet. \nAn internet gateway is attached to the VPC. The application and database run on Amazon EC2 \ninstances. The database servers are unable to access patches on the internet. \nA solutions architect needs to design a solution that maintains database security with the least \noperational overhead. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n145 \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Deploy a NAT gateway inside the public subnet for each Availability Zone and associate it with an", correct: true },
                { id: 1, text: "Deploy a NAT gateway inside the private subnet for each Availability Zone and associate it with", correct: false },
                { id: 2, text: "Deploy two NAT instances inside the public subnet for each Availability Zone and associate them", correct: false },
                { id: 3, text: "Deploy two NAT instances inside the private subnet for each Availability Zone and associate them", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nVPC with public and private subnets (NAT) The configuration for this scenario includes a virtual private cloud (VPC) with a public subnet and a private subnet. We recommend this scenario if you want to run a public-facing web application, while maintaining back-end servers that aren't publicly accessible. A common example is a multi- tier website, with the web servers in a public subnet and the database servers in a private subnet. You can set up security and routing so that the web servers can communicate with the database servers. The instances in the public subnet can send outbound traffic directly to the Internet, whereas the instances in the private subnet can't. Instead, the instances in the private subnet can access the Internet by using a network address translation (NAT) gateway that resides in the public subnet. The database servers can connect to the Internet for software updates using the NAT gateway, but the Internet cannot establish connections to the database servers. https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "A company has an on-premises data center that is running out of storage capacity. The company \nwants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution \nmust allow for immediate retrieval of data at no additional cost. \nHow can these requirements be met?",
            options: [
                { id: 0, text: "Deploy Amazon S3 Glacier Vault and enable expedited retrieval.", correct: false },
                { id: 1, text: "Deploy AWS Storage Gateway using cached volumes.", correct: false },
                { id: 2, text: "Deploy AWS Storage Gateway using stored volumes to store data locally.", correct: true },
                { id: 3, text: "Deploy AWS Direct Connect to connect with the on-premises data center.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nVolume Gateway provides an iSCSI target, which enables you to create block storage volumes and mount them as iSCSI devices from your on-premises or EC2 application servers. The Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "A company recently implemented hybrid cloud connectivity using AWS Direct Connect and is \nmigrating data to Amazon S3. \nThe company is looking for a fully managed solution that will automate and accelerate the \nreplication of data between the on-premises storage systems and AWS storage services. \nWhich solution should a solutions architect recommend to keep the data private?",
            options: [
                { id: 0, text: "Deploy an AWS DataSync agent tor the on-premises environment.", correct: true },
                { id: 1, text: "Deploy an AWS DataSync agent for the on-premises environment.", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway volume gateway for the on-premises environment.", correct: false },
                { id: 3, text: "Deploy an AWS Storage Gateway file gateway for the on-premises environment.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can use AWS DataSync with your Direct Connect link to access public service endpoints or private VPC endpoints. When using VPC endpoints, data transferred between the DataSync agent and AWS services does not traverse the public internet or need public IP addresses, increasing the security of data as it is copied over the network.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A solutions architect is designing the storage architecture for a new web application used for \nstonng and viewing engineering drawings. All application components will be deployed on the \nAWS infrastructure. \nThe application design must support caching to minimize the amount of time that users wait for \nthe engineering drawings to load. The application must be able to store petabytes of data. \nWhich combination of storage and caching should the solutions architect use?",
            options: [
                { id: 0, text: "Amazon S3 with Amazon CloudFront", correct: true },
                { id: 1, text: "Amazon S3 Glacier with Amazon ElastiCache", correct: false },
                { id: 2, text: "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront", correct: false },
                { id: 3, text: "AWS Storage Gateway with Amazon ElastiCache", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCloudFront for caching and S3 as the origin. Glacier is used for archiving which is not the case for this scenario.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "An operations team has a standard that states IAM policies should not be applied directly to \nusers. Some new members have not been following this standard. \nThe operation manager needs a way to easily identify the users with attached policies. \nWhat should a solutions architect do to accomplish this? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n147",
            options: [
                { id: 0, text: "Monitor using AWS CloudTrail", correct: false },
                { id: 1, text: "Create an AWS Config rule to run daily", correct: true },
                { id: 2, text: "Publish IAM user changes lo Amazon SNS", correct: false },
                { id: 3, text: "Run AWS Lambda when a user is modified", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA new AWS Config rule is deployed in the account after you enable AWS Security Hub. The AWS Config rule reacts to resource configuration and compliance changes and send these change items to AWS CloudWatch. When AWS CloudWatch receives the compliance change, a CloudWatch event rule triggers the AWS Lambda function.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A company is building applications in containers. \nThe company wants to migrate its on-premises development and operations services from its on-\npremises data center to AWS. \nManagement states that production system must be cloud agnostic and use the same \nconfiguration and administrator tools across production systems. \nA solutions architect needs to design a managed solution that will align open-source software. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Launch the containers on Amazon EC2 with EC2 instance worker nodes.", correct: false },
                { id: 1, text: "Launch the containers on Amazon Elastic Kubernetes Service (Amazon EKS) and EKS workers", correct: true },
                { id: 2, text: "Launch the containers on Amazon Elastic Containers service (Amazon ECS) with AWS Fargate", correct: false },
                { id: 3, text: "Launch the containers on Amazon Elastic Container Service (Amazon EC) with Amazon EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWhen talking about containerized applications, the leading technologies which will always come up during the conversation are Kubernetes and Amazon ECS (Elastic Container Service). While Kubernetes is an open-sourced container orchestration platform that was originally developed by Google, Amazon ECS is AWS' proprietary, managed container orchestration service.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 30,
            text: "A solutions architect is performing a security review of a recently migrated workload. \nThe workload is a web application that consists of Amazon EC2 instances in an Auto Scaling \ngroup behind an Application Load Balancer. \nThe solutions architect must improve the security posture and minimize the impact of a DDoS \nattack on resources. \nWhich solution is MOST effective?",
            options: [
                { id: 0, text: "Configure an AWS WAF ACL with rate-based rules.", correct: true },
                { id: 1, text: "Create a custom AWS Lambda function that adds identified attacks into a common vulnerability", correct: false },
                { id: 2, text: "Enable VPC Flow Logs and store them in Amazon S3.", correct: false },
                { id: 3, text: "Enable Amazon GuardDuty and configure findings written to Amazon CloudWatch.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS WAF is a web application firewall that helps detect and mitigate web application layer DDoS attacks by inspecting traffic inline. Application layer DDoS attacks use well-formed but malicious requests to evade mitigation and consume application resources. You can define custom security rules (also called web ACLs) that contain a set of conditions, rules, and actions to block attacking traffic. After you define web ACLs, you can apply them to CloudFront distributions, and web ACLs are evaluated in the priority order you specified when you configured them. Real-time metrics and sampled web requests are provided for each web ACL. https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos- attacks-by-using-amazon-cloudfront-and-amazon-route-53/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company is creating a prototype of an ecommerce website on AWS. The website consists of an \nApplication Load Balancer, an Auto Scaling group of Amazon EC2 instances for web servers, and \nan Amazon RDS for MySQL DB instance that runs with the Single-AZ configuration. \nThe website is slow to respond during searches of the product catalog. The product catalog is a \ngroup of tables in the MySQL database that the company does not update frequently. A solutions \narchitect has determined that the CPU utilization on the DB instance is high when product catalog \nsearches occur. \nWhat should the solutions architect recommend to improve the performance of the website during \nsearches of the product catalog?",
            options: [
                { id: 0, text: "Migrate the product catalog to an Amazon Redshift database.", correct: false },
                { id: 1, text: "Implement an Amazon ElastiCache for Redis cluster to cache the product catalog.", correct: true },
                { id: 2, text: "Add an additional scaling policy to the Auto Scaling group to launch additional EC2 instances", correct: false },
                { id: 3, text: "Turn on the Multi-AZ configuration for the DB instance.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCommon ElastiCache Use Cases and How ElastiCache Can Help : Whether serving the latest news, a top-10 leaderboard, a product catalog, or selling tickets to an event, speed is the name of the game. The success of your website and business is greatly affected by the speed at which you deliver content. https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company's application is running on Amazon EC2 instances within an Auto Scaling group \nbehind an Elastic Load Balancer. Based on the application's history, the company anticipates a \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n149 \nspike in traffic during a holiday each year. A solutions architect must design a strategy to ensure \nthat the Auto Scaling group proactively increases capacity to minimize any performance impact \non application users. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization", correct: false },
                { id: 1, text: "Create a recurring scheduled action to scale up the Auto Scaling group before the expected", correct: true },
                { id: 2, text: "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during", correct: false },
                { id: 3, text: "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon EC2 Auto Scaling supports sending Amazon SNS notifications when the following events occur. https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 33,
            text: "A company is creating an application that runs on containers in a VPC. The application stores \nand accesses data in an Amazon S3 bucket. During the development phase, the application will \nstore and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs \nand wants to prevent traffic from traversing the internet whenever possible. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Enable S3 Intelligent-Tiering for the S3 bucket.", correct: false },
                { id: 1, text: "Enable S3 Transfer Acceleration for the S3 bucket.", correct: false },
                { id: 2, text: "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in", correct: true },
                { id: 3, text: "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A solutions architect is tasked with transferring 750 TB of data from an on-premises network-\nattached file system located at a branch office Amazon S3 Glacier. \nThe migration must not saturate the on-premises 1 Mbps internet connection. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an AWS site-to-site VPN tunnel to an Amazon S3 bucket and transfer the files directly.", correct: false },
                { id: 1, text: "Order 10 AWS Snowball Edge Storage Optimized devices, and select an S3 Glacier vault as the", correct: false },
                { id: 2, text: "Mount the network-attached file system to an S3 bucket, and copy the files directly.", correct: false },
                { id: 3, text: "Order 10 AWS Snowball Edge Storage Optimized devices, and select an Amazon S3 bucket as", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo upload existing data to Amazon S3 Glacier (S3 Glacier), you might consider using one of the AWS Snowball device types to import data into Amazon S3, and then move it to the S3 Glacier storage class for archival using lifecycle rules. When you transition Amazon S3 objects to the S3 Glacier storage class, Amazon S3 internally uses S3 Glacier for durable storage at lower cost. Although the objects are stored in S3 Glacier, they remain Amazon S3 objects that you manage in Amazon S3, and you cannot access them directly through S3 Glacier. https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A company's website handles millions of requests each day, and the number of requests \ncontinues to increase. A solutions architect needs to improve the response time of the web \napplication. The solutions architect determines that the application needs to decrease latency \nwhen retrieving product details from the \nAmazon DynamoDB table. \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "Set up a DynamoDB Accelerator (DAX) cluster.", correct: true },
                { id: 1, text: "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application.", correct: false },
                { id: 2, text: "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web", correct: false },
                { id: 3, text: "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon DynamoDB is designed for scale and performance. In most cases, the DynamoDB response times can be measured in single-digit milliseconds. However, there are certain use cases that require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "A media company collects and analyzes user activity data on premises. The company wants to \nmigrate this capability to AWS. The user activity data store will continue to grow and will be \npetabytes in size. The company needs to build a highly available data ingestion solution that \nfacilitates on-demand analytics of existing data and new data with SQL. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Send activity data to an Amazon Kinesis data stream.", correct: false },
                { id: 1, text: "Send activity data to an Amazon Kinesis Data Firehose delivery stream.", correct: true },
                { id: 2, text: "Place activity data in an Amazon S3 bucket.", correct: false },
                { id: 3, text: "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon Kinesis Data Firehose is a data transfer service for loading streaming data into Amazon S3, Splunk, ElasticSearch, and RedShift. https://www.whizlabs.com/blog/aws-kinesis-data-streams-vs-aws-kinesis-data-firehose/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A company is using a centralized AWS account to store log data in various Amazon S3 buckets. \nA solutions architect needs to ensure that the data is encrypted at rest before the data is \nuploaded to the S3 buckets. The data also must be encrypted in transit. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.", correct: true },
                { id: 1, text: "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.", correct: false },
                { id: 2, text: "Create bucket policies that require the use of server-side encryption with S3 managed encryption", correct: false },
                { id: 3, text: "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nData protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit using Secure Socket Layer/Transport Layer Security (SSL/TLS) or client-side encryption. https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low- latency connection to the \napplication servers. A new company policy states all application-generated files must be copied to \nAWS. There is already a VPN connection to AWS. \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
            options: [
                { id: 0, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 1, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 2, text: "AWS Snowball", correct: false },
                { id: 3, text: "AWS Storage Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe files will be on the storgare gateway with low latency and copied to AWS as a second copy. FSx in AWS will not provide low latency for the on prem apps over a vpn to the FSx file system.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A company has an ordering application that stores customer information in Amazon RDS for \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n152 \nMySQL. During regular business hours, employees run one-time queries for reporting purposes. \nTimeouts are occurring during order processing because the reporting queries are taking a long \ntime to run. The company needs to eliminate the timeouts without preventing employees from \nperforming queries. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a read replica. Move reporting queries to the read replica.", correct: true },
                { id: 1, text: "Create a read replica. Distribute the ordering application to the primary DB instance and the read", correct: false },
                { id: 2, text: "Migrate the ordering application to Amazon DynamoDB with on-demand capacity.", correct: false },
                { id: 3, text: "Schedule the reporting queries for non-peak hours.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nReporting is OK to run on replicated data with some delay in replication.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A company runs a web application that is backed by Amazon RDS. A new database administrator \ncaused data loss by accidentally editing information in a database table. To help recover from this \ntype of incident, the company wants the ability to restore the database to its state from 5 minutes \nbefore any change within the last 30 days. \nWhich feature should the solutions architect include in the design to meet this requirement?",
            options: [
                { id: 0, text: "Read replicas", correct: false },
                { id: 1, text: "Manual snapshots", correct: false },
                { id: 2, text: "Automated backups", correct: true },
                { id: 3, text: "Multi-AZ deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nRDS creates automated backups of your volume snapshot in which you can recover to a specific point-in-time recovery. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBac kups.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A company uses Amazon RDS for PostgreSQL databases for its data tier. The company must \nimplement password rotation for the databases. \n \nWhich solution meets this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the password in AWS Secrets Manager.", correct: true },
                { id: 1, text: "Store the password in AWS Systems Manager Parameter Store.", correct: false },
                { id: 2, text: "Store the password in AWS Systems Manager Parameter Store.", correct: false },
                { id: 3, text: "Store the password in AWS Key Management Service (AWS KMS).", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company's facility has badge readers at every entrance throughout the building. When badges \nare scanned, the readers send a message over HTTPS to indicate who attempted to access that \nparticular entrance. \n \nA solutions architect must design a system to process these messages from the sensors. The \nsolution must be highly available, and the results must be made available for the company's \nsecurity team to analyze. \n \nWhich system architecture should the solutions architect recommend?",
            options: [
                { id: 0, text: "Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages.", correct: false },
                { id: 1, text: "Create an HTTPS endpoint in Amazon API Gateway.", correct: true },
                { id: 2, text: "Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function.", correct: false },
                { id: 3, text: "Create a gateway VPC endpoint for Amazon S3.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDeploy Amazon API Gateway as an HTTPS endpoint and AWS Lambda to process and save the messages to an Amazon DynamoDB table. This option provides a highly available and scalable solution that can easily handle large amounts of data. It also integrates with other AWS services, making it easier to analyze and visualize the data for the security team.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 43,
            text: "An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have \noutbound internet access, but the EC2 instance needs the ability to download monthly security \nupdates from an outside vendor. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an internet gateway, and attach it to the VPC.", correct: false },
                { id: 1, text: "Create a NAT gateway, and place it in a public subnet.", correct: true },
                { id: 2, text: "Create a NAT instance, and place it in the same subnet where the EC2 instance is located.", correct: false },
                { id: 3, text: "Create an internet gateway, and attach it to the VPC.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThis approach will allow the EC2 instance to access the internet and download the monthly security updates while still being located in a private subnet. By creating a NAT gateway and placing it in a public subnet, it will allow the instances in the private subnet to access the internet through the NAT gateway. And then, configure the private subnet route table to use the NAT gateway as the default route. This will ensure that all outbound traffic is directed through the NAT gateway, allowing the EC2 instance to access the internet while still maintaining the security of the private subnet.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A company has been running a web application with an Oracle relational database in an on-\npremises data center for the past 15 years. The company must migrate the database to AWS. \nThe company needs to reduce operational overhead without having to modify the application's \ncode. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use AWS Database Migration Service (AWS DMS) to migrate the database servers to Amazon", correct: true },
                { id: 1, text: "Use Amazon EC2 instances to migrate and operate the database servers.", correct: false },
                { id: 2, text: "Use AWS Database Migration Service (AWS DMS) to migrate the database servers to Amazon", correct: false },
                { id: 3, text: "Use an AWS Snowball Edge Storage Optimized device to migrate the data from Oracle to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDMS can be used for database migration(supports cross database migration too). RDS supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle. https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises- oracle-database-to-amazon-rds-for-oracle.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 45,
            text: "A company is running an application on Amazon EC2 instances. Traffic to the workload increases \nsubstantially during business hours and decreases afterward. The CPU utilization of an EC2 \ninstance is a strong indicator of end-user demand on the application. The company has \nconfigured an Auto Scaling group to have a minimum group size of 2 EC2 instances and a \nmaximum group size of 10 EC2 instances. \n \nThe company is concerned that the current scaling policy that is associated with the Auto Scaling \ngroup might not be correct. The company must avoid over-provisioning EC2 instances and \nincurring unnecessary costs. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure Amazon EC2 Auto Scaling to use a scheduled scaling plan and launch an additional 8", correct: false },
                { id: 1, text: "Configure AWS Auto Scaling to use a scaling plan that enables predictive scaling.", correct: true },
                { id: 2, text: "Configure a step scaling policy to add 4 EC2 instances at 50% CPU utilization and add another 4", correct: false },
                { id: 3, text: "Configure AWS Auto Scaling to have a desired capacity of 5 EC2 instances, and disable any", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nPredictive Scaling, now natively supported as an EC2 Auto Scaling policy, uses machine learning to schedule the right number of EC2 instances in anticipation of approaching traffic changes. Predictive Scaling predicts future traffic, including regularly-occurring spikes, and provisions the right number of EC2 instances in advance. Predictive Scaling's machine learning algorithms detect changes in daily and weekly patterns, automatically adjusting their forecasts. This removes the need for manual adjustment of Auto Scaling parameters as cyclicality changes over time, making Auto Scaling simpler to configure. Auto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "A company wants to use a custom distributed application that calculates various profit and loss \nscenarios. To achieve this goal, the company needs to provide a network connection between its \nAmazon EC2 instances. The connection must minimize latency and must maximize throughput \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Provision the application to use EC2 Dedicated Hosts of the same instance type.", correct: false },
                { id: 1, text: "Configure a placement group for EC2 instances that have the same instance type.", correct: true },
                { id: 2, text: "Use multiple AWS elastic network interfaces and link aggregation.", correct: false },
                { id: 3, text: "Configure AWS PrivateLink for the EC2 instances.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement- groups-cluster\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company needs to run a critical application on AWS. The company needs to use Amazon EC2 \nfor the application’s database. The database must be highly available and must fail over \nautomatically if a disruptive event occurs. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install", correct: true },
                { id: 1, text: "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an", correct: false },
                { id: 2, text: "Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2", correct: false },
                { id: 3, text: "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nConfigure the EC2 instances as a cluster) Cluster consist of one or more DB instances and a cluster volume that manages the data for those DB instances. Cluster Volume is a VIRTUAL DATABASE storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A company hosts its application on AWS. The company uses Amazon Cognito to manage users. \nWhen users log in to the application, the application fetches required data from Amazon \nDynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an \nAWS managed solution that will control access to the REST API to reduce development efforts. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user", correct: false },
                { id: 1, text: "For each user, create and assign an API key that must be sent with each request. Validate the", correct: false },
                { id: 2, text: "Send the user’s email address in the header with every request. Invoke an AWS Lambda function", correct: false },
                { id: 3, text: "Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUse the Amazon Cognito console, CLI/SDK, or API to create a user pool—or use one that's owned by another AWS account. https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with- cognito.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company is developing a marketing communications service that targets mobile app users. The \ncompany needs to send confirmation messages with Short Message Service (SMS) to its users. \nThe users must be able to reply to the SMS messages. The company must store the responses \nfor a year for analysis. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to", correct: false },
                { id: 1, text: "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS", correct: false },
                { id: 3, text: "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/pinpoint/product-details/sms/ Two-Way Messaging: Receive SMS messages from your customers and reply back to them in a chat-like interactive experience. With Amazon Pinpoint, you can create automatic responses when customers send you messages that contain certain keywords. You can even use Amazon Lex to create conversational bots. A majority of mobile phone users read incoming SMS messages almost immediately after receiving them. If you need to be able to provide your customers with urgent or important information, SMS messaging may be the right solution for you. You can use Amazon Pinpoint to create targeted groups of customers, and then send them campaign-based messages. You can also use Amazon Pinpoint to send direct messages, such as appointment confirmations, order updates, and one-time passwords.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "The customers of a finance company request appointments with financial advisors by sending \ntext messages. A web application that runs on Amazon EC2 instances accepts the appointment \nrequests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) \nqueue through the web application. Another application that runs on EC2 instances then sends \nmeeting invitations and meeting confirmation email messages to the customers. After successful \nscheduling, this application stores the meeting information in an Amazon DynamoDB database. \nAs the company expands, customers report that their meeting invitations are taking longer to \narrive. \nWhat should a solutions architect recommend to resolve this issue?",
            options: [
                { id: 0, text: "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.", correct: false },
                { id: 1, text: "Add an Amazon API Gateway API in front of the web application that accepts the appointment", correct: false },
                { id: 2, text: "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the", correct: false },
                { id: 3, text: "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo resolve the issue of longer delivery times for meeting invitations, the solutions architect can recommend adding an Auto Scaling group for the application that sends meeting invitations and configuring the Auto Scaling group to scale based on the depth of the SQS queue. This will allow the application to scale up as the number of appointment requests increases, improving the performance and delivery times of the meeting invitations.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 51,
            text: "A company offers a food delivery service that is growing rapidly. Because of the growth, the \ncompany’s order processing system is experiencing scaling problems during peak traffic hours. \nThe current architecture includes the following: \n \n- A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect \norders from the application \n- Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders \n \nThe order collection process occurs quickly, but the order fulfillment process can take longer. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n158 \nData must not be lost because of a scaling event. \nA solutions architect must ensure that the order collection process and the order fulfillment \nprocess can both scale properly during peak traffic hours. The solution must optimize utilization of \nthe company’s AWS resources. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling", correct: false },
                { id: 1, text: "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling", correct: false },
                { id: 2, text: "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection", correct: false },
                { id: 3, text: "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe number of instances in your Auto Scaling group can be driven by how long it takes to process a message and the acceptable amount of latency (queue delay). The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A company hosts multiple production applications. One of the applications consists of resources \nfrom Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon \nSNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All \ncompany resources are tagged with a tag name of “application” and a value that corresponds to \neach application. A solutions architect must provide the quickest solution for identifying all of the \ntagged components. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to generate a list of resources with the application tag.", correct: false },
                { id: 1, text: "Use the AWS CLI to query each service across all Regions to report the tagged components.", correct: false },
                { id: 2, text: "Run a query in Amazon CloudWatch Logs Insights to report on the components with the", correct: false },
                { id: 3, text: "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html Tags are words or phrases that act as metadata that you can use to identify and organize your AWS resources. A resource can have up to 50 user-applied tags. It can also have read-only system tags. Each tag consists of a key and one optional value.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "A company needs to export its database once a day to Amazon S3 for other teams to access. \nThe exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n159 \nvariable and changes rapidly. The data must be immediately available and must remain \naccessible for up to 3 months. The company needs the most cost-effective solution that will not \nincrease retrieval time. \nWhich S3 storage class should the company use to meet these requirements?",
            options: [
                { id: 0, text: "S3 Intelligent-Tiering", correct: true },
                { id: 1, text: "S3 Glacier Instant Retrieval", correct: false },
                { id: 2, text: "S3 Standard", correct: false },
                { id: 3, text: "S3 Standard-Infrequent Access (S3 Standard-IA)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nS3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier and after 90 days of no access to the Archive Instant Access tier.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A company is developing a new mobile app. The company must implement proper traffic filtering \nto protect its Application Load Balancer (ALB) against common application-level attacks, such as \ncross-site scripting or SQL injection. The company has minimal infrastructure and operational \nstaff. The company needs to reduce its share of the responsibility in managing, updating, and \nsecuring servers for its AWS environment. \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure AWS WAF rules and associate them with the ALB.", correct: true },
                { id: 1, text: "Deploy the application using Amazon S3 with public hosting enabled.", correct: false },
                { id: 2, text: "Deploy AWS Shield Advanced and add the ALB as a protected resource.", correct: false },
                { id: 3, text: "Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall,", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA solutions architect should recommend option A, which is to configure AWS WAF rules and associate them with the ALB. This will allow the company to apply traffic filtering at the application layer, which is necessary for protecting the ALB against common application-level attacks such as cross-site scripting or SQL injection. AWS WAF is a managed service that makes it easy to protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. The company can easily manage and update the rules to ensure the security of its application.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company’s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. \nThe company must convert these files to Apache Parquet format and must store the files in a \ntransformed data bucket. \nWhich solution will meet these requirements with the LEAST development effort?",
            options: [
                { id: 0, text: "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to", correct: false },
                { id: 1, text: "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and", correct: true },
                { id: 2, text: "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the", correct: false },
                { id: 3, text: "Create an AWS Lambda function to transform the data and output the data to the transformed", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types- for-converting-data-to-apache-parquet.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 56,
            text: "A company has a serverless website with millions of objects in an Amazon S3 bucket. The \ncompany uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company \ndid not set encryption on the S3 bucket before the objects were loaded. A solutions architect \nneeds to enable encryption for all existing objects and for all objects that are added to the S3 \nbucket in the future. \nWhich solution will meet these requirements with the LEAST amount of effort?",
            options: [
                { id: 0, text: "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download", correct: false },
                { id: 1, text: "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create", correct: true },
                { id: 2, text: "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the", correct: false },
                { id: 3, text: "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket’s objects. Sort", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nStep 1: S3 inventory to get object list Step 2 (If needed): Use S3 Select to filter Step 3: S3 object operations to encrypt the unencrypted objects. On the going object use default encryption. https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A company has a web server running on an Amazon EC2 instance in a public subnet with an \nElastic IP address. The default security group is assigned to the EC2 instance. The default \nnetwork ACL has been modified to block all traffic. A solutions architect needs to make the web \nserver accessible from everywhere on port 443. \nWhich combination of steps will accomplish this task? (Choose two.)",
            options: [
                { id: 0, text: "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.", correct: true },
                { id: 1, text: "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.", correct: false },
                { id: 2, text: "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.", correct: false },
                { id: 3, text: "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to", correct: false },
                { id: 4, text: "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo enable the connection to a service running on an instance, the associated network ACL must allow both: - Inbound traffic on the port that the service is listening on - Outbound traffic to ephemeral ports https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A solutions architect is designing a new API using Amazon API Gateway that will receive \nrequests from users. The volume of requests is highly variable; several hours can pass without \nreceiving a single request. The data processing will take place asynchronously, but should be \ncompleted within a few seconds after a request is made. \nWhich compute service should the solutions architect have the API invoke to deliver the \nrequirements at the lowest cost?",
            options: [
                { id: 0, text: "An AWS Glue job", correct: false },
                { id: 1, text: "An AWS Lambda function", correct: true },
                { id: 2, text: "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)", correct: false },
                { id: 3, text: "A containerized service hosted in Amazon ECS with Amazon EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAPI Gateway + Lambda is the perfect solution for modern applications with serverless architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 59,
            text: "A company runs an application on a group of Amazon Linux EC2 instances. For compliance \nreasons, the company must retain all application log files for 7 years. The log files will be \nanalyzed by a reporting tool that must be able to access all the files concurrently. \nWhich storage solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon EC2 instance store", correct: false },
                { id: 3, text: "Amazon S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon S3 - Requests to Amazon S3 can be authenticated or anonymous. Authenticated access requires credentials that AWS can use to authenticate your requests. When making REST API calls directly from your code, you create a signature using valid credentials and include the signature in your request. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. Reference: Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company has hired an external vendor to perform work in the company’s AWS account. The \nvendor uses an automated tool that is hosted in an AWS account that the vendor owns. The \nvendor does not have IAM access to the company’s AWS account. \nHow should a solutions architect grant this access to the vendor?",
            options: [
                { id: 0, text: "Create an IAM role in the company’s account to delegate access to the vendor’s IAM role. Attach", correct: true },
                { id: 1, text: "Create an IAM user in the company’s account with a password that meets the password", correct: false },
                { id: 2, text: "Create an IAM group in the company’s account. Add the tool’s IAM user from the vendor account", correct: false },
                { id: 3, text: "Create a new identity provider by choosing “AWS account” as the provider type in the IAM", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic \nKubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an \nAmazon DynamoDB table. A solutions architect must ensure that the application can interact with \nthe DynamoDB table without exposing traffic to the internet. \nWhich combination of steps should the solutions architect take to accomplish this goal? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Attach an IAM role that has sufficient privileges to the EKS pod.", correct: true },
                { id: 1, text: "Attach an IAM user that has sufficient privileges to the EKS pod.", correct: false },
                { id: 2, text: "Allow outbound connectivity to the DynamoDB table through the private subnets’ network ACLs.", correct: false },
                { id: 3, text: "Create a VPC endpoint for DynamoDB.", correct: false },
                { id: 4, text: "Embed the access keys in the Java Spring Boot code.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints- dynamodb.html https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-adds-support-to-assign-iam- permissions-to-kubernetes-service-accounts/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company recently migrated its web application to AWS by rehosting the application on Amazon \nEC2 instances in a single AWS Region. The company wants to redesign its application \narchitecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances \nrandomly. \nWhich combination of steps should the company take to meet these requirements? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n163",
            options: [
                { id: 0, text: "Create an Amazon Route 53 failover routing policy.", correct: false },
                { id: 1, text: "Create an Amazon Route 53 weighted routing policy.", correct: false },
                { id: 2, text: "Create an Amazon Route 53 multivalue answer routing policy.", correct: true },
                { id: 3, text: "Launch three EC2 instances: two instances in one Availability Zone and one instance in another", correct: false },
                { id: 4, text: "Launch four EC2 instances: two instances in one Availability Zone and two instances in another", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-policies/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company collects data from thousands of remote devices by using a RESTful web services \napplication that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, \ntransforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote \ndevices will increase into the millions soon. The company needs a highly scalable solution that \nminimizes operational overhead. \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Use AWS Glue to process the raw data in Amazon S3.", correct: true },
                { id: 1, text: "Use Amazon Route 53 to route traffic to different EC2 instances.", correct: false },
                { id: 2, text: "Add more EC2 instances to accommodate the increasing amount of incoming data.", correct: false },
                { id: 3, text: "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to", correct: false },
                { id: 4, text: "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n\"RESTful web services\" => API Gateway. \"EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket\" => GLUE with (Extract - Transform - Load)\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 64,
            text: "A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing \nCloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. \nThe CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy \nis in place to delete current objects after 3 years. \nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of \nobjects has continued to rise. However, the number of new CloudTrail logs that are delivered to \nthe S3 bucket has remained consistent. \nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?",
            options: [
                { id: 0, text: "Configure the organization’s centralized CloudTrail trail to expire objects after 3 years.", correct: false },
                { id: 1, text: "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.", correct: true },
                { id: 2, text: "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older", correct: false },
                { id: 3, text: "Configure the parent account as the owner of all objects that are delivered to the S3 bucket.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTo delete objects that are older than 3 years in the most cost-effective manner, the company should configure the S3 Lifecycle policy to delete previous versions as well as current versions. This will ensure that all versions of the objects, including the previous versions, are deleted after 3 years.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test14: [
        {
            id: 0,
            text: "A company has an API that receives real-time data from a fleet of monitoring devices. The API \nstores this data in an Amazon RDS DB instance for later analysis. The amount of data that the \nmonitoring devices send to the API fluctuates. During periods of heavy traffic, the API often \nreturns timeout errors. \nAfter an inspection of the logs, the company determines that the database is not capable of \nprocessing the volume of write traffic that comes from the API. A solutions architect must \nminimize the number of connections to the database and must ensure that data is not lost during \nperiods of heavy traffic. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Increase the size of the DB instance to an instance type that has more available memory.", correct: false },
                { id: 1, text: "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all", correct: false },
                { id: 2, text: "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS)", correct: true },
                { id: 3, text: "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing Amazon SQS will help minimize the number of connections to the database, as the API will write data to a queue instead of directly to the database. Additionally, using an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database will help ensure that data is not lost during periods of heavy traffic, as the queue will serve as a buffer between the API and the database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 1,
            text: "A company manages its own Amazon EC2 instances that run MySQL databases. The company \nis manually managing replication and scaling as demand increases or decreases. The company \nneeds a new solution that simplifies the process of adding or removing compute capacity to or \nfrom its database tier as needed. The solution also must offer improved performance, scaling, \nand durability with minimal effort from operations. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.", correct: true },
                { id: 1, text: "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.", correct: false },
                { id: 2, text: "Combine the databases into one larger MySQL database. Run the larger database on larger EC2", correct: false },
                { id: 3, text: "Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/rds/aurora/serverless/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A company is concerned that two NAT instances in use will no longer be able to support the \ntraffic needed for the company’s application. A solutions architect wants to implement a solution \nthat is highly available, fault tolerant, and automatically scalable. \nWhat should the solutions architect recommend?",
            options: [
                { id: 0, text: "Remove the two NAT instances and replace them with two NAT gateways in the same Availability", correct: false },
                { id: 1, text: "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different", correct: false },
                { id: 2, text: "Remove the two NAT instances and replace them with two NAT gateways in different Availability", correct: true },
                { id: 3, text: "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway's Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-basics\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The \napplication requires access to a database in VPC B. Both VPCs are in the same AWS account. \nWhich solution will provide the required access MOST securely?",
            options: [
                { id: 0, text: "Create a DB instance security group that allows all traffic from the public IP address of the", correct: false },
                { id: 1, text: "Configure a VPC peering connection between VPC A and VPC B.", correct: true },
                { id: 2, text: "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.", correct: false },
                { id: 3, text: "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-connectivity-instance-subnet- vpc/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company runs demonstration environments for its customers on Amazon EC2 instances. Each \nenvironment is isolated in its own VPC. The company’s operations team needs to be notified \nwhen RDP or SSH access to an environment has been established.",
            options: [
                { id: 0, text: "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems", correct: false },
                { id: 1, text: "Configure the EC2 instances with an IAM instance profile that has an IAM role with the", correct: false },
                { id: 2, text: "Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an", correct: true },
                { id: 3, text: "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nEC2 Instance State-change Notifications are not the same as RDP or SSH established connection notifications. Use Amazon CloudWatch Logs to monitor SSH access to your Amazon EC2 Linux instances so that you can monitor rejected (or established) SSH connection requests and take action. https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts- to-amazon-ec2-linux-instances/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company is building a new web-based customer relationship management application. The \napplication will use several Amazon EC2 instances that are backed by Amazon Elastic Block \nStore (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will \nalso use an Amazon Aurora database. All data for the application must be encrypted at rest and \nin transit. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit.", correct: false },
                { id: 1, text: "Use the AWS root account to log in to the AWS Management Console. Upload the company’s", correct: false },
                { id: 2, text: "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora", correct: true },
                { id: 3, text: "Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A solutions architect has created a new AWS account and must secure AWS account root user \naccess. \nWhich combination of actions will accomplish this? (Choose two.)",
            options: [
                { id: 0, text: "Ensure the root user uses a strong password.", correct: true },
                { id: 1, text: "Enable multi-factor authentication to the root user.", correct: false },
                { id: 2, text: "Store root user access keys in an encrypted Amazon S3 bucket.", correct: false },
                { id: 3, text: "Add the root user to a group containing administrative permissions.", correct: false },
                { id: 4, text: "Apply the required permissions to the root user with an inline policy document.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n\"Enable MFA\" The AWS Account Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html \"Choose a strong password\" Changing the AWS Account Root User Password https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_change-root.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A company has a three-tier application for image sharing. The application uses an Amazon EC2 \ninstance for the front-end layer, another EC2 instance for the application layer, and a third EC2 \ninstance for a MySQL database. A solutions architect must design a scalable and highly available \nsolution that requires the least amount of change to the application. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer.", correct: false },
                { id: 1, text: "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the", correct: false },
                { id: 2, text: "Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group", correct: false },
                { id: 3, text: "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nfor \"Highly available\": Multi-AZ & for \"least amount of changes to the application\": Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A company wants to experiment with individual AWS accounts for its engineer team. The \ncompany wants to be notified as soon as the Amazon EC2 instance usage for a given month \nexceeds a specific threshold for each account. \nWhat should a solutions architect do to meet this requirement MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances.", correct: false },
                { id: 1, text: "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2", correct: false },
                { id: 2, text: "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the", correct: true },
                { id: 3, text: "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Budgets allows you to create budgets for your AWS accounts and set alerts when usage exceeds a certain threshold. By creating a budget for each account, specifying the period as monthly and the scope as EC2 instances, you can effectively track the EC2 usage for each account and be notified when a threshold is exceeded. This solution is the most cost-effective option as it does not require additional resources such as Amazon Athena or Amazon EventBridge. https://aws.amazon.com/getting-started/hands-on/control-your-costs-free-tier-budgets/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 9,
            text: "A solutions architect needs to design a new microservice for a company’s application. Clients \nmust be able to call an HTTPS endpoint to reach the microservice. The microservice also must \nuse AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect \nwill write the logic for this microservice by using a single AWS Lambda function that is written in \nGo 1.x. \nWhich solution will deploy the function in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.", correct: true },
                { id: 1, text: "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company previously migrated its data warehouse solution to AWS. The company also has an \nAWS Direct Connect connection. Corporate office users query the data warehouse using a \nvisualization tool. The average size of a query returned by the data warehouse is 50 MB and \neach webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the \ndata warehouse are not cached. \nWhich solution provides the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Host the visualization tool on premises and query the data warehouse directly over the internet.", correct: false },
                { id: 1, text: "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the", correct: false },
                { id: 2, text: "Host the visualization tool on premises and query the data warehouse directly over a Direct", correct: false },
                { id: 3, text: "Host the visualization tool in the same AWS Region as the data warehouse and access it over a", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/directconnect/pricing/ https://aws.amazon.com/blogs/aws/aws-data-transfer-prices-reduced/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 11,
            text: "An online learning company is migrating to the AWS Cloud. The company maintains its student \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n169 \nrecords in a PostgreSQL database. The company needs a solution in which its data is available \nand online across multiple AWS Regions at all times. \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.", correct: false },
                { id: 1, text: "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-", correct: false },
                { id: 2, text: "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a", correct: true },
                { id: 3, text: "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A medical research lab produces data that is related to a new study. The lab wants to make the \ndata available with minimum latency to clinics across the country for their on-premises, file-based \napplications. The data files are stored in an Amazon S3 bucket that has read-only permissions for \neach clinic. \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each", correct: true },
                { id: 1, text: "Migrate the files to each clinic’s on-premises applications by using AWS DataSync for processing.", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at", correct: false },
                { id: 3, text: "Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic’s on-premises", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Storage Gateway is a service that connects an on-premises software appliance with cloud- based storage to provide seamless and secure integration between an organization's on- premises IT environment and AWS's storage infrastructure. By deploying a file gateway as a virtual machine on each clinic's premises, the medical research lab can provide low-latency access to the data stored in the S3 bucket while maintaining read-only permissions for each clinic. This solution allows the clinics to access the data files directly from their on-premises file- based applications without the need for data transfer or migration.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company is using a content management system that runs on a single Amazon EC2 instance. \nThe EC2 instance contains both the web server and the database software. The company must \nmake its website platform highly available and must enable the website to scale to meet user \ndemand. \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Move the database to Amazon RDS, and enable automatic backups. Manually launch another", correct: false },
                { id: 1, text: "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability", correct: false },
                { id: 2, text: "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an", correct: true },
                { id: 3, text: "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThis approach will provide both high availability and scalability for the website platform. By moving the database to Amazon Aurora with a read replica in another availability zone, it will provide a failover option for the database. The use of an Application Load Balancer and an Auto Scaling group across two availability zones allows for automatic scaling of the website to meet increased user demand. Additionally, creating an AMI from the original EC2 instance allows for easy replication of the instance in case of failure.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company is launching an application on AWS. The application uses an Application Load \nBalancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The \ninstances are in an Auto Scaling group for each environment. The company requires a \ndevelopment environment and a production environment. The production environment will have \nperiods of high traffic. \nWhich solution will configure the development environment MOST cost-effectively?",
            options: [
                { id: 0, text: "Reconfigure the target group in the development environment to have only one EC2 instance as a", correct: false },
                { id: 1, text: "Change the ALB balancing algorithm to least outstanding requests.", correct: false },
                { id: 2, text: "Reduce the size of the EC2 instances in both environments.", correct: false },
                { id: 3, text: "Reduce the maximum number of EC2 instances in the development environment’s Auto Scaling", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThis option will configure the development environment in the most cost-effective way as it reduces the number of instances running in the development environment and therefore reduces the cost of running the application. The development environment typically requires less resources than the production environment, and it is unlikely that the development environment will have periods of high traffic that would require a large number of instances. By reducing the maximum number of instances in the development environment's Auto Scaling group, the company can save on costs while still maintaining a functional development environment.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 15,
            text: "A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The \nEC2 instances are in private subnets. A solutions architect implements an internet-facing \nApplication Load Balancer (ALB) and specifies the EC2 instances as the target group. However, \nthe internet traffic is not reaching the EC2 instances. \nHow should the solutions architect reconfigure the architecture to resolve this issue? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n171",
            options: [
                { id: 0, text: "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to", correct: false },
                { id: 1, text: "Move the EC2 instances to public subnets. Add a rule to the EC2 instances’ security groups to", correct: false },
                { id: 2, text: "Update the route tables for the EC2 instances’ subnets to send 0.0.0.0/0 traffic through the", correct: false },
                { id: 3, text: "Create public subnets in each Availability Zone. Associate the public subnets with the ALB.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, \nthe database support team is reporting slow reads against the DB instance and recommends \nadding a read replica. \nWhich combination of actions should a solutions architect take before implementing this change? \n(Choose two.)",
            options: [
                { id: 0, text: "Enable binlog replication on the RDS primary node.", correct: false },
                { id: 1, text: "Choose a failover priority for the source DB instance.", correct: false },
                { id: 2, text: "Allow long-running transactions to complete on the source DB instance.", correct: true },
                { id: 3, text: "Create a global table and specify the AWS Regions where the table will be available.", correct: false },
                { id: 4, text: "Enable automatic backups on the source instance by setting the backup retention period to a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAn active, long-running transaction can slow the process of creating the read replica. We recommend that you wait for long-running transactions to complete before creating a read replica. If you create multiple read replicas in parallel from the same source DB instance, Amazon RDS takes only one snapshot at the start of the first create action. When creating a read replica, there are a few things to consider. First, you must enable automatic backups on the source DB instance by setting the backup retention period to a value other than 0. This requirement also applies to a read replica that is the source DB instance for another read replica. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company runs analytics software on Amazon EC2 instances. The software accepts job \nrequests from users to process data that has been uploaded to Amazon S3. Users report that \nsome submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances \nhave a consistent CPU utilization at or near 100%. The company wants to improve system \nperformance and scale the system based on user load. \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a copy of the instance. Place all instances behind an Application Load Balancer.", correct: false },
                { id: 1, text: "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.", correct: false },
                { id: 2, text: "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more", correct: false },
                { id: 3, text: "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nBy routing incoming requests to Amazon SQS, the company can decouple the job requests from the processing instances. This allows them to scale the number of instances based on the size of the queue, providing more resources when needed. Additionally, using an Auto Scaling group based on the queue size will automatically scale the number of instances up or down depending on the workload. Updating the software to read from the queue will allow it to process the job requests in a more efficient manner, improving the performance of the system.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A company is implementing a shared storage solution for a media application that is hosted in the \nAWS Cloud. The company needs the ability to use SMB clients to access data. The solution must \nbe fully managed. \nWhich AWS solution meets these requirements?",
            options: [
                { id: 0, text: "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required", correct: false },
                { id: 1, text: "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the", correct: false },
                { id: 2, text: "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the", correct: false },
                { id: 3, text: "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon FSx has native support for Windows file system features and for the industry-standard Server Message Block (SMB) protocol to access file storage over a network. https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company’s security team requests that network traffic be captured in VPC Flow Logs. The logs \nwill be frequently accessed for 90 days and then accessed intermittently. \nWhat should a solutions architect do to meet these requirements when configuring the logs?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90", correct: false },
                { id: 1, text: "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90", correct: false },
                { id: 2, text: "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and", correct: false },
                { id: 3, text: "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsConcepts.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A solutions architect needs to design a system to store client case files. The files are core \ncompany assets and are important. The number of files will grow over time. \nThe files must be simultaneously accessible from multiple application servers that run on Amazon \nEC2 instances. The solution must have built-in redundancy. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Amazon Elastic File System (Amazon EFS)", correct: true },
                { id: 1, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 2, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 3, text: "AWS Backup", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon EFS provides a simple, scalable, fully managed file system that can be simultaneously accessed from multiple EC2 instances and provides built-in redundancy. It is optimized for multiple EC2 instances to access the same files, and it is designed to be highly available, durable, and secure. It can scale up to petabytes of data and can handle thousands of concurrent connections, and is a cost-effective solution for storing and accessing large amounts of data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are \nattached to an IAM group. \n \n \n \n \n \nA cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer \nbe able to perform?",
            options: [
                { id: 0, text: "Deleting IAM users", correct: false },
                { id: 1, text: "Deleting directories", correct: false },
                { id: 2, text: "Deleting Amazon EC2 instances", correct: true },
                { id: 3, text: "Deleting logs from Amazon CloudWatch Logs", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThere is an explicit DENY on deleting directories in the second policy. So the only thing that can be deleted is EC2 instances as per the permission in the first policy.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A company is reviewing a recent migration of a three-tier application to a VPC. The security team \ndiscovers that the principle of least privilege is not being applied to Amazon EC2 security group \ningress and egress rules between the application tiers. \nWhat should a solutions architect do to correct this issue?",
            options: [
                { id: 0, text: "Create security group rules using the instance ID as the source or destination.", correct: false },
                { id: 1, text: "Create security group rules using the security group ID as the source or destination.", correct: true },
                { id: 2, text: "Create security group rules using the VPC CIDR blocks as the source or destination.", correct: false },
                { id: 3, text: "Create security group rules using the subnet CIDR blocks as the source or destination.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThe ID of a security group (referred to here as the specified security group). For example, the current security group, a security group from the same VPC, or a security group for a peered VPC. This allows traffic based on the private IP addresses of the resources associated with the specified security group. This does not add rules from the specified security group to the current security group. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "A solutions architect is implementing a document review application using an Amazon S3 bucket \nfor storage. The solution must prevent accidental deletion of the documents and ensure that all \nversions of the documents are available. Users must be able to download, modify, and upload \ndocuments. \nWhich combination of actions should be taken to meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Enable a read-only bucket ACL.", correct: false },
                { id: 1, text: "Enable versioning on the bucket.", correct: true },
                { id: 2, text: "Attach an IAM policy to the bucket.", correct: false },
                { id: 3, text: "Enable MFA Delete on the bucket.", correct: false },
                { id: 4, text: "Encrypt the bucket using AWS KMS.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTo prevent or mitigate future accidental deletions, consider the following features: - Enable versioning to keep historical versions of an object. - Enable cross-region replication of objects. - Enable MFA Delete to require multi-factor authentication (MFA) when deleting an object version.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "A company is building a solution that will report Amazon EC2 Auto Scaling events across all the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n175 \napplications in an AWS account. The company needs to use a serverless solution to store the \nEC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 \nto provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 \ninstance launches. \nHow should the company move the data to Amazon S3 to meet these requirements?",
            options: [
                { id: 0, text: "Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon", correct: true },
                { id: 1, text: "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to", correct: false },
                { id: 2, text: "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule.", correct: false },
                { id: 3, text: "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. One of the use cases is Data Lake: create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric- Streams.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "A company has an application that places hundreds of .csv files into an Amazon S3 bucket every \nhour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the \nfile to Apache Parquet format and place the output file into an S3 bucket. \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format,", correct: false },
                { id: 1, text: "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place", correct: false },
                { id: 2, text: "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application", correct: false },
                { id: 3, text: "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types- for-converting-data-to-apache-parquet.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A company is implementing new data retention policies for all databases that run on Amazon \nRDS DB instances. The company must retain daily backups for a minimum period of 2 years. The \nbackups must be consistent and restorable. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n176 \nWhich solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a", correct: true },
                { id: 1, text: "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot", correct: false },
                { id: 2, text: "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs", correct: false },
                { id: 3, text: "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company’s compliance team needs to move its file shares to AWS. The shares run on a \nWindows Server SMB file share. A self-managed on-premises Active Directory controls access to \nthe files and folders. \nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The \ncompany must ensure that the on-premises Active Directory groups restrict access to the FSx for \nWindows File Server SMB compliance shares, folders, and files after the move to AWS. The \ncompany has created an FSx for Windows File Server file system. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory", correct: false },
                { id: 1, text: "Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups", correct: false },
                { id: 2, text: "Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict", correct: false },
                { id: 3, text: "Join the file system to the Active Directory to restrict access.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nJoining the FSx for Windows File Server file system to the on-premises Active Directory will allow the company to use the existing Active Directory groups to restrict access to the file shares, folders, and files after the move to AWS. This option allows the company to continue using their existing access controls and management structure, making the transition to AWS more seamless.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company recently announced the deployment of its retail website to a global audience. The \nwebsite runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances \nrun in an Auto Scaling group across multiple Availability Zones. \nThe company wants to provide its customers with different versions of content based on the \ndevices that the customers use to access the website. \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Configure Amazon CloudFront to cache multiple versions of the content.", correct: true },
                { id: 1, text: "Configure a host header in a Network Load Balancer to forward traffic to different instances.", correct: false },
                { id: 2, text: "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent", correct: false },
                { id: 3, text: "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB).", correct: false },
                { id: 4, text: "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB).", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions \narchitect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s \nAmazon EC2 instances. Both VPCs are in the us-east-1 Region. \nThe solutions architect must implement a solution to provide the application’s EC2 instances with \naccess to the ElastiCache cluster. \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a peering connection between the VPCs. Add a route table entry for the peering", correct: true },
                { id: 1, text: "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route", correct: false },
                { id: 2, text: "Create a peering connection between the VPCs. Add a route table entry for the peering", correct: false },
                { id: 3, text: "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreating a peering connection between the VPCs allows the application's EC2 instances to communicate with the ElastiCache cluster directly and efficiently. This is the most cost-effective solution as it does not involve creating additional resources such as a Transit VPC, and it does not incur additional costs for traffic passing through the Transit VPC. Additionally, it is also more secure as it allows you to configure a more restrictive security group rule to allow inbound connection from only the application's security group.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 30,
            text: "A company is building an application that consists of several microservices. The company has \ndecided to use container technologies to deploy its software on AWS. The company needs a \nsolution that minimizes the amount of ongoing effort for maintenance and scaling. The company \ncannot manage additional infrastructure. \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.", correct: true },
                { id: 1, text: "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability", correct: false },
                { id: 2, text: "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch", correct: false },
                { id: 3, text: "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type.", correct: false },
                { id: 4, text: "Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by \nAmazon Route 53. The company occasionally experiences a timeout error when attempting to \nbrowse the application. The networking team finds that some DNS queries return IP addresses of \nunhealthy instances, resulting in the timeout error. \nWhat should a solutions architect implement to overcome these timeout errors?",
            options: [
                { id: 0, text: "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check", correct: false },
                { id: 1, text: "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health", correct: false },
                { id: 3, text: "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAn Application Load Balancer (ALB) allows you to distribute incoming traffic across multiple backend instances, and can automatically route traffic to healthy instances while removing traffic from unhealthy instances. By using an ALB in front of the EC2 instances and routing traffic to it from Route 53, the load balancer can perform health checks on the instances and only route traffic to healthy instances, which should help to reduce or eliminate timeout errors caused by unhealthy instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 32,
            text: "A solutions architect needs to design a highly available application consisting of web, application, \nand database tiers. HTTPS content delivery should be as close to the edge as possible, with the \nleast delivery time. \nWhich solution meets these requirements and is MOST secure?",
            options: [
                { id: 0, text: "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2", correct: false },
                { id: 1, text: "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in", correct: false },
                { id: 2, text: "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2", correct: true },
                { id: 3, text: "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThis solution meets the requirements for a highly available application with web, application, and database tiers, as well as providing edge-based content delivery. Additionally, it maximizes security by having the ALB in a private subnet, which limits direct access to the web servers, while still being able to serve traffic over the Internet via the public ALB. This will ensure that the web servers are not exposed to the public Internet, which reduces the attack surface and provides a secure way to access the application. https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A company has a popular gaming platform running on AWS. The application is sensitive to \nlatency because latency can impact the user experience and introduce unfair advantages to \nsome players. The application is deployed in every AWS Region. It runs on Amazon EC2 \ninstances that are part of Auto Scaling groups configured behind Application Load Balancers \n(ALBs). A solutions architect needs to implement a mechanism to monitor the health of the \napplication and redirect traffic to healthy endpoints. \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the", correct: true },
                { id: 1, text: "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure", correct: false },
                { id: 3, text: "Configure an Amazon DynamoDB database to serve as the data store for the application. Create", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nWhen you have an Application Load Balancer or Network Load Balancer that includes multiple target groups, Global Accelerator considers the load balancer endpoint to be healthy only if each target group behind the load balancer has at least one healthy target. If any single target group for the load balancer has only unhealthy targets, Global Accelerator considers the endpoint to be unhealthy. https://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoint-groups-health-check- options.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A company has one million users that use its mobile app. The company must analyze the data \nusage in near-real time. The company also must encrypt the data in near-real time and must \nstore the data in a centralized location in Apache Parquet format for further processing. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n180 \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon", correct: false },
                { id: 1, text: "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR", correct: false },
                { id: 2, text: "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create", correct: false },
                { id: 3, text: "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThis solution will meet the requirements with the least operational overhead as it uses Amazon Kinesis Data Firehose, which is a fully managed service that can automatically handle the data collection, data transformation, encryption, and data storage in near-real time. Kinesis Data Firehose can automatically store the data in Amazon S3 in Apache Parquet format for further processing. Additionally, it allows you to create an Amazon Kinesis Data Analytics application to analyze the data in near real-time, with no need to manage any infrastructure or invoke any Lambda function. This way you can process a large amount of data with the least operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "An ecommerce company has noticed performance degradation of its Amazon RDS based web \napplication. The performance degradation is attributed to an increase in the number of read-only \nSQL queries triggered by business analysts. A solutions architect needs to solve the problem with \nminimal changes to the existing web application. \nWhat should the solutions architect recommend?",
            options: [
                { id: 0, text: "Export the data to Amazon DynamoDB and have the business analysts run their queries.", correct: false },
                { id: 1, text: "Load the data into Amazon ElastiCache and have the business analysts run their queries.", correct: false },
                { id: 2, text: "Create a read replica of the primary database and have the business analysts run their queries.", correct: true },
                { id: 3, text: "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCreating a read replica of the primary RDS database will offload the read-only SQL queries from the primary database, which will help to improve the performance of the web application. Read replicas are exact copies of the primary database that can be used to handle read-only traffic, which will reduce the load on the primary database and improve the performance of the web application. This solution can be implemented with minimal changes to the existing web application, as the business analysts can continue to run their queries on the read replica without modifying the code.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "A company runs an internal browser-based application. The application runs on Amazon EC2 \ninstances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto \nScaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 \ninstances during work hours, but scales down to 2 instances overnight. Staff are complaining that \nthe application is very slow when the day begins, although it runs well by mid-morning. \n \nHow should the scaling be changed to address the staff complaints and keep costs to a \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n181 \nminimum?",
            options: [
                { id: 0, text: "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.", correct: false },
                { id: 1, text: "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown", correct: false },
                { id: 2, text: "Implement a target tracking action triggered at a lower CPU threshold, and decrease the", correct: true },
                { id: 3, text: "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto \nScaling group. An Amazon RDS for Oracle instance is the application' s data layer that uses \nOracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is \ncausing the EC2 instances to become overloaded and the RDS instance to run out of storage. \nThe Auto Scaling group does not have any scaling metrics and defines the minimum healthy \ninstance count only. The company predicts that traffic will continue to increase at a steady but \nunpredictable rate before leveling off. \n \nWhat should a solutions architect do to ensure the system can automatically scale for the \nincreased traffic? (Choose two.)",
            options: [
                { id: 0, text: "Configure storage Auto Scaling on the RDS for Oracle instance.", correct: true },
                { id: 1, text: "Migrate the database to Amazon Aurora to use Auto Scaling storage.", correct: false },
                { id: 2, text: "Configure an alarm on the RDS for Oracle instance for low free storage space.", correct: false },
                { id: 3, text: "Configure the Auto Scaling group to use the average CPU as the scaling metric.", correct: false },
                { id: 4, text: "Configure the Auto Scaling group to use the average free memory as the scaling metric.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "A company provides an online service for posting video content and transcoding it for use by any \nmobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) \nStandard to collect and store the videos so that multiple Amazon EC2 Linux instances can access \nthe video content for processing. As the popularity of the service has grown over time, the \nstorage costs have become too expensive. \n \nWhich storage solution is MOST cost-effective?",
            options: [
                { id: 0, text: "Use AWS Storage Gateway for files to store and process the video content.", correct: false },
                { id: 1, text: "Use AWS Storage Gateway for volumes to store and process the video content.", correct: false },
                { id: 2, text: "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to", correct: false },
                { id: 3, text: "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nA better solution would be to use a transcoding service like Amazon Elastic Transcoder to process the video content directly from Amazon S3. This would eliminate the need for storing the Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 39,
            text: "A company wants to create an application to store employee data in a hierarchical structured \nrelationship. The company needs a minimum-latency response to high-traffic queries for the \nemployee data and must protect any sensitive data. The company also needs to receive monthly \nemail messages if any financial information is present in the employee data. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3", correct: false },
                { id: 1, text: "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon", correct: true },
                { id: 2, text: "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to", correct: false },
                { id: 3, text: "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon", correct: false },
                { id: 4, text: "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A company has an application that is backed by an Amazon DynamoDB table. The company's \ncompliance requirements specify that database backups must be taken every month, must be \navailable for 6 months, and must be retained for 7 years. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month.", correct: true },
                { id: 1, text: "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month.", correct: false },
                { id: 2, text: "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB", correct: false },
                { id: 3, text: "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company is using Amazon CloudFront with its website. The company has enabled logging on \nthe CloudFront distribution, and logs are saved in one of the company's Amazon S3 buckets. The \ncompany needs to perform advanced analyses on the logs and build visualizations. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket.", correct: false },
                { id: 1, text: "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket.", correct: true },
                { id: 2, text: "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3", correct: false },
                { id: 3, text: "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nQuicksite creating data visualizations. https://docs.aws.amazon.com/quicksight/latest/user/welcome.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After \na routine compliance check, the company sets a standard that requires a recovery point objective \n(RPO) of less than 1 second for all its production databases. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Enable a Multi-AZ deployment for the DB instance.", correct: true },
                { id: 1, text: "Enable auto scaling for the DB instance in one Availability Zone.", correct: false },
                { id: 2, text: "Configure the DB instance in one Availability Zone, and create multiple read replicas in a", correct: false },
                { id: 3, text: "Configure the DB instance in one Availability Zone, and configure AWS Database Migration", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy using Multi-AZ deployment, the company can achieve an RPO of less than 1 second because the standby instance is always in sync with the primary instance, ensuring that data changes are continuously replicated.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 43,
            text: "A company runs a web application that is deployed on Amazon EC2 instances in the private \nsubnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets \ndirects web traffic to the EC2 instances. The company wants to implement new security \nmeasures to restrict inbound traffic from the ALB to the EC2 instances while preventing access \nfrom any other source inside or outside the private subnet of the EC2 instances. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure a route in a route table to direct traffic from the internet to the private IP addresses of", correct: false },
                { id: 1, text: "Configure the security group for the EC2 instances to only allow traffic that comes from the", correct: true },
                { id: 2, text: "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP", correct: false },
                { id: 3, text: "Configure the security group for the ALB to allow any TCP traffic on any port.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThis ensures that only the traffic originating from the ALB is allowed access to the EC2 instances in the private subnet, while denying any other traffic from other sources.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A research company runs experiments that are powered by a simulation application and a \nvisualization application. The simulation application runs on Linux and outputs intermediate data \nto an NFS share every 5 minutes. The visualization application is a Windows desktop application \nthat displays the simulation output and requires an SMB file system. \n \nThe company maintains two synchronized file systems. This strategy is causing data duplication \nand inefficient resource usage. The company needs to migrate the applications to AWS without \nmaking code changes to either application. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data", correct: false },
                { id: 1, text: "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure", correct: false },
                { id: 2, text: "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization", correct: false },
                { id: 3, text: "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon FSx for NetApp ONTAP is a fully-managed shared storage service built on NetApp's popular ONTAP file system. Amazon FSx for NetApp ONTAP provides the popular features, performance, and APIs of ONTAP file systems with the agility, scalability, and simplicity of a fully managed AWS service, making it easier for customers to migrate on-premises applications that rely on NAS appliances to AWS. FSx for ONTAP file systems are similar to on-premises NetApp clusters. Within each file system that you create, you also create one or more storage virtual machines (SVMs). These are isolated file servers each with their own endpoints for NFS, SMB, and management access, as well as authentication (for both administration and end-user data access). In turn, each SVM has one or more volumes which store your data. https://aws.amazon.com/de/blogs/storage/getting-started-cloud-file-storage-with-amazon-fsx-for- netapp-ontap-using-netapp-management-tools/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 45,
            text: "As part of budget planning, management wants a report of AWS billed items listed by user. The \ndata will be used to create department budgets. A solutions architect needs to determine the \nmost efficient way to obtain this report information. \n \nWhich solution meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n185",
            options: [
                { id: 0, text: "Run a query with Amazon Athena to generate the report.", correct: false },
                { id: 1, text: "Create a report in Cost Explorer and download the report.", correct: true },
                { id: 2, text: "Access the bill details from the billing dashboard and download the bill.", correct: false },
                { id: 3, text: "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company hosts its static website by using Amazon S3. The company wants to add a contact \nform to its webpage. The contact form will have dynamic server-side components for users to \ninput their name, email address, phone number, and user message. The company anticipates \nthat there will be fewer than 100 site visits each month. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to", correct: true },
                { id: 2, text: "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to", correct: false },
                { id: 3, text: "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL,", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites- using-aws-lambda-amazon-api-gateway-and-amazon-ses/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 47,
            text: "A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The \nstatic website uses a database backend. The company notices that the website does not reflect \nupdates that have been made in the website's Git repository. The company checks the \ncontinuous integration and continuous delivery (CI/CD) pipeline between the Git repository and \nAmazon S3. The company verifies that the webhooks are configured properly and that the CI/CD \npipeline is sending messages that indicate successful deployments. \n \nA solutions architect needs to implement a solution that displays the updates on the website. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Add an Application Load Balancer.", correct: false },
                { id: 1, text: "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.", correct: false },
                { id: 2, text: "Invalidate the CloudFront cache.", correct: true },
                { id: 3, text: "Use AWS Certificate Manager (ACM) to validate the website's SSL certificate.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nInvalidate the CloudFront cache: The solutions architect should invalidate the CloudFront cache to ensure that the latest version of the website is being served to users. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 48,
            text: "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers: an application tier, a business tier, and a database tier with \nMicrosoft SQL Server. The company wants to use specific features of SQL Server such as native \nbackups and Data Quality Services. The company also needs to share files for processing \nbetween the tiers. \n \nHow should a solutions architect design the architecture to meet these requirements?",
            options: [
                { id: 0, text: "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing", correct: false },
                { id: 1, text: "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file", correct: true },
                { id: 2, text: "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier", correct: false },
                { id: 3, text: "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nData Quality Services: If this feature is critical to your workload, consider choosing Amazon RDS Custom or Amazon EC2. https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/comparison.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 49,
            text: "A company is migrating a Linux-based web server group to AWS. The web servers must access \nfiles in a shared file store for some content. The company must not make any changes to the \napplication. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon S3 Standard bucket with access to the web servers.", correct: false },
                { id: 1, text: "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.", correct: false },
                { id: 2, text: "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on", correct: true },
                { id: 3, text: "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCreate an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers. To meet the requirements of providing a shared file store for Linux-based web servers without making changes to the application, using an Amazon EFS file system is the best solution. Amazon EFS is a managed NFS file system service that provides shared access to files across multiple Linux-based instances, which makes it suitable for this use case. Amazon S3 is not ideal for this scenario since it is an object storage service and not a file system, and it requires additional tools or libraries to mount the S3 bucket as a file system. Amazon CloudFront can be used to improve content delivery performance but is not necessary for this requirement. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that \nis located in the same AWS account. \n \nWhich solution will meet these requirements in the MOST secure manner?",
            options: [
                { id: 0, text: "Apply an S3 bucket policy that grants read access to the S3 bucket.", correct: false },
                { id: 1, text: "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to", correct: true },
                { id: 2, text: "Embed an access key and a secret key in the Lambda function's code to grant the required IAM", correct: false },
                { id: 3, text: "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThis is the most secure and recommended way to provide an AWS Lambda function with access to an S3 bucket. It involves creating an IAM role that the Lambda function assumes, and attaching an IAM policy to the role that grants the necessary permissions to read from the S3 bucket. https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are \nin an Auto Scaling group that scales in response to user demand. The company wants to \noptimize cost savings without making a long-term commitment. \n \nWhich EC2 instance purchasing option should a solutions architect recommend to meet these \nrequirements?",
            options: [
                { id: 0, text: "Dedicated Instances only", correct: false },
                { id: 1, text: "On-Demand Instances only", correct: false },
                { id: 2, text: "A mix of On-Demand Instances and Spot Instances", correct: true },
                { id: 3, text: "A mix of On-Demand Instances and Reserved Instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances- groups.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 52,
            text: "A media company uses Amazon CloudFront for its publicly available streaming video content. \nThe company wants to secure the video content that is hosted in Amazon S3 by controlling who \nhas access. Some of the company's users are using a custom HTTP client that does not support \ncookies. Some of the company's users are unable to change the hardcoded URLs that they are \nusing for access. \n \nWhich services or methods will meet these requirements with the LEAST impact to the users? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n188",
            options: [
                { id: 0, text: "Signed cookies", correct: true },
                { id: 1, text: "Signed URLs", correct: false },
                { id: 2, text: "AWS AppSync", correct: false },
                { id: 3, text: "JSON Web Token (JWT)", correct: false },
                { id: 4, text: "AWS Secrets Manager", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSigned URLs are URLs that grant temporary access to an S3 object. They include a signature that verifies the authenticity of the request, as well as an expiration date that limits the time during which the URL is valid. This solution will work for users who are using custom HTTP clients that do not support cookies. Signed cookies are similar to signed URLs, but they use cookies to grant temporary access to S3 objects. This solution will work for users who are unable to change the hardcoded URLs that they are using for access. https://aws.amazon.com/blogs/media/secure-content-using-cloudfront-functions/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A company is preparing a new data platform that will ingest real-time streaming data from multiple \nsources. The company needs to transform the data before writing the data to Amazon S3. The \ncompany needs the ability to use SQL to query the transformed data. \n \nWhich solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to", correct: true },
                { id: 1, text: "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS", correct: false },
                { id: 2, text: "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to", correct: false },
                { id: 3, text: "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use", correct: false },
                { id: 4, text: "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "A company has an on-premises volume backup solution that has reached its end of life. The \ncompany wants to use AWS as part of a new backup solution and wants to maintain local access \nto all the data while it is backed up on AWS. The company wants to ensure that the data backed \nup on AWS is automatically and securely transferred. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-", correct: false },
                { id: 1, text: "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the", correct: false },
                { id: 2, text: "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway", correct: false },
                { id: 3, text: "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nIn the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access. In the stored mode, your primary data is stored locally and your entire dataset is available for low- latency access while asynchronously backed up to AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. \nTraffic must not traverse the internet. \n \nHow should a solutions architect configure access to meet these requirements?",
            options: [
                { id: 0, text: "Create a private hosted zone by using Amazon Route 53.", correct: false },
                { id: 1, text: "Set up a gateway VPC endpoint for Amazon S3 in the VPC.", correct: true },
                { id: 2, text: "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.", correct: false },
                { id: 3, text: "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains \npersonally identifiable information (PII). The company wants to use the data in three applications. \nOnly one of the applications needs to process the PII. The PII must be removed before the other \ntwo applications process the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and", correct: false },
                { id: 1, text: "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object", correct: true },
                { id: 2, text: "Process the data and store the transformed data in three separate Amazon S3 buckets so that", correct: false },
                { id: 3, text: "Process the data and store the transformed data in three separate Amazon DynamoDB tables so", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon S3 Object Lambda allows you to add custom code to S3 GET requests, which means that you can modify the data before it is returned to the requesting application. In this case, you Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A development team has launched a new application that is hosted on Amazon EC2 instances \ninside a development VPC. A solutions architect needs to create a new VPC in the same \naccount. The new VPC will be peered with the development VPC. The VPC CIDR block for the \ndevelopment VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the \nnew VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. \n \nWhat is the SMALLEST CIDR block that meets these requirements?",
            options: [
                { id: 0, text: "10.0.1.0/32", correct: false },
                { id: 1, text: "192.168.0.0/24", correct: false },
                { id: 2, text: "192.168.1.0/32", correct: false },
                { id: 3, text: "10.0.1.0/24", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe allowed block size is between a /28 netmask and /16 netmask. The CIDR block must not overlap with any existing CIDR block that's associated with the VPC. https://docs.aws.amazon.com/vpc/latest/userguide/configure-your-vpc.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company deploys an application on five Amazon EC2 instances. An Application Load Balancer \n(ALB) distributes traffic to the instances by using a target group. The average CPU usage on \neach of the instances is below 10% most of the time, with occasional surges to 65%. \n \nA solutions architect needs to implement a solution to automate the scalability of the application. \nThe solution must optimize the cost of the architecture and must ensure that the application has \nenough CPU resources when surges occur. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization", correct: false },
                { id: 1, text: "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing", correct: true },
                { id: 2, text: "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing", correct: false },
                { id: 3, text: "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIt allows for automatic scaling based on the average CPU utilization of the EC2 instances in the target group. With the use of a target tracking scaling policy based on the ASGAverageCPUUtilization metric, the EC2 Auto Scaling group can ensure that the target value of 50% is maintained while scaling the number of instances in the group up or down as needed. This will help ensure that the application has enough CPU resources during surges without overprovisioning, thus optimizing the cost of the architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 59,
            text: "A company is running a critical business application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances run in an Auto Scaling group and access an \nAmazon RDS DB instance. \n \nThe design did not pass an operational review because the EC2 instances and the DB instance \nare all located in a single Availability Zone. A solutions architect must update the design to use a \nsecond Availability Zone. \n \nWhich solution will make the application highly available?",
            options: [
                { id: 0, text: "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the", correct: false },
                { id: 1, text: "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling", correct: false },
                { id: 2, text: "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the", correct: true },
                { id: 3, text: "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nA subnet must reside within a single Availability Zone. https://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,within% 20a%20single%20Availability%20Zone.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-\nmillisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds \nof Amazon EC2 instances that run Amazon Linux will distribute and process the data. \n \nWhich solution will meet the performance requirements?",
            options: [
                { id: 0, text: "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume' tiering policy to ALL.", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system", correct: true },
                { id: 2, text: "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system", correct: false },
                { id: 3, text: "Create an Amazon FSx for NetApp ONTAP file system. Set each volume's tiering policy to NONE.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCreate an Amazon S3 bucket to store the raw data Create an Amazon FSx for Lustre file system that uses persistent SSD storage Select the option to import data from and export data to Amazon S3 Mount the file system on the EC2 instances. Amazon FSx for Lustre uses SSD storage for sub-millisecond latencies and up to 6 GBps throughput, and can import data from and export data to Amazon S3. Additionally, the option to select persistent SSD storage will ensure that the data is stored on the disk and not lost if the file system is stopped.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A company needs to migrate a legacy application from an on-premises data center to the AWS \nCloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a \nweek. The application's database storage continues to grow over time. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to", correct: false },
                { id: 1, text: "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer", correct: false },
                { id: 2, text: "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer", correct: true },
                { id: 3, text: "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 62,
            text: "A university research laboratory needs to migrate 30 TB of data from an on-premises Windows \nfile server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that \nmany other departments in the university share. \n \nThe laboratory wants to implement a data migration service that will maximize the performance of \nthe data transfer. However, the laboratory needs to be able to control the amount of bandwidth \nthat the service uses to minimize the impact on other departments. The data migration must take \nplace within the next 5 days. \n \nWhich AWS solution will meet these requirements?",
            options: [
                { id: 0, text: "AWS Snowcone", correct: false },
                { id: 1, text: "Amazon FSx File Gateway", correct: false },
                { id: 2, text: "AWS DataSync", correct: true },
                { id: 3, text: "AWS Transfer Family", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nDataSync can be used to migrate data between on-premises Windows file servers and Amazon FSx for Windows File Server with its compatibility for Windows file systems. The laboratory needs to migrate a large amount of data (30 TB) within a relatively short timeframe (5 days) and limit the impact on other departments' network traffic. Therefore, AWS DataSync can meet these requirements by providing fast and efficient data transfer with network throttling capability to control bandwidth usage. https://docs.aws.amazon.com/datasync/latest/userguide/configure-bandwidth.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A company wants to create a mobile app that allows users to stream slow-motion video clips on \ntheir mobile devices. Currently, the app captures video clips and uploads the video clips in raw \nformat into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. \nHowever, the videos are large in their raw format. \n \nUsers are experiencing issues with buffering and playback on mobile devices. The company \nwants to implement solutions to maximize the performance and scalability of the app while \nminimizing operational overhead. \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Deploy Amazon CloudFront for content delivery and caching.", correct: true },
                { id: 1, text: "Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.", correct: false },
                { id: 2, text: "Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.", correct: false },
                { id: 3, text: "Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and", correct: false },
                { id: 4, text: "Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/elastictranscoder/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 64,
            text: "A company is launching a new application deployed on an Amazon Elastic Container Service \n(Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is \nmonitoring CPU and memory usage because it is expecting high traffic to the application upon its \nlaunch. However, the company wants to reduce costs when utilization decreases. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.", correct: false },
                { id: 1, text: "Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an", correct: false },
                { id: 2, text: "Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches", correct: false },
                { id: 3, text: "Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto- scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test15: [
        {
            id: 0,
            text: "A company recently created a disaster recovery site in a different AWS Region. The company \nneeds to transfer large amounts of data back and forth between NFS file systems in the two \nRegions on a periodic basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS DataSync.", correct: true },
                { id: 1, text: "Use AWS Snowball devices.", correct: false },
                { id: 2, text: "Set up an SFTP server on Amazon EC2.", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS).", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies moving large amounts of data between on-premises storage systems and AWS services. It can also transfer data between different AWS services, including different AWS Regions. DataSync provides a simple, scalable, and automated solution to transfer data, and it minimizes the operational overhead because it is fully managed by AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 1,
            text: "A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same \nVPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure \nthat enough funds are available before a stock can be purchased. The company has noticed in \nthe VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web \nservice over the internet instead of through the VPC. A solutions architect must implement a \nsolution so that the APIs communicate through the VPC. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
            options: [
                { id: 0, text: "Add an X-API-Key header in the HTTP header for authorization.", correct: false },
                { id: 1, text: "Use an interface endpoint.", correct: true },
                { id: 2, text: "Use a gateway endpoint.", correct: false },
                { id: 3, text: "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAn interface endpoint is a horizontally scaled, redundant VPC endpoint that provides private connectivity to a service. It is an elastic network interface with a private IP address that serves as an entry point for traffic destined to the AWS service. Interface endpoints are used to connect VPCs with AWS services. https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A company wants to run an in-memory database for a latency-sensitive application that runs on \nAmazon EC2 instances. The application processes more than 100,000 transactions each minute \nand requires high network throughput. A solutions architect needs to provide a cost-effective \nnetwork design that minimizes data transfer charges. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n195 \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a", correct: true },
                { id: 1, text: "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a", correct: false },
                { id: 2, text: "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a", correct: false },
                { id: 3, text: "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo achieve low latency, high throughput, and cost-effectiveness, the optimal solution is to launch EC2 instances as a placement group with the cluster strategy within the same Availability Zone.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 3,
            text: "A company that primarily runs its application servers on premises has decided to migrate to AWS. \nThe company wants to minimize its need to scale its Internet Small Computer Systems Interface \n(iSCSI) storage on premises. The company wants only its recently accessed data to remain \nstored locally. \n \nWhich AWS solution should the company use to meet these requirements?",
            options: [
                { id: 0, text: "Amazon S3 File Gateway", correct: false },
                { id: 1, text: "AWS Storage Gateway Tape Gateway", correct: false },
                { id: 2, text: "AWS Storage Gateway Volume Gateway stored volumes", correct: false },
                { id: 3, text: "AWS Storage Gateway Volume Gateway cached volumes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Storage Gateway Volume Gateway provides two configurations for connecting to iSCSI storage, namely, stored volumes and cached volumes. The stored volume configuration stores the entire data set on-premises and asynchronously backs up the data to AWS. The cached volume configuration stores recently accessed data on-premises, and the remaining data is stored in Amazon S3. Since the company wants only its recently accessed data to remain stored locally, the cached volume configuration would be the most appropriate. It allows the company to keep frequently accessed data on-premises and reduce the need for scaling its iSCSI storage while still providing access to all data through the AWS cloud. This configuration also provides low-latency access to frequently accessed data and cost-effective off-site backups for less frequently accessed data. https://docs.amazonaws.cn/en_us/storagegateway/latest/vgw/StorageGatewayConcepts.html#sto rage-gateway-cached-concepts\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company has multiple AWS accounts that use consolidated billing. The company runs several \nactive high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The \ncompany's finance team has access to AWS Trusted Advisor in the consolidated billing account \nand all other AWS accounts. \n \nThe finance team needs to use the appropriate AWS account to access the Trusted Advisor \ncheck recommendations for RDS. The finance team must review the appropriate Trusted Advisor \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n196 \ncheck to reduce RDS costs. \n \nWhich combination of steps should the finance team take to meet these requirements? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Use the Trusted Advisor recommendations from the account where the RDS instances are", correct: false },
                { id: 1, text: "Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS", correct: true },
                { id: 2, text: "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.", correct: false },
                { id: 3, text: "Review the Trusted Advisor check for Amazon RDS Idle DB Instances.", correct: false },
                { id: 4, text: "Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/trusted-advisor-cost-optimization/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A solutions architect needs to optimize storage costs. The solutions architect must identify any \nAmazon S3 buckets that are no longer being accessed or are rarely accessed. \n \nWhich solution will accomplish this goal with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity", correct: true },
                { id: 1, text: "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.", correct: false },
                { id: 2, text: "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access", correct: false },
                { id: 3, text: "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nS3 Storage Lens is a fully managed S3 storage analytics solution that provides a comprehensive view of object storage usage, activity trends, and recommendations to optimize costs. Storage Lens allows you to analyze object access patterns across all of your S3 buckets and generate detailed metrics and reports. https://aws.amazon.com/blogs/aws/s3-storage-lens/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 6,
            text: "A company sells datasets to customers who do research in artificial intelligence and machine \nlearning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket \nin the us-east-1 Region. The company hosts a web application that the customers use to \npurchase access to a given dataset. The web application is deployed on multiple Amazon EC2 \ninstances behind an Application Load Balancer. After a purchase is made, customers receive an \nS3 signed URL that allows access to the files. \n \nThe customers are distributed across North America and Europe. The company wants to reduce \nthe cost that is associated with data transfers and wants to maintain or improve performance. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n197",
            options: [
                { id: 0, text: "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3", correct: false },
                { id: 1, text: "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct", correct: true },
                { id: 2, text: "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between", correct: false },
                { id: 3, text: "Modify the web application to enable streaming of the datasets to end users. Configure the web", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTo reduce the cost associated with data transfers and maintain or improve performance, a solutions architect should use Amazon CloudFront, a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. Deploying a CloudFront distribution with the existing S3 bucket as the origin will allow the company to serve the data to customers from edge locations that are closer to them, reducing data transfer costs and improving performance. Directing customer requests to the CloudFront URL and switching to CloudFront signed URLs for access control will enable customers to access the data securely and efficiently. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A company is using AWS to design a web application that will process insurance quotes. Users \nwill request quotes from the application. Quotes must be separated by quote type, must be \nresponded to within 24 hours, and must not get lost. The solution must maximize operational \nefficiency and must minimize maintenance. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web", correct: false },
                { id: 1, text: "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 2, text: "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon", correct: true },
                { id: 3, text: "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nQuote types need to be separated: SNS message filtering can be used to publish messages to the appropriate SQS queue based on the quote type, ensuring that quotes are separated by type. Quotes must be responded to within 24 hours and must not get lost: SQS provides reliable and scalable queuing for messages, ensuring that quotes will not get lost and can be processed in a Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A company has an application that runs on several Amazon EC2 instances. Each EC2 instance \nhas multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The \napplication's EC2 instance configuration and data need to be backed up nightly. The application \nalso needs to be recoverable in a different AWS Region. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS", correct: false },
                { id: 1, text: "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to", correct: true },
                { id: 2, text: "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to", correct: false },
                { id: 3, text: "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/vi/blogs/aws/aws-backup-ec2-instances-efs-single-file-restore-and- cross-region-backup/ When you back up an EC2 instance, AWS Backup will protect all EBS volumes attached to the instance, and it will attach them to an AMI that stores all parameters from the original EC2 instance except for two.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "A company is building a mobile app on AWS. The company wants to expand its reach to millions \nof users. The company needs to build a platform so that authorized users can watch the \ncompany's content on their mobile devices. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS)", correct: false },
                { id: 1, text: "Set up IPsec VPN between the mobile app and the AWS environment to stream content.", correct: false },
                { id: 2, text: "Use Amazon CloudFront. Provide signed URLs to stream content.", correct: true },
                { id: 3, text: "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront supports signed URLs that provide authorized access to your content. This feature allows the company to control who can access their content and for how long, providing a secure Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A company hosts a three-tier web application that includes a PostgreSQL database The database \nstores the metadata from documents The company searches the metadata for key terms to \nretrieve documents that the company reviews in a report each month The documents are stored \nin Amazon S3 The documents are usually written only once, but they are updated frequency The \nreporting process takes a few hours with the use of relational queries The reporting process must \nnot affect any document modifications or the addition of new documents. \nWhat are the MOST operationally efficient solutions that meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read", correct: false },
                { id: 1, text: "Set up a new Amazon RDS for PostgreSQL Reserved Instance and an On-Demand read replica", correct: true },
                { id: 2, text: "Set up a new Amazon Aurora PostgreSQL DB cluster that includes a Reserved Instance and an", correct: false },
                { id: 3, text: "Set up a new Amazon RDS for PostgreSQL Multi-AZ Reserved Instance Configure the reporting", correct: false },
                { id: 4, text: "Set up a new Amazon DynamoDB table to store the documents Use a fixed write capacity to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company experienced a breach that affected several applications in its on-premises data \ncenter. The attacker took advantage of vulnerabilities in the custom applications that were \nrunning on the servers. The company is now migrating its applications to run on Amazon EC2 \ninstances. The company wants to implement a solution that actively scans for vulnerabilities on \nthe EC2 instances and sends a report that details the findings. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda", correct: false },
                { id: 1, text: "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities.", correct: false },
                { id: 2, text: "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an", correct: false },
                { id: 3, text: "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. https://aws.amazon.com/inspector/features/?nc=sn&loc=2 Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A company uses an Amazon EC2 instance to run a script to poll for and process messages in an \nAmazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational \ncosts while maintaining its ability to process a growing number of messages that are added to the \nqueue. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Increase the size of the EC2 instance to process messages faster.", correct: false },
                { id: 1, text: "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.", correct: false },
                { id: 2, text: "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.", correct: true },
                { id: 3, text: "Use AWS Systems Manager Run Command to run the script on demand.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy migrating the script to AWS Lambda, the company can take advantage of the auto-scaling feature of the service. AWS Lambda will automatically scale resources to match the size of the workload. This means that the company will not have to worry about provisioning or managing instances as the number of messages increases, resulting in lower operational costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 13,
            text: "A company uses a legacy application to produce data in CSV format. The legacy application \nstores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf \n(COTS) application that can perform complex SQL queries to analyze data that is stored in \nAmazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv \nfiles that the legacy application produces. \n \nThe company cannot update the legacy application to produce data in another format. The \ncompany needs to implement a solution so that the COTS application can use the data that the \nlegacy application produces. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure", correct: true },
                { id: 1, text: "Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files.", correct: false },
                { id: 2, text: "Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke", correct: false },
                { id: 3, text: "Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company recently migrated its entire IT environment to the AWS Cloud. The company \ndiscovers that users are provisioning oversized Amazon EC2 instances and modifying security \ngroup rules without using the appropriate change control process. A solutions architect must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n201 \ndevise a strategy to track and audit these inventory and configuration changes. \n \nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Enable AWS CloudTrail and use it for auditing.", correct: true },
                { id: 1, text: "Use data lifecycle policies for the Amazon EC2 instances.", correct: false },
                { id: 2, text: "Enable AWS Trusted Advisor and reference the security dashboard.", correct: false },
                { id: 3, text: "Enable AWS Config and create rules for auditing and compliance purposes.", correct: false },
                { id: 4, text: "Restore previous resource configurations with an AWS CloudFormation template.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems \nadministrators have used shared SSH keys to manage the instances. After a recent audit, the \ncompany's security team is mandating the removal of all shared keys. A solutions architect must \ndesign a solution that provides secure access to the EC2 instances. \n \nWhich solution will meet this requirement with the LEAST amount of administrative overhead?",
            options: [
                { id: 0, text: "Use AWS Systems Manager Session Manager to connect to the EC2 instances.", correct: true },
                { id: 1, text: "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.", correct: false },
                { id: 2, text: "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only", correct: false },
                { id: 3, text: "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Systems Manager Session Manager provides secure and auditable instance management without the need for any inbound connections or open ports. It allows you to manage your instances through an interactive one-click browser-based shell or through the AWS CLI. This means that you don't have to manage any SSH keys, and you don't have to worry about securing access to your instances as access is controlled through IAM policies.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A company is building a data analysis platform on AWS by using AWS Lake Formation. The \nplatform will ingest data from different sources such as Amazon S3 and Amazon RDS. The \ncompany needs a secure solution to prevent access to portions of the data that contain sensitive \ninformation.",
            options: [
                { id: 0, text: "Create an IAM role that includes permissions to access Lake Formation tables.", correct: true },
                { id: 1, text: "Create data filters to implement row-level security and cell-level security.", correct: false },
                { id: 2, text: "Create an AWS Lambda function that removes sensitive information before Lake Formation", correct: false },
                { id: 3, text: "Create an AWS Lambda function that perodically Queries and removes sensitive information from", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n202 \nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket \nare encrypted?",
            options: [
                { id: 0, text: "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.", correct: false },
                { id: 1, text: "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to", correct: false },
                { id: 2, text: "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header", correct: false },
                { id: 3, text: "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo ensure that all objects uploaded to an Amazon S3 bucket are encrypted, the solutions architect should update the bucket policy to deny any PutObject requests that do not have an x- amz-server-side-encryption header set. This will prevent any objects from being uploaded to the bucket unless they are encrypted using server-side encryption. https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A solutions architect is designing a multi-tier application for a company. The application's users \nupload images from a mobile device. The application generates a thumbnail of each image and \nreturns a message to the user to confirm that the image was uploaded successfully. \n \nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster \nresponse time to its users to notify them that the original image was received. The solutions \narchitect must design the application to asynchronously dispatch requests to the different \napplication tiers. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the", correct: false },
                { id: 1, text: "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration", correct: false },
                { id: 2, text: "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are", correct: true },
                { id: 3, text: "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 19,
            text: "A solution architect needs to assign a new microsoft for a company's application. Clients must be \nable to call an HTTPS endpoint to reach the micoservice. The microservice also must use AWS \nidentity and Access Management (IAM) to authentication calls. The soltions architect will write the \nlogic for this microservice by using a single AWS Lambda function that is written in Go 1.x. \nWhich solution will deploy the function in the in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n203",
            options: [
                { id: 0, text: "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.", correct: true },
                { id: 1, text: "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribuion. Deploy the function to CloudFront Functions. Specify", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A company wants to implement a disaster recovery plan for its primary on-premises file storage \nvolume. The file storage volume is mounted from an Internet Small Computer Systems Interface \n(iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes \n(TB) of data. \n \nThe company wants to ensure that end users retain immediate access to all file types from the \non-premises systems without experiencing latency. \n \nWhich solution will meet these requirements with the LEAST amount of change to the company's \nexisting infrastructure?",
            options: [
                { id: 0, text: "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set", correct: false },
                { id: 1, text: "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all", correct: false },
                { id: 2, text: "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10", correct: false },
                { id: 3, text: "Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A company is hosting a web application from an Amazon S3 bucket. The application uses \nAmazon Cognito as an identity provider to authenticate users and return a JSON Web Token \n(JWT) that provides access to protected resources that are stored in another S3 bucket. \n \nUpon deployment of the application, users report errors and are unable to access the protected \ncontent. A solutions architect must resolve this issue by providing proper permissions so that \nusers can access the protected content. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n204 \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the", correct: true },
                { id: 1, text: "Update the S3 ACL to allow the application to access the protected content.", correct: false },
                { id: 2, text: "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket", correct: false },
                { id: 3, text: "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/tutorial-create-identity-pool.html You have to create an custom role such as read-only.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "An image hosting company uploads its large assets to Amazon S3 Standard buckets. The \ncompany uses multipart upload in parallel by using S3 APIs and overwrites if the same object is \nuploaded again. For the first 30 days after upload, the objects will be accessed frequently. The \nobjects will be used less frequently after 30 days, but the access patterns for each object will be \ninconsistent. The company must optimize its S3 storage costs while maintaining high availability \nand resiliency of stored assets. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
            options: [
                { id: 0, text: "Move assets to S3 Intelligent-Tiering after 30 days.", correct: true },
                { id: 1, text: "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.", correct: false },
                { id: 2, text: "Configure an S3 Lifecycle policy to clean up expired object delete markers.", correct: false },
                { id: 3, text: "Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.", correct: false },
                { id: 4, text: "Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nS3 Intelligent-Tiering - Data with unknown, changing, or unpredictable access patterns and moves objects that have not been accessed in 30 consecutive days to the Infrequent Access tier. https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 \ninstances contain highly sensitive data and run in a private subnet. According to company policy, \nthe EC2 instances that run in the VPC can access only approved third-party software repositories \non the internet for software product updates that use the third party's URL. Other internet traffic \nmust be blocked. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Update the route table for the private subnet to route the outbound traffic to an AWS Network", correct: true },
                { id: 1, text: "Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on", correct: false },
                { id: 2, text: "Implement strict inbound security group rules. Configure an outbound rule that allows traffic only", correct: false },
                { id: 3, text: "Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSend the outbound connection from EC2 to Network Firewall. In Network Firewall, create stateful outbound rules to allow certain domains for software patch download and deny all other domains. https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata- examples.html#suricata-example-domain-filtering\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
            options: [
                { id: 0, text: "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2", correct: false },
                { id: 1, text: "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto", correct: false },
                { id: 2, text: "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache", correct: false },
                { id: 3, text: "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nStatic content can include images and style sheets that are the same across all users and are best cached at the edges of the content distribution network (CDN). Dynamic content includes information that changes frequently or is personalized based on user preferences, behavior, location or other factors - all content is sales requests.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions \narchitect needs to provide a solution that will run regular security scans across a large fleet of \nEC2 instances. The solution should also patch the EC2 instances on a regular schedule and \nprovide a report of each instance's patch status. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on", correct: false },
                { id: 1, text: "Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for", correct: false },
                { id: 2, text: "Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an", correct: false },
                { id: 3, text: "Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on Amazon Web Services (AWS). It automatically assesses applications for vulnerabilities or deviations from best practices. Amazon Inspector can be used to identify security issues and recommend fixes for them. It is an ideal solution for running regular security scans across a large fleet of EC2 instances. AWS Systems Manager Patch Manager is a service that helps you automate the process of patching Windows and Linux instances. It provides a simple, automated way to patch your instances with the latest security patches and updates. Patch Manager helps you maintain compliance with security policies and regulations by providing detailed reports on the patch status of your instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A company is planning to store data on Amazon RDS DB instances. The company must encrypt \nthe data at rest. \n \nWhat should a solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB", correct: true },
                { id: 1, text: "Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB", correct: false },
                { id: 2, text: "Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances", correct: false },
                { id: 3, text: "Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo encrypt data at rest in Amazon RDS, you can use the encryption feature of Amazon RDS, which uses AWS Key Management Service (AWS KMS). With this feature, Amazon RDS encrypts each database instance with a unique key. This key is stored securely by AWS KMS. You can manage your own keys or use the default AWS-managed keys. When you enable encryption for a DB instance, Amazon RDS encrypts the underlying storage, including the automated backups, read replicas, and snapshots.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The \ncompany's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Snowball.", correct: true },
                { id: 1, text: "Use AWS DataSync.", correct: false },
                { id: 2, text: "Use a secure VPN connection.", correct: false },
                { id: 3, text: "Use Amazon S3 Transfer Acceleration.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Snowball is a secure data transport solution that accelerates moving large amounts of data into and out of the AWS cloud. It can move up to 80 TB of data at a time, and provides a network bandwidth of up to 50 Mbps, so it is well-suited for the task. Additionally, it is secure and easy to use, making it the ideal solution for this migration. https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company needs to provide its employees with secure access to confidential and sensitive files. \nThe company wants to ensure that the files can be accessed only by authorized users. The files \nmust be downloaded securely to the employees' devices. \nThe files are stored in an on-premises Windows file server. However, due to an increase in \nremote usage, the file server is running out of capacity. \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security", correct: false },
                { id: 1, text: "Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon", correct: true },
                { id: 2, text: "Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow", correct: false },
                { id: 3, text: "Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIt provides a secure way for employees to access confidential and sensitive files from anywhere using AWS Client VPN. The Amazon FSx for Windows File Server file system is designed to provide native support for Windows file system features such as NTFS permissions, Active Directory integration, and Distributed File System (DFS). This means that the company can continue to use their on-premises Active Directory to manage user access to files.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A company's application runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability \nZones. On the first day of every month at midnight, the application becomes much slower when \nthe month-end financial calculation batch runs. This causes the CPU utilization of the EC2 \ninstances to immediately peak to 100%, which disrupts the application. \n \nWhat should a solutions architect recommend to ensure the application is able to handle the \nworkload and avoid downtime?",
            options: [
                { id: 0, text: "Configure an Amazon CloudFront distribution in front of the ALB.", correct: false },
                { id: 1, text: "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.", correct: false },
                { id: 2, text: "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.", correct: true },
                { id: 3, text: "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "A company wants to give a customer the ability to use on-premises Microsoft Active Directory to \ndownload files that are stored in Amazon S3. The customer's application uses an SFTP client to \ndownload the files. \n \nWhich solution will meet these requirements with the LEAST operational overhead and no \nchanges to the customer's application?",
            options: [
                { id: 0, text: "Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory", correct: true },
                { id: 1, text: "Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with", correct: false },
                { id: 2, text: "Set up AWS DataSync to synchronize between the on-premises location and the S3 location by", correct: false },
                { id: 3, text: "Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/transfer/latest/userguide/directory-services-users.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A company is experiencing sudden increases in demand. The company needs to provision large \nAmazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto \nScaling group. The company needs a solution that provides minimum initialization latency to meet \nthe demand. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step", correct: false },
                { id: 1, text: "Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot.", correct: true },
                { id: 2, text: "Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon", correct: false },
                { id: 3, text: "Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nEnabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows you to quickly create a new Amazon Machine Image (AMI) from a snapshot, which can help reduce the initialization latency when provisioning new instances. Once the AMI is provisioned, you can replace the AMI in the Auto Scaling group with the new AMI. This will ensure that new instances are launched from the updated AMI and are able to meet the increased demand quickly. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for \nstorage. The application tier is hosted on Amazon EC2 instances. The company's IT security \nguidelines mandate that the database credentials be encrypted and rotated every 14 days. \n \nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
            options: [
                { id: 0, text: "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets", correct: true },
                { id: 1, text: "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a", correct: false },
                { id: 2, text: "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)", correct: false },
                { id: 3, text: "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo implement password rotation lifecycles, use AWS Secrets Manager. You can rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle using Secrets Manager. https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials- amazon-rds-database-types-oracle/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 33,
            text: "A company has deployed a web application on AWS. The company hosts the backend database \non Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling \nneeds. The read replicas must lag no more than 1 second behind the primary DB instance. The \ndatabase routinely runs scheduled stored procedures. \n \nAs traffic on the website increases, the replicas experience additional lag during periods of peak \nload. A solutions architect must reduce the replication lag as much as possible. The solutions \narchitect must minimize changes to the application code and must minimize ongoing operational \noverhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas,", correct: true },
                { id: 1, text: "Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application", correct: false },
                { id: 2, text: "Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large,", correct: false },
                { id: 3, text: "Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can scale reads for your Amazon RDS for PostgreSQL DB instance by adding read replicas to the instance. As with other Amazon RDS database engines, RDS for PostgreSQL uses the native replication mechanisms of PostgreSQL to keep read replicas up to date with changes on the source DB. For general information about read replicas and Amazon RDS, see Working with read replicas. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.Re adReplicas.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a \nservice (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB \ncluster. \nThe DR plan must replicate data to a secondary AWS Region. \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB", correct: false },
                { id: 1, text: "Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB", correct: false },
                { id: 2, text: "Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora", correct: false },
                { id: 3, text: "Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAn Aurora DB cluster can contain up to 15 Aurora Replicas. The Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans WITHIN an AWS Region. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html You can replicate data across multiple Regions by using an Aurora global database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "A company has a custom application with embedded credentials that retrieves information from \nan Amazon RDS MySQL DB instance. Management says the application must be made more \nsecure with the least amount of programming effort. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load", correct: false },
                { id: 1, text: "Create credentials on the RDS for MySQL database for the application user and store the", correct: false },
                { id: 2, text: "Create credentials on the RDS for MySQL database for the application user and store the", correct: true },
                { id: 3, text: "Create credentials on the RDS for MySQL database for the application user and store the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://ws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically- with-aws-secrets-manager/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "A media company hosts its website on AWS. The website application's architecture includes a \nfleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is \nhosted on Amazon Aurora. The company's cybersecurity team reports that the application is \nvulnerable to SQL injection. \n \nHow should the company resolve this issue?",
            options: [
                { id: 0, text: "Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.", correct: true },
                { id: 1, text: "Create an ALB listener rule to reply to SQL injections with a fixed response.", correct: false },
                { id: 2, text: "Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.", correct: false },
                { id: 3, text: "Set up Amazon Inspector to block all SQL injection attempts automatically.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nProtect against SQL injection and cross-site scripting To protect your applications against SQL injection and cross-site scripting (XSS) attacks, use the built-in SQL injection and cross-site scripting engines. Remember that attacks can be performed on different parts of the HTTP request, such as the HTTP header, query string, or URI. Configure the AWS WAF rules to inspect different parts of the HTTP request against the built-in mitigation engines.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company \nwants to create a visualization in Amazon QuickSight by joining the data in the data lake with \noperational data that is stored in an Amazon Aurora MySQL database. The company wants to \nenforce column-level authorization so that the company's marketing team can access only a \nsubset of columns in the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine.", correct: false },
                { id: 1, text: "Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM", correct: false },
                { id: 2, text: "Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3.", correct: false },
                { id: 3, text: "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThis solution leverages AWS Lake Formation to ingest data from the Aurora MySQL database into the S3 data lake, while enforcing column-level access control for QuickSight users. Lake Formation can be used to create and manage the data lake's metadata and enforce security and governance policies, including column-level access control. This solution then uses Amazon Athena as the data source in QuickSight to query the data in the S3 data lake. This solution minimizes operational overhead by leveraging AWS services to manage and secure the data, and by using a standard query service (Amazon Athena) to provide a SQL interface to the data. https://aws.amazon.com/blogs/big-data/enforce-column-level-authorization-with-amazon- quicksight-and-aws-lake-formation/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 \ninstances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, \nbut the baseline CPU utilization that is noted on each run is at least 60%. The company needs to \nprovision the capacity 30 minutes before the jobs run. \n \nCurrently, engineers complete this task by manually modifying the Auto Scaling group \nparameters. The company does not have the resources to analyze the required capacity trends \nfor the Auto Scaling group counts. The company needs an automated way to modify the Auto \nScaling group's desired capacity. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based", correct: false },
                { id: 1, text: "Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired", correct: false },
                { id: 2, text: "Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based", correct: true },
                { id: 3, text: "Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 39,
            text: "A solutions architect is designing a company's disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n213",
            options: [
                { id: 0, text: "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the", correct: false },
                { id: 1, text: "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read", correct: false },
                { id: 2, text: "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster", correct: true },
                { id: 3, text: "Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to \nparse messages. The application cannot parse messages that are larger than 256 KB in size. \nThe company wants to implement a solution to give the application the ability to parse messages \nas large as 50 MB. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
            options: [
                { id: 0, text: "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256", correct: true },
                { id: 1, text: "Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.", correct: false },
                { id: 2, text: "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.", correct: false },
                { id: 3, text: "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS).", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo send messages larger than 256 KiB, you can use the Amazon SQS Extended Client Library for Java. This library allows you to send an Amazon SQS message that contains a reference to a message payload in Amazon S3. The maximum payload size is 2 GB. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas- messages.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A company wants to restrict access to the content of one of its main web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture and an authentication solution for fewer than 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as the company's user base grows while providing the lowest login \nlatency possible. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon", correct: true },
                { id: 1, text: "Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda", correct: false },
                { id: 2, text: "Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3", correct: false },
                { id: 3, text: "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon CloudFront is a global content delivery network (CDN) service that can securely deliver web content, videos, and APIs at scale. It integrates with Cognito for authentication and with Lambda@Edge for authorization, making it an ideal choice for serving web content globally. Lambda@Edge is a service that lets you run AWS Lambda functions globally closer to users, providing lower latency and faster response times. It can also handle authorization logic at the edge to secure content in CloudFront. For this scenario, Lambda@Edge can provide authorization for the web application while leveraging the low-latency benefit of running at the edge.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A company has an aging network-attached storage (NAS) array in its data center. The NAS array \npresents SMB shares and NFS shares to client workstations. The company does not want to \npurchase a new NAS array. The company also does not want to incur the cost of renewing the \nNAS array's support contract. Some of the data is accessed frequently, but much of the data is \ninactive. \n \nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 \nLifecycle policies, and maintains the same look and feel for the client workstations. The solutions \narchitect has identified AWS Storage Gateway as part of the solution. \n \nWhich type of storage gateway should the solutions architect provision to meet these \nrequirements?",
            options: [
                { id: 0, text: "Volume Gateway", correct: false },
                { id: 1, text: "Tape Gateway", correct: false },
                { id: 2, text: "Amazon FSx File Gateway", correct: false },
                { id: 3, text: "Amazon S3 File Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon S3 File Gateway provides a file interface to objects stored in S3. It can be used for a file- based interface with S3, which allows the company to migrate their NAS array data to S3 while maintaining the same look and feel for client workstations. Amazon S3 File Gateway supports SMB and NFS protocols, which will allow clients to continue to access the data using these protocols. Additionally, Amazon S3 Lifecycle policies can be used to automate the movement of data to lower-cost storage tiers, reducing the storage cost of inactive data. https://aws.amazon.com/about-aws/whats-new/2018/06/aws-storage-gateway-adds-smb-support- to-store-objects-in-amazon-s3/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A company has an application that is running on Amazon EC2 instances. A solutions architect \nhas standardized the company on a particular instance family and various instance sizes based \non the current needs of the company. \n \nThe company wants to maximize cost savings for the application over the next 3 years. The \ncompany needs to be able to change the instance family and sizes in the next 6 months based on \napplication popularity and usage. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n215 \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Compute Savings Plan", correct: true },
                { id: 1, text: "EC2 Instance Savings Plan", correct: false },
                { id: 2, text: "Zonal Reserved Instances", correct: false },
                { id: 3, text: "Standard Reserved Instances", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCompute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage. EC2 Instance Savings Plans provide the lowest prices, offering savings up to 72% in exchange for commitment to usage of individual instance families in a Region https://aws.amazon.com/savingsplans/compute-pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A company collects data from a large number of participants who use wearable devices. The \ncompany stores the data in an Amazon DynamoDB table and uses applications to analyze the \ndata. The data workload is constant and predictable. The company wants to stay at or below its \nforecasted budget for DynamoDB. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA).", correct: false },
                { id: 1, text: "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).", correct: true },
                { id: 2, text: "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high", correct: false },
                { id: 3, text: "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n\"The data workload is constant and predictable.\" https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html \"With provisioned capacity you pay for the provision of read and write capacity units for your DynamoDB tables. Whereas with DynamoDB on-demand you pay per request for the data reads and writes that your application performs on your tables.\"\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 45,
            text: "A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-\nsoutheast-3 Region. The database is encrypted with an AWS Key Management Service (AWS \nKMS) customer managed key. The company was recently acquired and must securely share a \nbackup of the database with the acquiring company's AWS account in ap-southeast-3. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new", correct: false },
                { id: 1, text: "Create a database snapshot. Add the acquiring company's AWS account to the KMS key policy.", correct: true },
                { id: 2, text: "Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring", correct: false },
                { id: 3, text: "Create a database snapshot. Download the database snapshot. Upload the database snapshot to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the \nus-east-1 Region to store customer transactions. The company needs high availability and \nautomatic recovery for the DB instance. \n \nThe company must also run reports on the RDS database several times a year. The report \nprocess causes transactions to take longer than usual to post to the customers' accounts. The \ncompany needs a solution that will improve the performance of the report process. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.", correct: true },
                { id: 1, text: "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in", correct: false },
                { id: 2, text: "Create a read replica of the DB instance in a different Availability Zone. Point all requests for", correct: false },
                { id: 3, text: "Migrate the database to RDS Custom.", correct: false },
                { id: 4, text: "Use RDS Proxy to limit reporting requests to the maintenance window.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 47,
            text: "A company is moving its data management application to AWS. The company wants to transition \nto an event-driven architecture. The architecture needs to be more distributed and to use \nserverless concepts while performing the different aspects of the workflow. The company also \nwants to minimize operational overhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process", correct: false },
                { id: 1, text: "Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances.", correct: false },
                { id: 2, text: "Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda", correct: false },
                { id: 3, text: "Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A company is designing the network for an online multi-player game. The game uses the UDP \nnetworking protocol and will be deployed in eight AWS Regions. The network architecture needs \nto minimize latency and packet loss to give end users a high-quality gaming experience. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Setup a transit gateway in each Region. Create inter-Region peering attachments between each", correct: false },
                { id: 1, text: "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.", correct: true },
                { id: 2, text: "Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.", correct: false },
                { id: 3, text: "Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nGlobal Accelerator supports the User Datagram Protocol (UDP) and Transmission Control Protocol (TCP), making it an excellent choice for an online multi-player game using UDP networking protocol. By setting up Global Accelerator with UDP listeners and endpoint groups in each Region, the network architecture can minimize latency and packet loss, giving end users a high-quality gaming experience.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A company hosts a three-tier web application on Amazon EC2 instances in a single Availability \nZone. The web application uses a self-managed MySQL database that is hosted on an EC2 \ninstance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL \ndatabase currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects \ntraffic of 1,000 IOPS for both reads and writes at peak traffic. \n \nThe company wants to minimize any disruptions, stabilize performance, and reduce costs while \nretaining the capacity for double the IOPS. The company wants to move the database tier to a \nfully managed solution that is highly available and fault tolerant. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block", correct: false },
                { id: 1, text: "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose", correct: true },
                { id: 2, text: "Use Amazon S3 Intelligent-Tiering access tiers.", correct: false },
                { id: 3, text: "Use two large EC2 instances to host the database in active-passive mode.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nRDS does not support IO2 or IO2express . GP2 can do the required IOPS. RDS supported Storage > https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html GP2 max IOPS > https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/general-purpose.html#gp2- performance Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 50,
            text: "A company hosts a serverless application on AWS. The application uses Amazon API Gateway, \nAWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase \nin application errors that result from database connection timeouts during times of peak traffic or \nunpredictable traffic. The company needs a solution that reduces the application failures with the \nleast amount of change to the code. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Reduce the Lambda concurrency rate.", correct: false },
                { id: 1, text: "Enable RDS Proxy on the RDS DB instance.", correct: true },
                { id: 2, text: "Resize the RDS DB instance class to accept more connections.", correct: false },
                { id: 3, text: "Migrate the database to Amazon DynamoDB with on-demand scaling.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/rds/proxy/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "A company is migrating an old application to AWS. The application runs a batch job every hour \nand is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. \nThe server has 64 virtual CPU (vCPU) and 512 GiB of memory. \n \nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Lambda with functional scaling.", correct: false },
                { id: 1, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.", correct: false },
                { id: 2, text: "Use Amazon Lightsail with AWS Auto Scaling.", correct: false },
                { id: 3, text: "Use AWS Batch on Amazon EC2.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Batch is a fully-managed service that can launch and manage the compute resources needed to execute batch jobs. It can scale the compute environment based on the size and timing of the batch jobs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 52,
            text: "A company stores its data objects in Amazon S3 Standard storage. A solutions architect has \nfound that 75% of the data is rarely accessed after 30 days. The company needs all the data to \nremain immediately accessible with the same high availability and resiliency, but the company \nwants to minimize storage costs. \n \nWhich storage solution will meet these requirements?",
            options: [
                { id: 0, text: "Move the data objects to S3 Glacier Deep Archive after 30 days.", correct: false },
                { id: 1, text: "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.", correct: true },
                { id: 2, text: "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.", correct: false },
                { id: 3, text: "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nMove the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days - will meet the requirements of keeping the data immediately accessible with high availability and resiliency, while minimizing storage costs. S3 Standard-IA is designed for infrequently accessed data, and it provides a lower storage cost than S3 Standard, while still offering the same low latency, high throughput, and high durability as S3 Standard.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A company has a three-tier application on AWS that ingests sensor data from its users’ devices. \nThe traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the \nweb tier, and finally to EC2 instances for the application tier. The application tier makes calls to a \ndatabase. \n \nWhat should a solutions architect do to improve the security of the data in transit?",
            options: [
                { id: 0, text: "Configure a TLS listener. Deploy the server certificate on the NLB.", correct: true },
                { id: 1, text: "Configure AWS Shield Advanced. Enable AWS WAF on the NLB.", correct: false },
                { id: 2, text: "Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.", correct: false },
                { id: 3, text: "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nNetwork Load Balancers now support TLS protocol. With this launch, you can now offload resource intensive decryption/encryption from your application servers to a high throughput, and low latency Network Load Balancer. Network Load Balancer is now able to terminate TLS traffic and set up connections with your targets either over TCP or TLS protocol. https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html https://exampleloadbalancer.com/nlbtls_demo.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A social media company runs its application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The \napplication has more than a billion images stored in an Amazon S3 bucket and processes \nthousands of images each second. The company wants to resize the images dynamically and \nserve appropriate formats to clients. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Install an external image management library on an EC2 instance. Use the image management", correct: false },
                { id: 1, text: "Create a CloudFront origin request policy. Use the policy to automatically resize images and to", correct: false },
                { id: 2, text: "Use a Lambda@Edge function with an external image management library. Associate the", correct: true },
                { id: 3, text: "Create a CloudFront response headers policy. Use the policy to automatically resize images and", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/cn/blogs/networking-and-content-delivery/resizing-images-with-amazon- cloudfront-lambdaedge-aws-cdn-blog/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 55,
            text: "A hospital needs to store patient records in an Amazon S3 bucket. The hospital's compliance \nteam must ensure that all protected health information (PHI) is encrypted in transit and at rest. \nThe compliance team must administer the encryption key for data at rest. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate", correct: false },
                { id: 1, text: "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted", correct: false },
                { id: 2, text: "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted", correct: true },
                { id: 3, text: "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nIt allows the compliance team to manage the KMS keys used for server-side encryption, thereby providing the necessary control over the encryption keys. Additionally, the use of the \"aws:SecureTransport\" condition on the bucket policy ensures that all connections to the S3 bucket are encrypted in transit.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A data analytics company wants to migrate its batch processing system to AWS. The company \nreceives thousands of small data files periodically during the day through FTP. A on-premises \nbatch job processes the data files overnight. However, the batch job takes hours to finish running. \nThe company wants the AWS solution to process incoming data files are possible with minimal \nchanges to the FTP clients that send the files. The solution must delete the incoming data files \nthe files have been processed successfully. Processing for each file needs to take 3-8 minutes. \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in", correct: false },
                { id: 1, text: "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon", correct: false },
                { id: 2, text: "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic", correct: true },
                { id: 3, text: "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A company is migrating a Linux-based web server group to AWS. The web servers must access \nfiles in a shared file store for some content. The company must not make any changes to the \napplication. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon S3 Standard bucket with access to the web servers.", correct: false },
                { id: 1, text: "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.", correct: false },
                { id: 2, text: "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on", correct: true },
                { id: 3, text: "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers, a business tier, and a database tier with Microsoft SQL Server. \nThe company wants to use specific features of SQL Server such as native backups and Data \nQuality Services. The company also needs to share files for process between the tiers. \n \nHow should a solution architect design the architecture to meet these requirements?",
            options: [
                { id: 0, text: "Host all three on Amazon instances. Use Mmazon FSx File Gateway for file sharing between", correct: false },
                { id: 1, text: "Host all three on Amazon EC2 instances. Use Amazon FSx for Windows file sharing between the", correct: true },
                { id: 2, text: "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier", correct: false },
                { id: 3, text: "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nData Quality Services: If this feature is critical to your workload, consider choosing Amazon RDS Custom or Amazon EC2. https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/comparison.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The \ncompany has VPCs with public subnets and private subnets in its AWS account. The EC2 \ninstances run in a private subnet in one of the VPCs. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for at least 1 year. The company expects the number of Lambda functions \nthat the application uses to increase during that time. The company wants to maximize its savings \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n222 \non all application resources and to keep network latency between the services low. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and", correct: false },
                { id: 1, text: "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and", correct: false },
                { id: 2, text: "Purchase a Compute Savings Plan. Optimize the Lambda functions duration and memory usage,", correct: true },
                { id: 3, text: "Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage,", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy purchasing a Compute Savings Plan, the company can save on the costs of running both EC2 instances and Lambda functions. The Lambda functions can be connected to the private subnet that contains the EC2 instances through a VPC endpoint for AWS services or a VPC peering connection. This provides direct network access to the EC2 instances while keeping the traffic within the private network, which helps to minimize network latency. Optimizing the Lambda functions’ duration, memory usage, number of invocations, and amount of data transferred can help to further minimize costs and improve performance. Additionally, using a private subnet helps to ensure that the EC2 instances are not directly accessible from the public internet, which is a security best practice.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company is building a mobile app on AWS. The company wants to expand its reach to millions \nof users. The company needs to build a platform so that authorized users can watch the \ncompany's content on their mobile devices. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS)", correct: false },
                { id: 1, text: "Set up IPsec VPN between the mobile app and the AWS environment to stream content.", correct: false },
                { id: 2, text: "Use Amazon CloudFront Provide signed URLs to stream content.", correct: true },
                { id: 3, text: "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront supports signed URLs that provide authorized access to your content. This feature allows the company to control who can access their content and for how long, providing a secure and scalable solution for millions of users. https://www.amazonaws.cn/en/cloudfront/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 61,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n223 \nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
            options: [
                { id: 0, text: "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2", correct: false },
                { id: 1, text: "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto", correct: false },
                { id: 2, text: "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache", correct: false },
                { id: 3, text: "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nStatic content can include images and style sheets that are the same across all users and are best cached at the edges of the content distribution network (CDN). Dynamic content includes information that changes frequently or is personalized based on user preferences, behavior, location or other factors - all content is sales requests.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company’s web application consists of an Amazon API Gateway API in front of an AWS \nLambda function and an Amazon DynamoDB database. The Lambda function handles the \nbusiness logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito \nuser pools to identify the individual users of the application. A solutions architect needs to update \nthe application so that only users who have a subscription can access premium content. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable API caching and throttling on the API Gateway API.", correct: false },
                { id: 1, text: "Set up AWS WAF on the API Gateway API Create a rule to filter users who have a subscription.", correct: false },
                { id: 2, text: "Apply fine-grained IAM permissions to the premium content in the DynamoDB table.", correct: false },
                { id: 3, text: "Implement API usage plans and API keys to limit the access of users who do not have a", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo meet the requirement with the least operational overhead, you can implement API usage plans and API keys to limit the access of users who do not have a subscription. This way, you can control access to your API Gateway APIs by requiring clients to submit valid API keys with requests. You can associate usage plans with API keys to configure throttling and quota limits on individual client accounts. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage- plans.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company's application runs on AWS. The application stores large documents in an Amazon S3 \nbucket that uses the S3 Standard-infrequent Access (S3 Standerd-IA) storage class. The \ncompany will continue paying to store the data but wants to save on its total S3 costs. The \ncompany wants authorized external users to have the ability to access the documents in \nmilliseconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure the S3 bucket to be a Requester Pays bucket.", correct: false },
                { id: 1, text: "Change the storage tier to S3 Standard for all existing and future objects.", correct: false },
                { id: 2, text: "Turn on S3 Transfer Acceleration tor the S3 Docket.", correct: false },
                { id: 3, text: "Use Amazon CloudFront to handle all the requests to the S3 bucket.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A company recently created a disaster recovery site in a different AWS Region. The company \nneeds to transfer large amounts of data back and forth between NFS file systems in the two \nRegions on a periodic basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS DataSync.", correct: true },
                { id: 1, text: "Use AWS Snowball devices", correct: false },
                { id: 2, text: "Set up an SFTP server on Amazon EC2", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies moving large amounts of data between on-premises storage systems and AWS services. It can also transfer data between different AWS services, including different AWS Regions. DataSync provides a simple, scalable, and automated solution to transfer data, and it minimizes the operational overhead because it is fully managed by AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test16: [
        {
            id: 0,
            text: "A company has an On-premises volume backup solution that has reached its end of life. The \ncompany wants to use AWS as part of a new backup solution and wants to maintain local access \nto all the data while it is backed up on AWS. The company wants to ensure that the data backed \nup on AWS is automatically and securely transferred. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-", correct: false },
                { id: 1, text: "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the", correct: false },
                { id: 2, text: "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway", correct: false },
                { id: 3, text: "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage software", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company runs an application on Amazon EC2 instances. The company needs to implement a \ndisaster recovery (DR) solution for the application. The DR solution needs to have a recovery \ntime objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible \nAWS resources during normal operations. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a", correct: false },
                { id: 1, text: "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a", correct: true },
                { id: 2, text: "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary", correct: false },
                { id: 3, text: "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nOption B would be the most operationally efficient solution for implementing a DR solution for the application, meeting the requirement of an RTO of less than 4 hours and using the fewest possible AWS resources during normal operations. By creating Amazon Machine Images (AMIs) to back up the EC2 instances and copying them to a secondary AWS Region, the company can ensure that they have a reliable backup in the event of a disaster. By using AWS CloudFormation to automate infrastructure deployment in the secondary Region, the company can minimize the amount of time and effort required to set up the DR solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company hosts a multiplayer gaming application on AWS. The company wants the application \nto read data with sub-millisecond latency and run one-time queries on historical data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the", correct: false },
                { id: 1, text: "Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older", correct: false },
                { id: 2, text: "Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed.", correct: true },
                { id: 3, text: "Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nDynamoDB supports some of the world's largest scale applications by providing consistent, single-digit millisecond response times at any scale. You can build applications with virtually unlimited throughput and storage. https://aws.amazon.com/dynamodb/dax/?nc1=h_ls\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "A company has a regional subscription-based streaming service that runs in a single AWS \nRegion. The architecture consists of web servers and application servers on Amazon EC2 \ninstances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The \narchitecture includes an Amazon Aurora database cluster that extends across multiple Availability \nZones. \n \nThe company wants to expand globally and to ensure that its application has minimal downtime.",
            options: [
                { id: 0, text: "Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in", correct: true },
                { id: 1, text: "Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL", correct: false },
                { id: 2, text: "Deploy the web tier and the applicatin tier to a second Region. Create an Aurora PostSQL", correct: false },
                { id: 3, text: "Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. \nThe company does not want to use the default domain name for the distribution. Instead, the \ncompany wants to use a different domain name for the distribution. \n \nWhich solution will deploy the certificate with icurring any additional costs?",
            options: [
                { id: 0, text: "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-", correct: false },
                { id: 1, text: "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-", correct: true },
                { id: 2, text: "Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-east-", correct: false },
                { id: 3, text: "Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A solutions architect is designing a company’s disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the", correct: false },
                { id: 1, text: "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read", correct: false },
                { id: 2, text: "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster", correct: true },
                { id: 3, text: "Store the schedule backup of the MySQL database in an Amazon S3 bucket that is configured for", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A rapidly growing global ecommerce company is hosting its web application on AWS. The web \napplication includes static content and dynamic content. The website stores online transaction \nprocessing (OLTP) data in an Amazon RDS database. The website's users are experiencing slow \npage loads. \n \nWhich combination of actions should a solutions architect take to resolve this issue? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Configure an Amazon Redshift cluster.", correct: false },
                { id: 1, text: "Set up an Amazon CloudFront distribution", correct: true },
                { id: 2, text: "Host the dynamic web content in Amazon S3", correct: false },
                { id: 3, text: "Create a read replica for the RDS DB instance.", correct: false },
                { id: 4, text: "Configure a Multi-AZ deployment for the RDS DB instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTo resolve the issue of slow page loads for a rapidly growing e-commerce website hosted on AWS, a solutions architect can take the following two actions: 1. Set up an Amazon CloudFront distribution 2. Create a read replica for the RDS DB instance Configuring an Amazon Redshift cluster is not relevant to this issue since Redshift is a data warehousing service and is typically used for the analytical processing of large amounts of data. Hosting the dynamic web content in Amazon S3 may not necessarily improve performance since S3 is an object storage service, not a web application server. While S3 can be used to host static Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A solutions architect wants all new users to have specific complexity requirements and mandatory \nrotation periods tor IAM user passwords. \n \nWhat should the solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Set an overall password policy for the entire AWS account.", correct: true },
                { id: 1, text: "Set a password policy for each IAM user in the AWS account.", correct: false },
                { id: 2, text: "Use third-party vendor software to set password requirements.", correct: false },
                { id: 3, text: "Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nTo accomplish this, the solutions architect should set an overall password policy for the entire AWS account. This policy will apply to all IAM users in the account, including new users.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company wants to deploy a new public web application on AWS. The application includes a \nweb server tier that uses Amazon EC2 instances. The application also includes a database tier \nthat uses an Amazon RDS for MySQL DB instance. \n \nThe application must be secure and accessible for global customers that have dynamic IP \naddresses. \n \nHow should a solutions architect configure the security groups to meet these requirements?",
            options: [
                { id: 0, text: "Configure the security group tor the web servers to allow inbound traffic on port 443 from", correct: true },
                { id: 1, text: "Configure the security group for the web servers to allow inbound traffic on port 443 from the IP", correct: false },
                { id: 2, text: "Configure the security group for the web servers to allow inbound traffic on port 443 from the IP", correct: false },
                { id: 3, text: "Configure the security group for the web servers to allow inbound traffic on port 443 from", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A company is planning to migrate a commercial off-the-shelf application from is on-premises data \ncenter to AWS. The software has a software licensing model using sockets and cores with \npredictable capacity and uptime requirements. The company wants to use its existing licenses, \nwhich were purchased earlier this year. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n229 \n \nWhich Amazon EC2 pricing option is the MOST cost-effective?",
            options: [
                { id: 0, text: "Dedicated Reserved Hosts", correct: true },
                { id: 1, text: "Dedicated On-Demand Hosts", correct: false },
                { id: 2, text: "Dedicated Reserved Instances", correct: false },
                { id: 3, text: "Dedicated On-Demand Instances", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDedicated Host Reservations provide a billing discount compared to running On-Demand Dedicated Hosts. Reservations are available in three payment options. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "An ecommerce company is experiencing an increase in user traffic. The company's store is \ndeployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a \nseparate database tier. As traffic increases, the company notices that the architecture is causing \nsignificant delays in sending timely marketing and order confirmation email to users. The \ncompany wants to reduce the time it spends resolving complex email delivery issues and \nminimize operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a separate application tier using EC2 instances dedicated to email processing.", correct: false },
                { id: 1, text: "Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).", correct: true },
                { id: 2, text: "Configure the web instance to send email through Amazon Simple Notification Service (Amazon", correct: false },
                { id: 3, text: "Create a separate application tier using EC2 instances dedicated to email processing. Place the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon SES is a cost-effective and scalable email service that enables businesses to send and receive email using their own email addresses and domains. Configuring the web instance to send email through Amazon SES is a simple and effective solution that can reduce the time spent resolving complex email delivery issues and minimize operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon \nEC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database \ntier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier \nrequires access to the database to retrieve product information. \n \nThe web application is not working as intended. The web application reports that it cannot \nconnect to the database. The database is confirmed to be up and running. All configurations for \nthe network ACLs, security groups, and route tables are still in their default states. \n \nWhat should a solutions architect recommend to fix the application?",
            options: [
                { id: 0, text: "Add an explicit rule to the private subnet's network ACL to allow traffic from the web tier's EC2", correct: false },
                { id: 1, text: "Add a route in the VPC route table to allow traffic between the web tier’s EC2 instances and the", correct: false },
                { id: 2, text: "Deploy the web tier's EC2 instances and the database tier's RDS instance into two separate", correct: false },
                { id: 3, text: "Add an inbound rule to the security group of the database tier's RDS instance to allow traffic from", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nBy default, all inbound traffic to an RDS instance is blocked. Therefore, an inbound rule needs to be added to the security group of the RDS instance to allow traffic from the security group of the web tier's EC2 instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company is running a multi-tier ecommerce web application in the AWS Cloud. The application \nruns on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon \nRDS is configured with the latest generation DB instance with 2,000 GB of storage in a General \nPurpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database \nperformance affects the application during periods of high demand. \n \nA database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the \napplication performance always degrades when the number of read and write IOPS is higher than \n20,000. \n \nWhat should a solutions architect do to improve the application performance?",
            options: [
                { id: 0, text: "Replace the volume with a magnetic volume.", correct: false },
                { id: 1, text: "Increase the number of IOPS on the gp3 volume.", correct: false },
                { id: 2, text: "Replace the volume with a Provisioned IOPS SSD (Io2) volume.", correct: false },
                { id: 3, text: "Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo improve the application performance, you can replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes. This will increase the number of IOPS available to the database and improve performance. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A company is deploying a new application on Amazon EC2 instances. The application writes data \nto Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all \ndata that is written to the EBS volumes is encrypted at rest. \n \nWhich solution will meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.", correct: false },
                { id: 1, text: "Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.", correct: true },
                { id: 2, text: "Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that", correct: false },
                { id: 3, text: "Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWhen you create an EBS volume, you can specify whether to encrypt the volume. If you choose to encrypt the volume, all data written to the volume is automatically encrypted at rest using AWS-managed keys. You can also use customer-managed keys (CMKs) stored in AWS KMS to encrypt and protect your EBS volumes. You can create encrypted EBS volumes and attach them to EC2 instances to ensure that all data written to the volumes is encrypted at rest.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A company is moving its data management application to AWS. The company wants to transition \nto an event-driven architecture. The architecture needs to be more distributed and to use \nserverless concepts while performing the different aspects of the workflow. The company also \nwants to minimize operational overhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process", correct: false },
                { id: 1, text: "Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 Instances.", correct: false },
                { id: 2, text: "Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda", correct: false },
                { id: 3, text: "Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nStep 3: Create a State Machine Use the Step Functions console to create a state machine that invokes the Lambda function that you created earlier in Step 1. https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-creating-lambda-state- machine.html In Step Functions, a workflow is called a state machine, which is a series of event-driven steps. Each step in a workflow is called a state.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "An image-hosting company stores its objects in Amazon S3 buckets. The company wants to \navoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the \nentire AWS account need to remain private. \n \nWhich solution will meal these requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action", correct: false },
                { id: 1, text: "Use AWS Trusted Advisor to find publicly accessible S3 Dockets. Configure email notifications In", correct: false },
                { id: 2, text: "Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple", correct: false },
                { id: 3, text: "Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public- access.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A financial company hosts a web application on AWS. The application uses an Amazon API \nGateway Regional API endpoint to give users the ability to retrieve current stock prices. The \ncompany's security team has noticed an increase in the number of API requests. The security \nteam is concerned that HTTP flood attacks might take the application offline. \nA solutions architect must design a solution to protect the application from this type of attack. \n \nWhich solution meats these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with", correct: false },
                { id: 1, text: "Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the", correct: true },
                { id: 2, text: "Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA rate-based rule in AWS WAF allows the security team to configure thresholds that trigger rate- based rules, which enable AWS WAF to track the rate of requests for a specified time period and then block them automatically when the threshold is exceeded. This provides the ability to prevent HTTP flood attacks with minimal operational overhead. https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A payment processing company records all voice communication with its customers and stores \nthe audio files in an Amazon S3 bucket. The company needs to capture the text from the audio \nfiles. The company must remove from the text any personally identifiable information (Pll) that \nbelongs to customers. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function", correct: false },
                { id: 1, text: "When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an", correct: false },
                { id: 2, text: "Configure an Amazon Transcribe transcription job with Pll redaction turned on. When an audio file", correct: true },
                { id: 3, text: "Create an Amazon Connect contact flow that ingests the audio files with transcription turned on.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A company is migrating its on-premises workload to the AWS Cloud. The company already uses \nseveral Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution \nthat automatically starts and stops the EC2 instances and D6 instances outside of business \nhours. The solution must minimize cost and infrastructure maintenance. \n \nWhich solution will meet these requirement?",
            options: [
                { id: 0, text: "Scale the EC2 instances by using elastic resize Scale the DB instances to zero outside of", correct: false },
                { id: 1, text: "Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2", correct: false },
                { id: 2, text: "Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and", correct: false },
                { id: 3, text: "Create an AWS Lambda function that will start and stop the EC2 instances and DB instances.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe most efficient solution for automatically starting and stopping EC2 instances and DB instances on a schedule while minimizing cost and infrastructure maintenance is to create an AWS Lambda function and configure Amazon EventBridge to invoke the function on a schedule.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 19,
            text: "A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The \ninstances run in an Auto Scaling group behind an Application Load Balancer (ALB). All \necommerce data is stored in an Amazon RDS for ManaDB Multi-AZ DB instance. The company \nwants to optimize customer session management during transactions. The application must store \nsession data durably. \n \nWhich solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Turn on the sticky sessions feature (session affinity) on the ALB", correct: true },
                { id: 1, text: "Use an Amazon DynamoDB table to store customer session information", correct: false },
                { id: 2, text: "Deploy an Amazon Cognito user pool to manage user session information", correct: false },
                { id: 3, text: "Deploy an Amazon ElastiCache for Redis cluster to store customer session information", correct: false },
                { id: 4, text: "Use AWS Systems Manager Application Manager in the application to manage user session", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/caching/session-management/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records \nfor analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be \nup to 10 GB in size. Based on the number of sales events, the job can take up to an hour to \ncomplete. The CPU and memory usage of the job are constant and are known in advance. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n234 \nA solutions architect needs to minimize the amount of operational effort that is needed for the job \nto run. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the", correct: false },
                { id: 1, text: "Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the", correct: false },
                { id: 2, text: "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch", correct: true },
                { id: 3, text: "Create an Amazon Elastic Container Service (Amazon ECS) duster with an Amazon EC2 launch", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A solutions architect must migrate a Windows Internet Information Services (IIS) web application \nto AWS. The application currently relies on a file share hosted in the user's on-premises network-\nattached storage (NAS). The solutions architect has proposed migrating the MS web servers to \nAmazon EC2 instances in multiple Availability Zones that are connected to the storage solution, \nand configuring an Elastic Load Balancer attached to the instances. \nWhich replacement to the on-premises file share is MOST resilient and durable?",
            options: [
                { id: 0, text: "Migrate the file share to Amazon RDS", correct: false },
                { id: 1, text: "Migrate the file share to AWS Storage Gateway", correct: false },
                { id: 2, text: "Migrate the file share to Amazon FSx for Windows File Server", correct: true },
                { id: 3, text: "Migrate the file share to Amazon Elastic File System (Amazon EFS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon FSx makes it easy and cost effective to launch, run, and scale feature-rich, high- performance file systems in the cloud.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A company wants to restrict access to the content of one of its man web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture end an authentication solution for fewer tian 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as to company's user base grows while providing lowest login latency \npossible.",
            options: [
                { id: 0, text: "Use Amazon Cognito tor authentication. Use Lambda#Edge tor authorization. Use Amazon", correct: true },
                { id: 1, text: "Use AWS Directory Service for Microsoft Active Directory tor authentication. Use AWS Lambda", correct: false },
                { id: 2, text: "Usa Amazon Cognito for authentication. Use AWS Lambda tor authorization. Use Amazon S3", correct: false },
                { id: 3, text: "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "An ecommerce company is building a distributed application that involves several serverless \nfunctions and AWS services to complete order-processing tasks. These tasks require manual \napprovals as part of the workflow. A solutions architect needs to design an architecture for the \norder-processing application. The solution must be able to combine multiple AWS Lambda \nfunctions into responsive serverless applications. The solution also must orchestrate data and \nservices that run on Amazon EC2 instances, containers, or on-premises servers. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Step Functions to build the application.", correct: true },
                { id: 1, text: "Integrate all the application components in an AWS Glue job", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to build the application", correct: false },
                { id: 3, text: "Use AWS Lambda functions and Amazon EventBridge (Amazon CloudWatch Events) events to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Step Functions is a fully managed service that makes it easy to build applications by coordinating the components of distributed applications and microservices using visual workflows. With Step Functions, you can combine multiple AWS Lambda functions into responsive serverless applications and orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Step Functions also allows for manual approvals as part of the workflow. This solution meets all the requirements with the least operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based \napplication for users around the world. The application is hosted on redundant servers in the \ncompany's on-premises data centers in the United States. Asia, and Europe. The company's \ncompliance requirements state that the application must be hosted on premises. The company \nwants to improve the performance and availability of the application. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "A Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-", correct: true },
                { id: 1, text: "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-", correct: false },
                { id: 2, text: "Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-", correct: false },
                { id: 3, text: "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nQ: How is AWS Global Accelerator different from Amazon CloudFront? A: AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A company runs an application on Amazon EC2 Linux instances across multiple Availability \nZones. The application needs a storage layer that is highly available and Portable Operating \nSystem Interface (POSIX)-compliant. The storage layer must provide maximum data durability \nand must be shareable across the EC2 instances. The data in the storage layer will be accessed \nfrequently for the first 30 days and will be accessed infrequently after that time. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently", correct: false },
                { id: 1, text: "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently", correct: false },
                { id: 2, text: "Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a Lifecycle", correct: true },
                { id: 3, text: "Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a Lifecycle", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/efs/features/infrequent-access/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "A company wants to migrate its 1 PB on-premises image repository to AWS. The images will be \nused by a serverless web application. Images stored in the repository are rarely accessed, but \nthey must be immediately available Additionally, the images must be encrypted at rest and \nprotected from accidental deletion. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Implement client-side encryption and store the images in an Amazon S3 Glacier vault. Set a vault", correct: false },
                { id: 1, text: "Store the images in an Amazon S3 bucket in the S3 Standard-Infrequent Access (S3 Standard-", correct: true },
                { id: 2, text: "Store the images in an Amazon FSx for Windows File Server file share. Configure the Amazon", correct: false },
                { id: 3, text: "Store the images in an Amazon Elastic File System (Amazon EFS) file share in the Infrequent", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company runs an application that receives data from thousands of geographically dispersed \nremote devices that use UDP. The application processes the data immediately and sends a \nmessage back to the device if necessary. No data is stored. \n \nThe company needs a solution that minimizes latency for the data transmission from the devices. \nThe solution also must provide rapid failover to another AWS Region. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in", correct: false },
                { id: 1, text: "Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions", correct: true },
                { id: 2, text: "Use AWS Global Accelerator Create an Application Load Balancer (ALB) in each of the two", correct: false },
                { id: 3, text: "Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nGeographically dispersed (related to UDP) - Global Accelerator - multiple entrances worldwide to the AWS network to provide better transfer rates. UDP - NLB (Network Load Balancer).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "An ecommerce company is running a multi-tier application on AWS. The front-end and backend \ntiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend \ntier communicates with the RDS instance. There are frequent calls to return identical datasets \nfrom the database that are causing performance slowdowns. \n \nWhich action should be taken to improve the performance of the backend?",
            options: [
                { id: 0, text: "Implement Amazon SNS to store the database calls.", correct: false },
                { id: 1, text: "Implement Amazon ElasticCache to cache the large database.", correct: true },
                { id: 2, text: "Implement an RDS for MySQL read replica to cache database calls.", correct: false },
                { id: 3, text: "Implement Amazon Kinesis Data Firehose to stream the calls to the database.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nKey term is identical datasets from the database it means caching can solve this issue by cached in frequently used dataset from DB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A hospital is designing a new application that gathers symptoms from patients. The hospital has \ndecided to use Amazon Simple Queue Service (Amazon SOS) and Amazon Simple Notification \nService (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure \ndesign Data must be encrypted at test and in transit. Only authorized personnel of the hospital \nshould be able to access the data. \n \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Turn on server-side encryption on the SQS components. Update tie default key policy to restrict", correct: false },
                { id: 1, text: "Turn on server-side encryption on the SNS components by using an AWS Key Management", correct: true },
                { id: 2, text: "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to", correct: false },
                { id: 3, text: "Turn on server-side encryption on the SOS components by using an AWS Key Management", correct: false },
                { id: 4, text: "Turn on server-side encryption on the SOS components by using an AWS Key Management", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nFor a customer managed KMS key, you must configure the key policy to add permissions for each queue producer and consumer. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-key- management.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A solutions architect is creating a new VPC design. There are two public subnets for the load \nbalancer, two private subnets for web servers, and two private subnets for MySQL. The web \nservers use only HTTPS. The solutions architect has already created a security group for the load \nbalancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the \nleast access required to still be able to perform its tasks. \n \nWhich additional configuration strategy should the solutions architect use to meet these \nrequirements?",
            options: [
                { id: 0, text: "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security", correct: false },
                { id: 1, text: "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network", correct: false },
                { id: 2, text: "Create a security group for the web servers and allow port 443 from the load balancer. Create a", correct: true },
                { id: 3, text: "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nLoad balancer is public facing accepting all traffic coming towards the VPC (0.0.0.0/0). The web server needs to trust traffic originating from the ALB. The DB will only trust traffic originating from the Web server on port 3306 for Mysql.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company wants to use Amazon S3 for the secondary copy of its on-premises dataset. The \ncompany would rarely need to access this copy. The storage solution's cost should be minimal. \n \nWhich storage solution meets these requirements?",
            options: [
                { id: 0, text: "S3 Standard", correct: false },
                { id: 1, text: "S3 Intelligent-Tiering", correct: false },
                { id: 2, text: "S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 3, text: "S3 One Zone-Infrequent Access (S3 One Zone-IA)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A solutions architect is designing a two-tiered architecture that includes a public subnet and a \ndatabase subnet. The web servers in the public subnet must be open to the internet on port 443. \nThe Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the \nweb servers on port 3306. \n \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on", correct: false },
                { id: 1, text: "Create a security group for the DB instance. Add a rule to allow traffic from the public subnet", correct: false },
                { id: 2, text: "Create a security group for the web servers in the public subnet. Add a rule to allow traffic from", correct: true },
                { id: 3, text: "Create a security group for the DB instance. Add a rule to allow traffic from the web servers'", correct: false },
                { id: 4, text: "Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 33,
            text: "A company has an application that collects data from loT sensors on automobiles. The data is \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n240 \nstreamed and stored in Amazon S3 through Amazon Kinesis Date Firehose. The data produces \ntrillions of S3 objects each year. Each morning, the company uses the data from the previous 30 \ndays to retrain a suite of machine learning (ML) models. \n \nFour times each year, the company uses the data from the previous 12 months to perform \nanalysis and train other ML models. The data must be available with minimal delay for up to 1 \nyear. After 1 year, the data must be retained for archival purposes. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to", correct: false },
                { id: 1, text: "Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically", correct: false },
                { id: 2, text: "Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle", correct: false },
                { id: 3, text: "Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A company recently deployed a new auditing system to centralize information about operating \nsystem versions patching and installed software for Amazon EC2 instances. A solutions architect \nmust ensure all instances provisioned through EC2 Auto Scaling groups successfully send \nreports to the auditing system as soon as they are launched and terminated. \n \nWhich solution achieves these goals MOST efficiently?",
            options: [
                { id: 0, text: "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send", correct: false },
                { id: 1, text: "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system", correct: true },
                { id: 2, text: "Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send", correct: false },
                { id: 3, text: "Run a custom script on the instance operating system to send data to the audit system. Configure", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon EC2 Auto Scaling offers the ability to add lifecycle hooks to your Auto Scaling groups. These hooks let you create solutions that are aware of events in the Auto Scaling instance lifecycle, and then perform a custom action on instances when the corresponding lifecycle event occurs. https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to \nthe database come from serverless applications. Application traffic to the database changes \nsignificantly at random intervals. At limes of high demand, users report that their applications \nexperience database connection rejection errors. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n241 \n \nWhich solution will resolve this issue with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through", correct: true },
                { id: 1, text: "Deploy Amazon ElastCache for Memcached between the users' application and the DB instance.", correct: false },
                { id: 2, text: "Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the", correct: false },
                { id: 3, text: "Configure Multi-AZ for the DB instance. Configure the users' application to switch between the DB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nMany applications, including those built on modern serverless architectures, can have a large number of open connections to the database server and may open and close database connections at a high rate, exhausting database memory and compute resources. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. https://aws.amazon.com/pt/rds/proxy/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A solutions architect is designing the architecture for a software demonstration environment. The \nenvironment will run on Amazon EC2 instances in an Auto Scaling group behind an Application \nLoad Balancer (ALB). The system will experience significant increases in traffic during working \nhours but Is not required to operate on weekends. \n \nWhich combination of actions should the solutions architect take to ensure that the system can \nscale to meet demand? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Auto Scaling to adjust the ALB capacity based on request rate.", correct: false },
                { id: 1, text: "Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.", correct: false },
                { id: 2, text: "Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.", correct: false },
                { id: 3, text: "Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU", correct: true },
                { id: 4, text: "Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired", correct: false },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A company has deployed a serverless application that invokes an AWS Lambda function when \nnew documents are uploaded to an Amazon S3 bucket. The application uses the Lambda \nfunction to process the documents. After a recent marketing campaign, the company noticed that \nthe application did not process many of the documents. \n \nWhat should a solutions architect do to improve the architecture of this application?",
            options: [
                { id: 0, text: "Set the Lambda function's runtime timeout value to 15 minutes.", correct: false },
                { id: 1, text: "Configure an S3 bucket replication policy. Stage the documents m the S3 bucket for later", correct: false },
                { id: 2, text: "Deploy an additional Lambda function Load balance the processing of the documents across the", correct: false },
                { id: 3, text: "Create an Amazon Simple Queue Service (Amazon SOS) queue. Send the requests to the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo improve the architecture of this application, the best solution would be to use Amazon Simple Queue Service (Amazon SQS) to buffer the requests and decouple the S3 bucket from the Lambda function. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. By using Amazon SQS, the architecture is decoupled and the Lambda function can process the documents in a scalable and fault-tolerant manner.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 \nand needs the required permissions to perform the task. The developer already has an IAM user \nwith valid IAM credentials required for Amazon S3. \n \nWhat should a solutions architect do to grant the permissions?",
            options: [
                { id: 0, text: "Add required IAM permissions in the resource policy of the Lambda function.", correct: false },
                { id: 1, text: "Create a signed request using the existing IAM credentials in the Lambda function", correct: false },
                { id: 2, text: "Create a new IAM user and use the existing IAM credentials in the Lambda function.", correct: false },
                { id: 3, text: "Create an IAM execution role with the required permissions and attach the IAM rote to the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo grant the necessary permissions to an AWS Lambda function to upload files to Amazon S3, a solutions architect should create an IAM execution role with the required permissions and attach the IAM role to the Lambda function. This approach follows the principle of least privilege and ensures that the Lambda function can only access the resources it needs to perform its specific task.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company has a large dataset for its online advertising business stored in an Amazon RDS for \nMySQL DB instance in a single Availability Zone. The company wants business reporting queries \nto run without impacting the write operations to the production DB instance. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Deploy RDS read replicas to process the business reporting queries.", correct: true },
                { id: 1, text: "Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.", correct: false },
                { id: 2, text: "Scale up the DB instance to a larger instance type to handle write operations and queries.", correct: false },
                { id: 3, text: "Deploy the OB distance in multiple Availability Zones to process the business reporting queries.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A meteorological startup company has a custom web application to sell weather data to its users \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n243 \nonline. The company uses Amazon DynamoDB to store is data and wants to build a new service \nthat sends an alert to the managers of four Internal teams every time a new weather event is \nrecorded. The company does not want true new service to affect the performance of the current \napplication. \n \nWhat should a solutions architect do to meet these requirement with the LEAST amount of \noperational overhead?",
            options: [
                { id: 0, text: "Use DynamoDB transactions to write new event data to the table. Configure the transactions to", correct: false },
                { id: 1, text: "Have the current application publish a message to four Amazon Simple Notification Service", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a mingle Amazon", correct: true },
                { id: 3, text: "Add a custom attribute to each record to flag new items. Write a cron job that scans the table", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe best solution to meet these requirements with the least amount of operational overhead is to enable Amazon DynamoDB Streams on the table and use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe. This solution requires minimal configuration and infrastructure setup, and Amazon DynamoDB Streams provide a low-latency way to capture changes to the DynamoDB table. The triggers automatically capture the changes and publish them to the SNS topic, which notifies the internal teams.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company is developing a real-time multiplayer game that uses UDP for communications \nbetween the client and servers. In an Auto Scaling group Spikes in demand are anticipated during \nthe day, so the game server platform must adapt accordingly. Developers want to store gamer \nscores and other non-relational data in a database solution that will scale without intervention. \n \nWhich solution should a solutions architect recommend?",
            options: [
                { id: 0, text: "Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage.", correct: false },
                { id: 1, text: "Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data", correct: true },
                { id: 2, text: "Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data", correct: false },
                { id: 3, text: "Use an Application Load Balancer for traffic distribution and Amazon DynamoDB global tables for", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA Network Load Balancer can handle UDP traffic, and Amazon DynamoDB on-demand can provide automatic scaling without intervention.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host \na digital media streaming application. The EKS cluster will use a managed node group that is \nbacked by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must \nencrypt all data at rest by using a customer managed key that is stored in AWS Key Management \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n244 \nService (AWS KMS). \n \nWhich combination of actions will meet this requirement with the LEAST operational overhead? \n(Choose two.)",
            options: [
                { id: 0, text: "Use a Kubernetes plugin that uses the customer managed key to perform data encryption.", correct: false },
                { id: 1, text: "After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the", correct: false },
                { id: 2, text: "Enable EBS encryption by default in the AWS Region where the EKS cluster will be created.", correct: true },
                { id: 3, text: "Create the EKS cluster. Create an IAM role that has a policy that grants permission to the", correct: false },
                { id: 4, text: "Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 43,
            text: "A company has a web application with sporadic usage patterns. There is heavy usage at the \nbeginning of each month moderate usage at the start of each week and unpredictable usage \nduring the week. The application consists of a web server and a MySQL database server running \ninside the data center. The company would like to move the application to the AWS Cloud and \nneeds to select a cost-effective database platform that will not require database modifications. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Amazon DynamoDB", correct: false },
                { id: 1, text: "Amazon RDS for MySQL", correct: false },
                { id: 2, text: "MySQL-compatible Amazon Aurora Serverless", correct: true },
                { id: 3, text: "MySQL deployed on Amazon EC2 in an Auto Scaling group", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon RDS for MySQL is a fully-managed relational database service that makes it easy to set up, operate, and scale MySQL deployments in the cloud. Amazon Aurora Serverless is an on- demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible edition), where the database will automatically start up, shut down, and scale capacity up or down based on your application’s needs. It is a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A company uses a payment processing system that requires messages for a particular payment \nID to be received in the same order that they were sent. Otherwise, the payments might be \nprocessed incorrectly. \n \nWhich actions should a solutions architect take to meet this requirement? (Choose two.)",
            options: [
                { id: 0, text: "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key", correct: false },
                { id: 1, text: "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.", correct: true },
                { id: 2, text: "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as", correct: false },
                { id: 3, text: "Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the", correct: false },
                { id: 4, text: "Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 45,
            text: "An IAM user made several configuration changes to AWS resources in their company's account \nduring a production deployment last week. A solutions architect learned that a couple of security \ngroup rules are not configured as desired. The solutions architect wants to confirm which IAM \nuser was responsible for making changes. \n \nWhich service should the solutions architect use to find the desired information?",
            options: [
                { id: 0, text: "Amazon GuardDuty", correct: false },
                { id: 1, text: "Amazon Inspector", correct: false },
                { id: 2, text: "AWS CloudTrail", correct: true },
                { id: 3, text: "AWS Config", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe best option is to use AWS CloudTrail to find the desired information. AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of AWS account activities. CloudTrail can be used to log all changes made to resources in an AWS account, including changes made by IAM users, EC2 instances, AWS management console, and other AWS services. By using CloudTrail, the solutions architect can identify the IAM user who made the configuration changes to the security group rules.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company runs a public three-Tier web application in a VPC. The application runs on Amazon \nEC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets \nneed to communicate with a license server over the internet. The company needs a managed \nsolution that minimizes operational maintenance. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Provision a NAT instance in a public subnet. Modify each private subnets route table with a", correct: false },
                { id: 1, text: "Provision a NAT instance in a private subnet. Modify each private subnet's route table with a", correct: false },
                { id: 2, text: "Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a", correct: true },
                { id: 3, text: "Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAs the company needs a managed solution that minimizes operational maintenance - NAT Gateway is a public subnet is the answer.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n246 \nA company needs to transfer 600 TB of data from its on-premises network-attached storage \n(NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is \nsensitive and must be encrypted in transit. The company's internet connection can support an \nupload speed of 100 Mbps. \n \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon S3 multi-part upload functionality to transfer the fees over HTTPS.", correct: false },
                { id: 1, text: "Create a VPN connection between the on-premises NAS system and the nearest AWS Region.", correct: false },
                { id: 2, text: "Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized", correct: true },
                { id: 3, text: "Set up a 10 Gbps AWS Direct Connect connection between the company location and the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe best option is to use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices and use the devices to transfer the data to Amazon S3. Snowball Edge is a petabyte-scale data transfer device that can help transfer large amounts of data securely and quickly. Using Snowball Edge can be the most cost-effective solution for transferring large amounts of data over long distances and can help meet the requirement of transferring 600 TB of data within two weeks.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A company needs a backup strategy for its three-tier stateless web application. The web \napplication runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling \npolicy that is configured to respond to scaling events. The database tier runs on Amazon RDS for \nPostgreSQL. The web application does not require temporary local storage on the EC2 instances. \nThe company’s recovery point objective (RPO) is 2 hours. \n \nThe backup strategy must maximize scalability and optimize resource utilization for this \nenvironment. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and", correct: false },
                { id: 1, text: "Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 2, text: "Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable", correct: true },
                { id: 3, text: "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe web application does not require temporary local storage on the EC2 instances => No EBS snapshot is required, retaining the latest AMI is enough.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 49,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n247 \nA company needs to ingest and handle large amounts of streaming data that its application \ngenerates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis \nData Streams, which is configured with default settings. Every other day, the application \nconsumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) \nprocessing. The company observes that Amazon S3 is not receiving all the data that the \napplication sends to Kinesis Data Streams. \n \nWhat should a solutions architect do to resolve this issue?",
            options: [
                { id: 0, text: "Update the Kinesis Data Streams default settings by modifying the data retention period.", correct: true },
                { id: 1, text: "Update the application to use the Kinesis Producer Library (KPL) lo send the data to Kinesis Data", correct: false },
                { id: 2, text: "Update the number of Kinesis shards lo handle the throughput of me data that is sent to Kinesis", correct: false },
                { id: 3, text: "Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html The\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 \ninstances runs several 1-hour tasks on a schedule. These tasks were written by different teams \nand have no common programming language. The company is concerned about performance \nand scalability while these tasks run on a single instance. A solutions architect needs to \nimplement a solution to resolve these concerns. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge", correct: true },
                { id: 1, text: "Convert the EC2 instance to a container. Use AWS App Runner to create the container on", correct: false },
                { id: 2, text: "Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon", correct: false },
                { id: 3, text: "Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nLambda functions are short lived; the Lambda max timeout is 900 seconds (15 minutes). This can be difficult to manage and can cause issues in production applications. We'll take a look at AWS Lambda timeout limits, timeout errors, monitoring timeout errors, and how to apply best practices to handle them effectively.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "A company wants to migrate an Oracle database to AWS. The database consists of a single table \nthat contains millions of geographic information systems (GIS) images that are high resolution \nand are identified by a geographic code. When a natural disaster occurs tens of thousands of \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n248 \nimages get updated every few minutes. Each geographic code has a single image or row that is \nassociated with it. The company wants a solution that is highly available and scalable during such \nevents. \n \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Store the images and geographic codes in a database table. Use Oracle running on an Amazon", correct: false },
                { id: 1, text: "Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as", correct: true },
                { id: 2, text: "Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB", correct: false },
                { id: 3, text: "Store the images in Amazon S3 buckets Store geographic codes and image S3 URLs in a", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 52,
            text: "A company has implemented a self-managed DNS service on AWS. The solution consists of the \nfollowing: \n \n- Amazon EC2 instances in different AWS Regions \n- Endpoints of a standard accelerator in AWS Global Accelerator \n \nThe company wants to protect the solution against DDoS attacks. \n \nWhat should a solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.", correct: true },
                { id: 1, text: "Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.", correct: false },
                { id: 2, text: "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the", correct: false },
                { id: 3, text: "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Shield is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS. AWS Shield Standard is automatically enabled to all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service. AWS Shield Advanced provides additional protections against more sophisticated and larger attacks for your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The \ncompany uses Amazon EC2 Windows Server instances behind an Application Load Balancer to \nhost its dynamic application. The company needs a highly available storage solution for the \napplication. The application consists of static files and dynamic server-side code. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n249",
            options: [
                { id: 0, text: "Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.", correct: true },
                { id: 1, text: "Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.", correct: false },
                { id: 2, text: "Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS", correct: false },
                { id: 3, text: "Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows", correct: false },
                { id: 4, text: "Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://www.techtarget.com/searchaws/tip/Amazon-FSx-vs-EFS-Compare-the-AWS-file-services FSx is built for high performance and submillisecond latency using solid-state drive storage volumes. This design enables users to select storage capacity and latency independently. Thus, even a subterabyte file system can have 256 Mbps or higher throughput and support volumes up to 64 TB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A company is migrating an old application to AWS. The application runs a batch job every hour \nand is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. \nThe server has 64 virtual CPU (vCPU) and 512 GiB of memory. \n \nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Lambda with functional scaling", correct: false },
                { id: 1, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate", correct: false },
                { id: 2, text: "Use Amazon Lightsail with AWS Auto Scaling", correct: false },
                { id: 3, text: "Use AWS Batch on Amazon EC2", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUse AWS Batch on Amazon EC2. AWS Batch is a fully managed batch processing service that can be used to easily run batch jobs on Amazon EC2 instances. It can scale the number of instances to match the workload, allowing the batch job to be completed in the desired time frame with minimal operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company hosts a frontend application that uses an Amazon API Gateway API backend that is \nintegrated with AWS Lambda. When the API receives requests, the Lambda function loads many \nlibraries. Then the Lambda function connects to an Amazon RDS database, processes the data, \nand returns the data to the frontend application. The company wants to ensure that response \nlatency is as low as possible for all its users with the fewest number of changes to the company's \noperations. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Establish a connection between the frontend application and the database to make queries faster", correct: false },
                { id: 1, text: "Configure provisioned concurrency for the Lambda function that handles the requests.", correct: true },
                { id: 2, text: "Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.", correct: false },
                { id: 3, text: "Increase the size of the database to increase the number of connections Lambda can establish at", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nConfigure provisioned concurrency for the Lambda function that handles the requests. Provisioned concurrency allows you to set the amount of compute resources that are available to the Lambda function, so that it can handle more requests at once and reduce latency. Caching the results of the queries in Amazon S3 could also help to reduce latency, but it would not be as effective as setting up provisioned concurrency. Increasing the size of the database would not help to reduce latency, as this would not increase the number of connections the Lambda function could establish, and establishing a direct connection between the frontend application and the database would bypass the API, which would not be the best solution either.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 56,
            text: "A company is building a game system that needs to send unique events to separate leaderboard, \nmatchmaking, and authentication services concurrently. The company needs an AWS event-\ndriven system that guarantees the order of the events. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Amazon EventBridge event bus", correct: false },
                { id: 1, text: "Amazon Simple Notification Service (Amazon SNS) FIFO topics", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS) standard topics", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS) FIFO queues", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "An ecommerce company is running a multi-tier application on AWS. The front-end and backend \ntiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend \ntier communicates with the RDS instance. There are frequent calls to return identical datasets \nfrom the database that are causing performance slowdowns. \n \nWhich action should be taken to improve the performance of the backend?",
            options: [
                { id: 0, text: "Implement Amazon SNS to store the database calls.", correct: false },
                { id: 1, text: "Implement Amazon ElastiCache to cache the large datasets.", correct: true },
                { id: 2, text: "Implement an RDS for MySQL read replica to cache database calls.", correct: false },
                { id: 3, text: "Implement Amazon Kinesis Data Firehose to stream the calls to the database.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nKey term is identical datasets from the database it means caching can solve this issue by cached in frequently used dataset from DB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources. A solutions architect \nwants the deployment engineer to perform job activities while following the principle of least \nprivilege. \n \nWhich combination of actions should the solutions architect take to accomplish this goal? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n251",
            options: [
                { id: 0, text: "Have the deployment engineer use AWS account root user credentials for performing AWS", correct: false },
                { id: 1, text: "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the", correct: false },
                { id: 2, text: "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the", correct: false },
                { id: 3, text: "Create a new IAM user for the deployment engineer and add the IAM user to a group that has an", correct: true },
                { id: 4, text: "Create an IAM role for the deployment engineer to explicitly define the permissions specific to the", correct: false },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A company is implementing a shared storage solution for a gaming application that is hosted in \nthe AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution \nmust be fully managed. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file", correct: false },
                { id: 1, text: "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client", correct: false },
                { id: 2, text: "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support", correct: false },
                { id: 3, text: "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company has a business system that generates hundreds of reports each day. The business \nsystem saves the reports to a network share in CSV format. The company needs to store this \ndata in the AWS Cloud in near-real time for analysis. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the", correct: false },
                { id: 1, text: "Create an Amazon S3 File Gateway. Update the business system to use a new network share", correct: true },
                { id: 2, text: "Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the", correct: false },
                { id: 3, text: "Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/storagegateway/file/?nc1=h_ls Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 61,
            text: "A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 \nbuckets and is accessed with varying frequency. The company does not know access patterns for \nall the data. The company needs to implement a solution for each S3 bucket to optimize the cost \nof S3 usage. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3", correct: true },
                { id: 1, text: "Use the S3 storage class analysis tool to determine the correct tier for each object in the S3", correct: false },
                { id: 2, text: "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3", correct: false },
                { id: 3, text: "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/s3/storage-classes/intelligent-tiering/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 62,
            text: "A solutions architect needs to allow team members to access Amazon S3 buckets in two different \nAWS accounts: a development account and a production account. The team currently has access \nto S3 buckets in the development account by using unique IAM users that are assigned to an IAM \ngroup that has appropriate permissions in the account. \n \nThe solutions architect has created an IAM role in the production account. The role has a policy \nthat grants access to an S3 bucket in the production account. \n \nWhich solution will meet these requirements while complying with the principle of least privilege?",
            options: [
                { id: 0, text: "Attach the Administrator Access policy to the development account users.", correct: false },
                { id: 1, text: "Add the development account as a principal in the trust policy of the role in the production", correct: true },
                { id: 2, text: "Turn off the S3 Block Public Access feature on the S3 bucket in the production account.", correct: false },
                { id: 3, text: "Create a user in the production account with unique credentials for each team member.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy adding the development account as a principal in the trust policy of the IAM role in the production account, you are allowing users from the development account to assume the role in the production account. This allows the team members to access the S3 bucket in the production account without granting them unnecessary privileges.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 \nworkloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that \nprevents any resources from being created in any other Region. A security policy requires the \ncompany to encrypt all data at rest. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n253 \nAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) \nvolumes for EC2 instances without encrypting the volumes. The company wants any new EC2 \ninstances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS \nvolumes. The company wants a solution that will have minimal effect on employees who create \nEBS volumes. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "In the Amazon EC2 console, select the EBS encryption account attribute and define a default", correct: false },
                { id: 1, text: "Create an IAM permission boundary. Attach the permission boundary to the root organizational", correct: false },
                { id: 2, text: "Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the", correct: true },
                { id: 3, text: "Update the IAM policies for each account to deny the ec2:CreateVolume action when the", correct: false },
                { id: 4, text: "In the Organizations management account, specify the Default EBS volume encryption setting.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nSCP that denies the ec2:CreateVolume action when the ec2:Encrypted condition equals false. This will prevent users and service accounts in member accounts from creating unencrypted EBS volumes in the ap-southeast-2 Region.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming \ndatabase administrative tasks for production database workloads. The company wants to ensure \nthat its database is highly available and will provide automatic failover support in most scenarios \nin less than 40 seconds. The company wants to offload reads off of the primary instance and \nkeep costs as low as possible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the", correct: false },
                { id: 1, text: "Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read", correct: false },
                { id: 2, text: "Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the", correct: false },
                { id: 3, text: "Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.ht ml\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test17: [
        {
            id: 0,
            text: "A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux \ninstances that run with elastic IP addresses to accept traffic from trusted IP sources on the \ninternet. The SFTP service is backed by shared storage that is attached to the instances. User \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n254 \naccounts are created and managed as Linux users in the SFTP servers. \n \nThe company wants a serverless option that provides high IOPS performance and highly \nconfigurable security. The company also wants to maintain control over user permissions. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS", correct: false },
                { id: 1, text: "Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS", correct: true },
                { id: 2, text: "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp- servers/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company is developing a new machine learning (ML) model solution on AWS. The models are \ndeveloped as independent microservices that fetch approximately 1 GB of model data from \nAmazon S3 at startup and load the data into memory. Users access the models through an \nasynchronous API. Users can send a request or a batch of requests and specify where the \nresults should be sent. \n \nThe company provides models to hundreds of users. The usage patterns for the models are \nirregular. Some models could be unused for days or weeks. Other models could receive batches \nof thousands of requests at a time. \n \nWhich design should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS", correct: false },
                { id: 1, text: "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as", correct: false },
                { id: 2, text: "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.", correct: false },
                { id: 3, text: "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/blogs/containers/amazon-elastic-container-service-ecs-auto-scaling- using-custom-metrics/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A solutions architect wants to use the following JSON text as an identity-based policy to grant \nspecific permissions: \n \n \n \nWhich IAM principals can the solutions architect attach this policy to? (Choose two.)",
            options: [
                { id: 0, text: "Role", correct: true },
                { id: 1, text: "Group", correct: false },
                { id: 2, text: "Organization", correct: false },
                { id: 3, text: "Amazon Elastic Container Service (Amazon ECS) resource", correct: false },
                { id: 4, text: "Amazon EC2 resource", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A company is running a custom application on Amazon EC2 On-Demand Instances. The \napplication has frontend nodes that need to run 24 hours a day, 7 days a week and backend \nnodes that need to run only for a short time based on workload. The number of backend nodes \nvaries during the day. \n \nThe company needs to scale out and scale in more instances based on workload. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.", correct: false },
                { id: 1, text: "Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.", correct: true },
                { id: 2, text: "Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.", correct: false },
                { id: 3, text: "Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A company uses high block storage capacity to runs its workloads on premises. The company's \ndaily peak input and output transactions per second are not more than 15,000 IOPS. The \ncompany wants to migrate the workloads to Amazon EC2 and to provision disk performance \nindependent of storage capacity. \n \nWhich Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements \nMOST cost-effectively?",
            options: [
                { id: 0, text: "GP2 volume type", correct: false },
                { id: 1, text: "io2 volume type", correct: false },
                { id: 2, text: "GP3 volume type", correct: true },
                { id: 3, text: "io1 volume type", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBoth GP2 and GP3 has max IOPS 16000 but GP3 is cost effective. https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and- save-up-to-20-on-costs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 5,
            text: "A company needs to store data from its healthcare application. The application's data frequently \nchanges. A new regulation requires audit access at all levels of the stored data. \n \nThe company hosts the application on an on-premises infrastructure that is running out of storage \ncapacity. A solutions architect must securely migrate the existing data to AWS while satisfying the \nnew regulation. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data", correct: true },
                { id: 1, text: "Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log", correct: false },
                { id: 2, text: "Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS", correct: false },
                { id: 3, text: "Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/datasync/latest/userguide/encryption-in-transit.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A solutions architect is implementing a complex Java application with a MySQL database. The \nJava application must be deployed on Apache Tomcat and must be highly available. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with", correct: false },
                { id: 1, text: "Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment", correct: true },
                { id: 2, text: "Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow", correct: false },
                { id: 3, text: "Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Elastic Beanstalk provides an easy and quick way to deploy, manage, and scale applications. It supports a variety of platforms, including Java and Apache Tomcat. By using Elastic Beanstalk, the solutions architect can upload the Java application and configure the environment to run Apache Tomcat.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. \nThe Lambda function needs permissions to read and write to the DynamoDB table. \n \nWhich solution will give the Lambda function access to the DynamoDB table MOST securely?",
            options: [
                { id: 0, text: "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user", correct: false },
                { id: 1, text: "Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that", correct: true },
                { id: 2, text: "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user", correct: false },
                { id: 3, text: "Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nOption B suggests creating an IAM role that includes Lambda as a trusted service, meaning the role is specifically designed for Lambda functions. The role should have a policy attached to it that grants the required read and write access to the DynamoDB table.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "The following IAM policy is attached to an IAM group. This is the only policy applied to the group. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n258 \n \n \nWhat are the effective IAM permissions of this policy for group members?",
            options: [
                { id: 0, text: "Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements", correct: false },
                { id: 1, text: "Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they", correct: false },
                { id: 2, text: "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for", correct: false },
                { id: 3, text: "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. \nThese .csv files must be converted into images and must be made available as soon as possible \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n259 \nfor the automatic generation of graphical reports. \n \nThe images become irrelevant after 1 month, but the .csv files must be kept to train machine \nlearning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
            options: [
                { id: 0, text: "Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the", correct: false },
                { id: 1, text: "Design an AWS Lambda function that converts the .csv files into images and stores the images in", correct: true },
                { id: 2, text: "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files", correct: false },
                { id: 3, text: "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files", correct: false },
                { id: 4, text: "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class- amazon-s3-glacier-flexible-retrieval/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has developed a new video game as a web application. The application is in a three-\ntier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players \nwill compete concurrently online. The game's developers want to display a top-10 scoreboard in \nnear-real time and offer the ability to stop and restore the game while preserving the current \nscores. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web", correct: false },
                { id: 1, text: "Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web", correct: true },
                { id: 2, text: "Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard", correct: false },
                { id: 3, text: "Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/jp/blogs/news/building-a-real-time-gaming-leaderboard-with-amazon- elasticache-for-redis/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "An ecommerce company wants to use machine learning (ML) algorithms to build and train \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n260 \nmodels. The company will use the models to visualize complex scenarios and to detect trends in \ncustomer data. The architecture team wants to integrate its ML models with a reporting platform \nto analyze the augmented data and use the data directly in its business intelligence dashboards. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch", correct: false },
                { id: 1, text: "Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.", correct: true },
                { id: 2, text: "Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train", correct: false },
                { id: 3, text: "Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon SageMaker is a fully managed service that provides a complete set of tools and capabilities for building, training, and deploying ML models. It simplifies the end-to-end ML workflow and reduces operational overhead by handling infrastructure provisioning, model training, and deployment. To visualize the data and integrate it into business intelligence dashboards, Amazon QuickSight can be used. QuickSight is a cloud-native business intelligence service that allows users to easily create interactive visualizations, reports, and dashboards from various data sources, including the augmented data generated by the ML models.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company is running its production and nonproduction environment workloads in multiple AWS \naccounts. The accounts are in an organization in AWS Organizations. The company needs to \ndesign a solution that will prevent the modification of cost usage tags. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a custom AWS Config rule to prevent tag modification except by authorized principals.", correct: false },
                { id: 1, text: "Create a custom trail in AWS CloudTrail to prevent tag modification.", correct: false },
                { id: 2, text: "Create a service control policy (SCP) to prevent tag modification except by authorized principals.", correct: true },
                { id: 3, text: "Create custom Amazon CloudWatch logs to prevent tag modification.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/organizations/latest/userguide/orgs_manage_policies_scps_e xamples_tagging.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 13,
            text: "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 \ninstances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon \nDynamoDB table. The company wants to ensure the application can be made available in \nanotherAWS Region with minimal downtime. \n \nWhat should a solutions architect do to meet these requirements with the LEAST amount of \ndowntime?",
            options: [
                { id: 0, text: "Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the", correct: false },
                { id: 1, text: "Create an AWS CloudFormation template to create EC2 instances, load balancers, and", correct: false },
                { id: 2, text: "Create an AWS CloudFormation template to create EC2 instances and a load balancer to be", correct: false },
                { id: 3, text: "Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company needs to migrate a MySQL database from its on-premises data center to AWS within \n2 weeks. The database is 20 TB in size. The company wants to complete the migration with \nminimal downtime. \n \nWhich solution will migrate the database MOST cost-effectively?",
            options: [
                { id: 0, text: "Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service", correct: true },
                { id: 1, text: "Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with", correct: false },
                { id: 2, text: "Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database", correct: false },
                { id: 3, text: "Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 15,
            text: "A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB \ninstance. The company successfully launched a new product. The workload on the database has \nincreased. The company wants to accommodate the larger workload without adding \ninfrastructure. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB", correct: true },
                { id: 1, text: "Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.", correct: false },
                { id: 2, text: "Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB", correct: false },
                { id: 3, text: "Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n\"without adding infrastructure\" means scaling vertically and choosing larger instance. \"MOST cost-effectively\" reserved instances\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 16,
            text: "A company operates an ecommerce website on Amazon EC2 instances behind an Application \nLoad Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues \nrelated to a high request rate from illegitimate external systems with changing IP addresses. The \nsecurity team is worried about potential DDoS attacks against the website. The company must \nblock the illegitimate incoming requests in a way that has a minimal impact on legitimate users. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Deploy Amazon Inspector and associate it with the ALB.", correct: false },
                { id: 1, text: "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.", correct: true },
                { id: 2, text: "Deploy rules to the network ACLs associated with the ALB to block the incomingtraffic.", correct: false },
                { id: 3, text: "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS WAF (Web Application Firewall) is a service that provides protection for web applications against common web exploits. By associating AWS WAF with the Application Load Balancer (ALB), you can inspect incoming traffic and define rules to allow or block requests based on various criteria.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A company wants to share accounting data with an external auditor. The data is stored in an \nAmazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account \nand requires its own copy of the database. \n \nWhat is the MOST secure way for the company to share the database with the auditor?",
            options: [
                { id: 0, text: "Create a read replica of the database. Configure IAM standard database authentication to grant", correct: false },
                { id: 1, text: "Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new", correct: false },
                { id: 2, text: "Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's", correct: false },
                { id: 3, text: "Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe most secure way for the company to share the database with the auditor is option D: Create an encrypted snapshot of the database, share the snapshot with the auditor, and allow access to the AWS Key Management Service (AWS KMS) encryption key. By creating an encrypted snapshot, the company ensures that the database data is protected at rest. Sharing the encrypted snapshot with the auditor allows them to have their own copy of the Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A solutions architect configured a VPC that has a small range of IP addresses. The number of \nAmazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP \naddresses for future workloads. \n \nWhich solution resolves this issue with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional", correct: true },
                { id: 1, text: "Create a second VPC with additional subnets. Use a peering connection to connect the second", correct: false },
                { id: 2, text: "Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first", correct: false },
                { id: 3, text: "Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou assign a single CIDR IP address range as the primary CIDR block when you create a VPC and can add up to four secondary CIDR blocks after creation of the VPC.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A company used an Amazon RDS for MySQL DB instance during application testing. Before \nterminating the DB instance at the end of the test cycle, a solutions architect created two \nbackups. The solutions architect created the first backup by using the mysqldump utility to create \na database dump. The solutions architect created the second backup by enabling the final DB \nsnapshot option on RDS termination. \n \nThe company is now planning for a new test cycle and wants to create a new DB instance from \nthe most recent backup. The company has chosen a MySQL-compatible edition ofAmazon \nAurora to host the DB instance. \n \nWhich solutions will create the new DB instance? (Choose two.)",
            options: [
                { id: 0, text: "Import the RDS snapshot directly into Aurora.", correct: true },
                { id: 1, text: "Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.", correct: false },
                { id: 2, text: "Upload the database dump to Amazon S3. Then import the database dump into Aurora.", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.", correct: false },
                { id: 4, text: "Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RD SMySQL.Import.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an \nApplication Load Balancer. The instances run in an Auto Scaling group across multiple \nAvailability Zones. The company observes that the Auto Scaling group launches more On-\nDemand Instances when the application's end users access high volumes of static web content. \nThe company wants to optimize cost. \n \nWhat should a solutions architect do to redesign the application MOST cost-effectively?",
            options: [
                { id: 0, text: "Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.", correct: false },
                { id: 1, text: "Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3", correct: true },
                { id: 3, text: "Create an AWS Lambda function behind an Amazon API Gateway API to host the static website", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy leveraging Amazon CloudFront, you can cache and serve the static web content from edge locations worldwide, reducing the load on your EC2 instances. This can help lower the number of On-Demand Instances required to handle high volumes of static web content requests. Storing the static content in an Amazon S3 bucket and using CloudFront as a content delivery network (CDN) improves performance and reduces costs by reducing the load on your EC2 instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "A company stores several petabytes of data across multiple AWS accounts. The company uses \nAWS Lake Formation to manage its data lake. The company's data science team wants to \nsecurely share selective data from its accounts with the company's engineering team for \nanalytical purposes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Copy the required data to a common account. Create an IAM access role in that account. Grant", correct: false },
                { id: 1, text: "Use the Lake Formation permissions Grant command in each account where the data is stored to", correct: false },
                { id: 2, text: "Use AWS Data Exchange to privately publish the required data to the required engineering team", correct: false },
                { id: 3, text: "Use Lake Formation tag-based access control to authorize and grant cross-account permissions", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nBy utilizing Lake Formation's tag-based access control, you can define tags and tag-based policies to grant selective access to the required data for the engineering team accounts. This approach allows you to control access at a granular level without the need to copy or move the Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A company wants to host a scalable web application on AWS. The application will be accessed \nby users from different geographic regions of the world. Application users will be able to \ndownload and upload unique data up to gigabytes in size. The development team wants a cost-\neffective solution to minimize upload and download latency and maximize performance. \n \nWhat should a solutions architect do to accomplish this?",
            options: [
                { id: 0, text: "Use Amazon S3 with Transfer Acceleration to host the application.", correct: true },
                { id: 1, text: "Use Amazon S3 with CacheControl headers to host the application.", correct: false },
                { id: 2, text: "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.", correct: false },
                { id: 3, text: "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/transfer-acceleration.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A company has hired a solutions architect to design a reliable architecture for its application. The \napplication consists of one Amazon RDS DB instance and two manually provisioned Amazon \nEC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. \n \nAn employee recently deleted the DB instance, and the application was unavailable for 24 hours \nas a result. The company is concerned with the overall reliability of its environment. \n \nWhat should the solutions architect do to maximize reliability of the application's infrastructure?",
            options: [
                { id: 0, text: "Delete one EC2 instance and enable termination protection on the other EC2 instance. Update", correct: false },
                { id: 1, text: "Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances", correct: true },
                { id: 2, text: "Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda", correct: false },
                { id: 3, text: "Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUpdate the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in \nits corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct \nConnect connection. \n \nAfter an audit from a regulator, the company has 90 days to move the data to the cloud. The \ncompany needs to move the data efficiently and without disruption. The company still needs to be \nable to access and update the data during the transfer window. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start", correct: true },
                { id: 1, text: "Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an", correct: false },
                { id: 2, text: "Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the", correct: false },
                { id: 3, text: "Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy leveraging AWS DataSync in combination with AWS Direct Connect, the company can efficiently and securely transfer its 700 terabytes of data to an Amazon S3 bucket without disruption. The solution allows continued access and updates to the data during the transfer window, ensuring business continuity throughout the migration process.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal \nrequirement to retain all new and existing data in Amazon S3 for 7 years. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data", correct: false },
                { id: 1, text: "Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention", correct: false },
                { id: 2, text: "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention", correct: false },
                { id: 3, text: "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nTo replicate existing object/data in S3 Bucket to bring them to compliance, optionally we use \"S3 Batch Replication\", so option D is the most appropriate, especially if we have big data in S3.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A company has a stateless web application that runs on AWS Lambda functions that are invoked \nby Amazon API Gateway. The company wants to deploy the application across multiple AWS \nRegions to provide Regional failover capabilities. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n267 \n \nWhat should a solutions architect do to route traffic to multiple Regions?",
            options: [
                { id: 0, text: "Create Amazon Route 53 health checks for each Region. Use an active-active failover", correct: true },
                { id: 1, text: "Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health", correct: false },
                { id: 2, text: "Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region.", correct: false },
                { id: 3, text: "Create an Application Load Balancer in the primary Region. Set the target group to point to the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company has two VPCs named Management and Production. The Management VPC uses \nVPNs through a customer gateway to connect to a single device in the data center. The \nProduction VPC uses a virtual private gateway with two attached AWS Direct Connect \nconnections. The Management and Production VPCs both use a single VPC peering connection \nto allow communication between the applications. \n \nWhat should a solutions architect do to mitigate any single point of failure in this architecture?",
            options: [
                { id: 0, text: "Add a set of VPNs between the Management and Production VPCs.", correct: false },
                { id: 1, text: "Add a second virtual private gateway and attach it to the Management VPC.", correct: false },
                { id: 2, text: "Add a second set of VPNs to the Management VPC from a second customer gateway device.", correct: true },
                { id: 3, text: "Add a second VPC peering connection between the Management VPC and the Production VPC.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nRedundant VPN connections: Instead of relying on a single device in the data center, the Management VPC should have redundant VPN connections established through multiple customer gateways. This will ensure high availability and fault tolerance in case one of the VPN connections or customer gateways fails.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company runs its application on an Oracle database. The company plans to quickly migrate to \nAWS because of limited resources for the database, backup administration, and data center \nmaintenance. The application uses third-party database features that require privileged access. \n \nWhich solution will help the company migrate the database to AWS MOST cost-effectively?",
            options: [
                { id: 0, text: "Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud", correct: false },
                { id: 1, text: "Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to", correct: true },
                { id: 2, text: "Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize", correct: false },
                { id: 3, text: "Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 29,
            text: "A company has a three-tier web application that is in a single server. The company wants to \nmigrate the application to the AWS Cloud. The company also wants the application to align with \nthe AWS Well-Architected Framework and to be consistent with AWS recommended best \npractices for security, scalability, and resiliency. \n \nWhich combination of solutions will meet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Create a VPC across two Availability Zones with the application's existing architecture. Host the", correct: false },
                { id: 1, text: "Set up security groups and network access control lists (network ACLs) to control access to the", correct: false },
                { id: 2, text: "Create a VPC across two Availability Zones. Refactor the application to host the web tier,", correct: true },
                { id: 3, text: "Use a single Amazon RDS database. Allow database access only from the application tier", correct: false },
                { id: 4, text: "Use Elastic Load Balancers in front of the web tier. Control access by using security groups", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company is migrating its applications and databases to the AWS Cloud. The company will use \nAmazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. \n \nWhich activities will be managed by the company's operational team? (Choose three.)",
            options: [
                { id: 0, text: "Management of the Amazon RDS infrastructure layer, operating system, and platforms", correct: false },
                { id: 1, text: "Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window", correct: true },
                { id: 2, text: "Configuration of additional software components on Amazon ECS for monitoring, patch", correct: false },
                { id: 3, text: "Installation of patches for all minor and major database versions for Amazon RDS", correct: false },
                { id: 4, text: "Ensure the physical security of the Amazon RDS infrastructure in the data center", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and \ntakes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. \nThe CPU utilization of the instance is low except for short surges during which the job uses the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n269 \nmaximum CPU available. The company wants to optimize the costs to run the job. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic", correct: false },
                { id: 1, text: "Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon", correct: true },
                { id: 2, text: "Use AWS App2Container (A2C) to containerize the job. Install the container in the existing", correct: false },
                { id: 3, text: "Configure the existing schedule to stop the EC2 instance at the completion of the job and restart", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 \nbuckets. Because of regulatory requirements, the company must retain backup files for a specific \ntime period. The company must not alter the files for the duration of the retention period. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the", correct: false },
                { id: 1, text: "Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.", correct: false },
                { id: 2, text: "Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle", correct: false },
                { id: 3, text: "Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A company has resources across multiple AWS Regions and accounts. A newly hired solutions \narchitect discovers a previous employee did not provide details about the resources inventory. \nThe solutions architect needs to build and map the relationship details of the various workloads \nacross all accounts. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Use AWS Systems Manager Inventory to generate a map view from the detailed view report.", correct: false },
                { id: 1, text: "Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads", correct: false },
                { id: 2, text: "Use Workload Discovery on AWS to generate architecture diagrams of the workloads.", correct: true },
                { id: 3, text: "Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nWorkload Discovery on AWS is a service that helps visualize and understand the architecture of your workloads across multiple AWS accounts and Regions. It automatically discovers and maps the relationships between resources, providing an accurate representation of the architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "A company uses AWS Organizations. The company wants to operate some of its AWS accounts \nwith different budgets. The company wants to receive alerts and automatically prevent \nprovisioning of additional resources on AWS accounts when the allocated budget threshold is met \nduring a specific period. \n \nWhich combination of solutions will meet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports", correct: false },
                { id: 1, text: "Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the", correct: true },
                { id: 2, text: "Create an IAM user for AWS Budgets to run budget actions with the required permissions.", correct: false },
                { id: 3, text: "Create an IAM role for AWS Budgets to run budget actions with the required permissions.", correct: false },
                { id: 4, text: "Add an alert to notify the company when each account meets its budget threshold. Add a budget", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/awsaccountbilling/latest/aboutv2/view-billing-dashboard.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A company runs applications on Amazon EC2 instances in one AWS Region. The company \nwants to back up the EC2 instances to a second Region. The company also wants to provision \nEC2 resources in the second Region and manage the EC2 instances centrally from one AWS \naccount. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second", correct: false },
                { id: 1, text: "Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances.", correct: false },
                { id: 2, text: "Create a backup plan by using AWS Backup. Configure cross-Region backup to the second", correct: true },
                { id: 3, text: "Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nUsing AWS Backup, you can create backup plans that automate the backup process for your EC2 instances. By configuring cross-Region backup, you can ensure that backups are replicated Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A company that uses AWS is building an application to transfer data to a product manufacturer. \nThe company has its own identity provider (IdP). The company wants the IdP to authenticate \napplication users while the users use the application to transfer data. The company must use \nApplicability Statement 2 (AS2) protocol. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.", correct: false },
                { id: 1, text: "Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service", correct: false },
                { id: 2, text: "Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP", correct: true },
                { id: 3, text: "Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nTo authenticate your users, you can use your existing identity provider with AWS Transfer Family. You integrate your identity provider using an AWS Lambda function, which authenticates and authorizes your users for access to Amazon S3 or Amazon Elastic File System (Amazon EFS). https://docs.aws.amazon.com/transfer/latest/userguide/custom-identity-provider-users.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback \nservice. The application requires 1 GB of memory and 2 GB of storage for its computation \nresources. The application will require that the data is in a relational format. \n \nWhich additional combination ofAWS services will meet these requirements with the LEAST \nadministrative effort? (Choose two.)",
            options: [
                { id: 0, text: "Amazon EC2", correct: false },
                { id: 1, text: "AWS Lambda", correct: true },
                { id: 2, text: "Amazon RDS", correct: false },
                { id: 3, text: "Amazon DynamoDB", correct: false },
                { id: 4, text: "Amazon Elastic Kubernetes Services (Amazon EKS)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n\"The application will require that the data is in a relational format\" so DynamoDB is out. RDS is the choice. Lambda is severless.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging \npolicy adds department tags to AWS resources when the company creates tags. \n \nAn accounting team needs to determine spending on Amazon EC2 consumption. The accounting \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n272 \nteam must determine which departments are responsible for the costs regardless ofAWS \naccount. The accounting team has access to AWS Cost Explorer for all AWS accounts within the \norganization and needs to access all reports from Cost Explorer. \n \nWhich solution meets these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "From the Organizations management account billing console, activate a user-defined cost", correct: true },
                { id: 1, text: "From the Organizations management account billing console, activate an AWS-defined cost", correct: false },
                { id: 2, text: "From the Organizations member account billing console, activate a user-defined cost allocation", correct: false },
                { id: 3, text: "From the Organizations member account billing console, activate an AWS-defined cost allocation", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy activating a user-defined cost allocation tag named \"department\" and creating a cost report in Cost Explorer that groups by the tag name and filters by EC2, the accounting team will be able to track and attribute costs to specific departments across all AWS accounts within the organization. This approach allows for consistent cost allocation and reporting regardless of the AWS account structure.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company wants to securely exchange data between its software as a service (SaaS) \napplication Salesforce account and Amazon S3. The company must encrypt the data at rest by \nusing AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The \ncompany must also encrypt the data in transit. The company has enabled API access for the \nSalesforce account.",
            options: [
                { id: 0, text: "Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.", correct: false },
                { id: 1, text: "Create an AWS Step Functions workflow. Define the task to transfer the data securely from", correct: false },
                { id: 2, text: "Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.", correct: true },
                { id: 3, text: "Create a custom connector for Salesforce to transfer the data securely from Salesforce to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon AppFlow is a fully managed integration service that allows you to securely transfer data between different SaaS applications and AWS services. It provides built-in encryption options and supports encryption in transit using SSL/TLS protocols. With AppFlow, you can configure the data transfer flow from Salesforce to Amazon S3, ensuring data encryption at rest by utilizing AWS KMS CMKs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple \nAmazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon \nDynamoDB. The app communicates by using TCP traffic and UDP traffic between the users and \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n273 \nthe servers. The application will be used globally. The company wants to ensure the lowest \npossible latency for all users. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer", correct: false },
                { id: 1, text: "Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB)", correct: true },
                { id: 2, text: "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load", correct: false },
                { id: 3, text: "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Global Accelerator is a better solution for the mobile gaming app than CloudFront.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company has an application that processes customer orders. The company hosts the \napplication on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. \nOccasionally when traffic is high the workload does not process orders fast enough. \n \nWhat should a solutions architect do to write the orders reliably to the database as quickly as \npossible?",
            options: [
                { id: 0, text: "Increase the instance size of the EC2 instance when traffic is high. Write orders to Amazon", correct: false },
                { id: 1, text: "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in", correct: true },
                { id: 2, text: "Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database", correct: false },
                { id: 3, text: "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy decoupling the write operation from the processing operation using SQS, you ensure that the orders are reliably stored in the queue, regardless of the processing capacity of the EC2 instances. This allows the processing to be performed at a scalable rate based on the available EC2 instances, improving the overall reliability and speed of order processing.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "An IoT company is releasing a mattress that has sensors to collect data about a user's sleep. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n274 \nsensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data \nevery night for each mattress. The company must process and summarize the data for each \nmattress. The results need to be available as soon as possible. Data processing will require 1 GB \nof memory and will finish within 30 seconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use AWS Glue with a Scala job", correct: false },
                { id: 1, text: "Use Amazon EMR with an Apache Spark script", correct: false },
                { id: 2, text: "Use AWS Lambda with a Python script", correct: true },
                { id: 3, text: "Use AWS Glue with a PySpark job", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Lambda charges you based on the number of invocations and the execution time of your function. Since the data processing job is relatively small (2 MB of data), Lambda is a cost- effective choice. You only pay for the actual usage without the need to provision and maintain infrastructure.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A company hosts an online shopping application that stores all orders in an Amazon RDS for \nPostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and \nhas asked a solutions architect to recommend an approach to minimize database downtime \nwithout requiring any changes to the application code. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Convert the existing database instance to a Multi-AZ deployment by modifying the database", correct: true },
                { id: 1, text: "Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and", correct: false },
                { id: 2, text: "Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon", correct: false },
                { id: 3, text: "Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCompared to other solutions that involve creating new instances, restoring snapshots, or setting up replication manually, converting to a Multi-AZ deployment is a simpler and more streamlined approach with lower overhead. Overall, option A offers a cost-effective and efficient way to minimize database downtime without requiring significant changes or additional complexities.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 44,
            text: "A company is developing an application to support customer demands. The company wants to \ndeploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability \nZone. The company also wants to give the application the ability to write to multiple block storage \nvolumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application \navailability. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n275",
            options: [
                { id: 0, text: "Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 1, text: "Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon", correct: false },
                { id: 2, text: "Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS)", correct: true },
                { id: 3, text: "Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nMulti-Attach is supported exclusively on Provisioned IOPS SSD (io1 and io2) volumes.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company designed a stateless two-tier application that uses Amazon EC2 in a single \nAvailability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants \nto ensure the application is highly available. \n \nWhat should a solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load", correct: true },
                { id: 1, text: "Configure the application to take snapshots of the EC2 instances and send them to a different", correct: false },
                { id: 2, text: "Configure the application to use Amazon Route 53 latency-based routing to feed requests to the", correct: false },
                { id: 3, text: "Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy combining Multi-AZ EC2 Auto Scaling and an Application Load Balancer, you achieve high availability for the EC2 instances hosting your stateless two-tier application.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A company uses AWS Organizations. A member account has purchased a Compute Savings \nPlan. Because of changes in the workloads inside the member account, the account no longer \nreceives the full benefit of the Compute Savings Plan commitment. The company uses less than \n50% of its purchased compute power.",
            options: [
                { id: 0, text: "Turn on discount sharing from the Billing Preferences section of the account console in the", correct: false },
                { id: 1, text: "Turn on discount sharing from the Billing Preferences section of the account console in the", correct: true },
                { id: 2, text: "Migrate additional compute workloads from another AWS account to the account that has the", correct: false },
                { id: 3, text: "Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 47,
            text: "A company is developing a microservices application that will provide a search catalog for \ncustomers. The company must use REST APIs to present the frontend of the application to users. \nThe REST APIs must access the backend services that the company hosts in containers in \nprivate VPC subnets. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic", correct: false },
                { id: 1, text: "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic", correct: true },
                { id: 2, text: "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic", correct: false },
                { id: 3, text: "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-private-integration.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A company stores raw collected data in an Amazon S3 bucket. The data is used for several types \nof analytics on behalf of the company's customers. The type of analytics requested determines \nthe access pattern on the S3 objects. \n \nThe company cannot predict or control the access pattern. The company wants to reduce its S3 \ncosts. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access", correct: false },
                { id: 1, text: "Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3", correct: false },
                { id: 2, text: "Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering", correct: true },
                { id: 3, text: "Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The \napplications must initiate communications with other external applications using the internet. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n277 \nHowever the company's security policy states that any external service cannot initiate a \nconnection to the EC2 instances. \n \nWhat should a solutions architect recommend to resolve this issue?",
            options: [
                { id: 0, text: "Create a NAT gateway and make it the destination of the subnet's route table", correct: false },
                { id: 1, text: "Create an internet gateway and make it the destination of the subnet's route table", correct: false },
                { id: 2, text: "Create a virtual private gateway and make it the destination of the subnet's route table", correct: false },
                { id: 3, text: "Create an egress-only internet gateway and make it the destination of the subnet's route table", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAn egress-only internet gateway (EIGW) is specifically designed for IPv6-only VPCs and provides outbound IPv6 internet access while blocking inbound IPv6 traffic. It satisfies the requirement of preventing external services from initiating connections to the EC2 instances while allowing the instances to initiate outbound communications.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A company is creating an application that runs on containers in a VPC. The application stores \nand accesses data in an Amazon S3 bucket. During the development phase, the application will \nstore and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs \nand wants to prevent traffic from traversing the internet whenever possible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Enable S3 Intelligent-Tiering for the S3 bucket", correct: false },
                { id: 1, text: "Enable S3 Transfer Acceleration for the S3 bucket", correct: false },
                { id: 2, text: "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in", correct: true },
                { id: 3, text: "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nPrevent traffic from traversing the internet = Gateway VPC endpoint for S3.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company has a mobile chat application with a data store based in Amazon DynamoDB. Users \nwould like new messages to be read with as little latency as possible. A solutions architect needs \nto design an optimal solution that requires minimal application changes. \n \nWhich method should the solutions architect select?",
            options: [
                { id: 0, text: "Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code", correct: true },
                { id: 1, text: "Add DynamoDB read replicas to handle the increased read load. Update the application to point", correct: false },
                { id: 2, text: "Double the number of read capacity units for the new messages table in DynamoDB. Continue to", correct: false },
                { id: 3, text: "Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The website serves static content. Website traffic is increasing, and the company is \nconcerned about a potential increase in cost.",
            options: [
                { id: 0, text: "Create an Amazon CloudFront distribution to cache state files at edge locations", correct: true },
                { id: 1, text: "Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve", correct: false },
                { id: 2, text: "Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache", correct: false },
                { id: 3, text: "Create a second ALB in an alternative AWS Region. Route user traffic to the closest Region to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon CloudFront: CloudFront is a content delivery network (CDN) service that caches content at edge locations worldwide. By creating a CloudFront distribution, static content from the website can be cached at edge locations, reducing the load on the EC2 instances and improving the overall performance. Caching Static Files: Since the website serves static content, caching these files at CloudFront edge locations can significantly reduce the number of requests forwarded to the EC2 instances. This helps to lower the overall cost by offloading traffic from the instances and reducing the data transfer costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "A company has multiple VPCs across AWS Regions to support and run workloads that are \nisolated from workloads in other Regions. Because of a recent application launch requirement, \nthe company's VPCs must communicate with all other VPCs across all Regions. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
            options: [
                { id: 0, text: "Use VPC peering to manage VPC communication in a single Region. Use VPC peering across", correct: false },
                { id: 1, text: "Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and", correct: false },
                { id: 2, text: "Use AWS Transit Gateway to manage VPC communication in a single Region and Transit", correct: true },
                { id: 3, text: "Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Transit Gateway: Transit Gateway is a highly scalable service that simplifies network connectivity between VPCs and on-premises networks. By using a Transit Gateway in a single Region, you can centralize VPC communication management and reduce administrative effort. Transit Gateway Peering: Transit Gateway supports peering connections across AWS Regions, allowing you to establish connectivity between VPCs in different Regions without the need for complex VPC peering configurations. This simplifies the management of VPC communications across Regions. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "A company is designing a containerized application that will use Amazon Elastic Container \nService (Amazon ECS). The application needs to access a shared file system that is highly \ndurable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 \nhours. The file system needs to provide a mount target m each Availability Zone within a Region. \n \nA solutions architect wants to use AWS Backup to manage the replication to another Region. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server with a Multi-AZ deployment", correct: false },
                { id: 1, text: "Amazon FSx for NetApp ONTAP with a Multi-AZ deployment", correct: false },
                { id: 2, text: "Amazon Elastic File System (Amazon EFS) with the Standard storage class", correct: true },
                { id: 3, text: "Amazon FSx for OpenZFS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/efs/faq/ Q: What is Amazon EFS Replication? EFS Replication can replicate your file system data to another Region or within the same Region without requiring additional infrastructure or a custom process. Amazon EFS Replication automatically and transparently replicates your data to a second file system in a Region or AZ of your choice. You can use the Amazon EFS console, AWS CLI, and APIs to activate replication on an existing file system. EFS Replication is continual and provides a recovery point objective (RPO) and a recovery time objective (RTO) of minutes, helping you meet your compliance and business continuity goals.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company is expecting rapid growth in the near future. A solutions architect needs to configure \nexisting users and grant permissions to new users on AWS. The solutions architect has decided \nto create IAM groups. The solutions architect will add the new users to IAM groups based on \ndepartment. \n \nWhich additional action is the MOST secure way to grant permissions to the new users?",
            options: [
                { id: 0, text: "Apply service control policies (SCPs) to manage access permissions", correct: false },
                { id: 1, text: "Create IAM roles that have least privilege permission. Attach the roles to the IAM groups", correct: false },
                { id: 2, text: "Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups", correct: true },
                { id: 3, text: "Create IAM roles. Associate the roles with a permissions boundary that defines the maximum", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_manage_attach-policy.html Attaching a policy to an IAM user group.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. \nAn administrator has created the following IAM policy to provide access to the bucket and applied \nthat policy to the group. The group is not able to delete objects in the bucket. The company \nfollows least-privilege access rules. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n280 \n \n \nWhich statement should a solutions architect add to the policy to correct bucket access?",
            options: [
                { id: 0, text: "B.", correct: false },
                { id: 2, text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.", correct: false },
                { id: 3, text: "Answer: D", correct: false },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A law firm needs to share information with the public. The information includes hundreds of files \nthat must be publicly readable. Modifications or deletions of the files by anyone before a \ndesignated future date are prohibited. \n \nWhich solution will meet these requirements in the MOST secure way?",
            options: [
                { id: 0, text: "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Grant read-", correct: false },
                { id: 1, text: "Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a", correct: true },
                { id: 2, text: "Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run", correct: false },
                { id: 3, text: "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Select the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company is making a prototype of the infrastructure for its new website by manually \nprovisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an \nApplication Load Balancer and an Amazon RDS database. After the configuration has been \nthoroughly validated, the company wants the capability to immediately deploy the infrastructure \nfor development and production use in two Availability Zones in an automated fashion. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Systems Manager to replicate and provision the prototype infrastructure in two", correct: false },
                { id: 1, text: "Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy", correct: true },
                { id: 2, text: "Use AWS Config to record the inventory of resources that are used in the prototype infrastructure.", correct: false },
                { id: 3, text: "Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS CloudFormation is a service that allows you to define and provision infrastructure as code. This means that you can create a template that describes the resources you want to create, and then use CloudFormation to deploy those resources in an automated fashion. In this case, the solutions architect should define the infrastructure as a template by using the prototype infrastructure as a guide. The template should include resources for an Auto Scaling group, an Application Load Balancer, and an Amazon RDS database. Once the template is created, the solutions architect can use CloudFormation to deploy the infrastructure in two Availability Zones.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object \nstorage. The chief information security officer has directed that no application traffic between the \ntwo services should traverse the public internet. \n \nWhich capability should the solutions architect use to meet the compliance requirements?",
            options: [
                { id: 0, text: "AWS Key Management Service (AWS KMS)", correct: false },
                { id: 1, text: "VPC endpoint", correct: true },
                { id: 2, text: "Private subnet", correct: false },
                { id: 3, text: "Virtual private gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA VPC endpoint enables you to privately access AWS services without requiring internet gateways, NAT gateways, VPN connections, or AWS Direct Connect connections. It allows you to connect your VPC directly to supported AWS services, such as Amazon S3, over a private connection within the AWS network. By creating a VPC endpoint for Amazon S3, the traffic between your EC2 instances and S3 will stay within the AWS network and won't traverse the public internet. This provides a more secure and compliant solution, as the data transfer remains within the private network boundaries.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for \nMySQL server forms the database layer Amazon ElastiCache forms the cache layer. The \ncompany wants a caching strategy that adds or updates data in the cache when a customer adds \nan item to the database. The data in the cache must always match the data in the database. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Implement the lazy loading caching strategy", correct: false },
                { id: 1, text: "Implement the write-through caching strategy", correct: true },
                { id: 2, text: "Implement the adding TTL caching strategy", correct: false },
                { id: 3, text: "Implement the AWS AppConfig caching strategy", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIn the write-through caching strategy, when a customer adds or updates an item in the database, Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A company wants to migrate 100 GB of historical data from an on-premises location to an \nAmazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on \npremises. The company needs to encrypt the data in transit to the S3 bucket. The company will \nstore new data directly in Amazon S3. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket", correct: false },
                { id: 1, text: "Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket", correct: true },
                { id: 2, text: "Use AWS Snowball to move the data to an S3 bucket", correct: false },
                { id: 3, text: "Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies and automates the process of moving data between on-premises storage and Amazon S3. It provides secure and efficient data transfer with built-in encryption, ensuring that the data is encrypted in transit. By using AWS DataSync, the company can easily migrate the 100 GB of historical data from their on-premises location to an S3 bucket. DataSync will handle the encryption of data in transit and ensure secure transfer.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company containerized a Windows job that runs on .NET 6 Framework under a Windows \ncontainer. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. \nThe job's runtime varies between 1 minute and 3 minutes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create an AWS Lambda function based on the container image of the job. Configure Amazon", correct: false },
                { id: 1, text: "Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling", correct: false },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a", correct: true },
                { id: 3, text: "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy leveraging AWS Fargate and ECS, you can achieve cost-effective scaling and resource allocation for your containerized Windows job running on .NET 6 Framework in the AWS Cloud. The serverless nature of Fargate ensures that you only pay for the actual resources consumed by your containers, allowing for efficient cost management.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 63,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n284 \nA company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
            options: [
                { id: 0, text: "Create a new organization in AWS Organizations with all features turned on. Create the new", correct: true },
                { id: 1, text: "Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-", correct: false },
                { id: 2, text: "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity", correct: false },
                { id: 3, text: "Create a new organization in AWS Organizations. Configure the organization's authentication", correct: false },
                { id: 4, text: "Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM", correct: false },
                { id: 0, text: "By creating a new organization in AWS Organizations, you can establish a consolidated multi-", correct: false },
                { id: 4, text: "Setting up AWS IAM Identity Center (AWS Single Sign-On) within the organization enables", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA. By creating a new organization in AWS Organizations, you can establish a consolidated multi- account architecture. This allows you to create and manage multiple AWS accounts for different business units under a single organization. E. Setting up AWS IAM Identity Center (AWS Single Sign-On) within the organization enables you to integrate it with the company's corporate directory service. This integration allows for centralized authentication, where users can sign in using their corporate credentials and access the AWS accounts within the organization. Together, these actions create a centralized, multi-account architecture that leverages AWS Organizations for account management and AWS IAM Identity Center (AWS Single Sign-On) for authentication and access control.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company is looking for a solution that can store video archives in AWS from old news footage. \nThe company needs to minimize costs and will rarely need to restore these files. When the files \nare needed, they must be available in a maximum of five minutes. \n \nWhat is the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Store the video archives in Amazon S3 Glacier and use Expedited retrievals.", correct: true },
                { id: 1, text: "Store the video archives in Amazon S3 Glacier and use Standard retrievals.", correct: false },
                { id: 2, text: "Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).", correct: false },
                { id: 3, text: "Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy choosing Expedited retrievals in Amazon S3 Glacier, you can reduce the retrieval time to minutes, making it suitable for scenarios where quick access is required. Expedited retrievals come with a higher cost per retrieval compared to standard retrievals but provide faster access to your archived data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test18: [
        {
            id: 0,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n285 \nA company is building a three-tier application on AWS. The presentation tier will serve a static \nwebsite The logic tier is a containerized application. This application will store data in a relational \ndatabase. The company wants to simplify deployment and to reduce operational costs. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS)", correct: true },
                { id: 1, text: "Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon", correct: false },
                { id: 2, text: "Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS)", correct: false },
                { id: 3, text: "Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon S3 is a highly scalable and cost-effective storage service that can be used to host static website content. It provides durability, high availability, and low latency access to the static files. Amazon ECS with AWS Fargate eliminates the need to manage the underlying infrastructure. It allows you to run containerized applications without provisioning or managing EC2 instances. This reduces operational overhead and provides scalability. By using a managed Amazon RDS cluster for the database, you can offload the management tasks such as backups, patching, and monitoring to AWS. This reduces the operational burden and ensures high availability and durability of the database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company seeks a storage solution for its application. The solution must be highly available and \nscalable. The solution also must function as a file system be mountable by multiple Linux \ninstances in AWS and on premises through native protocols, and have no minimum size \nrequirements. The company has set up a Site-to-Site VPN for access from its on-premises \nnetwork to its VPC. \n \nWhich storage solution meets these requirements?",
            options: [
                { id: 0, text: "Amazon FSx Multi-AZ deployments", correct: false },
                { id: 1, text: "Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes", correct: false },
                { id: 2, text: "Amazon Elastic File System (Amazon EFS) with multiple mount targets", correct: true },
                { id: 3, text: "Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon EFS is a fully managed file system service that provides scalable, shared storage for Amazon EC2 instances. It supports the Network File System version 4 (NFSv4) protocol, which is a native protocol for Linux-based systems. EFS is designed to be highly available, durable, and scalable.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A 4-year-old media company is using the AWS Organizations all features feature set to organize \nits AWS accounts. According to the company's finance team, the billing information on the \nmember accounts must not be accessible to anyone, including the root user of the member \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n286 \naccounts. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Add all finance team users to an IAM group. Attach an AWS managed policy named Billing to the", correct: false },
                { id: 1, text: "Attach an identity-based policy to deny access to the billing information to all users, including the", correct: false },
                { id: 2, text: "Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to", correct: true },
                { id: 3, text: "Convert from the Organizations all features feature set to the Organizations consolidated billing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nService control policy are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization's access control guidelines. SCPs are available only in an organization that has all features enabled.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "An ecommerce company runs an application in the AWS Cloud that is integrated with an on-\npremises warehouse solution. The company uses Amazon Simple Notification Service (Amazon \nSNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application \ncan process the orders. The local data center team has detected that some of the order \nmessages were not received. \n \nA solutions architect needs to retain messages that are not delivered and analyze the messages \nfor up to 14 days. \n \nWhich solution will meet these requirements with the LEAST development effort?",
            options: [
                { id: 0, text: "Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target", correct: false },
                { id: 1, text: "Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days", correct: false },
                { id: 2, text: "Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service", correct: true },
                { id: 3, text: "Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe message retention period in Amazon SQS can be set between 1 minute and 14 days (the default is 4 days). Therefore, you can configure your SQS DLQ to retain undelivered SNS messages for 14 days. This will enable you to analyze undelivered messages with the least development effort.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 4,
            text: "A gaming company uses Amazon DynamoDB to store user information such as geographic \nlocation, player data, and leaderboards. The company needs to configure continuous backups to \nan Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n287 \nof the application and must not affect the read capacity units (RCUs) that are defined for the \ntable. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.", correct: false },
                { id: 1, text: "Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-", correct: true },
                { id: 2, text: "Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the", correct: false },
                { id: 3, text: "Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nContinuous backups is a native feature of DynamoDB, it works at any scale without having to manage servers or clusters and allows you to export data across AWS Regions and accounts to any point-in-time in the last 35 days at a per-second granularity. Plus, it doesn't affect the read capacity or the availability of your production tables. https://aws.amazon.com/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake- amazon-s3/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A solutions architect is designing an asynchronous application to process credit card data \nvalidation requests for a bank. The application must be secure and be able to process each \nrequest at least once. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)", correct: true },
                { id: 1, text: "Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 2, text: "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/zh_tw/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs- least-privilege-policy.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 6,
            text: "A company has multiple AWS accounts for development work. Some staff consistently use \noversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the \ndevelopment accounts. The company wants to centrally restrict the creation of AWS resources in \nthese accounts. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n288 \nWhich solution will meet these requirements with the LEAST development effort?",
            options: [
                { id: 0, text: "Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the", correct: false },
                { id: 1, text: "Use AWS Organizations to organize the accounts into organizational units (OUs). Define and", correct: true },
                { id: 2, text: "Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2", correct: false },
                { id: 3, text: "Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Organizations: AWS Organizations is a service that helps you centrally manage multiple AWS accounts. It enables you to group accounts into organizational units (OUs) and apply policies across those accounts. Service Control Policies (SCPs): SCPs in AWS Organizations allow you to define fine-grained permissions and restrictions at the account or OU level. By attaching an SCP to the development accounts, you can control the creation and usage of EC2 instance types. Least Development Effort: Option B requires minimal development effort as it leverages the built- in features of AWS Organizations and SCPs. You can define the SCP to restrict the use of oversized EC2 instance types and apply it to the appropriate OUs or accounts.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A company wants to use artificial intelligence (AI) to determine the quality of its customer service \ncalls. The company currently manages calls in four different languages, including English. The \ncompany will offer new languages in the future. The company does not have the resources to \nregularly maintain machine learning (ML) models. \n \nThe company needs to create written sentiment analysis reports from the customer service call \nrecordings. The customer service call recording text must be translated into English. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Use Amazon Comprehend to translate the audio recordings into English.", correct: false },
                { id: 1, text: "Use Amazon Lex to create the written sentiment analysis reports.", correct: false },
                { id: 2, text: "Use Amazon Polly to convert the audio recordings into text.", correct: false },
                { id: 3, text: "Use Amazon Transcribe to convert the audio recordings in any language into text.", correct: true },
                { id: 4, text: "Use Amazon Translate to translate text in any language to English.", correct: false },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon Transcribe will convert the audio recordings into text, Amazon Translate will translate the text into English, and Amazon Comprehend will perform sentiment analysis on the translated text to generate sentiment analysis reports.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A company uses Amazon EC2 instances to host its internal systems. As part of a deployment \noperation, an administrator tries to use the AWS CLI to terminate an EC2 instance. However, the \nadministrator receives a 403 (Access Denied) error message. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n289 \nThe administrator is using an IAM role that has the following IAM policy attached: \n \n \n \nWhat is the cause of the unsuccessful request?",
            options: [
                { id: 0, text: "The EC2 instance has a resource-based policy with a Deny statement.", correct: false },
                { id: 1, text: "The principal has not been specified in the policy statement.", correct: false },
                { id: 2, text: "The \"Action\" field does not grant the actions that are required to terminate the EC2 instance.", correct: false },
                { id: 3, text: "The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A company is conducting an internal audit. The company wants to ensure that the data in an \nAmazon S3 bucket that is associated with the company's AWS Lake Formation data lake does \nnot contain sensitive customer or employee data. The company wants to discover personally \nidentifiable information (PII) or financial information, including passport numbers and credit card \nnumbers. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n290 \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security", correct: false },
                { id: 1, text: "Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the", correct: false },
                { id: 2, text: "Configure Amazon Macie to run a data discovery job that uses managed identifiers for the", correct: true },
                { id: 3, text: "Use Amazon S3 Select to run a report across the S3 bucket.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon Macie is a service that helps discover, classify, and protect sensitive data stored in AWS. It uses machine learning algorithms and managed identifiers to detect various types of sensitive information, including personally identifiable information (PII) and financial information. By configuring Amazon Macie to run a data discovery job with the appropriate managed identifiers for the required data types (such as passport numbers and credit card numbers), the company can identify and classify any sensitive data present in the S3 bucket.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company uses on-premises servers to host its applications. The company is running out of \nstorage capacity. The applications use both block storage and NFS storage. The company needs \na high-performing solution that supports local caching without re-architecting its existing \napplications. \n \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Mount Amazon S3 as a file system to the on-premises servers.", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway file gateway to replace NFS storage.", correct: true },
                { id: 2, text: "Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.", correct: false },
                { id: 3, text: "Deploy an AWS Storage Gateway volume gateway to replace the block storage.", correct: false },
                { id: 4, text: "Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy combining the deployment of an AWS Storage Gateway file gateway and an AWS Storage Gateway volume gateway, the company can address both its block storage and NFS storage needs, while leveraging local caching capabilities for improved performance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket \nin the same AWS Region. The service is deployed on Amazon EC2 instances within the private \nsubnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public \nsubnet. However, the company wants a solution that will reduce the data output costs. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the", correct: false },
                { id: 1, text: "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the", correct: false },
                { id: 2, text: "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the", correct: true },
                { id: 3, text: "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nA VPC gateway endpoint allows you to privately access Amazon S3 from within your VPC without using a NAT gateway or NAT instance. By provisioning a VPC gateway endpoint for S3, the service in the private subnet can directly communicate with S3 without incurring data transfer costs for traffic going through a NAT gateway.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 12,
            text: "A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize \napplication changes, the company stores the pictures as the latest version of an S3 object. The \ncompany needs to retain only the two most recent versions of the pictures. \n \nThe company wants to reduce costs. The company has identified the S3 bucket as a large \nexpense. \n \nWhich solution will reduce the S3 costs with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.", correct: true },
                { id: 1, text: "Use an AWS Lambda function to check for older versions and delete all but the two most recent", correct: false },
                { id: 2, text: "Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent", correct: false },
                { id: 3, text: "Deactivate versioning on the S3 bucket and retain the two most recent versions.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nS3 Lifecycle policies allow you to define rules that automatically transition or expire objects based on their age or other criteria. By configuring an S3 Lifecycle policy to delete expired object versions and retain only the two most recent versions, you can effectively manage the storage costs while maintaining the desired retention policy. This solution is highly automated and requires minimal operational overhead as the lifecycle management is handled by S3 itself.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 13,
            text: "A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The \ncompany's average connection utilization is less than 10%. A solutions architect must \nrecommend a solution that will reduce the cost without compromising security. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS", correct: false },
                { id: 1, text: "Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.", correct: false },
                { id: 2, text: "Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with", correct: false },
                { id: 3, text: "Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nFor Dedicated Connections, 1 Gbps, 10 Gbps, and 100 Gbps ports are available. For Hosted Connections, connection speeds of 50 Mbps, 100 Mbps, 200 Mbps, 300 Mbps, 400 Mbps, 500 Mbps, 1 Gbps, 2 Gbps, 5 Gbps and 10 Gbps may be ordered from approved AWS Direct Connect Partners.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A company has multiple Windows file servers on premises. The company wants to migrate and \nconsolidate its files into an Amazon FSx for Windows File Server file system. File permissions \nmust be preserved to ensure that access rights do not change. \n \nWhich solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the", correct: true },
                { id: 1, text: "Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule", correct: false },
                { id: 2, text: "Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3.", correct: false },
                { id: 3, text: "Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS", correct: false },
                { id: 4, text: "Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA - This option involves deploying DataSync agents on your on-premises file servers and using DataSync to transfer the data directly to the FSx for Windows File Server. DataSync ensures that file permissions are preserved during the migration process. D - This option involves using an AWS Snowcone device, a portable data transfer device. You would connect the Snowcone device to your on-premises network, launch DataSync agents on the device, and schedule DataSync tasks to transfer the data to FSx for Windows File Server. DataSync handles the migration process while preserving file permissions.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company wants to ingest customer payment data into the company's data lake in Amazon S3. \nThe company receives payment data every minute on average. The company wants to analyze \nthe payment data in real time. Then the company wants to ingest the data into the data lake. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to ingest data. Use AWS Lambda to analyze the data in real", correct: false },
                { id: 1, text: "Use AWS Glue to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real", correct: false },
                { id: 2, text: "Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to", correct: true },
                { id: 3, text: "Use Amazon API Gateway to ingest data. Use AWS Lambda to analyze the data in real time.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy leveraging the combination of Amazon Kinesis Data Firehose and Amazon Kinesis Data Analytics, you can efficiently ingest and analyze the payment data in real time without the need for manual processing or additional infrastructure management. This solution provides a streamlined and scalable approach to handle continuous data ingestion and analysis requirements.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A company runs a website that uses a content management system (CMS) on Amazon EC2. The \nCMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance \nfor the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) \nvolume that is mounted inside the EC2 instance. \n \nWhich combination of actions should a solutions architect take to improve the performance and \nresilience of the website? (Choose two.)",
            options: [
                { id: 0, text: "Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance", correct: false },
                { id: 1, text: "Share the website images by using an NFS share from the primary EC2 instance. Mount this", correct: false },
                { id: 2, text: "Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is", correct: true },
                { id: 3, text: "Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to", correct: false },
                { id: 4, text: "Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nBy combining the use of Amazon EFS for shared file storage and Amazon CloudFront for content delivery, you can achieve improved performance and resilience for the website.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company runs an infrastructure monitoring service. The company is building a new feature that \nwill enable the service to monitor data in customer AWS accounts. The new feature will call AWS \nAPIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch \nmetrics. \n \nWhat should the company do to obtain access to customer accounts in the MOST secure way?",
            options: [
                { id: 0, text: "Ensure that the customers create an IAM role in their account with read-only EC2 and", correct: true },
                { id: 1, text: "Create a serverless API that implements a token vending machine to provide temporary AWS", correct: false },
                { id: 2, text: "Ensure that the customers create an IAM user in their account with read-only EC2 and", correct: false },
                { id: 3, text: "Ensure that the customers create an Amazon Cognito user in their account to use an IAM role", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy having customers create an IAM role with the necessary permissions in their own accounts, the company can use AWS Identity and Access Management (IAM) to establish cross-account access. The trust policy allows the company's AWS account to assume the customer's IAM role temporarily, granting access to the specified resources (EC2 instances and CloudWatch metrics) within the customer's account. This approach follows the principle of least privilege, as the company only requests the necessary permissions and does not require long-term access keys or user credentials from the customers.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS \naccounts. The company's networking team has its own AWS account to manage the cloud \nnetwork. \n \nWhat is the MOST operationally efficient solution to connect the VPCs?",
            options: [
                { id: 0, text: "Set up VPC peering connections between each VPC. Update each associated subnet's route", correct: false },
                { id: 1, text: "Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the", correct: false },
                { id: 2, text: "Create an AWS Transit Gateway in the networking team's AWS account. Configure static routes", correct: true },
                { id: 3, text: "Deploy VPN gateways in each VPC. Create a transit VPC in the networking team's AWS account", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nWS Transit Gateway is a highly scalable and centralized hub for connecting multiple VPCs, on- premises networks, and remote networks. It simplifies network connectivity by providing a single entry point and reducing the number of connections required. In this scenario, deploying an AWS Transit Gateway in the networking team's AWS account allows for efficient management and control over the network connectivity across multiple VPCs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 \ninstances run in an Auto Scaling group that uses On-Demand billing. If a job fails on one \ninstance, another instance will reprocess the job. The batch jobs run between 12:00 AM and \n06:00 AM local time every day. \n \nWhich solution will provide EC2 instances to meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto", correct: false },
                { id: 1, text: "Purchase a 1-year Reserved Instance for the specific instance type and operating system of the", correct: false },
                { id: 2, text: "Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances.", correct: true },
                { id: 3, text: "Create a new launch template for the Auto Scaling group. Increase the instance size. Set a policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nPurchasing a 1-year Savings Plan (option A) or a 1-year Reserved Instance (option B) may provide cost savings, but they are more suitable for long-running, steady-state workloads. Since your batch jobs run for a specific period each day, using Spot Instances with the ability to scale out based on CPU usage is a more cost-effective choice.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A social media company is building a feature for its website. The feature will give users the ability \nto upload photos. The company expects significant increases in demand during large events and \nmust ensure that the website can handle the upload traffic from users. \n \nWhich solution meets these requirements with the MOST scalability?",
            options: [
                { id: 0, text: "Upload files from the user's browser to the application servers. Transfer the files to an Amazon S3", correct: false },
                { id: 1, text: "Provision an AWS Storage Gateway file gateway. Upload files directly from the user's browser to", correct: false },
                { id: 2, text: "Generate Amazon S3 presigned URLs in the application. Upload files directly from the user's", correct: true },
                { id: 3, text: "Provision an Amazon Elastic File System (Amazon EFS) file system. Upload files directly from the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThis approach allows users to upload files directly to S3 without passing through the application servers, reducing the load on the application and improving scalability. It leverages the client-side capabilities to handle the file uploads and offloads the processing to S3.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A company has a web application for travel ticketing. The application is based on a database that \nruns in a single data center in North America. The company wants to expand the application to \nserve a global user base. The company needs to deploy the application to multiple AWS Regions. \nAverage latency must be less than 1 second on updates to the reservation database. \n \nThe company wants to have separate deployments of its web platform across multiple Regions. \nHowever, the company must maintain a single primary reservation database that is globally \nconsistent. \n \nWhich solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Convert the application to use Amazon DynamoDB. Use a global table for the center reservation", correct: true },
                { id: 1, text: "Migrate the database to an Amazon Aurora MySQL database. Deploy Aurora Read Replicas in", correct: false },
                { id: 2, text: "Migrate the database to an Amazon RDS for MySQL database. Deploy MySQL read replicas in", correct: false },
                { id: 3, text: "Migrate the application to an Amazon Aurora Serverless database. Deploy instances of the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nUsing DynamoDB's global tables feature, you can achieve a globally consistent reservation database with low latency on updates, making it suitable for serving a global user base. The automatic replication provided by DynamoDB eliminates the need for manual synchronization between Regions.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 \ninstances that run in the us-west-1 Region. The company manually backs up the workloads to \ncreate an image as needed. \n \nIn the event of a natural disaster in the us-west-1 Region, the company wants to recover \nworkloads quickly in the us-west-2 Region. The company wants no more than 24 hours of data \nloss on the EC2 instances. The company also wants to automate any backups of the EC2 \ninstances. \n \nWhich solutions will meet these requirements with the LEAST administrative effort? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup", correct: false },
                { id: 1, text: "Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup", correct: true },
                { id: 2, text: "Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup. Create a backup plan", correct: false },
                { id: 3, text: "Create a backup vault by using AWS Backup. Use AWS Backup to create a backup plan for the", correct: false },
                { id: 4, text: "Create a backup vault by using AWS Backup. Use AWS Backup to create a backup plan for the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSolutions are both automated and require no manual intervention to create or copy backups。\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A company operates a two-tier application for image processing. The application uses two \nAvailability Zones, each with one public subnet and one private subnet. An Application Load \nBalancer (ALB) for the web tier uses the public subnets. Amazon EC2 instances for the \napplication tier use the private subnets. \n \nUsers report that the application is running more slowly than expected. A security audit of the web \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n297 \nserver log files shows that the application is receiving millions of illegitimate requests from a small \nnumber of IP addresses. A solutions architect needs to resolve the immediate performance \nproblem while the company investigates a more permanent solution. \n \nWhat should the solutions architect recommend to meet this requirement?",
            options: [
                { id: 0, text: "Modify the inbound security group for the web tier. Add a deny rule for the IP addresses that are", correct: false },
                { id: 1, text: "Modify the network ACL for the web tier subnets. Add an inbound deny rule for the IP addresses", correct: true },
                { id: 2, text: "Modify the inbound security group for the application tier. Add a deny rule for the IP addresses", correct: false },
                { id: 3, text: "Modify the network ACL for the application tier subnets. Add an inbound deny rule for the IP", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIn this scenario, the security audit reveals that the application is receiving millions of illegitimate requests from a small number of IP addresses. To address this issue, it is recommended to modify the network ACL (Access Control List) for the web tier subnets. By adding an inbound deny rule specifically targeting the IP addresses that are consuming resources, the network ACL can block the illegitimate traffic at the subnet level before it reaches the web servers. This will help alleviate the excessive load on the web tier and improve the application's performance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "A global marketing company has applications that run in the ap-southeast-2 Region and the eu-\nwest-1 Region. Applications that run in a VPC in eu-west-1 need to communicate securely with \ndatabases that run in a VPC in ap-southeast-2. \n \nWhich network design will meet these requirements?",
            options: [
                { id: 0, text: "Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC.", correct: false },
                { id: 1, text: "Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC.", correct: false },
                { id: 2, text: "Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1", correct: true },
                { id: 3, text: "Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap-", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nYou cannot reference the security group of a peer VPC that's in a different Region. Instead, use the CIDR block of the peer VPC. https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n298 \nA company is developing software that uses a PostgreSQL database schema. The company \nneeds to configure multiple development environments and databases for the company's \ndevelopers. On average, each development environment is used for half of the 8-hour workday. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure each development environment with its own Amazon Aurora PostgreSQL database", correct: false },
                { id: 1, text: "Configure each development environment with its own Amazon RDS for PostgreSQL Single-AZ", correct: false },
                { id: 2, text: "Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-", correct: true },
                { id: 3, text: "Configure each development environment with its own Amazon S3 bucket by using Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nWith Aurora Serverless, you create a database, specify the desired database capacity range, and connect your applications. You pay on a per-second basis for the database capacity that you use when the database is active, and migrate between standard and serverless configurations with a few steps in the Amazon Relational Database Service (Amazon RDS) console.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "A company uses AWS Organizations with resources tagged by account. The company also uses \nAWS Backup to back up its AWS infrastructure resources. The company needs to back up all \nAWS resources. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Config to identify all untagged resources. Tag the identified resources", correct: true },
                { id: 1, text: "Use AWS Config to identify all resources that are not running. Add those resources to the backup", correct: false },
                { id: 2, text: "Require all AWS account owners to review their resources to identify the resources that need to", correct: false },
                { id: 3, text: "Use Amazon Inspector to identify all noncompliant resources.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThis solution allows you to leverage AWS Config to identify any untagged resources within your AWS Organizations accounts. Once identified, you can programmatically apply the necessary tags to indicate the backup requirements for each resource. By using tags in the backup plan configuration, you can ensure that only the tagged resources are included in the backup process, reducing operational overhead and ensuring all necessary resources are backed up.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "A social media company wants to allow its users to upload images in an application that is hosted \nin the AWS Cloud. The company needs a solution that automatically resizes the images so that \nthe images can be displayed on multiple device types. The application experiences unpredictable \ntraffic patterns throughout the day. The company is seeking a highly available solution that \nmaximizes scalability. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n299",
            options: [
                { id: 0, text: "Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the", correct: true },
                { id: 1, text: "Create a static website hosted in Amazon CloudFront that invokes AWS Step Functions to resize", correct: false },
                { id: 2, text: "Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance.", correct: false },
                { id: 3, text: "Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy using Amazon S3 and AWS Lambda together, you can create a serverless architecture that provides highly scalable and available image resizing capabilities. Here's how the solution would work: Set up an Amazon S3 bucket to store the original images uploaded by users. Configure an event trigger on the S3 bucket to invoke an AWS Lambda function whenever a new image is uploaded. The Lambda function can be designed to retrieve the uploaded image, perform the necessary resizing operations based on device requirements, and store the resized images back in the S3 bucket or a different bucket designated for resized images. Configure the Amazon S3 bucket to make the resized images publicly accessible for serving to users.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company is running a microservices application on Amazon EC2 instances. The company \nwants to migrate the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster \nfor scalability. The company must configure the Amazon EKS control plane with endpoint private \naccess set to true and endpoint public access set to false to maintain security compliance. The \ncompany must also put the data plane in private subnets. However, the company has received \nerror notifications because the node cannot join the cluster. \n \nWhich solution will allow the node to join the cluster?",
            options: [
                { id: 0, text: "Grant the required permission in AWS Identity and Access Management (IAM) to the", correct: false },
                { id: 1, text: "Create interface VPC endpoints to allow nodes to access the control plane.", correct: true },
                { id: 2, text: "Recreate nodes in the public subnet. Restrict security groups for EC2 nodes.", correct: false },
                { id: 3, text: "Allow outbound traffic in the security group of the nodes.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy creating interface VPC endpoints, you can enable the necessary communication between the Amazon EKS control plane and the nodes in private subnets. This solution ensures that the control plane maintains endpoint private access (set to true) and endpoint public access (set to false) for security compliance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A company is migrating an on-premises application to AWS. The company wants to use Amazon \nRedshift as a solution. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n300 \nWhich use cases are suitable for Amazon Redshift in this scenario? (Choose three.)",
            options: [
                { id: 0, text: "Supporting data APIs to access data with traditional, containerized, and event-driven applications", correct: false },
                { id: 1, text: "Supporting client-side and server-side encryption", correct: true },
                { id: 2, text: "Building analytics workloads during specified hours and when the application is not active", correct: false },
                { id: 3, text: "Caching data to reduce the pressure on the backend database", correct: false },
                { id: 4, text: "Scaling globally to support petabytes of data and tens of millions of requests per minute", correct: false },
                { id: 1, text: "Supporting client-side and server-side encryption: Amazon Redshift supports both client-side", correct: false },
                { id: 2, text: "Building analytics workloads during specified hours and when the application is not active:", correct: false },
                { id: 4, text: "Scaling globally to support petabytes of data and tens of millions of requests per minute:", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nB. Supporting client-side and server-side encryption: Amazon Redshift supports both client-side and server-side encryption for improved data security. C. Building analytics workloads during specified hours and when the application is not active: Amazon Redshift is optimized for running complex analytic queries against very large datasets, making it a good choice for this use case. E. Scaling globally to support petabytes of data and tens of millions of requests per minute: Amazon Redshift is designed to handle petabytes of data, and to deliver fast query and I/O performance for virtually any size dataset.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company provides an API interface to customers so the customers can retrieve their financial \ninformation. 舎e company expects a larger number of requests during peak usage times of the \nyear. \n \nThe company requires the API to respond consistently with low latency to ensure customer \nsatisfaction. The company needs to provide a compute host for the API. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).", correct: false },
                { id: 1, text: "Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.", correct: true },
                { id: 2, text: "Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS)", correct: false },
                { id: 3, text: "Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIn the context of the given scenario, where the company wants low latency and consistent performance for their API during peak usage times, it would be more suitable to use provisioned concurrency. By allocating a specific number of concurrent executions, the company can ensure that there are enough function instances available to handle the expected load and minimize the impact of cold starts. This will result in lower latency and improved performance for the API.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 \nbucket for archival purposes. \n \nWhich solution will meet this requirement with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session", correct: true },
                { id: 1, text: "Install the Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Export the logs", correct: false },
                { id: 2, text: "Create a Systems Manager document to upload all server logs to a central S3 bucket. Use", correct: false },
                { id: 3, text: "Install an Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Create a", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 32,
            text: "An application uses an Amazon RDS MySQL DB instance. The RDS database is becoming low \non disk space. A solutions architect wants to increase the disk space without downtime. \n \nWhich solution meets these requirements with the LEAST amount of effort?",
            options: [
                { id: 0, text: "Enable storage autoscaling in RDS", correct: true },
                { id: 1, text: "Increase the RDS database instance size", correct: false },
                { id: 2, text: "Change the RDS database instance storage type to Provisioned IOPS", correct: false },
                { id: 3, text: "Back up the RDS database, increase the storage capacity, restore the database, and stop the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nEnabling storage autoscaling allows RDS to automatically adjust the storage capacity based on the application's needs. When the storage usage exceeds a predefined threshold, RDS will automatically increase the allocated storage without requiring manual intervention or causing downtime. This ensures that the RDS database has sufficient disk space to handle the increasing storage requirements.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A consulting company provides professional services to customers worldwide. The company \nprovides solutions and tools for customers to expedite gathering and analyzing data on AWS. The \ncompany needs to centrally manage and deploy a common set of solutions and tools for \ncustomers to use for self-service purposes. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create AWS CloudFormation templates for the customers.", correct: false },
                { id: 1, text: "Create AWS Service Catalog products for the customers.", correct: true },
                { id: 2, text: "Create AWS Systems Manager templates for the customers.", correct: false },
                { id: 3, text: "Create AWS Config items for the customers.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Service Catalog allows you to create and manage catalogs of IT services that can be deployed within your organization. With Service Catalog, you can define a standardized set of products (solutions and tools in this case) that customers can self-service provision. By creating Service Catalog products, you can control and enforce the deployment of approved and validated Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "A company is designing a new web application that will run on Amazon EC2 Instances. The \napplication will use Amazon DynamoDB for backend data storage. The application traffic will be \nunpredictable. The company expects that the application read and write throughput to the \ndatabase will be moderate to high. The company needs to scale in response to application traffic. \n \nWhich DynamoDB table configuration will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure DynamoDB with provisioned read and write by using the DynamoDB Standard table", correct: false },
                { id: 1, text: "Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.", correct: true },
                { id: 2, text: "Configure DynamoDB with provisioned read and write by using the DynamoDB Standard", correct: false },
                { id: 3, text: "Configure DynamoDB in on-demand mode by using the DynamoDB Standard Infrequent Access", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Service Catalog allows you to create and manage catalogs of IT services that can be deployed within your organization. With Service Catalog, you can define a standardized set of products (solutions and tools in this case) that customers can self-service provision. By creating Service Catalog products, you can control and enforce the deployment of approved and validated solutions and tools.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A retail company has several businesses. The IT team for each business manages its own AWS \naccount. Each team account is part of an organization in AWS Organizations. Each team \nmonitors its product inventory levels in an Amazon DynamoDB table in the team's own AWS \naccount. \n \nThe company is deploying a central inventory reporting application into a shared AWS account. \nThe application must be able to read items from all the teams' DynamoDB tables. \n \nWhich authentication option will meet these requirements MOST securely?",
            options: [
                { id: 0, text: "Integrate DynamoDB with AWS Secrets Manager in the inventory application account. Configure", correct: false },
                { id: 1, text: "In every business account, create an IAM user that has programmatic access. Configure the", correct: false },
                { id: 2, text: "In every business account, create an IAM role named BU_ROLE with a policy that gives the role", correct: true },
                { id: 3, text: "Integrate DynamoDB with AWS Certificate Manager (ACM). Generate identity certificates to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nIAM Roles: IAM roles provide a secure way to grant permissions to entities within AWS. By creating an IAM role in each business account named BU_ROLE with the necessary permissions to access the DynamoDB table, the access can be controlled at the IAM role level. Cross-Account Access: By configuring a trust policy in the BU_ROLE that trusts a specific role in the inventory application account (APP_ROLE), you establish a trusted relationship between the two accounts. Least Privilege: By creating a specific IAM role (BU_ROLE) in each business account and granting it access only to the required DynamoDB table, you can ensure that each team's table is accessed with the least privilege principle. Security Token Service (STS): The use of STS AssumeRole API operation in the inventory application account allows the application to assume the cross-account role (BU_ROLE) in each business account.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon \nEKS). The company's workload is not consistent throughout the day. The company wants \nAmazon EKS to scale in and out according to the workload. \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
            options: [
                { id: 0, text: "Use an AWS Lambda function to resize the EKS cluster.", correct: false },
                { id: 1, text: "Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.", correct: true },
                { id: 2, text: "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.", correct: false },
                { id: 3, text: "Use Amazon API Gateway and connect it to Amazon EKS.", correct: false },
                { id: 4, text: "Use AWS App Mesh to observe network activity.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy combining the Kubernetes Cluster Autoscaler (option C) to manage the number of nodes in the cluster and enabling horizontal pod autoscaling (option B) with the Kubernetes Metrics Server, you can achieve automatic scaling of your EKS cluster and container applications based on workload demand. This approach minimizes operational overhead as it leverages built-in Kubernetes functionality and automation mechanisms.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A company runs a microservice-based serverless web application. The application must be able \nto retrieve data from multiple Amazon DynamoDB tables A solutions architect needs to give the \napplication the ability to retrieve the data with no impact on the baseline performance of the \napplication. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "AWS AppSync pipeline resolvers", correct: false },
                { id: 1, text: "Amazon CloudFront with Lambda@Edge functions", correct: false },
                { id: 2, text: "Edge-optimized Amazon API Gateway with AWS Lambda functions", correct: false },
                { id: 3, text: "Amazon Athena Federated Query with a DynamoDB connector", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "A company wants to analyze and troubleshoot Access Denied errors and Unauthorized errors \nthat are related to IAM permissions. The company has AWS CloudTrail turned on. \n \nWhich solution will meet these requirements with the LEAST effort?",
            options: [
                { id: 0, text: "Use AWS Glue and write custom scripts to query CloudTrail logs for the errors.", correct: false },
                { id: 1, text: "Use AWS Batch and write custom scripts to query CloudTrail logs for the errors.", correct: false },
                { id: 2, text: "Search CloudTrail logs with Amazon Athena queries to identify the errors.", correct: true },
                { id: 3, text: "Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n\"Using Athena with CloudTrail logs is a powerful way to enhance your analysis of AWS service activity.\" https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company wants to add its existing AWS usage cost to its operation cost dashboard. A solutions \narchitect needs to recommend a solution that will give the company access to its usage cost \nprogrammatically. The company must be able to access cost data for the current year and \nforecast costs for the next 12 months. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Access usage cost-related data by using the AWS Cost Explorer API with pagination.", correct: true },
                { id: 1, text: "Access usage cost-related data by using downloadable AWS Cost Explorer report .csv files.", correct: false },
                { id: 2, text: "Configure AWS Budgets actions to send usage cost data to the company through FTP.", correct: false },
                { id: 3, text: "Create AWS Budgets reports for usage cost data. Send the data to the company through SMTP.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nYou can view your costs and usage using the Cost Explorer user interface free of charge. You can also access your data programmatically using the Cost Explorer API. Each paginated API request incurs a charge of $0.01. You can't disable Cost Explorer after you enable it. https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-cost- explorer/interfaces/costexplorerpaginationconfiguration.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A solutions architect is reviewing the resilience of an application. The solutions architect notices \nthat a database administrator recently failed over the application's Amazon Aurora PostgreSQL \ndatabase writer instance as part of a scaling exercise. The failover resulted in 3 minutes of \ndowntime for the application. \n \nWhich solution will reduce the downtime for scaling exercises with the LEAST operational \noverhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n305",
            options: [
                { id: 0, text: "Create more Aurora PostgreSQL read replicas in the cluster to handle the load during failover.", correct: false },
                { id: 1, text: "Set up a secondary Aurora PostgreSQL cluster in the same AWS Region. During failover, update", correct: false },
                { id: 2, text: "Create an Amazon ElastiCache for Memcached cluster to handle the load during failover.", correct: false },
                { id: 3, text: "Set up an Amazon RDS proxy for the database. Update the application to use the proxy endpoint.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon RDS proxy allows you to automatically route write request to the healthy writer, minimizing downtime.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company has a regional subscription-based streaming service that runs in a single AWS \nRegion. The architecture consists of web servers and application servers on Amazon EC2 \ninstances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The \narchitecture includes an Amazon Aurora global database cluster that extends across multiple \nAvailability Zones. \n \nThe company wants to expand globally and to ensure that its application has minimal downtime. \n \nWhich solution will provide the MOST fault tolerance?",
            options: [
                { id: 0, text: "Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in", correct: false },
                { id: 1, text: "Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL", correct: false },
                { id: 2, text: "Deploy the web tier and the application tier to a second Region. Create an Aurora PostgreSQL", correct: false },
                { id: 3, text: "Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAws Aurora Global Database allows you to read and write from any region in the global cluster. This enables you to distribute read and write workloads globally, improving performance and reducing latency. Data is replicated synchronously across regions, ensuring strong consistency.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A data analytics company wants to migrate its batch processing system to AWS. The company \nreceives thousands of small data files periodically during the day through FTP. An on-premises \nbatch job processes the data files overnight. However, the batch job takes hours to finish running. \n \nThe company wants the AWS solution to process incoming data files as soon as possible with \nminimal changes to the FTP clients that send the files. The solution must delete the incoming \ndata files after the files have been processed successfully. Processing for each file needs to take \n3-8 minutes. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n306 \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in", correct: false },
                { id: 1, text: "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon", correct: false },
                { id: 2, text: "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic", correct: false },
                { id: 3, text: "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 43,
            text: "A company is migrating its workloads to AWS. The company has transactional and sensitive data \nin its databases. The company wants to use AWS Cloud solutions to increase security and \nreduce operational overhead for the databases. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the databases to Amazon EC2. Use an AWS Key Management Service (AWS KMS)", correct: false },
                { id: 1, text: "Migrate the databases to Amazon RDS Configure encryption at rest.", correct: true },
                { id: 2, text: "Migrate the data to Amazon S3 Use Amazon Macie for data security and protection", correct: false },
                { id: 3, text: "Migrate the database to Amazon RDS. Use Amazon CloudWatch Logs for data security and", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A company has an online gaming application that has TCP and UDP multiplayer gaming \ncapabilities. The company uses Amazon Route 53 to point the application traffic to multiple \nNetwork Load Balancers (NLBs) in different AWS Regions. The company needs to improve \napplication performance and decrease latency for the online game in preparation for user growth. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Add an Amazon CloudFront distribution in front of the NLBs. Increase the Cache-Control max-age", correct: false },
                { id: 1, text: "Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-", correct: false },
                { id: 2, text: "Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use", correct: true },
                { id: 3, text: "Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company needs to integrate with a third-party data feed. The data feed sends a webhook to \nnotify an external service when new data is ready for consumption. A developer wrote an AWS \nLambda function to retrieve data when the company receives a webhook callback. The developer \nmust make the Lambda function available for the third party to call. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create a function URL for the Lambda function. Provide the Lambda function URL to the third", correct: true },
                { id: 1, text: "Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the ALB URL", correct: false },
                { id: 2, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to the", correct: false },
                { id: 3, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A company has a workload in an AWS Region. Customers connect to and access the workload \nby using an Amazon API Gateway REST API. The company uses Amazon Route 53 as its DNS \nprovider. The company wants to provide individual and secure URLs for all customers. \n \nWhich combination of steps will meet these requirements with the MOST operational efficiency? \n(Choose three.)",
            options: [
                { id: 0, text: "Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53", correct: true },
                { id: 1, text: "Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a", correct: false },
                { id: 2, text: "Create hosted zones for each customer as required in Route 53. Create zone records that point", correct: false },
                { id: 3, text: "Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager", correct: false },
                { id: 4, text: "Create multiple API endpoints for each customer in API Gateway.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A company stores data in Amazon S3. According to regulations, the data must not contain \npersonally identifiable information (PII). The company recently discovered that S3 buckets have \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n308 \nsome objects that contain PII. The company needs to automatically detect PII in S3 buckets and \nto notify the company's security team. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type", correct: true },
                { id: 1, text: "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type", correct: false },
                { id: 2, text: "Use Amazon Macie. Create an Amazon EventBridge rule to filter the", correct: false },
                { id: 3, text: "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A company wants to build a logging solution for its multiple AWS accounts. The company \ncurrently stores the logs from all accounts in a centralized account. The company has created an \nAmazon S3 bucket in the centralized account to store the VPC flow logs and AWS CloudTrail \nlogs. All logs must be highly available for 30 days for frequent analysis, retained for an additional \n60 days for backup purposes, and deleted 90 days after creation. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Transition objects to the S3 Standard storage class 30 days after creation. Write an expiration", correct: false },
                { id: 1, text: "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days", correct: false },
                { id: 2, text: "Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation. Write", correct: true },
                { id: 3, text: "Transition objects to the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class 30", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 49,
            text: "A company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its \nworkloads. All secrets that are stored in Amazon EKS must be encrypted in the Kubernetes etcd \nkey-value store. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a new AWS Key Management Service (AWS KMS) key. Use AWS Secrets Manager to", correct: false },
                { id: 1, text: "Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS", correct: true },
                { id: 2, text: "Create the Amazon EKS cluster with default options. Use the Amazon Elastic Block Store", correct: false },
                { id: 3, text: "Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A company wants to provide data scientists with near real-time read-only access to the \ncompany's production Amazon RDS for PostgreSQL database. The database is currently \nconfigured as a Single-AZ database. The data scientists use complex queries that will not affect \nthe production database. The company needs a solution that is highly available. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Scale the existing production database in a maintenance window to provide enough power for the", correct: false },
                { id: 1, text: "Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary", correct: false },
                { id: 2, text: "Change the setup from a Single-AZ to a Multi-AZ instance deployment. Provide two additional", correct: false },
                { id: 3, text: "Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nMulti-AZ instance: the standby instance doesn't serve any read or write traffic. Multi-AZ DB cluster: consists of primary instance running in one AZ serving read-write traffic and two other standby running in two different AZs serving read traffic. https://aws.amazon.com/blogs/database/choose-the-right-amazon-rds-deployment-option-single- az-instance-multi-az-instance-or-multi-az-database-cluster/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 51,
            text: "A company runs a three-tier web application in the AWS Cloud that operates across three \nAvailability Zones. The application architecture has an Application Load Balancer, an Amazon \nEC2 web server that hosts user session states, and a MySQL database that runs on an EC2 \ninstance. The company expects sudden increases in application traffic. The company wants to be \nable to scale to meet future application capacity demands and to ensure high availability across \nall three Availability Zones. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.", correct: true },
                { id: 1, text: "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.", correct: false },
                { id: 2, text: "Migrate the MySQL database to Amazon DynamoDB Use DynamoDB Accelerator (DAX) to", correct: false },
                { id: 3, text: "Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone. Use", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nMemcached is best suited for caching data, while Redis is better for storing data that needs to be persisted. If you need to store data that needs to be accessed frequently, such as user profiles, session data, and application settings, then Redis is the better choice.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 52,
            text: "A global video streaming company uses Amazon CloudFront as a content distribution network \n(CDN). The company wants to roll out content in a phased manner across multiple countries. The \ncompany needs to ensure that viewers who are outside the countries to which the company rolls \nout content are not able to view the content. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Add geographic restrictions to the content in CloudFront by using an allow list. Set up a custom", correct: true },
                { id: 1, text: "Set up a new URL tor restricted content. Authorize access by using a signed URL and cookies.", correct: false },
                { id: 2, text: "Encrypt the data for the content that the company distributes. Set up a custom error message.", correct: false },
                { id: 3, text: "Create a new URL for restricted content. Set up a time-restricted access policy for signed URLs.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) \nconfiguration. The company's core production business application uses Microsoft SQL Server \nStandard, which runs on a virtual machine (VM). The application has a recovery point objective \n(RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes. The DR \nsolution needs to minimize costs wherever possible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure a multi-site active/active setup between the on-premises server and AWS by using", correct: false },
                { id: 1, text: "Configure a warm standby Amazon RDS for SQL Server database on AWS. Configure AWS", correct: true },
                { id: 2, text: "Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.", correct: false },
                { id: 3, text: "Use third-party backup software to capture backups every night. Store a secondary set of", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n311 \nA company has an on-premises server that uses an Oracle database to process and store \ncustomer information. The company wants to use an AWS database service to achieve higher \navailability and to improve application performance. The company also wants to offload reporting \nfrom its primary database system. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in", correct: false },
                { id: 1, text: "Use Amazon RDS in a Single-AZ deployment to create an Oracle database. Create a read replica", correct: false },
                { id: 2, text: "Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database.", correct: true },
                { id: 3, text: "Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A company wants to build a web application on AWS. Client access requests to the website are \nnot predictable and can be idle for a long time. Only customers who have paid a subscription fee \ncan have the ability to sign in and use the web application. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
            options: [
                { id: 0, text: "Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create", correct: true },
                { id: 1, text: "Create an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load", correct: false },
                { id: 2, text: "Create an Amazon Cognito user pool to authenticate users.", correct: false },
                { id: 3, text: "Create an Amazon Cognito identity pool to authenticate users.", correct: false },
                { id: 4, text: "Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A media company uses an Amazon CloudFront distribution to deliver content over the internet. \nThe company wants only premium customers to have access to the media streams and file \ncontent. The company stores all content in an Amazon S3 bucket. The company also delivers \ncontent on demand to customers for a specific purpose, such as movie rentals or music \ndownloads. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Generate and provide S3 signed cookies to premium customers.", correct: false },
                { id: 1, text: "Generate and provide CloudFront signed URLs to premium customers.", correct: true },
                { id: 2, text: "Use origin access control (OAC) to limit the access of non-premium customers.", correct: false },
                { id: 3, text: "Generate and activate field-level encryption to block non-premium customers.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A company runs Amazon EC2 instances in multiple AWS accounts that are individually bled. The \ncompany recently purchased a Savings Pian. Because of changes in the company's business \nrequirements, the company has decommissioned a large number of EC2 instances. The \ncompany wants to use its Savings Plan discounts on its other AWS accounts. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "From the AWS Account Management Console of the management account, turn on discount", correct: true },
                { id: 1, text: "From the AWS Account Management Console of the account that purchased the existing Savings", correct: false },
                { id: 2, text: "From the AWS Organizations management account, use AWS Resource Access Manager (AWS", correct: false },
                { id: 3, text: "Create an organization in AWS Organizations in a new payer account. Invite the other AWS", correct: false },
                { id: 4, text: "Create an organization in AWS Organizations in the existing AWS account with the existing EC2", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 58,
            text: "A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API \nGateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A \nsolutions architect needs to create a solution that has minimal effects on customers and minimal \ndata loss to release the new version of APIs. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point", correct: true },
                { id: 1, text: "Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format.", correct: false },
                { id: 2, text: "Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format.", correct: false },
                { id: 3, text: "Create a new API Gateway endpoint with new versions of the API definitions. Create a custom", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A recent analysis of a company's IT expenses highlights the need to reduce backup costs. The \ncompany's chief information officer wants to simplify the on-premises backup infrastructure and \nreduce costs by eliminating the use of physical backup tapes. The company must preserve the \nexisting investment in the on-premises backup applications and workflows. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.", correct: false },
                { id: 1, text: "Set up an Amazon EFS file system that connects with the backup applications using the NFS", correct: false },
                { id: 2, text: "Set up an Amazon EFS file system that connects with the backup applications using the iSCSI", correct: false },
                { id: 3, text: "Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/storagegateway/vtl/?nc1=h_ls\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A company has data collection sensors at different locations. The data collection sensors stream \na high volume of data to the company. The company wants to design a platform on AWS to \ningest and process high-volume streaming data. The solution must be scalable and support data \ncollection in near real time. The company must store the data in Amazon S3 for future reporting. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.", correct: true },
                { id: 1, text: "Use AWS Glue to deliver streaming data to Amazon S3.", correct: false },
                { id: 2, text: "Use AWS Lambda to deliver streaming data and store the data to Amazon S3.", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon Kinesis Data Firehose: Capture, transform, and load data streams into AWS data stores (S3) in near real-time.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 61,
            text: "A company has separate AWS accounts for its finance, data analytics, and development \ndepartments. Because of costs and security concerns, the company wants to control which \nservices each AWS account can use. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n314",
            options: [
                { id: 0, text: "Use AWS Systems Manager templates to control which AWS services each department can use.", correct: false },
                { id: 1, text: "Create organization units (OUs) for each department in AWS Organizations. Attach service", correct: true },
                { id: 2, text: "Use AWS CloudFormation to automatically provision only the AWS services that each department", correct: false },
                { id: 3, text: "Set up a list of products in AWS Service Catalog in the AWS accounts to manage and control the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company has created a multi-tier application for its ecommerce website. The website uses an \nApplication Load Balancer that resides in the public subnets, a web tier in the public subnets, and \na MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database \nneeds to retrieve product catalog and pricing information that is hosted on the internet by a third-\nparty provider. A solutions architect must devise a strategy that maximizes security without \nincreasing operational overhead. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Deploy a NAT instance in the VPC. Route all the internet-based traffic through the NAT instance.", correct: false },
                { id: 1, text: "Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all", correct: true },
                { id: 2, text: "Configure an internet gateway and attach it to the VPModify the private subnet route table to", correct: false },
                { id: 3, text: "Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda \nenvironment variables. A solutions architect needs to ensure that the required permissions are in \nplace to decrypt and use the environment variables. \n \nWhich steps must the solutions architect take to implement the correct permissions? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Add AWS KMS permissions in the Lambda resource policy.", correct: false },
                { id: 1, text: "Add AWS KMS permissions in the Lambda execution role.", correct: true },
                { id: 2, text: "Add AWS KMS permissions in the Lambda function policy.", correct: false },
                { id: 3, text: "Allow the Lambda execution role in the AWS KMS key policy.", correct: false },
                { id: 4, text: "Allow the Lambda resource policy in the AWS KMS key policy.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTo decrypt environment variables encrypted with AWS KMS, Lambda needs to be granted permissions to call KMS APIs. This is done in two places: The Lambda execution role needs kms:Decrypt and kms:GenerateDataKey permissions added. The execution role governs what AWS services the function code can access. The KMS key policy needs to allow the Lambda execution role to have kms:Decrypt and kms:GenerateDataKey permissions for that specific key. This allows the execution role to use that Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company has a financial application that produces reports. The reports average 50 KB in size \nand are stored in Amazon S3. The reports are frequently accessed during the first week after \nproduction and must be stored for several years. The reports must be retrievable within 6 hours. \n \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.", correct: true },
                { id: 1, text: "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Standard-Infrequent", correct: false },
                { id: 2, text: "Use S3 Intelligent-Tiering. Configure S3 Intelligent-Tiering to transition the reports to S3", correct: false },
                { id: 3, text: "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier Deep Archive", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon S3 Glacier: Expedited Retrieval: Provides access to data within 1-5 minutes. Standard Retrieval: Provides access to data within 3-5 hours. Bulk Retrieval: Provides access to data within 5-12 hours. Amazon S3 Glacier Deep Archive: Standard Retrieval: Provides access to data within 12 hours. Bulk Retrieval: Provides access to data within 48 hours.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test19: [
        {
            id: 0,
            text: "A company needs to optimize the cost of its Amazon EC2 instances. The company also needs to \nchange the type and family of its EC2 instances every 2-3 months. \n \nWhat should the company do to meet these requirements?",
            options: [
                { id: 0, text: "Purchase Partial Upfront Reserved Instances for a 3-year term.", correct: false },
                { id: 1, text: "Purchase a No Upfront Compute Savings Plan for a 1-year term.", correct: true },
                { id: 2, text: "Purchase All Upfront Reserved Instances for a 1-year term.", correct: false },
                { id: 3, text: "Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nEC2 Instance Savings Plans give you the flexibility to change your usage between instances WITHIN a family in that region. https://aws.amazon.com/savingsplans/compute-pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 1,
            text: "A solutions architect needs to review a company's Amazon S3 buckets to discover personally \nidentifiable information (PII). The company stores the PII data in the us-east-1 Region and us-\nwest-2 Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n316",
            options: [
                { id: 0, text: "Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.", correct: true },
                { id: 1, text: "Configure AWS Security Hub for all Regions. Create an AWS Config rule to analyze the data that", correct: false },
                { id: 2, text: "Configure Amazon Inspector to analyze the data that is in Amazon S3.", correct: false },
                { id: 3, text: "Configure Amazon GuardDuty to analyze the data that is in Amazon S3.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon Macie is designed specifically for discovering and classifying sensitive data like PII in S3. This makes it the optimal service to use. Macie can be enabled directly in the required Regions rather than enabling it across all Regions which is unnecessary. This minimizes overhead. Macie can be set up to automatically scan the specified S3 buckets on a schedule. No need to create separate jobs. Security Hub is for security monitoring across AWS accounts, not specific for PII discovery. More overhead than needed. Inspector and GuardDuty are not built for PII discovery in S3 buckets. They provide broader security capabilities.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A company's SAP application has a backend SQL Server database in an on-premises \nenvironment. The company wants to migrate its on-premises application and database server to \nAWS. The company needs an instance type that meets the high demands of its SAP database. \nOn-premises performance data shows that both the SAP application and the database have high \nmemory utilization. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use the compute optimized instance family for the application. Use the memory optimized", correct: false },
                { id: 1, text: "Use the storage optimized instance family for both the application and the database.", correct: false },
                { id: 2, text: "Use the memory optimized instance family for both the application and the database.", correct: true },
                { id: 3, text: "Use the high performance computing (HPC) optimized instance family for the application. Use the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nSince both the app and database have high memory needs, the memory optimized family like R5 instances meet those requirements well. Using the same instance family simplifies management and operations, rather than mixing instance types. Compute optimized instances may not provide enough memory for the SAP app's needs. Storage optimized is overkill for the database's compute and memory needs. HPC is overprovisioned for the SAP app.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "A company runs an application in a VPC with public and private subnets. The VPC extends \nacross multiple Availability Zones. The application runs on Amazon EC2 instances in private \nsubnets. The application uses an Amazon Simple Queue Service (Amazon SQS) queue. \n \nA solutions architect needs to design a secure solution to establish a connection between the \nEC2 instances and the SQS queue. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n317 \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private", correct: true },
                { id: 1, text: "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public", correct: false },
                { id: 2, text: "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public", correct: false },
                { id: 3, text: "Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAn interface VPC endpoint is a private way to connect to AWS services without having to expose your VPC to the public internet. This is the most secure way to connect to Amazon SQS from the private subnets. Configuring the endpoint to use the private subnets ensures that the traffic between the EC2 instances and the SQS queue is only within the VPC. This helps to protect the traffic from being intercepted by a malicious actor. Adding a security group to the endpoint that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets further restricts the traffic to only the authorized sources. This helps to prevent unauthorized access to the SQS queue.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A solutions architect is using an AWS CloudFormation template to deploy a three-tier web \napplication. The web application consists of a web tier and an application tier that stores and \nretrieves user data in Amazon DynamoDB tables. The web and application tiers are hosted on \nAmazon EC2 instances, and the database tier is not publicly accessible. The application EC2 \ninstances need to access the DynamoDB tables without exposing API credentials in the template. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an IAM role to read the DynamoDB tables. Associate the role with the application", correct: false },
                { id: 1, text: "Create an IAM role that has the required permissions to read and write from the DynamoDB", correct: true },
                { id: 2, text: "Use the parameter section in the AWS CloudFormation template to have the user input access", correct: false },
                { id: 3, text: "Create an IAM user in the AWS CloudFormation template that has the required permissions to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A solutions architect manages an analytics application. The application stores large amounts of \nsemistructured data in an Amazon S3 bucket. The solutions architect wants to use parallel data \nprocessing to process the data more quickly. The solutions architect also wants to use \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n318 \ninformation that is stored in an Amazon Redshift database to enrich the data. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon Redshift data to", correct: false },
                { id: 1, text: "Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to", correct: true },
                { id: 2, text: "Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3", correct: false },
                { id: 3, text: "Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUse Amazon EMR to process the semi-structured data in Amazon S3. EMR provides a managed Hadoop framework optimized for processing large datasets in S3. EMR supports parallel data processing across multiple nodes to speed up the processing. EMR can integrate directly with Amazon Redshift using the EMR-Redshift integration. This allows querying the Redshift data from EMR and joining it with the S3 data. This enables enriching the semi-structured S3 data with the information stored in Redshift.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A company has two VPCs that are located in the us-west-2 Region within the same AWS \naccount. The company needs to allow network traffic between these VPCs. Approximately 500 \nGB of data transfer will occur between the VPCs each month. \n \nWhat is the MOST cost-effective solution to connect these VPCs?",
            options: [
                { id: 0, text: "Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to", correct: false },
                { id: 1, text: "Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each", correct: false },
                { id: 2, text: "Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use", correct: true },
                { id: 3, text: "Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nVPC peering provides private connectivity between VPCs without using public IP space. Data transferred between peered VPCs is free as long as they are in the same region. 500 GB/month inter-VPC data transfer fits within peering free tier. Transit Gateway (Option A) incurs hourly charges plus data transfer fees. More costly than peering. Site-to-Site VPN (Option B) incurs hourly charges and data transfer fees. More expensive than peering. Direct Connect (Option D) has high hourly charges and would be overkill for this use case.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company hosts multiple applications on AWS for different product lines. The applications use \ndifferent compute resources, including Amazon EC2 instances and Application Load Balancers. \nThe applications run in different AWS accounts under the same organization in AWS \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n319 \nOrganizations across multiple AWS Regions. Teams for each product line have tagged each \ncompute resource in the individual accounts. \n \nThe company wants more details about the cost for each product line from the consolidated \nbilling feature in Organizations. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Select a specific AWS generated tag in the AWS Billing console.", correct: false },
                { id: 1, text: "Select a specific user-defined tag in the AWS Billing console.", correct: true },
                { id: 2, text: "Select a specific user-defined tag in the AWS Resource Groups console.", correct: false },
                { id: 3, text: "Activate the selected tag from each AWS account.", correct: false },
                { id: 4, text: "Activate the selected tag from the Organizations management account.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUser-defined tags were created by each product team to identify resources. Selecting the relevant tag in the Billing console will group costs. The tag must be activated from the Organizations management account to consolidate billing across all accounts. AWS generated tags are predefined by AWS and won't align to product lines. Resource Groups (Option C) helps manage resources but not billing. Activating the tag from each account (Option D) is not needed since Organizations centralizes billing.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 8,
            text: "A company's solutions architect is designing an AWS multi-account solution that uses AWS \nOrganizations. The solutions architect has organized the company's accounts into organizational \nunits (OUs). \n \nThe solutions architect needs a solution that will identify any changes to the OU hierarchy. The \nsolution also needs to notify the company's operations team of any changes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Provision the AWS accounts by using AWS Control Tower. Use account drift notifications to", correct: true },
                { id: 1, text: "Provision the AWS accounts by using AWS Control Tower. Use AWS Config aggregated rules to", correct: false },
                { id: 2, text: "Use AWS Service Catalog to create accounts in Organizations. Use an AWS CloudTrail", correct: false },
                { id: 3, text: "Use AWS CloudFormation templates to create accounts in Organizations. Use the drift detection", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe key advantages you highlight of Control Tower are convincing: Fully managed service simplifies multi-account setup. Built-in account drift notifications detect OU changes automatically. More scalable and less complex than Config rules or CloudTrail. Better security and compliance guardrails than custom options. Lower operational overhead compared to other solution Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "A company's website handles millions of requests each day, and the number of requests \ncontinues to increase. A solutions architect needs to improve the response time of the web \napplication. The solutions architect determines that the application needs to decrease latency \nwhen retrieving product details from the Amazon DynamoDB table. \n \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "Set up a DynamoDB Accelerator (DAX) cluster. Route all read requests through DAX.", correct: true },
                { id: 1, text: "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application.", correct: false },
                { id: 2, text: "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web", correct: false },
                { id: 3, text: "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDAX provides a DynamoDB-compatible caching layer to reduce read latency. It is purpose-built for accelerating DynamoDB workloads. Using DAX requires minimal application changes - only read requests are routed through it. DAX handles caching logic automatically without needing complex integration code. ElastiCache Redis/Memcached (Options B/C) require more integration work to sync DynamoDB data. Using Lambda and Streams to populate ElastiCache (Option D) is a complex event-driven approach requiring ongoing maintenance. DAX plugs in seamlessly to accelerate DynamoDB with very little operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A solutions architect needs to ensure that API calls to Amazon DynamoDB from Amazon EC2 \ninstances in a VPC do not travel across the internet. \n \nWhich combination of steps should the solutions architect take to meet this requirement? \n(Choose two.)",
            options: [
                { id: 0, text: "Create a route table entry for the endpoint.", correct: true },
                { id: 1, text: "Create a gateway endpoint for DynamoDB.", correct: false },
                { id: 2, text: "Create an interface endpoint for Amazon EC2.", correct: false },
                { id: 3, text: "Create an elastic network interface for the endpoint in each of the subnets of the VPC.", correct: false },
                { id: 4, text: "Create a security group entry in the endpoint's security group to provide access.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company runs its applications on both Amazon Elastic Kubernetes Service (Amazon EKS) \nclusters and on-premises Kubernetes clusters. The company wants to view all clusters and \nworkloads from a central location. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n321",
            options: [
                { id: 0, text: "Use Amazon CloudWatch Container Insights to collect and group the cluster information.", correct: false },
                { id: 1, text: "Use Amazon EKS Connector to register and connect all Kubernetes clusters.", correct: true },
                { id: 2, text: "Use AWS Systems Manager to collect and view the cluster information.", correct: false },
                { id: 3, text: "Use Amazon EKS Anywhere as the primary cluster to view the other clusters with native", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nYou can use Amazon EKS Connector to register and connect any conformant Kubernetes cluster to AWS and visualize it in the Amazon EKS console. After a cluster is connected, you can see the status, configuration, and workloads for that cluster in the Amazon EKS console. https://docs.aws.amazon.com/eks/latest/userguide/eks-connector.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company is building an ecommerce application and needs to store sensitive customer \ninformation. The company needs to give customers the ability to complete purchase transactions \non the website. The company also needs to ensure that sensitive customer data is protected, \neven from database administrators. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Store sensitive data in an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS", correct: false },
                { id: 1, text: "Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management Service (AWS", correct: true },
                { id: 2, text: "Store sensitive data in Amazon S3. Use AWS Key Management Service (AWS KMS) server-side", correct: false },
                { id: 3, text: "Store sensitive data in Amazon FSx for Windows Server. Mount the file share on application", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nRDS MySQL provides a fully managed database service well suited for an ecommerce application. AWS KMS client-side encryption allows encrypting sensitive data before it hits the database. The data remains encrypted at rest. This protects sensitive customer data from database admins and privileged users. EBS encryption (Option A) protects data at rest but not in use. IAM roles don't prevent admin access. S3 (Option C) encrypts data at rest on the server side. Bucket policies don't restrict admin access. FSx file permissions (Option D) don't prevent admin access to unencrypted data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company has an on-premises MySQL database that handles transactional data. The company \nis migrating the database to the AWS Cloud. The migrated database must maintain compatibility \nwith the company's applications that use the database. The migrated database also must scale \nautomatically during periods of increased demand. \n \nWhich migration solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n322",
            options: [
                { id: 0, text: "Use native MySQL tools to migrate the database to Amazon RDS for MySQL. Configure elastic", correct: false },
                { id: 1, text: "Migrate the database to Amazon Redshift by using the mysqldump utility. Turn on Auto Scaling", correct: false },
                { id: 2, text: "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora.", correct: true },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nDMS provides an easy migration path from MySQL to Aurora while minimizing downtime. Aurora is a MySQL-compatible relational database service that will maintain compatibility with the company's applications. Aurora Auto Scaling allows the database to automatically scale up and down based on demand to handle increased workloads. RDS MySQL (Option A) does not scale as well as the Aurora architecture. Redshift (Option B) is for analytics, not transactional data, and may not be compatible. DynamoDB (Option D) is a NoSQL datastore and lacks MySQL compatibility.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. \nThe instances host applications that use a hierarchical directory structure. The applications need \nto read and write rapidly and concurrently to shared storage. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket. Allow access from all the EC2 instances in the VPC.", correct: false },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system", correct: true },
                { id: 2, text: "Create a file system on a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon", correct: false },
                { id: 3, text: "Create file systems on Amazon Elastic Block Store (Amazon EBS) volumes that are attached to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nHow is Amazon EFS different than Amazon S3? Amazon EFS provides shared access to data using a traditional file sharing permissions model and hierarchical directory structure via the NFSv4 protocol. Applications that access data using a standard file system interface provided through the operating system can use Amazon EFS to take advantage of the scalability and reliability of file storage in the cloud without writing any new code or adjusting applications. Amazon S3 is an object storage platform that uses a simple API for storing and accessing data. Applications that do not require a file system structure and are designed to work with object storage can use Amazon S3 as a massively scalable, durable, low-cost object storage solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A solutions architect is designing a workload that will store hourly energy consumption by \nbusiness tenants in a building. The sensors will feed a database through HTTP requests that will \nadd up usage for each tenant. The solutions architect must use managed services when possible. \nThe workload will receive more features in the future as the solutions architect adds independent \ncomponents. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n323 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors,", correct: true },
                { id: 1, text: "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2", correct: false },
                { id: 2, text: "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors,", correct: false },
                { id: 3, text: "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A solutions architect is designing the storage architecture for a new web application used for \nstoring and viewing engineering drawings. All application components will be deployed on the \nAWS infrastructure. \n \nThe application design must support caching to minimize the amount of time that users wait for \nthe engineering drawings to load. The application must be able to store petabytes of data. \n \nWhich combination of storage and caching should the solutions architect use?",
            options: [
                { id: 0, text: "Amazon S3 with Amazon CloudFront", correct: true },
                { id: 1, text: "Amazon S3 Glacier with Amazon ElastiCache", correct: false },
                { id: 2, text: "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront", correct: false },
                { id: 3, text: "AWS Storage Gateway with Amazon ElastiCache", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "An Amazon EventBridge rule targets a third-party API. The third-party API has not received any \nincoming traffic. A solutions architect needs to determine whether the rule conditions are being \nmet and if the rule's target is being invoked. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Check for metrics in Amazon CloudWatch in the namespace for AWS/Events.", correct: true },
                { id: 1, text: "Review events in the Amazon Simple Queue Service (Amazon SQS) dead-letter queue.", correct: false },
                { id: 2, text: "Check for the events in Amazon CloudWatch Logs.", correct: false },
                { id: 3, text: "Check the trails in AWS CloudTrail for the EventBridge events.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A company has a large workload that runs every Friday evening. The workload runs on Amazon \nEC2 instances that are in two Availability Zones in the us-east-1 Region. Normally, the company \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n324 \nmust run no more than two instances at all times. However, the company wants to scale up to six \ninstances each Friday to handle a regularly repeating increased workload. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a reminder in Amazon EventBridge to scale the instances.", correct: false },
                { id: 1, text: "Create an Auto Scaling group that has a scheduled action.", correct: true },
                { id: 2, text: "Create an Auto Scaling group that uses manual scaling.", correct: false },
                { id: 3, text: "Create an Auto Scaling group that uses automatic scaling.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company is creating a REST API. The company has strict requirements for the use of TLS. The \ncompany requires TLSv1.3 on the API endpoints. The company also requires a specific public \nthird-party certificate authority (CA) to sign the TLS certificate. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use a local machine to create a certificate that is signed by the third-party CImport the certificate", correct: false },
                { id: 1, text: "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA.", correct: true },
                { id: 2, text: "Use AWS Certificate Manager (ACM) to create a certificate that is signed by the third-party CA.", correct: false },
                { id: 3, text: "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Certificate Manager (ACM) is a service that lets you easily provision, manage, and deploy SSL/TLS certificates for use with AWS services and your internal resources. By creating a certificate in ACM that is signed by the third-party CA, the company can meet its requirement for a specific public third-party CA to sign the TLS certificate.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A company runs an application on AWS. The application receives inconsistent amounts of usage. \nThe application uses AWS Direct Connect to connect to an on-premises MySQL-compatible \ndatabase. The on-premises database consistently uses a minimum of 2 GiB of memory. \n \nThe company wants to migrate the on-premises database to a managed AWS service. The \ncompany wants to use auto scaling capabilities to manage unexpected workload increases. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Provision an Amazon DynamoDB database with default read and write capacity settings.", correct: false },
                { id: 1, text: "Provision an Amazon Aurora database with a minimum capacity of 1 Aurora capacity unit (ACU).", correct: false },
                { id: 2, text: "Provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1 Aurora", correct: true },
                { id: 3, text: "Provision an Amazon RDS for MySQL database with 2 GiB of memory.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAurora Serverless v2 provides auto-scaling so the database can handle inconsistent workloads and spikes automatically without admin intervention. It can scale down to zero when not in use to minimize costs. The minimum 1 ACU capacity is sufficient to replace the on-prem 2 GiB database based on the info given. Serverless capabilities reduce admin overhead for capacity management. DynamoDB lacks MySQL compatibility and requires more hands-on management. RDS and provisioned Aurora require manually resizing instances to scale, increasing admin overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A company wants to use an event-driven programming model with AWS Lambda. The company \nwants to reduce startup latency for Lambda functions that run on Java 11. The company does not \nhave strict latency requirements for the applications. The company wants to reduce cold starts \nand outlier latencies when a function scales up. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure Lambda provisioned concurrency.", correct: false },
                { id: 1, text: "Increase the timeout of the Lambda functions.", correct: false },
                { id: 2, text: "Increase the memory of the Lambda functions.", correct: false },
                { id: 3, text: "Configure Lambda SnapStart.1", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nLambda SnapStart for Java can improve startup performance for latency-sensitive applications by up to 10x at no extra cost, typically with no changes to your function code. https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 22,
            text: "A financial services company launched a new application that uses an Amazon RDS for MySQL \ndatabase. The company uses the application to track stock market trends. The company needs to \noperate the application for only 2 hours at the end of each week. The company needs to optimize \nthe cost of running the database. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Migrate the existing RDS for MySQL database to an Aurora Serverless v2 MySQL database", correct: true },
                { id: 1, text: "Migrate the existing RDS for MySQL database to an Aurora MySQL database cluster.", correct: false },
                { id: 2, text: "Migrate the existing RDS for MySQL database to an Amazon EC2 instance that runs MySQL.", correct: false },
                { id: 3, text: "Migrate the existing RDS for MySQL database to an Amazon Elastic Container Service (Amazon", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAurora Serverless v2 scales compute capacity automatically based on actual usage, down to zero when not in use. This minimizes costs for intermittent usage. Since it only runs for 2 hours per week, the application is ideal for a serverless architecture like Aurora Serverless. Aurora Serverless v2 charges per second when the database is active, unlike RDS which charges hourly. Aurora Serverless provides higher availability than self-managed MySQL on EC2 or ECS. Using reserved EC2 instances or ECS still incurs charges when not in use versus the fine-grained scaling of serverless. Standard Aurora clusters have a minimum capacity unlike the auto-scaling serverless architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A company deploys its applications on Amazon Elastic Kubernetes Service (Amazon EKS) \nbehind an Application Load Balancer in an AWS Region. The application needs to store data in a \nPostgreSQL database engine. The company wants the data in the database to be highly \navailable. The company also needs increased capacity for read workloads. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create an Amazon DynamoDB database table configured with global tables.", correct: false },
                { id: 1, text: "Create an Amazon RDS database with Multi-AZ deployments.", correct: false },
                { id: 2, text: "Create an Amazon RDS database with Multi-AZ DB cluster deployment.", correct: true },
                { id: 3, text: "Create an Amazon RDS database configured with cross-Region read replicas.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nDB cluster deployment can scale read workloads by adding read replicas. This provides increased capacity for read workloads without impacting the write workload.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "A company is building a RESTful serverless web application on AWS by using Amazon API \nGateway and AWS Lambda. The users of this web application will be geographically distributed, \nand the company wants to reduce the latency of API requests to these users. \n \nWhich type of endpoint should a solutions architect use to meet these requirements?",
            options: [
                { id: 0, text: "Private endpoint", correct: false },
                { id: 1, text: "Regional endpoint", correct: false },
                { id: 2, text: "Interface VPC endpoint", correct: false },
                { id: 3, text: "Edge-optimized endpoint", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAn edge-optimized API endpoint typically routes requests to the nearest CloudFront Point of Presence (POP), which could help in cases where your clients are geographically distributed. This is the default endpoint type for API Gateway REST APIs. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint- types.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A company uses an Amazon CloudFront distribution to serve content pages for its website. The \ncompany needs to ensure that clients use a TLS certificate when accessing the company's \nwebsite. The company wants to automate the creation and renewal of the TLS certificates. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Use a CloudFront security policy to create a certificate.", correct: false },
                { id: 1, text: "Use a CloudFront origin access control (OAC) to create a certificate.", correct: false },
                { id: 2, text: "Use AWS Certificate Manager (ACM) to create a certificate. Use DNS validation for the domain.", correct: true },
                { id: 3, text: "Use AWS Certificate Manager (ACM) to create a certificate. Use email validation for the domain.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Certificate Manager (ACM) provides free public TLS/SSL certificates and handles certificate renewals automatically. Using DNS validation with ACM is operationally efficient since it automatically makes changes to Route 53 rather than requiring manual validation steps. ACM integrates natively with CloudFront distributions for delivering HTTPS content. CloudFront security policies and origin access controls do not issue TLS certificates. Email validation requires manual steps to approve the domain validation emails for each renewal.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A company deployed a serverless application that uses Amazon DynamoDB as a database layer. \nThe application has experienced a large increase in users. The company wants to improve \ndatabase response time from milliseconds to microseconds and to cache requests to the \ndatabase. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use DynamoDB Accelerator (DAX).", correct: true },
                { id: 1, text: "Migrate the database to Amazon Redshift.", correct: false },
                { id: 2, text: "Migrate the database to Amazon RDS.", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Redis.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement - from milliseconds to microseconds - even at millions of requests per second.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 27,
            text: "A company runs an application that uses Amazon RDS for PostgreSQL. The application receives \ntraffic only on weekdays during business hours. The company wants to optimize costs and \nreduce operational overhead based on this usage. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use the Instance Scheduler on AWS to configure start and stop schedules.", correct: true },
                { id: 1, text: "Turn off automatic backups. Create weekly manual snapshots of the database.", correct: false },
                { id: 2, text: "Create a custom AWS Lambda function to start and stop the database based on minimum CPU", correct: false },
                { id: 3, text: "Purchase All Upfront reserved DB instances.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe Instance Scheduler on AWS solution automates the starting and stopping of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) instances. This solution helps reduce operational costs by stopping resources that are not in use and starting them when they are needed. The cost savings can be significant if you leave all of your instances running at full utilization continuously. https://aws.amazon.com/solutions/implementations/instance-scheduler-on-aws/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 28,
            text: "A company uses locally attached storage to run a latency-sensitive application on premises. The \ncompany is using a lift and shift method to move the application to the AWS Cloud. The company \ndoes not want to change the application architecture. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for Lustre", correct: false },
                { id: 1, text: "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon", correct: false },
                { id: 2, text: "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for", correct: false },
                { id: 3, text: "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 29,
            text: "A company runs a stateful production application on Amazon EC2 instances. The application \nrequires at least two EC2 instances to always be running. \n \nA solutions architect needs to design a highly available and fault-tolerant architecture for the \napplication. The solutions architect creates an Auto Scaling group of EC2 instances. \n \nWhich set of additional steps should the solutions architect take to meet these requirements?",
            options: [
                { id: 0, text: "Set the Auto Scaling group's minimum capacity to two. Deploy one On-Demand Instance in one", correct: false },
                { id: 1, text: "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one", correct: true },
                { id: 2, text: "Set the Auto Scaling group's minimum capacity to two. Deploy four Spot Instances in one", correct: false },
                { id: 3, text: "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy setting the Auto Scaling group's minimum capacity to four, the architect ensures that there are always at least two running instances. Deploying two On-Demand Instances in each of two Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 30,
            text: "An ecommerce company uses Amazon Route 53 as its DNS provider. The company hosts its \nwebsite on premises and in the AWS Cloud. The company's on-premises data center is near the \nus-west-1 Region. The company uses the eu-central-1 Region to host the website. The company \nwants to minimize load time for the website as much as possible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the on-premises data", correct: true },
                { id: 1, text: "Set up a simple routing policy that routes all traffic that is near eu-central-1 to eu-central-1 and", correct: false },
                { id: 2, text: "Set up a latency routing policy. Associate the policy with us-west-1.", correct: false },
                { id: 3, text: "Set up a weighted routing policy. Split the traffic evenly between eu-central-1 and the on-", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A company has 5 PB of archived data on physical tapes. The company needs to preserve the \ndata on the tapes for another 10 years for compliance purposes. The company wants to migrate \nto AWS in the next 6 months. The data center that stores the tapes has a 1 Gbps uplink internet \nconnectivity. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Read the data from the tapes on premises. Stage the data in a local NFS storage. Use AWS", correct: false },
                { id: 1, text: "Use an on-premises backup application to read the data from the tapes and to write directly to", correct: false },
                { id: 2, text: "Order multiple AWS Snowball devices that have Tape Gateway. Copy the physical tapes to virtual", correct: true },
                { id: 3, text: "Configure an on-premises Tape Gateway. Create virtual tapes in the AWS Cloud. Use backup", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "A company is deploying an application that processes large quantities of data in parallel. The \ncompany plans to use Amazon EC2 instances for the workload. The network architecture must be \nconfigurable to prevent groups of nodes from sharing the same underlying hardware. \n \nWhich networking solution meets these requirements?",
            options: [
                { id: 0, text: "Run the EC2 instances in a spread placement group.", correct: false },
                { id: 1, text: "Group the EC2 instances in separate accounts.", correct: false },
                { id: 2, text: "Configure the EC2 instances with dedicated tenancy.", correct: true },
                { id: 3, text: "Configure the EC2 instances with shared tenancy.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nConfiguring the EC2 instances with dedicated tenancy ensures that each instance will run on isolated, single-tenant hardware. This meets the requirement to prevent groups of nodes from sharing underlying hardware. A spread placement group only provides isolation at the Availability Zone level. Instances could still share hardware within an AZ.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 \ncapacity in a failover AWS Region. Business requirements state that the DR strategy must meet \ncapacity in the failover Region. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Purchase On-Demand Instances in the failover Region.", correct: false },
                { id: 1, text: "Purchase an EC2 Savings Plan in the failover Region.", correct: false },
                { id: 2, text: "Purchase regional Reserved Instances in the failover Region.", correct: false },
                { id: 3, text: "Purchase a Capacity Reservation in the failover Region.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "A company has five organizational units (OUs) as part of its organization in AWS Organizations. \nEach OU correlates to the five businesses that the company owns. The company's research and \ndevelopment (R&D) business is separating from the company and will need its own organization. \nA solutions architect creates a separate new management account for this purpose. \n \nWhat should the solutions architect do next in the new management account?",
            options: [
                { id: 0, text: "Have the R&D AWS account be part of both organizations during the transition.", correct: false },
                { id: 1, text: "Invite the R&D AWS account to be part of the new organization after the R&D AWS account has", correct: true },
                { id: 2, text: "Create a new R&D AWS account in the new organization. Migrate resources from the prior R&D", correct: false },
                { id: 3, text: "Have the R&D AWS account join the new organization. Make the new management account a", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/mt/migrating-accounts-between-aws-organizations-with- consolidated-billing-to-all-features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "A company is designing a solution to capture customer activity in different web applications to \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n331 \nprocess analytics and make predictions. Customer activity in the web applications is \nunpredictable and can increase suddenly. The company requires a solution that integrates with \nother web applications. The solution must include an authorization step for security purposes. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service", correct: false },
                { id: 1, text: "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that", correct: false },
                { id: 2, text: "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that", correct: true },
                { id: 3, text: "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda- authorizer.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances \nthat run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective \n(RPO) and recovery time objective (RTO) are 24 hours. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a cross-Region read replica and promote the read replica to the primary instance.", correct: false },
                { id: 1, text: "Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.", correct: false },
                { id: 2, text: "Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.", correct: false },
                { id: 3, text: "Copy automatic snapshots to another Region every 24 hours.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon RDS creates and saves automated backups of your DB instance or Multi-AZ DB cluster during the backup window of your DB instance. RDS creates a storage volume snapshot of your DB instance, backing up the entire DB instance and not just individual databases. RDS saves the automated backups of your DB instance according to the backup retention period that you specify. If necessary, you can recover your DB instance to any point in time during the backup retention period.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 37,
            text: "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an \nApplication Load Balancer that has sticky sessions enabled. The web server currently hosts the \nuser session state. The company wants to ensure high availability and avoid user session state \nloss in the event of a web server outage. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n332",
            options: [
                { id: 0, text: "Use an Amazon ElastiCache for Memcached instance to store the session data. Update the", correct: false },
                { id: 1, text: "Use Amazon ElastiCache for Redis to store the session state. Update the application to use", correct: true },
                { id: 2, text: "Use an AWS Storage Gateway cached volume to store session data. Update the application to", correct: false },
                { id: 3, text: "Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nElastiCache Redis provides in-memory caching that can deliver microsecond latency for session data. Redis supports replication and multi-AZ which can provide high availability for the cache. The application can be updated to store session data in ElastiCache Redis rather than locally on the web servers. If a web server fails, the user can be routed via the load balancer to another web server which can retrieve their session data from the highly available ElastiCache Redis cluster.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company migrated a MySQL database from the company's on-premises data center to an \nAmazon RDS for MySQL DB instance. The company sized the RDS DB instance to meet the \ncompany's average daily workload. Once a month, the database performs slowly when the \ncompany runs queries for a report. The company wants to have the ability to run reports and \nmaintain the performance of the daily workloads. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a read replica of the database. Direct the queries to the read replica.", correct: true },
                { id: 1, text: "Create a backup of the database. Restore the backup to another DB instance. Direct the queries", correct: false },
                { id: 2, text: "Export the data to Amazon S3. Use Amazon Athena to query the S3 bucket.", correct: false },
                { id: 3, text: "Resize the DB instance to accommodate the additional workload.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A company runs a container application by using Amazon Elastic Kubernetes Service (Amazon \nEKS). The application includes microservices that manage customers and place orders. The \ncompany needs to route incoming requests to the appropriate microservices. \n \nWhich solution will meet this requirement MOST cost-effectively?",
            options: [
                { id: 0, text: "Use the AWS Load Balancer Controller to provision a Network Load Balancer.", correct: false },
                { id: 1, text: "Use the AWS Load Balancer Controller to provision an Application Load Balancer.", correct: false },
                { id: 2, text: "Use an AWS Lambda function to connect the requests to Amazon EKS.", correct: false },
                { id: 3, text: "Use Amazon API Gateway to connect the requests to Amazon EKS.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAPI Gateway provides an entry point to your microservices. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 40,
            text: "A company uses AWS and sells access to copyrighted images. The company's global customer \nbase needs to be able to access these images quickly. The company must deny access to users \nfrom specific countries. The company wants to minimize costs as much as possible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket", correct: false },
                { id: 1, text: "Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a", correct: false },
                { id: 2, text: "Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the", correct: false },
                { id: 3, text: "Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A solutions architect is designing a highly available Amazon ElastiCache for Redis based \nsolution. The solutions architect needs to ensure that failures do not result in performance \ndegradation or loss of data locally and within an AWS Region. The solution needs to provide high \navailability at the node level and at the Region level. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Multi-AZ Redis replication groups with shards that contain multiple nodes.", correct: true },
                { id: 1, text: "Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on.", correct: false },
                { id: 2, text: "Use a Multi-AZ Redis cluster with more than one read replica in the replication group.", correct: false },
                { id: 3, text: "Use Redis shards that contain multiple nodes with Auto Scaling turned on.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company plans to migrate to AWS and use Amazon EC2 On-Demand Instances for its \napplication. During the migration testing phase, a technical team observes that the application \ntakes a long time to launch and load memory to become fully productive. \n \nWhich solution will reduce the launch time of the application during the next testing phase?",
            options: [
                { id: 0, text: "Launch two or more EC2 On-Demand Instances. Turn on auto scaling features and make the", correct: false },
                { id: 1, text: "Launch EC2 Spot Instances to support the application and to scale the application so it is", correct: false },
                { id: 2, text: "Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling", correct: true },
                { id: 3, text: "Launch EC2 On-Demand Instances with Capacity Reservations. Start additional EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nWith Amazon EC2 hibernation enabled, you can maintain your EC2 instances in a \"pre-warmed\" state so these can get to a productive state faster.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company's applications run on Amazon EC2 instances in Auto Scaling groups. The company \nnotices that its applications experience sudden traffic increases on random days of the week. The \ncompany wants to maintain application performance during sudden traffic increases. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use manual scaling to change the size of the Auto Scaling group.", correct: false },
                { id: 1, text: "Use predictive scaling to change the size of the Auto Scaling group.", correct: false },
                { id: 2, text: "Use dynamic scaling to change the size of the Auto Scaling group.", correct: true },
                { id: 3, text: "Use schedule scaling to change the size of the Auto Scaling group.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nDynamic Scaling - This is yet another type of Auto Scaling in which the number of EC2 instances is changed automatically depending on the signals received. Dynamic Scaling is a good choice when there is a high volume of unpredictable traffic.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "An ecommerce application uses a PostgreSQL database that runs on an Amazon EC2 instance. \nDuring a monthly sales event, database usage increases and causes database connection issues \nfor the application. The traffic is unpredictable for subsequent monthly sales events, which \nimpacts the sales forecast. The company needs to maintain performance when there is an \nunpredictable increase in traffic. \n \nWhich solution resolves this issue in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Migrate the PostgreSQL database to Amazon Aurora Serverless v2.", correct: true },
                { id: 1, text: "Enable auto scaling for the PostgreSQL database on the EC2 instance to accommodate", correct: false },
                { id: 2, text: "Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a larger instance type.", correct: false },
                { id: 3, text: "Migrate the PostgreSQL database to Amazon Redshift to accommodate increased usage.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAurora Serverless v2 got autoscaling, highly available and cheaper when compared to the other options.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 45,
            text: "A company hosts an internal serverless application on AWS by using Amazon API Gateway and \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n335 \nAWS Lambda. The company's employees report issues with high latency when they begin using \nthe application each day. The company wants to reduce latency. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Increase the API Gateway throttling limit.", correct: false },
                { id: 1, text: "Set up a scheduled scaling to increase Lambda provisioned concurrency before employees begin", correct: true },
                { id: 2, text: "Create an Amazon CloudWatch alarm to initiate a Lambda function as a target for the alarm at", correct: false },
                { id: 3, text: "Increase the Lambda function memory.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/compute/scheduling-aws-lambda-provisioned-concurrency-for- recurring-peak-usage/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A research company uses on-premises devices to generate data for analysis. The company \nwants to use the AWS Cloud to analyze the data. The devices generate .csv files and support \nwriting the data to an SMB file share. Company analysts must be able to use SQL commands to \nquery the data. The analysts will run queries periodically throughout the day. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
            options: [
                { id: 0, text: "Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode.", correct: true },
                { id: 1, text: "Deploy an AWS Storage Gateway on premises in Amazon FSx File Gateway made.", correct: false },
                { id: 2, text: "Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3.", correct: false },
                { id: 3, text: "Set up an Amazon EMR cluster with EMR File System (EMRFS) to query the data that is in", correct: false },
                { id: 4, text: "Set up an Amazon Redshift cluster to query the data that is in Amazon S3. Provide access to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon- s3/ https://aws.amazon.com/storagegateway/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 47,
            text: "A company wants to use Amazon Elastic Container Service (Amazon ECS) clusters and Amazon \nRDS DB instances to build and run a payment processing application. The company will run the \napplication in its on-premises data center for compliance purposes. \n \nA solutions architect wants to use AWS Outposts as part of the solution. The solutions architect is \nworking with the company's operational team to build the application. \n \nWhich activities are the responsibility of the company's operational team? (Choose three.)",
            options: [
                { id: 0, text: "Providing resilient power and network connectivity to the Outposts racks", correct: true },
                { id: 1, text: "Managing the virtualization hypervisor, storage systems, and the AWS services that run on", correct: false },
                { id: 2, text: "Physical security and access controls of the data center environment", correct: false },
                { id: 3, text: "Availability of the Outposts infrastructure including the power supplies, servers, and networking", correct: false },
                { id: 4, text: "Physical maintenance of Outposts components", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-outposts-high-availability-design/aws- outposts-high-availability-design.html With Outposts, you are responsible for providing resilient power and network connectivity to the Outpost racks to meet your availability requirements for workloads running on Outposts. You are responsible for the physical security and access controls of the data center environment. You must provide sufficient power, space, and cooling to keep the Outpost operational and network connections to connect the Outpost back to the Region. Since Outpost capacity is finite and determined by the size and number of racks AWS installs at your site, you must decide how much EC2, EBS, and S3 on Outposts capacity you need to run your initial workloads, accommodate future growth, and to provide extra capacity to mitigate server failures and maintenance events.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A company is planning to migrate a TCP-based application into the company's VPC. The \napplication is publicly accessible on a nonstandard TCP port through a hardware appliance in the \ncompany's data center. This public endpoint can process up to 3 million requests per second with \nlow latency. The company requires the same level of performance for the new public endpoint in \nAWS. \n \nWhat should a solutions architect recommend to meet this requirement?",
            options: [
                { id: 0, text: "Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the", correct: true },
                { id: 1, text: "Deploy an Application Load Balancer (ALB). Configure the ALB to be publicly accessible over the", correct: false },
                { id: 2, text: "Deploy an Amazon CloudFront distribution that listens on the TCP port that the application", correct: false },
                { id: 3, text: "Deploy an Amazon API Gateway API that is configured with the TCP port that the application", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSince the company requires the same level of performance for the new public endpoint in AWS. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n337 \ncompany wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora", correct: false },
                { id: 1, text: "Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read", correct: true },
                { id: 2, text: "Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster.", correct: false },
                { id: 3, text: "Use the pg_dump utility to back up the RDS for PostgreSQL database. Restore the backup to a", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating .html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 50,
            text: "A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon \nElastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 \ninstance can be recovered after a disaster. \n \nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
            options: [
                { id: 0, text: "Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS", correct: false },
                { id: 1, text: "Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic", correct: false },
                { id: 2, text: "Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS", correct: true },
                { id: 3, text: "Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Backup automates backup of resources like EBS volumes. It allows defining backup policies for groups of resources. This removes the need to manually create backups for each resource. The AWS Backup API and CLI allow programmatic control of backup plans and restores. This enables restoring hundreds of EC2 instances programmatically after a disaster instead of manually. AWS Backup handles cleanup of old backups based on policies to minimize storage costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 51,
            text: "A company recently migrated to the AWS Cloud. The company wants a serverless solution for \nlarge-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, \nmedia files, sales transactions, and IoT sensor data that is stored in Amazon S3. The company \nwants the solution to process thousands of items in the dataset in parallel. \n \nWhich solution will meet these requirements with the MOST operational efficiency? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n338",
            options: [
                { id: 0, text: "Use the AWS Step Functions Map state in Inline mode to process the data in parallel.", correct: false },
                { id: 1, text: "Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.", correct: true },
                { id: 2, text: "Use AWS Glue to process the data in parallel.", correct: false },
                { id: 3, text: "Use several AWS Lambda functions to process the data in parallel.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Step Functions allows you to orchestrate and scale distributed processing using the Map state. The Map state can process items in a large dataset in parallel by distributing the work across multiple resources. Using the Map state in Distributed mode will automatically handle the parallel processing and scaling. Step Functions will add more workers to process the data as needed. Step Functions is serverless so there are no servers to manage. It will scale up and down automatically based on demand.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a \n500 Mbps uplink to the internet. Other on-premises applications share the uplink. The company \ncan use 80% of the internet bandwidth for this one-time migration task. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure AWS DataSync to migrate the data to Amazon S3 and to automatically verify the data.", correct: false },
                { id: 1, text: "Use rsync to transfer the data directly to Amazon S3.", correct: false },
                { id: 2, text: "Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.", correct: false },
                { id: 3, text: "Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A company has several on-premises Internet Small Computer Systems Interface (ISCSI) network \nstorage servers. The company wants to reduce the number of these servers by moving to the \nAWS Cloud. A solutions architect must provide low-latency access to frequently used data and \nreduce the dependency on on-premises servers with a minimal number of infrastructure changes. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy an Amazon S3 File Gateway.", correct: false },
                { id: 1, text: "Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.", correct: false },
                { id: 3, text: "Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A solutions architect is designing an application that will allow business users to upload objects to \nAmazon S3. The solution needs to maximize object durability. Objects also must be readily \navailable at any time and for any length of time. Users will access objects frequently within the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n339 \nfirst 30 days after the objects are uploaded, but users are much less likely to access objects that \nare older than 30 days. \n \nWhich solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3", correct: false },
                { id: 1, text: "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3", correct: true },
                { id: 2, text: "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One", correct: false },
                { id: 3, text: "Store all the objects in S3 Intelligent-Tiering with an S3 Lifecycle rule to transition the objects to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A company has migrated a two-tier application from its on-premises data center to the AWS \nCloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General \nPurpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to \nprocess and store documents in the database as binary large objects (blobs) with an average \ndocument size of 6 MB. \n \nThe database size has grown over time, reducing the performance and increasing the cost of \nstorage. The company must improve the database performance and needs a solution that is \nhighly available and resilient. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage", correct: false },
                { id: 1, text: "Increase the RDS DB instance size. Increase the storage capacity to 24 TiChange the storage", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store", correct: true },
                { id: 3, text: "Create an Amazon DynamoDB table. Update the application to use DynamoDB. Use AWS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A company has an application that serves clients that are deployed in more than 20.000 retail \nstorefront locations around the world. The application consists of backend web services that are \nexposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind \nan Application Load Balancer (ALB). The retail locations communicate with the web application \nover the public internet. The company allows each retail location to register the IP address that \nthe retail location has been allocated by its local ISP. \n \nThe company's security team recommends to increase the security of the application endpoint by \nrestricting access to only the IP addresses registered by the retail locations. \n \nWhat should a solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n340",
            options: [
                { id: 0, text: "Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic.", correct: true },
                { id: 1, text: "Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the", correct: false },
                { id: 2, text: "Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization", correct: false },
                { id: 3, text: "Configure the network ACL on the subnet that contains the public interface of the ALB. Update", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAssociate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A company is building a data analysis platform on AWS by using AWS Lake Formation. The \nplatform will ingest data from different sources such as Amazon S3 and Amazon RDS. The \ncompany needs a secure solution to prevent access to portions of the data that contain sensitive \ninformation. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an IAM role that includes permissions to access Lake Formation tables.", correct: false },
                { id: 1, text: "Create data filters to implement row-level security and cell-level security.", correct: true },
                { id: 2, text: "Create an AWS Lambda function that removes sensitive information before Lake Formation", correct: false },
                { id: 3, text: "Create an AWS Lambda function that periodically queries and removes sensitive information from", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nLake Formation data filters allow restricting access to rows or cells in data tables based on conditions. This allows preventing access to sensitive data. Data filters are implemented within Lake Formation and do not require additional coding or Lambda functions. Lambda functions to pre-process data or purge tables would require ongoing development and maintenance. IAM roles only provide user-level permissions, not row or cell level security. Data filters give granular access control over Lake Formation data with minimal configuration, avoiding complex custom code.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source \ndata into Amazon S3 buckets so that the data can be processed in the future. According to \ncompliance laws, the data must not be transmitted over the public internet. Servers in the \ncompany's on-premises data center will consume the output from an application that runs on the \nEC2 instances. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection", correct: false },
                { id: 1, text: "Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection", correct: true },
                { id: 2, text: "Set up an AWS Transit Gateway connection from the VPC to the S3 buckets. Create an AWS", correct: false },
                { id: 3, text: "Set up proxy EC2 instances that have routes to NAT gateways. Configure the proxy EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nGateway VPC Endpoint = no internet to access S3. Direct Connect = secure access to VPC.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A company has an application with a REST-based interface that allows data to be received in \nnear-real time from a third-party vendor. Once received, the application processes and stores the \ndata for further analysis. The application is running on Amazon EC2 instances. \n \nThe third-party vendor has received many 503 Service Unavailable Errors when sending data to \nthe application. When the data volume spikes, the compute capacity reaches its maximum limit \nand the application is unable to process all requests. \n \nWhich design should a solutions architect recommend to provide a more scalable solution?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda", correct: true },
                { id: 1, text: "Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances", correct: false },
                { id: 3, text: "Repackage the application as a container. Deploy the application using Amazon Elastic Container", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nKinesis Data Streams provides an auto-scaling stream that can handle large amounts of streaming data ingestion and throughput. This removes the bottlenecks around receiving the data. AWS Lambda can process and store the data in a scalable serverless manner, avoiding EC2 capacity limits. API Gateway adds API management capabilities but does not improve the underlying scalability of the EC2 application. SNS is for event publishing/notifications, not large scale data ingestion. ECS still relies on EC2 capacity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 60,
            text: "A company has an application that runs on Amazon EC2 instances in a private subnet. The \napplication needs to process sensitive information from an Amazon S3 bucket. The application \nmust not use the internet to connect to the S3 bucket. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an internet gateway. Update the S3 bucket policy to allow access from the internet", correct: false },
                { id: 1, text: "Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN", correct: false },
                { id: 2, text: "Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway.", correct: false },
                { id: 3, text: "Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container \napplication. The EKS cluster stores sensitive information in the Kubernetes secrets object. The \ncompany wants to ensure that the information is encrypted. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use the container application to encrypt the information by using AWS Key Management Service", correct: false },
                { id: 1, text: "Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS", correct: true },
                { id: 2, text: "Implement an AWS Lambda function to encrypt the information by using AWS Key Management", correct: false },
                { id: 3, text: "Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2020/03/amazon-eks-adds-envelope-encryption- for-secrets-with-aws-kms/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company is designing a new multi-tier web application that consists of the following \ncomponents: \n \n- Web and application servers that run on Amazon EC2 instances as part \nof Auto Scaling groups \n- An Amazon RDS DB instance for data storage \n \nA solutions architect needs to limit access to the application servers so that only the web servers \ncan access them. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy AWS PrivateLink in front of the application servers. Configure the network ACL to allow", correct: false },
                { id: 1, text: "Deploy a VPC endpoint in front of the application servers. Configure the security group to allow", correct: false },
                { id: 2, text: "Deploy a Network Load Balancer with a target group that contains the application servers' Auto", correct: false },
                { id: 3, text: "Deploy an Application Load Balancer with a target group that contains the application servers'", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAn Application Load Balancer (ALB) allows directing traffic to the application servers and provides access control via security groups. Security groups act as a firewall at the instance level and can control access to the application servers from the web servers. Network ACLs work at the subnet level and are less flexible for security groups for instance-level access control. VPC endpoints are used to provide private access to AWS services, not for access between EC2 instances. AWS PrivateLink provides private connectivity between VPCs, which is not required in this single VPC scenario.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service \n(Amazon EKS). The application has a microservices architecture. The company needs to \nimplement a solution that collects, aggregates, and summarizes metrics and logs from the \napplication in a centralized location. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the", correct: false },
                { id: 1, text: "Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh", correct: false },
                { id: 2, text: "Configure AWS CloudTrail to capture data events. Query CloudTrail by using Amazon", correct: false },
                { id: 3, text: "Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAmazon CloudWatch Application Insights facilitates observability for your applications and underlying AWS resources. It helps you set up the best monitors for your application resources to continuously analyze data for signs of problems with your applications.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 64,
            text: "A company has deployed its newest product on AWS. The product runs in an Auto Scaling group \nbehind a Network Load Balancer. The company stores the product's objects in an Amazon S3 \nbucket. \n \nThe company recently experienced malicious attacks against its systems. The company needs a \nsolution that continuously monitors for malicious activity in the AWS account, workloads, and \naccess patterns to the S3 bucket. The solution must also report suspicious activity and display \nthe information on a dashboard. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n344",
            options: [
                { id: 0, text: "Configure Amazon Macie to monitor and report findings to AWS Config.", correct: false },
                { id: 1, text: "Configure Amazon Inspector to monitor and report findings to AWS CloudTrail.", correct: false },
                { id: 2, text: "Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.", correct: true },
                { id: 3, text: "Configure AWS Config to monitor and report findings to Amazon EventBridge.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. It analyzes AWS CloudTrail, VPC Flow Logs, and DNS logs. GuardDuty can detect threats like instance or S3 bucket compromise, malicious IP addresses, or unusual API calls. Findings can be sent to AWS Security Hub which provides a centralized security dashboard and alerts. Amazon Macie and Amazon Inspector do not monitor the breadth of activity that GuardDuty does. They focus more on data security and application vulnerabilities respectively. AWS Config monitors for resource configuration changes, not malicious activity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test20: [
        {
            id: 0,
            text: "A company wants to migrate an on-premises data center to AWS. The data center hosts a \nstorage server that stores data in an NFS-based file system. The storage server holds 200 GB of \ndata. The company needs to migrate the data without interruption to existing services. Multiple \nresources in AWS must be able to access the data by using the NFS protocol. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
            options: [
                { id: 0, text: "Create an Amazon FSx for Lustre file system.", correct: false },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) file system.", correct: true },
                { id: 2, text: "Create an Amazon S3 bucket to receive the data.", correct: false },
                { id: 3, text: "Manually use an operating system copy command to push the data into the AWS destination.", correct: false },
                { id: 4, text: "Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon EFS provides a scalable, high performance NFS file system that can be accessed from multiple resources in AWS. AWS DataSync can perform the migration from the on-prem NFS server to EFS without interruption to existing services. This avoids having to manually move the data which could cause downtime. DataSync incrementally syncs changed data. EFS and DataSync together provide a cost-optimized approach compared to using S3 or FSx, while still meeting the requirements. Manually copying 200 GB of data to AWS would be slow and risky compared to using DataSync.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 1,
            text: "A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that \nhave an SMB file share mounted as a volume in the us-east-1 Region. The company has a \nrecovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned \nservice disruptions. The company needs to replicate the file system to the us-west-2 Region. The \nreplicated data must not be deleted by any user for 5 years. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n345",
            options: [
                { id: 0, text: "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2", correct: false },
                { id: 1, text: "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment", correct: false },
                { id: 2, text: "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment", correct: true },
                { id: 3, text: "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A solutions architect is designing a security solution for a company that wants to provide \ndevelopers with individual AWS accounts through AWS Organizations, while also maintaining \nstandard security controls. Because the individual developers will have AWS account root user-\nlevel access to their own accounts, the solutions architect wants to ensure that the mandatory \nAWS CloudTrail configuration that is applied to new developer accounts is not modified. \n \nWhich action meets these requirements?",
            options: [
                { id: 0, text: "Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.", correct: false },
                { id: 1, text: "Create a new trail in CloudTrail from within the developer accounts with the organization trails", correct: false },
                { id: 2, text: "Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the", correct: true },
                { id: 3, text: "Create a service-linked role for CloudTrail with a policy condition that allows changes only from", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A company is planning to deploy a business-critical application in the AWS Cloud. The application \nrequires durable storage with consistent, low-latency performance. \n \nWhich type of storage should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Instance store volume", correct: false },
                { id: 1, text: "Amazon ElastiCache for Memcached cluster", correct: false },
                { id: 2, text: "Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume", correct: true },
                { id: 3, text: "Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/ebs/volume-types/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-\nwest-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. \n \nWhich solution will meet this requirement with the LEAST operational effort?",
            options: [
                { id: 0, text: "Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from", correct: true },
                { id: 1, text: "Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify", correct: false },
                { id: 2, text: "Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle", correct: false },
                { id: 3, text: "Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2015/03/amazon-s3-introduces-cross-region- replication/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A company is creating a new web application for its subscribers. The application will consist of a \nstatic single page and a persistent database layer. The application will have millions of users for 4 \nhours in the morning, but the application will have only a few thousand users during the rest of \nthe day. The company's data architects have requested the ability to rapidly evolve their schema. \n \nWhich solutions will meet these requirements and provide the MOST scalability? (Choose two.)",
            options: [
                { id: 0, text: "Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.", correct: true },
                { id: 1, text: "Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.", correct: false },
                { id: 2, text: "Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is", correct: false },
                { id: 3, text: "Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution", correct: false },
                { id: 4, text: "Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nFor tables using on-demand mode, DynamoDB instantly accommodates customers’ workloads as they ramp up or down to any previously observed traffic level. If the level of traffic hits a new peak, DynamoDB adapts rapidly to accommodate the workload. https://aws.amazon.com/blogs/aws/amazon-dynamodb-on-demand-no-capacity-planning-and- pay-per-request-pricing/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A company uses Amazon API Gateway to manage its REST APIs that third-party service \nproviders access. The company must protect the REST APIs from SQL injection and cross-site \nscripting attacks. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
            options: [
                { id: 0, text: "Configure AWS Shield.", correct: false },
                { id: 1, text: "Configure AWS WAF.", correct: true },
                { id: 2, text: "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in", correct: false },
                { id: 3, text: "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A company wants to provide users with access to AWS resources. The company has 1,500 users \nand manages their access to on-premises resources through Active Directory user groups on the \ncorporate network. However, the company does not want users to have to maintain another \nidentity to access the resources. A solutions architect must manage user access to the AWS \nresources while preserving access to the on-premises resources. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an IAM user for each user in the company. Attach the appropriate policies to each user.", correct: false },
                { id: 1, text: "Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies", correct: false },
                { id: 2, text: "Define cross-account roles with the appropriate policies attached. Map the roles to the Active", correct: false },
                { id: 3, text: "Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/identity/saml/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company is hosting a website behind multiple Application Load Balancers. The company has \ndifferent distribution rights for its content around the world. A solutions architect needs to ensure \nthat users are served the correct content without violating distribution rights. \n \nWhich configuration should the solutions architect choose to meet these requirements?",
            options: [
                { id: 0, text: "Configure Amazon CloudFront with AWS WAF.", correct: false },
                { id: 1, text: "Configure Application Load Balancers with AWS WAF", correct: false },
                { id: 2, text: "Configure Amazon Route 53 with a geolocation policy", correct: true },
                { id: 3, text: "Configure Amazon Route 53 with a geoproximity routing policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "A company stores its data on premises. The amount of data is growing beyond the company's \navailable capacity. \n \nThe company wants to migrate its data from the on-premises location to an Amazon S3 bucket. \nThe company needs a solution that will automatically validate the integrity of the data after the \ntransfer. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online", correct: false },
                { id: 1, text: "Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the", correct: true },
                { id: 2, text: "Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the", correct: false },
                { id: 3, text: "Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDuring a transfer, AWS DataSync always checks the integrity of your data, but you can specify how and when this verification happens with the following options: Verify only the data transferred (recommended) – DataSync calculates the checksum of transferred files and metadata at the source location. https://docs.aws.amazon.com/datasync/latest/userguide/configure-data-verification-options.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A company wants to migrate two DNS servers to AWS. The servers host a total of approximately \n200 zones and receive 1 million requests each day on average. The company wants to maximize \navailability while minimizing the operational overhead that is related to the management of the \ntwo servers. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Create 200 new hosted zones in the Amazon Route 53 console Import zone files.", correct: true },
                { id: 1, text: "Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch", correct: false },
                { id: 2, text: "Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure", correct: false },
                { id: 3, text: "Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-domain-in-use.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A global company runs its applications in multiple AWS accounts in AWS Organizations. The \ncompany's applications use multipart uploads to upload data to multiple Amazon S3 buckets \nacross AWS Regions. The company wants to report on incomplete multipart uploads for cost \ncompliance purposes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure AWS Config with a rule to report the incomplete multipart upload object count.", correct: false },
                { id: 1, text: "Create a service control policy (SCP) to report the incomplete multipart upload object count.", correct: false },
                { id: 2, text: "Configure S3 Storage Lens to report the incomplete multipart upload object count.", correct: true },
                { id: 3, text: "Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nS3 storage lenses can be used to find incomplete multipart uploads: https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting- incomplete-multipart-uploads-to-lower-amazon-s3-costs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company runs a production database on Amazon RDS for MySQL. The company wants to \nupgrade the database version for security compliance reasons. Because the database contains \ncritical data, the company wants a quick solution to upgrade and test functionality without losing \nany data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.", correct: false },
                { id: 1, text: "Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS", correct: false },
                { id: 2, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new", correct: false },
                { id: 3, text: "Use Amazon RDS Blue/Green Deployments to deploy and test production changes.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nYou can make changes to the RDS DB instances in the green environment without affecting production workloads. For example, you can upgrade the major or minor DB engine version, upgrade the underlying file system configuration, or change database parameters in the staging environment. You can thoroughly test changes in the green environment. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments- overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A solutions architect is creating a data processing job that runs once daily and can take up to 2 \nhours to complete. If the job is interrupted, it has to restart from the beginning. \n \nHow should the solutions architect address this issue in the MOST cost-effective manner?",
            options: [
                { id: 0, text: "Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron", correct: false },
                { id: 1, text: "Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.", correct: false },
                { id: 2, text: "Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon", correct: true },
                { id: 3, text: "Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 14,
            text: "A social media company wants to store its database of user profiles, relationships, and \ninteractions in the AWS Cloud. The company needs an application to monitor any changes in the \ndatabase. The application needs to analyze the relationships between the data entities and to \nprovide recommendations to users. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process", correct: false },
                { id: 1, text: "Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the", correct: true },
                { id: 2, text: "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon", correct: false },
                { id: 3, text: "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWith Amazon Neptune, you can create sophisticated, interactive graph applications that can query billions of relationships in milliseconds. https://aws.amazon.com/neptune/features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company is creating a new application that will store a large amount of data. The data will be \nanalyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed \nacross multiple Availability Zones. The needed amount of storage space will continue to grow for \nthe next 6 months. \n \nWhich storage solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the", correct: false },
                { id: 1, text: "Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume", correct: false },
                { id: 2, text: "Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system", correct: true },
                { id: 3, text: "Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nShared File System: Amazon EFS allows multiple Amazon EC2 instances to mount the same file system simultaneously, making it easy for multiple instances to access and modify the data concurrently. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-\nAZ DB instance. Increases in traffic are causing performance problems. The company determines \nthat database queries are the primary reason for the slow performance. \n \nWhat should a solutions architect do to improve the application's performance?",
            options: [
                { id: 0, text: "Serve read traffic from the Multi-AZ standby replica.", correct: false },
                { id: 1, text: "Configure the DB instance to use Transfer Acceleration.", correct: false },
                { id: 2, text: "Create a read replica from the source DB instance. Serve read traffic from the read replica.", correct: true },
                { id: 3, text: "Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAfter you create a read replica from a source DB instance, the source becomes the primary DB instance. When you make updates to the primary DB instance, Amazon RDS copies them asynchronously to the read replica. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company collects 10 GB of telemetry data daily from various machines. The company stores \nthe data in an Amazon S3 bucket in a source data account. \n \nThe company has hired several consulting agencies to use this data for analysis. Each agency \nneeds read access to the data for its analysts. The company must share the data from the source \ndata account by choosing a solution that maximizes security and operational efficiency. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure S3 global tables to replicate data for each agency.", correct: false },
                { id: 1, text: "Make the S3 bucket public for a limited time. Inform only the agencies.", correct: false },
                { id: 2, text: "Configure cross-account access for the S3 bucket to the accounts that the agencies own.", correct: true },
                { id: 3, text: "Set up an IAM user for each analyst in the source data account. Grant each user access to the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS \nfile shares. Applications that run on Amazon EC2 instances access the file shares. The company \nneeds a storage disaster recovery (DR) solution in a secondary Region. The data that is \nreplicated in the secondary Region needs to be accessed by using the same protocols as the \nprimary Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3", correct: false },
                { id: 1, text: "Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the", correct: false },
                { id: 2, text: "Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate", correct: true },
                { id: 3, text: "Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nYou can use NetApp SnapMirror to schedule periodic replication of your FSx for ONTAP file system to or from a second file system. This capability is available for both in-Region and cross- Region deployments. https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/scheduled-replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A development team is creating an event-based application that uses AWS Lambda functions. \nEvents will be generated when files are added to an Amazon S3 bucket. The development team \ncurrently has Amazon Simple Notification Service (Amazon SNS) configured as the event target \nfrom Amazon S3. \n \nWhat should a solutions architect do to process the events from Amazon S3 in a scalable way?",
            options: [
                { id: 0, text: "Create an SNS subscription that processes the event in Amazon Elastic Container Service", correct: false },
                { id: 1, text: "Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service", correct: false },
                { id: 2, text: "Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon", correct: true },
                { id: 3, text: "Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS).", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon SQS is designed for event-driven and scalable message processing. It can handle large volumes of messages and automatically scales based on the incoming workload. This allows for better load distribution and scaling as compared to direct Lambda invocation.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 20,
            text: "A solutions architect is designing a new service behind Amazon API Gateway. The request \npatterns for the service will be unpredictable and can change suddenly from 0 requests to over \n500 per second. The total size of the data that needs to be persisted in a backend database is \ncurrently less than 1 GB with unpredictable future growth. Data can be queried using simple key-\nvalue requests. \n \nWhich combination ofAWS services would meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "AWS Fargate", correct: false },
                { id: 1, text: "AWS Lambda", correct: true },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon EC2 Auto Scaling", correct: false },
                { id: 4, text: "MySQL-compatible Amazon Aurora", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A company collects and shares research data with the company's employees all over the world. \nThe company wants to collect and store the data in an Amazon S3 bucket and process the data \nin the AWS Cloud. The company will share the data with the company's employees. The \ncompany needs a secure solution in the AWS Cloud that minimizes operational overhead. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the", correct: true },
                { id: 1, text: "Create an IAM user for each employee. Create an IAM policy for each employee to allow S3", correct: false },
                { id: 2, text: "Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow", correct: false },
                { id: 3, text: "Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A company is building a new furniture inventory application. The company has deployed the \napplication on a fleet ofAmazon EC2 instances across multiple Availability Zones. The EC2 \ninstances run behind an Application Load Balancer (ALB) in their VPC. \n \nA solutions architect has observed that incoming traffic seems to favor one EC2 instance, \nresulting in latency for some requests. \n \nWhat should the solutions architect do to resolve this issue?",
            options: [
                { id: 0, text: "Disable session affinity (sticky sessions) on the ALB", correct: true },
                { id: 1, text: "Replace the ALB with a Network Load Balancer", correct: false },
                { id: 2, text: "Increase the number of EC2 instances in each Availability Zone", correct: false },
                { id: 3, text: "Adjust the frequency of the health checks on the ALB's target group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A company has an application workflow that uses an AWS Lambda function to download and \ndecrypt files from Amazon S3. These files are encrypted using AWS Key Management Service \n(AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required \npermissions are set correctly. \n \nWhich combination of actions accomplish this? (Choose two.)",
            options: [
                { id: 0, text: "Attach the kms:decrypt permission to the Lambda function's resource policy", correct: false },
                { id: 1, text: "Grant the decrypt permission for the Lambda IAM role in the KMS key's policy", correct: true },
                { id: 2, text: "Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.", correct: false },
                { id: 3, text: "Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda", correct: false },
                { id: 4, text: "Create a new IAM role with the kms:decrypt permission and attach the execution role to the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "A company wants to monitor its AWS costs for financial review. The cloud operations team is \ndesigning an architecture in the AWS Organizations management account to query AWS Cost \nand Usage Reports for all member accounts. The team must run this query once a month and \nprovide a detailed analysis of the bill. \n \nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
            options: [
                { id: 0, text: "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis.", correct: false },
                { id: 1, text: "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3", correct: true },
                { id: 2, text: "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3 Use", correct: false },
                { id: 3, text: "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/big-data/analyze-amazon-s3-storage-costs-using-aws-cost-and- usage-reports-amazon-s3-inventory-and-amazon-athena/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 25,
            text: "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto \nScaling group in the AWS Cloud. The application will transmit data by using UDP packets. The \ncompany wants to ensure that the application can scale out and in as traffic increases and \ndecreases. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Attach a Network Load Balancer to the Auto Scaling group.", correct: true },
                { id: 1, text: "Attach an Application Load Balancer to the Auto Scaling group.", correct: false },
                { id: 2, text: "Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately.", correct: false },
                { id: 3, text: "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A company runs several websites on AWS for its different brands. Each website generates tens \nof gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution \nto give the company's developers the ability to analyze traffic patterns across all the company's \nwebsites. This analysis by the developers will occur on demand once a week over the course of \nseveral months. The solution must support queries with standard SQL. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n355 \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Store the logs in Amazon S3. Use Amazon Athena tor analysis.", correct: true },
                { id: 1, text: "Store the logs in Amazon RDS. Use a database client for analysis.", correct: false },
                { id: 2, text: "Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis.", correct: false },
                { id: 3, text: "Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 27,
            text: "An international company has a subdomain for each country that the company operates in. The \nsubdomains are formatted as example.com, country1.example.com, and country2.example.com. \nThe company's workloads are behind an Application Load Balancer. The company wants to \nencrypt the website data that is in transit. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top", correct: true },
                { id: 1, text: "Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top", correct: false },
                { id: 2, text: "Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the", correct: false },
                { id: 3, text: "Validate domain ownership by email address. Switch to DNS validation by adding the required", correct: false },
                { id: 4, text: "Validate domain ownership for the domain by adding the required DNS records to the DNS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company is required to use cryptographic keys in its on-premises key manager. The key \nmanager is outside of the AWS Cloud because of regulatory and compliance requirements. The \ncompany wants to manage encryption and decryption by using cryptographic keys that are \nretained outside of the AWS Cloud and that support a variety of external key managers from \ndifferent vendors. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS CloudHSM key store backed by a CloudHSM cluster.", correct: false },
                { id: 1, text: "Use an AWS Key Management Service (AWS KMS) external key store backed by an external key", correct: true },
                { id: 2, text: "Use the default AWS Key Management Service (AWS KMS) managed key store.", correct: false },
                { id: 3, text: "Use a custom key store backed by an AWS CloudHSM cluster.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/kms/latest/developerguide/keystore-external.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A solutions architect needs to host a high performance computing (HPC) workload in the AWS \nCloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel \naccess to a shared file system to enable distributed processing of large datasets. Datasets will be \naccessed across multiple instances simultaneously. The workload requires access latency within \n1 ms. After processing has completed, engineers will need access to the dataset for manual \npostprocessing. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from", correct: false },
                { id: 1, text: "Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly", correct: false },
                { id: 2, text: "Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket", correct: true },
                { id: 3, text: "Configure AWS Resource Access Manager to share an Amazon S3 bucket so that it can be", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon FSx for Lustre is a fully managed, high-performance file system optimized for HPC workloads. It is designed to deliver sub-millisecond latencies and high throughput, making it ideal for applications that require parallel access to shared storage, such as simulations and data analytics.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "A gaming company is building an application with Voice over IP capabilities. The application will \nserve traffic to users across the world. The application needs to be highly available with an \nautomated failover across AWS Regions. The company wants to minimize the latency of users \nwithout relying on IP address caching on user devices. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Global Accelerator with health checks.", correct: true },
                { id: 1, text: "Use Amazon Route 53 with a geolocation routing policy.", correct: false },
                { id: 2, text: "Create an Amazon CloudFront distribution that includes multiple origins.", correct: false },
                { id: 3, text: "Create an Application Load Balancer that uses path-based routing.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-benefits-of-migrating.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A weather forecasting company needs to process hundreds of gigabytes of data with sub-\nmillisecond latency. The company has a high performance computing (HPC) environment in its \ndata center and wants to expand its forecasting capabilities. \n \nA solutions architect must identify a highly available cloud storage solution that can handle large \namounts of sustained throughput. Files that are stored in the solution should be accessible to \nthousands of compute instances that will simultaneously access and process the entire dataset. \n \nWhat should the solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n357",
            options: [
                { id: 0, text: "Use Amazon FSx for Lustre scratch file systems.", correct: false },
                { id: 1, text: "Use Amazon FSx for Lustre persistent file systems.", correct: true },
                { id: 2, text: "Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.", correct: false },
                { id: 3, text: "Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "An ecommerce company runs a PostgreSQL database on premises. The database stores data by \nusing high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O \ntransactions per second do not exceed 15,000 IOPS. The company wants to migrate the \ndatabase to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of \ndisk storage capacity. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.", correct: false },
                { id: 1, text: "Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.", correct: false },
                { id: 2, text: "Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.", correct: true },
                { id: 3, text: "Configure the EBS magnetic volume type to achieve maximum IOPS.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to \nAWS. The company's online application uses the database to process transactions. The data \nanalysis team uses the same production database to run reports for analytical processing. The \ncompany wants to reduce operational overhead by moving to managed services wherever \npossible. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes", correct: true },
                { id: 1, text: "Migrate to Microsoft SQL Server on Amazon EC2. Use Always On read replicas for reporting", correct: false },
                { id: 2, text: "Migrate to Amazon DynamoDB. Use DynamoDB on-demand replicas for reporting purposes", correct: false },
                { id: 3, text: "Migrate to Amazon Aurora MySQL. Use Aurora read replicas for reporting purposes", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "A company stores a large volume of image files in an Amazon S3 bucket. The images need to be \nreadily available for the first 180 days. The images are infrequently accessed for the next 180 \ndays. After 360 days, the images need to be archived but must be available instantly upon \nrequest. After 5 years, only auditors can access the images. The auditors must be able to retrieve \nthe images within 12 hours. The images cannot be lost during this process. \n \nA developer will use S3 Standard storage for the first 180 days. The developer needs to configure \nan S3 Lifecycle rule. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n358 \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3", correct: true },
                { id: 1, text: "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3", correct: false },
                { id: 2, text: "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3", correct: false },
                { id: 3, text: "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A company has a large data workload that runs for 6 hours each day. The company cannot lose \nany data while the process is running. A solutions architect is designing an Amazon EMR cluster \nconfiguration to support this critical data workload. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure a long-running cluster that runs the primary node and core nodes on On-Demand", correct: false },
                { id: 1, text: "Configure a transient cluster that runs the primary node and core nodes on On-Demand", correct: true },
                { id: 2, text: "Configure a transient cluster that runs the primary node on an On-Demand Instance and the core", correct: false },
                { id: 3, text: "Configure a long-running cluster that runs the primary node on an On-Demand Instance, the core", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nA transient cluster provides cost savings because it runs only during the computation time, and it provides scalability and flexibility in a cloud environment.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A company maintains an Amazon RDS database that maps users to cost centers. The company \nhas accounts in an organization in AWS Organizations. The company needs a solution that will \ntag all resources that are created in a specific AWS account in the organization. The solution \nmust tag each resource with the cost center ID of the user who created the resource. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Move the specific AWS account to a new organizational unit (OU) in Organizations from the", correct: false },
                { id: 1, text: "Create an AWS Lambda function to tag the resources after the Lambda function looks up the", correct: true },
                { id: 2, text: "Create an AWS CloudFormation stack to deploy an AWS Lambda function. Configure the", correct: false },
                { id: 3, text: "Create an AWS Lambda function to tag the resources with a default value. Configure an Amazon", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 37,
            text: "A company recently migrated its web application to the AWS Cloud. The company uses an \nAmazon EC2 instance to run multiple processes to host the application. The processes include \nan Apache web server that serves static content. The Apache web server makes requests to a \nPHP application that uses a local Redis server for user sessions. \n \nThe company wants to redesign the architecture to be highly available and to use AWS managed \nsolutions. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Elastic Beanstalk to host the static content and the PHP application. Configure Elastic", correct: false },
                { id: 1, text: "Use AWS Lambda to host the static content and the PHP application. Use an Amazon API", correct: false },
                { id: 2, text: "Keep the backend code on the EC2 instance. Create an Amazon ElastiCache for Redis cluster", correct: false },
                { id: 3, text: "Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company runs a web application on Amazon EC2 instances in an Auto Scaling group that has a \ntarget group. The company designed the application to work with session affinity (sticky sessions) \nfor a better user experience. \n \nThe application must be available publicly over the internet as an endpoint. A WAF must be \napplied to the endpoint for additional security. Session affinity (sticky sessions) must be \nconfigured on the endpoint. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Create a public Network Load Balancer. Specify the application target group.", correct: false },
                { id: 1, text: "Create a Gateway Load Balancer. Specify the application target group.", correct: false },
                { id: 2, text: "Create a public Application Load Balancer. Specify the application target group.", correct: true },
                { id: 3, text: "Create a second target group. Add Elastic IP addresses to the EC2 instances.", correct: false },
                { id: 4, text: "Create a web ACL in AWS WAF. Associate the web ACL with the endpoint", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company runs a website that stores images of historical events. Website users need the ability \nto search and view images based on the year that the event in the image occurred. On average, \nusers request each image only once or twice a year. The company wants a highly available \nsolution to store and deliver the images to users. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Store images in Amazon Elastic Block Store (Amazon EBS). Use a web server that runs on", correct: false },
                { id: 1, text: "Store images in Amazon Elastic File System (Amazon EFS). Use a web server that runs on", correct: false },
                { id: 2, text: "Store images in Amazon S3 Standard. Use S3 Standard to directly deliver images by using a", correct: false },
                { id: 3, text: "Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3 Standard-IA", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 40,
            text: "A company has multiple AWS accounts in an organization in AWS Organizations that different \nbusiness units use. The company has multiple offices around the world. The company needs to \nupdate security group rules to allow new office CIDR ranges or to remove old CIDR ranges \nacross the organization. The company wants to centralize the management of security group \nrules to minimize the administrative overhead that updating CIDR ranges requires. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create VPC security groups in the organization's management account. Update the security", correct: false },
                { id: 1, text: "Create a VPC customer managed prefix list that contains the list of CIDRs. Use AWS Resource", correct: true },
                { id: 2, text: "Create an AWS managed prefix list. Use an AWS Security Hub policy to enforce the security", correct: false },
                { id: 3, text: "Create security groups in a central administrative AWS account. Create an AWS Firewall", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 41,
            text: "A company uses an on-premises network-attached storage (NAS) system to provide file shares to \nits high performance computing (HPC) workloads. The company wants to migrate its latency-\nsensitive HPC workloads and its storage to the AWS Cloud. The company must be able to \nprovide NFS and SMB multi-protocol access from the file system. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n361 \nWhich solution will meet these requirements with the LEAST latency? (Choose two.)",
            options: [
                { id: 0, text: "Deploy compute optimized EC2 instances into a cluster placement group.", correct: true },
                { id: 1, text: "Deploy compute optimized EC2 instances into a partition placement group.", correct: false },
                { id: 2, text: "Attach the EC2 instances to an Amazon FSx for Lustre file system.", correct: false },
                { id: 3, text: "Attach the EC2 instances to an Amazon FSx for OpenZFS file system.", correct: false },
                { id: 4, text: "Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A company is relocating its data center and wants to securely transfer 50 TB of data to AWS \nwithin 2 weeks. The existing data center has a Site-to-Site VPN connection to AWS that is 90% \nutilized. \n \nWhich AWS service should a solutions architect use to meet these requirements?",
            options: [
                { id: 0, text: "AWS DataSync with a VPC endpoint", correct: false },
                { id: 1, text: "AWS Direct Connect", correct: false },
                { id: 2, text: "AWS Snowball Edge Storage Optimized", correct: true },
                { id: 3, text: "AWS Storage Gateway", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company hosts an application on Amazon EC2 On-Demand Instances in an Auto Scaling \ngroup. Application peak hours occur at the same time each day. Application users report slow \napplication performance at the start of peak hours. The application performs normally 2-3 hours \nafter peak hours begin. The company wants to ensure that the application works properly at the \nstart of peak hours. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer to distribute traffic properly to the instances.", correct: false },
                { id: 1, text: "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on", correct: false },
                { id: 2, text: "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on", correct: false },
                { id: 3, text: "Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 44,
            text: "A company runs applications on AWS that connect to the company's Amazon RDS database. \nThe applications scale on weekends and at peak times of the year. The company wants to scale \nthe database more effectively for its applications that connect to the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB with connection pooling with a target group configuration for the", correct: false },
                { id: 1, text: "Use Amazon RDS Proxy with a target group for the database. Change the applications to use the", correct: true },
                { id: 2, text: "Use a custom proxy that runs on Amazon EC2 as an intermediary to the database. Change the", correct: false },
                { id: 3, text: "Use an AWS Lambda function to provide connection pooling with a target group configuration for", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that \nAmazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. \nHowever, the company does not purchase additional EBS storage every month. The company \nwants to optimize monthly costs for its current storage usage. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use logs in Amazon CloudWatch Logs to monitor the storage utilization of Amazon EBS. Use", correct: false },
                { id: 1, text: "Use a custom script to monitor space usage. Use Amazon EBS Elastic Volumes to reduce the", correct: false },
                { id: 2, text: "Delete all expired and unused snapshots to reduce snapshot costs.", correct: false },
                { id: 3, text: "Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "A company is developing a new application on AWS. The application consists of an Amazon \nElastic Container Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for \nthe application, and an Amazon RDS for MySQL database that contains the dataset for the \napplication. The dataset contains sensitive information. The company wants to ensure that only \nthe ECS cluster can access the data in the RDS for MySQL database and the data in the S3 \nbucket. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt", correct: false },
                { id: 1, text: "Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3", correct: false },
                { id: 2, text: "Create an S3 bucket policy that restricts bucket access to the ECS task execution role. Create a", correct: false },
                { id: 3, text: "Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nThe most comprehensive solution as it leverages VPC endpoints for both Amazon RDS and Amazon S3, along with proper network-level controls to restrict access to only the necessary resources from the ECS cluster.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A company has a web application that runs on premises. The application experiences latency \nissues during peak hours. The latency issues occur twice each month. At the start of a latency \nissue, the application's CPU utilization immediately increases to 10 times its normal amount. \n \nThe company wants to migrate the application to AWS to improve latency. The company also \nwants to scale the application automatically when application demand increases. The company \nwill use AWS Elastic Beanstalk for application deployment. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited", correct: false },
                { id: 1, text: "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the", correct: false },
                { id: 2, text: "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the", correct: false },
                { id: 3, text: "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 48,
            text: "A company has customers located across the world. The company wants to use automation to \nsecure its systems and network infrastructure. The company's security team must be able to track \nand audit all incremental changes to the infrastructure. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Organizations to set up the infrastructure. Use AWS Config to track changes.", correct: false },
                { id: 1, text: "Use AWS CloudFormation to set up the infrastructure. Use AWS Config to track changes.", correct: true },
                { id: 2, text: "Use AWS Organizations to set up the infrastructure. Use AWS Service Catalog to track changes.", correct: false },
                { id: 3, text: "Use AWS CloudFormation to set up the infrastructure. Use AWS Service Catalog to track", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A startup company is hosting a website for its customers on an Amazon EC2 instance. The \nwebsite consists of a stateless Python application and a MySQL database. The website serves \nonly a small amount of traffic. The company is concerned about the reliability of the instance and \nneeds to migrate to a highly available architecture. The company cannot modify the application \ncode. \n \nWhich combination of actions should a solutions architect take to achieve high availability for the \nwebsite? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n364",
            options: [
                { id: 0, text: "Provision an internet gateway in each Availability Zone in use.", correct: false },
                { id: 1, text: "Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.", correct: true },
                { id: 2, text: "Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.", correct: false },
                { id: 3, text: "Use AWS DataSync to synchronize the database data across multiple EC2 instances.", correct: false },
                { id: 4, text: "Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A company is moving its data and applications to AWS during a multiyear migration project. The \ncompany wants to securely access data on Amazon S3 from the company's AWS Region and \nfrom the company's on-premises location. The data must not traverse the internet. The company \nhas established an AWS Direct Connect connection between its Region and its on-premises \nlocation. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create gateway endpoints for Amazon S3. Use the gateway endpoints to securely access the", correct: false },
                { id: 1, text: "Create a gateway in AWS Transit Gateway to access Amazon S3 securely from the Region and", correct: false },
                { id: 2, text: "Create interface endpoints for Amazon S3. Use the interface endpoints to securely access the", correct: true },
                { id: 3, text: "Use an AWS Key Management Service (AWS KMS) key to access the data securely from the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company created a new organization in AWS Organizations. The organization has multiple \naccounts for the company's development teams. The development team members use AWS IAM \nIdentity Center (AWS Single Sign-On) to access the accounts. For each of the company's \napplications, the development teams must use a predefined application name to tag resources \nthat are created. \n \nA solutions architect needs to design a solution that gives the development team the ability to \ncreate resources only if the application name tag has an approved value. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an IAM group that has a conditional Allow policy that requires the application name tag to", correct: false },
                { id: 1, text: "Create a cross-account role that has a Deny policy for any resource that has the application name", correct: false },
                { id: 2, text: "Create a resource group in AWS Resource Groups to validate that the tags are applied to all", correct: false },
                { id: 3, text: "Create a tag policy in Organizations that has a list of allowed application names.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A company runs its databases on Amazon RDS for PostgreSQL. The company wants a secure \nsolution to manage the master user password by rotating the password every 30 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon EventBridge to schedule a custom AWS Lambda function to rotate the password", correct: false },
                { id: 1, text: "Use the modify-db-instance command in the AWS CLI to change the password.", correct: false },
                { id: 2, text: "Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password", correct: true },
                { id: 3, text: "Integrate AWS Systems Manager Parameter Store with Amazon RDS for PostgreSQL to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "A company performs tests on an application that uses an Amazon DynamoDB table. The tests \nrun for 4 hours once a week. The company knows how many read and write operations the \napplication performs to the table each second during the tests. The company does not currently \nuse DynamoDB for any other use case. A solutions architect needs to optimize the costs for the \ntable. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Choose on-demand mode. Update the read and write capacity units appropriately.", correct: true },
                { id: 1, text: "Choose provisioned mode. Update the read and write capacity units appropriately.", correct: false },
                { id: 2, text: "Purchase DynamoDB reserved capacity for a 1-year term.", correct: false },
                { id: 3, text: "Purchase DynamoDB reserved capacity for a 3-year term.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nWith provisioned capacity mode, you specify the number of reads and writes per second that you expect your application to require, and you are billed based on that. Furthermore if you can forecast your capacity requirements you can also reserve a portion of DynamoDB provisioned capacity and optimize your costs even further. https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 54,
            text: "A company runs its applications on Amazon EC2 instances. The company performs periodic \nfinancial assessments of its AWS costs. The company recently identified unusual spending. \n \nThe company needs a solution to prevent unusual spending. The solution must monitor costs and \nnotify responsible stakeholders in the event of unusual spending. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n366",
            options: [
                { id: 0, text: "Use an AWS Budgets template to create a zero spend budget.", correct: false },
                { id: 1, text: "Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management", correct: true },
                { id: 2, text: "Create AWS Pricing Calculator estimates for the current running workload pricing details.", correct: false },
                { id: 3, text: "Use Amazon CloudWatch to monitor costs and to identify unusual spending.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Cost Anomaly Detection is designed to automatically detect unusual spending patterns based on machine learning algorithms. It can identify anomalies and send notifications when it detects unexpected changes in spending. This aligns well with the requirement to prevent unusual spending and notify stakeholders. https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A marketing company receives a large amount of new clickstream data in Amazon S3 from a \nmarketing campaign. The company needs to analyze the clickstream data in Amazon S3 quickly. \nThen the company needs to determine whether to process the data further in the data pipeline. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create external tables in a Spark catalog. Configure jobs in AWS Glue to query the data.", correct: false },
                { id: 1, text: "Configure an AWS Glue crawler to crawl the data. Configure Amazon Athena to query the data.", correct: false },
                { id: 2, text: "Create external tables in a Hive metastore. Configure Spark jobs in Amazon EMR to query the", correct: false },
                { id: 3, text: "Configure an AWS Glue crawler to crawl the data. Configure Amazon Kinesis Data Analytics to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service, and Athena is a serverless query service that allows you to analyze data directly in Amazon S3 using SQL queries. By configuring an AWS Glue crawler to crawl the data, you can create a schema for the data, and then use Athena to query the data directly without the need to load it into a separate database. This minimizes operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 56,
            text: "A company runs an SMB file server in its data center. The file server stores large files that the \ncompany frequently accesses for up to 7 days after the file creation date. After 7 days, the \ncompany needs to be able to access the files with a maximum retrieval time of 24 hours. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.", correct: false },
                { id: 1, text: "Create an Amazon S3 File Gateway to increase the company's storage space. Create an S3", correct: true },
                { id: 2, text: "Create an Amazon FSx File Gateway to increase the company's storage space. Create an", correct: false },
                { id: 3, text: "Configure access to Amazon S3 for each user. Create an S3 Lifecycle policy to transition the data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 file gateway supports SMB and S3 Glacier Deep Archive can retrieve data within 12 hours. https://aws.amazon.com/storagegateway/file/s3/ https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/amazon-s3- glacier.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A company runs a web application on Amazon EC2 instances in an Auto Scaling group. The \napplication uses a database that runs on an Amazon RDS for PostgreSQL DB instance. The \napplication performs slowly when traffic increases. The database experiences a heavy read load \nduring periods of high traffic. \n \nWhich actions should a solutions architect take to resolve these performance issues? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Turn on auto scaling for the DB instance.", correct: false },
                { id: 1, text: "Create a read replica for the DB instance. Configure the application to send read traffic to the", correct: true },
                { id: 2, text: "Convert the DB instance to a Multi-AZ DB instance deployment. Configure the application to send", correct: false },
                { id: 3, text: "Create an Amazon ElastiCache cluster. Configure the application to cache query results in the", correct: false },
                { id: 4, text: "Configure the Auto Scaling group subnets to ensure that the EC2 instances are provisioned in the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nBy creating a read replica, you offload read traffic from the primary DB instance to the replica, distributing the load and improving overall performance during periods of heavy read traffic. Amazon ElastiCache can be used to cache frequently accessed data, reducing the load on the database. This is particularly effective for read-heavy workloads, as it allows the application to retrieve data from the cache rather than making repeated database queries.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) \nvolumes to run an application. The company creates one snapshot of each EBS volume every \nday to meet compliance requirements. The company wants to implement an architecture that \nprevents the accidental deletion of EBS volume snapshots. The solution must not change the \nadministrative rights of the storage administrator user. \n \nWhich solution will meet these requirements with the LEAST administrative effort?",
            options: [
                { id: 0, text: "Create an IAM role that has permission to delete snapshots. Attach the role to a new EC2", correct: false },
                { id: 1, text: "Create an IAM policy that denies snapshot deletion. Attach the policy to the storage administrator", correct: false },
                { id: 2, text: "Add tags to the snapshots. Create retention rules in Recycle Bin for EBS snapshots that have the", correct: false },
                { id: 3, text: "Lock the EBS snapshots to prevent deletion.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 \ninstances, and databases that are deployed in an Amazon VPC. The company wants to capture \ninformation about traffic to and from the network interfaces in near real time in its Amazon VPC. \nThe company wants to send the information to Amazon OpenSearch Service for analysis. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data", correct: false },
                { id: 1, text: "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data", correct: true },
                { id: 2, text: "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use", correct: false },
                { id: 3, text: "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nVPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. By configuring VPC Flow Logs to send the log data to a log group in Amazon CloudWatch Logs, you can then use Amazon Kinesis Data Firehose to stream the logs from the log group to Amazon OpenSearch Service for analysis. This approach provides near real-time streaming of logs to the analytics service.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 60,
            text: "A company is developing an application that will run on a production Amazon Elastic Kubernetes \nService (Amazon EKS) cluster. The EKS cluster has managed node groups that are provisioned \nwith On-Demand Instances. \n \nThe company needs a dedicated EKS cluster for development work. The company will use the \ndevelopment cluster infrequently to test the resiliency of the application. The EKS cluster must \nmanage all the nodes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a managed node group that contains only Spot Instances.", correct: true },
                { id: 1, text: "Create two managed node groups. Provision one node group with On-Demand Instances.", correct: false },
                { id: 2, text: "Create an Auto Scaling group that has a launch configuration that uses Spot Instances. Configure", correct: false },
                { id: 3, text: "Create a managed node group that contains only On-Demand Instances.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 61,
            text: "A company stores sensitive data in Amazon S3. A solutions architect needs to create an \nencryption solution. The company needs to fully control the ability of users to create, rotate, and \ndisable encryption keys with minimal effort for any data that must be encrypted. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store", correct: false },
                { id: 1, text: "Create a customer managed key by using AWS Key Management Service (AWS KMS). Use the", correct: true },
                { id: 2, text: "Create an AWS managed key by using AWS Key Management Service (AWS KMS). Use the", correct: false },
                { id: 3, text: "Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThis option allows you to create a customer managed key using AWS KMS. With a customer managed key, you have full control over key lifecycle management, including the ability to create, rotate, and disable keys with minimal effort. SSE-KMS also integrates with AWS Identity and Access Management (IAM) for fine-grained access control.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company wants to back up its on-premises virtual machines (VMs) to AWS. The company's \nbackup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3 \nbackups must be retained for 30 days and must be automatically deleted after 30 days. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Create an S3 bucket that has S3 Object Lock enabled.", correct: true },
                { id: 1, text: "Create an S3 bucket that has object versioning enabled.", correct: false },
                { id: 2, text: "Configure a default retention period of 30 days for the objects.", correct: false },
                { id: 3, text: "Configure an S3 Lifecycle policy to protect the objects for 30 days.", correct: false },
                { id: 4, text: "Configure an S3 Lifecycle policy to expire the objects after 30 days.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 63,
            text: "A solutions architect needs to copy files from an Amazon S3 bucket to an Amazon Elastic File \nSystem (Amazon EFS) file system and another S3 bucket. The files must be copied continuously. \nNew files are added to the original S3 bucket consistently. The copied files should be overwritten \nonly if the source file changes. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n370 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.", correct: true },
                { id: 1, text: "Create an AWS Lambda function. Mount the file system to the function. Set up an S3 event", correct: false },
                { id: 2, text: "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.", correct: false },
                { id: 3, text: "Launch an Amazon EC2 instance in the same VPC as the file system. Mount the file system.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS DataSync is designed for efficient and reliable copying of data between different storage solutions. By setting up an AWS DataSync task with the transfer mode set to transfer only data that has changed, you ensure that only the new or modified files are copied. This minimizes data transfer and operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store \n(Amazon EBS) volumes. The company must ensure that all data is encrypted at rest by using \nAWS Key Management Service (AWS KMS). The company must be able to control rotation of the \nencryption keys. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a customer managed key. Use the key to encrypt the EBS volumes.", correct: true },
                { id: 1, text: "Use an AWS managed key to encrypt the EBS volumes. Use the key to configure automatic key", correct: false },
                { id: 2, text: "Create an external KMS key with imported key material. Use the key to encrypt the EBS volumes.", correct: false },
                { id: 3, text: "Use an AWS owned key to encrypt the EBS volumes.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
    ],
    test21: [
        {
            id: 0,
            text: "A company needs a solution to enforce data encryption at rest on Amazon EC2 instances. The \nsolution must automatically identify noncompliant resources and enforce compliance policies on \nfindings. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store", correct: true },
                { id: 1, text: "Use AWS Key Management Service (AWS KMS) to manage access to encrypted Amazon Elastic", correct: false },
                { id: 2, text: "Use Amazon Macie to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes.", correct: false },
                { id: 3, text: "Use Amazon inspector to detect unencrypted Amazon Elastic Block Store (Amazon EBS)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nBy creating an IAM policy that allows users to create only encrypted EBS volumes, you proactively prevent the creation of unencrypted volumes. Using AWS Config, you can set up rules to detect noncompliant resources, and AWS Systems Manager Automation can be used for automated remediation. This approach provides a proactive and automated solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company is migrating its multi-tier on-premises application to AWS. The application consists of \na single-node MySQL database and a multi-node web tier. The company must minimize changes \nto the application during the migration. The company wants to improve application resiliency after \nthe migration. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application", correct: true },
                { id: 1, text: "Migrate the database to Amazon EC2 instances in an Auto Scaling group behind a Network Load", correct: false },
                { id: 2, text: "Migrate the database to an Amazon RDS Multi-AZ deployment.", correct: false },
                { id: 3, text: "Migrate the web tier to an AWS Lambda function.", correct: false },
                { id: 4, text: "Migrate the database to an Amazon DynamoDB table.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nWeb Tier Migration (Option A): Migrating the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) provides horizontal scalability, automatic scaling, and improved resiliency. Auto Scaling helps in managing and maintaining the desired number of EC2 instances based on demand, and the ALB distributes incoming traffic across multiple instances. Database Migration to Amazon RDS Multi-AZ (Option C): Migrating the database to Amazon RDS in a Multi-AZ deployment provides high availability and automatic failover. In a Multi-AZ deployment, Amazon RDS maintains a standby replica in a different Availability Zone, and in the event of a failure, it automatically promotes the replica to the primary instance. This enhances the resiliency of the database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company wants to migrate its web applications from on premises to AWS. The company is \nlocated close to the eu-central-1 Region. Because of regulations, the company cannot launch \nsome of its applications in eu-central-1. The company wants to achieve single-digit millisecond \nlatency. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the applications in eu-central-1. Extend the company's VPC from eu-central-1 to an edge", correct: false },
                { id: 1, text: "Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1", correct: true },
                { id: 2, text: "Deploy the applications in eu-central-1. Extend the company's VPC from eu-central-1 to the", correct: false },
                { id: 3, text: "Deploy the applications in AWS Wavelength Zones by extending the company's VPC from eu-", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Local Zones are a type of AWS infrastructure deployment that place compute, storage, database, and other select services closer to large population, industry, and IT centers, enabling you to deliver applications that require single-digit millisecond latency to end-users.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "A company's ecommerce website has unpredictable traffic and uses AWS Lambda functions to \ndirectly access a private Amazon RDS for PostgreSQL DB instance. The company wants to \nmaintain predictable database performance and ensure that the Lambda invocations do not \noverload the database with too many connections. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Point the client driver at an RDS custom endpoint. Deploy the Lambda functions inside a VPC.", correct: false },
                { id: 1, text: "Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions inside a VPC.", correct: true },
                { id: 2, text: "Point the client driver at an RDS custom endpoint. Deploy the Lambda functions outside a VPC.", correct: false },
                { id: 3, text: "Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions outside a VPC.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company is creating an application. The company stores data from tests of the application in \nmultiple on-premises locations. \n \nThe company needs to connect the on-premises locations to VPCs in an AWS Region in the \nAWS Cloud. The number of accounts and VPCs will increase during the next year. The network \narchitecture must simplify the administration of new connections and must provide the ability to \nscale. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Create a peering connection between the VPCs. Create a VPN connection between the VPCs", correct: false },
                { id: 1, text: "Launch an Amazon EC2 instance. On the instance, include VPN software that uses a VPN", correct: false },
                { id: 2, text: "Create a transit gateway. Create VPC attachments for the VPC connections. Create VPN", correct: false },
                { id: 3, text: "Create an AWS Direct Connect connection between the on-premises locations and a central", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n373 \nA company that uses AWS needs a solution to predict the resources needed for manufacturing \nprocesses each month. The solution must use historical values that are currently stored in an \nAmazon S3 bucket. The company has no machine learning (ML) experience and wants to use a \nmanaged service for the training and predictions. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Deploy an Amazon SageMaker model. Create a SageMaker endpoint for inference.", correct: false },
                { id: 1, text: "Use Amazon SageMaker to train a model by using the historical data in the S3 bucket.", correct: true },
                { id: 2, text: "Configure an AWS Lambda function with a function URL that uses Amazon SageMaker endpoints", correct: false },
                { id: 3, text: "Configure an AWS Lambda function with a function URL that uses an Amazon Forecast predictor", correct: false },
                { id: 4, text: "Train an Amazon Forsecast predictor by using the historical data in the S3 bucket.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A company manages AWS accounts in AWS Organizations. AWS IAM Identity Center (AWS \nSingle Sign-On) and AWS Control Tower are configured for the accounts. The company wants to \nmanage multiple user permissions across all the accounts. \n \nThe permissions will be used by multiple IAM users and must be split between the developer and \nadministrator teams. Each team requires different permissions. The company wants a solution \nthat includes new users that are hired on both teams. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create individual users in IAM Identity Center for each account. Create separate developer and", correct: false },
                { id: 1, text: "Create individual users in IAM Identity Center for each account. Create separate developer and", correct: false },
                { id: 2, text: "Create individual users in IAM Identity Center. Create new developer and administrator groups in", correct: true },
                { id: 3, text: "Create individual users in IAM Identity Center. Create new permission sets that include the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/controltower/latest/userguide/sso.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume \nencryption strategy. The company also wants to minimize the cost and configuration effort \nrequired to operate the volume encryption check. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n374 \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Use", correct: false },
                { id: 1, text: "Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Run", correct: false },
                { id: 2, text: "Create an AWS Identity and Access Management (IAM) policy that requires the use of tags on", correct: false },
                { id: 3, text: "Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nYou could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed- rules.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company regularly uploads GB-sized files to Amazon S3. After the company uploads the files, \nthe company uses a fleet of Amazon EC2 Spot Instances to transcode the file format. The \ncompany needs to scale throughput when the company uploads data from the on-premises data \ncenter to Amazon S3 and when the company downloads data from Amazon S3 to the EC2 \ninstances. \n \nWhich solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use the S3 bucket access point instead of accessing the S3 bucket directly.", correct: false },
                { id: 1, text: "Upload the files into multiple S3 buckets.", correct: false },
                { id: 2, text: "Use S3 multipart uploads.", correct: true },
                { id: 3, text: "Fetch multiple byte-ranges of an object in parallel.", correct: false },
                { id: 4, text: "Add a random prefix to each object when uploading the files.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 9,
            text: "A solutions architect is designing a shared storage solution for a web application that is deployed \nacross multiple Availability Zones. The web application runs on Amazon EC2 instances that are in \nan Auto Scaling group. The company plans to make frequent changes to the content. The \nsolution must have strong consistency in returning the new content as soon as the changes \noccur. \n \nWhich solutions meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI)", correct: false },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on", correct: true },
                { id: 2, text: "Create a shared Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on", correct: false },
                { id: 3, text: "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the", correct: false },
                { id: 4, text: "Create an Amazon S3 bucket to store the web content. Set the metadata for the Cache-Control", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A company is deploying an application in three AWS Regions using an Application Load \nBalancer. Amazon Route 53 will be used to distribute traffic between these Regions. \n \nWhich Route 53 configuration should a solutions architect use to provide the MOST high-\nperforming experience?",
            options: [
                { id: 0, text: "Create an A record with a latency policy.", correct: true },
                { id: 1, text: "Create an A record with a geolocation policy.", correct: false },
                { id: 2, text: "Create a CNAME record with a failover policy.", correct: false },
                { id: 3, text: "Create a CNAME record with a geoproximity policy.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nLBR (Latency Based Routing) is a new feature for Amazon Route 53 that helps you improve your application’s performance for a global audience. You can run applications in multiple AWS regions and Amazon Route 53, using dozens of edge locations worldwide, will route end users to the AWS region that provides the lowest latency. https://aws.amazon.com/route53/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company has a web application that includes an embedded NoSQL database. The application \nruns on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in \nan Amazon EC2 Auto Scaling group in a single Availability Zone. \n \nA recent increase in traffic requires the application to be highly available and for the database to \nbe eventually consistent. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Replace the ALB with a Network Load Balancer. Maintain the embedded NoSQL database with", correct: false },
                { id: 1, text: "Replace the ALB with a Network Load Balancer. Migrate the embedded NoSQL database to", correct: false },
                { id: 2, text: "Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Maintain the", correct: false },
                { id: 3, text: "Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Migrate the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A company is building a shopping application on AWS. The application offers a catalog that \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n376 \nchanges once each month and needs to scale with traffic volume. The company wants the lowest \npossible latency from the application. Data from each user's shopping cart needs to be highly \navailable. User session data must be available even if the user is disconnected and reconnects. \n \nWhat should a solutions architect do to ensure that the shopping cart data is preserved at all \ntimes?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer to enable the sticky sessions feature (session affinity) for", correct: false },
                { id: 1, text: "Configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and", correct: true },
                { id: 2, text: "Configure Amazon OpenSearch Service to cache catalog data from Amazon DynamoDB and", correct: false },
                { id: 3, text: "Configure an Amazon EC2 instance with Amazon Elastic Block Store (Amazon EBS) storage for", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company is building a microservices-based application that will be deployed on Amazon Elastic \nKubernetes Service (Amazon EKS). The microservices will interact with each other. The company \nwants to ensure that the application is observable to identify performance issues in the future. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the application to use Amazon ElastiCache to reduce the number of requests that are", correct: false },
                { id: 1, text: "Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters.", correct: true },
                { id: 2, text: "Configure AWS CloudTrail to review the API calls. Build an Amazon QuickSight dashboard to", correct: false },
                { id: 3, text: "Use AWS Trusted Advisor to understand the performance of the application.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon CloudWatch Container Insights: This service provides monitoring and troubleshooting capabilities for containerized applications. It collects and aggregates metrics, logs, and events from Amazon EKS clusters and containers. This helps in monitoring the performance and health of microservices.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "A company needs to provide customers with secure access to its data. The company processes \ncustomer data and stores the results in an Amazon S3 bucket. \n \nAll the data is subject to strong regulations and security requirements. The data must be \nencrypted at rest. Each customer must be able to access only their data from their AWS account. \nCompany employees must not be able to access the data. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Provision an AWS Certificate Manager (ACM) certificate for each customer. Encrypt the data", correct: false },
                { id: 1, text: "Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt", correct: false },
                { id: 2, text: "Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt", correct: true },
                { id: 3, text: "Provision an AWS Certificate Manager (ACM) certificate for each customer. Encrypt the data", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A solutions architect creates a VPC that includes two public subnets and two private subnets. A \ncorporate security mandate requires the solutions architect to launch all Amazon EC2 instances \nin a private subnet. However, when the solutions architect launches an EC2 instance that runs a \nweb server on ports 80 and 443 in a private subnet, no external internet traffic can connect to the \nserver. \n \nWhat should the solutions architect do to resolve this issue?",
            options: [
                { id: 0, text: "Attach the EC2 instance to an Auto Scaling group in a private subnet. Ensure that the DNS record", correct: false },
                { id: 1, text: "Provision an internet-facing Application Load Balancer (ALB) in a public subnet. Add the EC2", correct: true },
                { id: 2, text: "Launch a NAT gateway in a private subnet. Update the route table for the private subnets to add", correct: false },
                { id: 3, text: "Ensure that the security group that is attached to the EC2 instance allows HTTP traffic on port 80", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A company is deploying a new application to Amazon Elastic Kubernetes Service (Amazon EKS) \nwith an AWS Fargate cluster. The application needs a storage solution for data persistence. The \nsolution must be highly available and fault tolerant. The solution also must be shared between \nmultiple application containers. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create Amazon Elastic Block Store (Amazon EBS) volumes in the same Availability Zones where", correct: false },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) file system. Register the file system in a", correct: true },
                { id: 2, text: "Create an Amazon Elastic Block Store (Amazon EBS) volume. Register the volume in a", correct: false },
                { id: 3, text: "Create Amazon Elastic File System (Amazon EFS) file systems in the same Availability Zones", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A company has an application that uses Docker containers in its local data center. The \napplication runs on a container host that stores persistent data in a volume on the host. The \ncontainer instances use the stored persistent data. \n \nThe company wants to move the application to a fully managed service because the company \ndoes not want to manage any servers or storage infrastructure. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed nodes. Create an", correct: false },
                { id: 1, text: "Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create", correct: true },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create", correct: false },
                { id: 3, text: "Use Amazon Elastic Container Service (Amazon ECS) with an Amazon EC2 launch type. Create", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nMounting S3 in Fargate is not supported commonly. You'd have to make it manually. EFS is very well supported with Fargate. https://stackoverflow.com/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A gaming company wants to launch a new internet-facing application in multiple AWS Regions. \nThe application will use the TCP and UDP protocols for communication. The company needs to \nprovide high availability and minimum latency for global users. \n \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
            options: [
                { id: 0, text: "Create internal Network Load Balancers in front of the application in each Region.", correct: true },
                { id: 1, text: "Create external Application Load Balancers in front of the application in each Region.", correct: false },
                { id: 2, text: "Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each", correct: false },
                { id: 3, text: "Configure Amazon Route 53 to use a geolocation routing policy to distribute the traffic.", correct: false },
                { id: 4, text: "Configure Amazon CloudFront to handle the traffic and route requests to the application in each", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A city has deployed a web application running on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The application's users have reported sporadic performance, which \nappears to be related to DDoS attacks originating from random IP addresses. The city needs a \nsolution that requires minimal configuration changes and provides an audit trail for the DDoS \nsources. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Enable an AWS WAF web ACL on the ALB, and configure rules to block traffic from unknown", correct: false },
                { id: 1, text: "Subscribe to Amazon Inspector. Engage the AWS DDoS Response Team (DRT) to integrate", correct: false },
                { id: 2, text: "Subscribe to AWS Shield Advanced. Engage the AWS DDoS Response Team (DRT) to integrate", correct: true },
                { id: 3, text: "Create an Amazon CloudFront distribution for the application, and set the ALB as the origin.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A company copies 200 TB of data from a recent ocean survey onto AWS Snowball Edge Storage \nOptimized devices. The company has a high performance computing (HPC) cluster that is hosted \non AWS to look for oil and gas deposits. A solutions architect must provide the cluster with \nconsistent sub-millisecond latency and high-throughput access to the data on the Snowball Edge \nStorage Optimized devices. The company is sending the devices back to AWS. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an AWS Storage", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an Amazon FSx for", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket and an Amazon Elastic File System (Amazon EFS) file system.", correct: false },
                { id: 3, text: "Create an Amazon FSx for Lustre file system. Import the data directly into the FSx for Lustre file", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n380 \nA company has NFS servers in an on-premises data center that need to periodically back up \nsmall amounts of data to Amazon S3. \n \nWhich solution meets these requirements and is MOST cost-effective?",
            options: [
                { id: 0, text: "Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.", correct: false },
                { id: 1, text: "Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.", correct: true },
                { id: 2, text: "Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon", correct: false },
                { id: 3, text: "Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 22,
            text: "An online video game company must maintain ultra-low latency for its game servers. The game \nservers run on Amazon EC2 instances. The company needs a solution that can handle millions of \nUDP internet traffic requests each second. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer with the required protocol and ports for the internet", correct: false },
                { id: 1, text: "Configure a Gateway Load Balancer for the internet traffic. Specify the EC2 instances as the", correct: false },
                { id: 2, text: "Configure a Network Load Balancer with the required protocol and ports for the internet traffic.", correct: true },
                { id: 3, text: "Launch an identical set of game servers on EC2 instances in separate AWS Regions. Route", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A company runs a three-tier application in a VPC. The database tier uses an Amazon RDS for \nMySQL DB instance. \n \nThe company plans to migrate the RDS for MySQL DB instance to an Amazon Aurora \nPostgreSQL DB cluster. The company needs a solution that replicates the data changes that \nhappen during the migration to the new database. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the", correct: true },
                { id: 1, text: "Use AWS Database Migration Service (AWS DMS) Schema Conversion to create an Aurora", correct: false },
                { id: 2, text: "Configure an Aurora MySQL read replica for the RDS for MySQL DB instance.", correct: false },
                { id: 3, text: "Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to", correct: false },
                { id: 4, text: "Promote the Aurora PostgreSQL read replica to a standalone Aurora PostgreSQL DB cluster", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company hosts a database that runs on an Amazon RDS instance that is deployed to multiple \nAvailability Zones. The company periodically runs a script against the database to report new \nentries that are added to the database. The script that runs against the database negatively \naffects the performance of a critical application. The company needs to improve application \nperformance with minimal costs. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Add functionality to the script to identify the instance that has the fewest active connections.", correct: false },
                { id: 1, text: "Create a read replica of the database. Configure the script to query only the read replica to report", correct: false },
                { id: 2, text: "Instruct the development team to manually export the new entries for the day in the database at", correct: true },
                { id: 3, text: "Use Amazon ElastiCache to cache the common queries that the script runs against the database.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A company is using an Application Load Balancer (ALB) to present its application to the internet. \nThe company finds abnormal traffic access patterns across the application. A solutions architect \nneeds to improve visibility into the infrastructure to help the company understand these \nabnormalities better. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
            options: [
                { id: 0, text: "Create a table in Amazon Athena for AWS CloudTrail logs. Create a query for the relevant", correct: false },
                { id: 1, text: "Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.", correct: true },
                { id: 2, text: "Enable ALB access logging to Amazon S3. Open each file in a text editor, and search each line", correct: false },
                { id: 3, text: "Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 \ninstances in private subnets must be able to connect to the public internet through the NAT \ngateways. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create public NAT gateways in the same private subnets as the EC2 instances.", correct: false },
                { id: 1, text: "Create private NAT gateways in the same private subnets as the EC2 instances.", correct: false },
                { id: 2, text: "Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.", correct: true },
                { id: 3, text: "Create private NAT gateways in public subnets in the same VPCs as the EC2 instances.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nPublic NAT GW in Public Subnet to have access to internet. Private NAT GW is used for VPC or on-prem.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company has an organization in AWS Organizations. The company runs Amazon EC2 \ninstances across four AWS accounts in the root organizational unit (OU). There are three \nnonproduction accounts and one production account. The company wants to prohibit users from \nlaunching EC2 instances of a certain size in the nonproduction accounts. The company has \ncreated a service control policy (SCP) to deny access to launch instances that use the prohibited \ntypes. \n \nWhich solutions to deploy the SCP will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Attach the SCP to the root OU for the organization.", correct: false },
                { id: 1, text: "Attach the SCP to the three nonproduction Organizations member accounts.", correct: true },
                { id: 2, text: "Attach the SCP to the Organizations management account.", correct: false },
                { id: 3, text: "Create an OU for the production account. Attach the SCP to the OU. Move the production", correct: false },
                { id: 4, text: "Create an OU for the required accounts. Attach the SCP to the OU. Move the nonproduction", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company's website hosted on Amazon EC2 instances processes classified data stored in \nAmazon S3. Due to security concerns, the company requires a private and secure connection \nbetween its EC2 resources and Amazon S3. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Set up S3 bucket policies to allow access from a VPC endpoint.", correct: true },
                { id: 1, text: "Set up an IAM policy to grant read-write access to the S3 bucket.", correct: false },
                { id: 2, text: "Set up a NAT gateway to access resources outside the private subnet.", correct: false },
                { id: 3, text: "Set up an access key ID and a secret access key to access the S3 bucket.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nA VPC endpoint enables customers to privately connect to supported AWS services.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "An ecommerce company runs its application on AWS. The application uses an Amazon Aurora \nPostgreSQL cluster in Multi-AZ mode for the underlying database. During a recent promotional \ncampaign, the application experienced heavy read load and write load. Users experienced \ntimeout issues when they attempted to access the application. \n \nA solutions architect needs to make the application architecture more scalable and highly \navailable. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n383 \n \nWhich solution will meet these requirements with the LEAST downtime?",
            options: [
                { id: 0, text: "Create an Amazon EventBridge rule that has the Aurora cluster as a source. Create an AWS", correct: false },
                { id: 1, text: "Modify the Aurora cluster and activate the zero-downtime restart (ZDR) feature. Use Database", correct: false },
                { id: 2, text: "Add additional reader instances to the Aurora cluster. Create an Amazon RDS Proxy target group", correct: true },
                { id: 3, text: "Create an Amazon ElastiCache for Redis cache. Replicate data from the Aurora cluster to Redis", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company is designing a web application on AWS. The application will use a VPN connection \nbetween the company's existing data centers and the company's VPCs. \n \nThe company uses Amazon Route 53 as its DNS service. The application must use private DNS \nrecords to communicate with the on-premises services from a VPC. \n \nWhich solution will meet these requirements in the MOST secure manner?",
            options: [
                { id: 0, text: "Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate the resolver", correct: true },
                { id: 1, text: "Create a Route 53 Resolver inbound endpoint. Create a resolver rule. Associate the resolver rule", correct: false },
                { id: 2, text: "Create a Route 53 private hosted zone. Associate the private hosted zone with the VPC.", correct: false },
                { id: 3, text: "Create a Route 53 public hosted zone. Create a record for each service to allow service", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company is running a photo hosting service in the us-east-1 Region. The service enables users \nacross multiple countries to upload and view photos. Some photos are heavily viewed for months, \nand others are viewed for less than a week. The application allows uploads of up to 20 MB for \neach photo. The service uses the photo metadata to determine which photos to display to each \nuser. \n \nWhich solution provides the appropriate user access MOST cost-effectively?",
            options: [
                { id: 0, text: "Store the photos in Amazon DynamoDB. Turn on DynamoDB Accelerator (DAX) to cache", correct: false },
                { id: 1, text: "Store the photos in the Amazon S3 Intelligent-Tiering storage class. Store the photo metadata", correct: false },
                { id: 2, text: "Store the photos in the Amazon S3 Standard storage class. Set up an S3 Lifecycle policy to move", correct: false },
                { id: 3, text: "Store the photos in the Amazon S3 Glacier storage class. Set up an S3 Lifecycle policy to move", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "A company runs a highly available web application on Amazon EC2 instances behind an \nApplication Load Balancer. The company uses Amazon CloudWatch metrics. \n \nAs the traffic to the web application increases, some EC2 instances become overloaded with \nmany outstanding requests. The CloudWatch metrics show that the number of requests \nprocessed and the time to receive the responses from some EC2 instances are both higher \ncompared to other EC2 instances. The company does not want new requests to be forwarded to \nthe EC2 instances that are already overloaded. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use the round robin routing algorithm based on the RequestCountPerTarget and", correct: false },
                { id: 1, text: "Use the least outstanding requests algorithm based on the RequestCountPerTarget and", correct: true },
                { id: 2, text: "Use the round robin routing algorithm based on the RequestCount and TargetResponseTime", correct: false },
                { id: 3, text: "Use the least outstanding requests algorithm based on the RequestCount and", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A company uses Amazon EC2, AWS Fargate, and AWS Lambda to run multiple workloads in the \ncompany's AWS account. The company wants to fully make use of its Compute Savings Plans. \nThe company wants to receive notification when coverage of the Compute Savings Plans drops. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create a daily budget for the Savings Plans by using AWS Budgets. Configure the budget with a", correct: true },
                { id: 1, text: "Create a Lambda function that runs a coverage report against the Savings Plans. Use Amazon", correct: false },
                { id: 2, text: "Create an AWS Budgets report for the Savings Plans budget. Set the frequency to daily.", correct: false },
                { id: 3, text: "Create a Savings Plans alert subscription. Enable all notification options. Enter an email address", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/sp-usingBudgets.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A company runs a real-time data ingestion solution on AWS. The solution consists of the most \nrecent version of Amazon Managed Streaming for Apache Kafka (Amazon MSK). The solution is \ndeployed in a VPC in private subnets across three Availability Zones. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n385 \n \nA solutions architect needs to redesign the data ingestion solution to be publicly available over \nthe internet. The data in transit must also be encrypted. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Configure public subnets in the existing VPC. Deploy an MSK cluster in the public subnets.", correct: true },
                { id: 1, text: "Create a new VPC that has public subnets. Deploy an MSK cluster in the public subnets. Update", correct: false },
                { id: 2, text: "Deploy an Application Load Balancer (ALB) that uses private subnets. Configure an ALB security", correct: false },
                { id: 3, text: "Deploy a Network Load Balancer (NLB) that uses private subnets. Configure an NLB listener for", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A company wants to migrate an on-premises legacy application to AWS. The application ingests \ncustomer order files from an on-premises enterprise resource planning (ERP) system. The \napplication then uploads the files to an SFTP server. The application uses a scheduled job that \nchecks for order files every hour. \n \nThe company already has an AWS account that has connectivity to the on-premises network. The \nnew application on AWS must support integration with the existing ERP system. The new \napplication must be secure and resilient and must use the SFTP protocol to process orders from \nthe ERP system immediately. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an AWS Transfer Family SFTP internet-facing server in two Availability Zones. Use", correct: false },
                { id: 1, text: "Create an AWS Transfer Family SFTP internet-facing server in one Availability Zone. Use", correct: false },
                { id: 2, text: "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon", correct: false },
                { id: 3, text: "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 36,
            text: "A company's applications use Apache Hadoop and Apache Spark to process data on premises. \nThe existing infrastructure is not scalable and is complex to manage. \n \nA solutions architect must design a scalable solution that reduces operational complexity. The \nsolution must keep the data processing on premises. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n386 \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Site-to-Site VPN to access the on-premises Hadoop Distributed File System (HDFS)", correct: false },
                { id: 1, text: "Use AWS DataSync to connect to the on-premises Hadoop Distributed File System (HDFS)", correct: false },
                { id: 2, text: "Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR", correct: true },
                { id: 3, text: "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Create an Amazon", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company is migrating a large amount of data from on-premises storage to AWS. Windows, \nMac, and Linux based Amazon EC2 instances in the same AWS Region will access the data by \nusing SMB and NFS storage protocols. The company will access a portion of the data routinely. \nThe company will access the remaining data infrequently. \n \nThe company needs to design a solution to host the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering.", correct: false },
                { id: 1, text: "Create an Amazon FSx for ONTAP instance. Create an FSx for ONTAP file system with a root", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket that uses S3 Intelligent-Tiering. Migrate the data to the S3 bucket", correct: true },
                { id: 3, text: "Create an Amazon FSx for OpenZFS file system. Migrate the data to the new volume.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "A manufacturing company runs its report generation application on AWS. The application \ngenerates each report in about 20 minutes. The application is built as a monolith that runs on a \nsingle Amazon EC2 instance. The application requires frequent updates to its tightly coupled \nmodules. The application becomes complex to maintain as the company adds new features. \n \nEach time the company patches a software module, the application experiences downtime. \nReport generation must restart from the beginning after any interruptions. The company wants to \nredesign the application so that the application can be flexible, scalable, and gradually improved. \nThe company wants to minimize application downtime. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Run the application on AWS Lambda as a single function with maximum provisioned", correct: false },
                { id: 1, text: "Run the application on Amazon EC2 Spot Instances as microservices with a Spot Fleet default", correct: true },
                { id: 2, text: "Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with", correct: false },
                { id: 3, text: "Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 39,
            text: "A company wants to rearchitect a large-scale web application to a serverless microservices \narchitecture. The application uses Amazon EC2 instances and is written in Python. \n \nThe company selected one component of the web application to test as a microservice. The \ncomponent supports hundreds of requests each second. The company wants to create and test \nthe microservice on an AWS solution that supports Python. The solution must also scale \nautomatically and require minimal infrastructure and minimal operational support. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use a Spot Fleet with auto scaling of EC2 instances that run the most recent Amazon Linux", correct: false },
                { id: 1, text: "Use an AWS Elastic Beanstalk web server environment that has high availability configured.", correct: false },
                { id: 2, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS). Launch Auto Scaling groups of self-", correct: true },
                { id: 3, text: "Use an AWS Lambda function that runs custom developed code.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A company has an AWS Direct Connect connection from its on-premises location to an AWS \naccount. The AWS account has 30 different VPCs in the same AWS Region. The VPCs use \nprivate virtual interfaces (VIFs). Each VPC has a CIDR block that does not overlap with other \nnetworks under the company's control. \n \nThe company wants to centrally manage the networking architecture while still allowing each VPC \nto communicate with all other VPCs and on-premises networks. \n \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "Create a transit gateway, and associate the Direct Connect connection with a new transit VIF.", correct: true },
                { id: 1, text: "Create a Direct Connect gateway. Recreate the private VIFs to use the new gateway. Associate", correct: false },
                { id: 2, text: "Create a transit VPConnect the Direct Connect connection to the transit VPCreate a peering", correct: false },
                { id: 3, text: "Create AWS Site-to-Site VPN connections from on premises to each VPC. Ensure that both VPN", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company has applications that run on Amazon EC2 instances. The EC2 instances connect to \nAmazon RDS databases by using an IAM role that has associated policies. The company wants \nto use AWS Systems Manager to patch the EC2 instances without disrupting the running \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n388 \napplications. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a new IAM role. Attach the AmazonSSMManagedInstanceCore policy to the new IAM role.", correct: false },
                { id: 1, text: "Create an IAM user. Attach the AmazonSSMManagedInstanceCore policy to the IAM user.", correct: false },
                { id: 2, text: "Enable Default Host Configuration Management in Systems Manager to manage the EC2", correct: true },
                { id: 3, text: "Remove the existing policies from the existing IAM role. Add the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon \nEKS) and the Kubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout \nthe day. A solutions architect notices that the number of nodes does not automatically scale out \nwhen the existing nodes have reached maximum capacity in the cluster, which causes \nperformance issues. \n \nWhich solution will resolve this issue with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Scale out the nodes by tracking the memory usage.", correct: false },
                { id: 1, text: "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.", correct: true },
                { id: 2, text: "Use an AWS Lambda function to resize the EKS cluster automatically.", correct: false },
                { id: 3, text: "Use an Amazon EC2 Auto Scaling group to distribute the workload.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 \nobjects are each typically around 50 GB in size and are frequently replaced with multipart uploads \nby their global application. The number and size of S3 objects remain constant, but the \ncompany's S3 storage costs are increasing each month. \n \nHow should a solutions architect reduce costs in this situation?",
            options: [
                { id: 0, text: "Switch from multipart uploads to Amazon S3 Transfer Acceleration.", correct: false },
                { id: 1, text: "Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.", correct: true },
                { id: 2, text: "Configure S3 inventory to prevent objects from being archived too quickly.", correct: false },
                { id: 3, text: "Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A company has deployed a multiplayer game for mobile devices. The game requires live location \ntracking of players based on latitude and longitude. The data store for the game must support \nrapid updates and retrieval of locations. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n389 \nThe game uses an Amazon RDS for PostgreSQL DB instance with read replicas to store the \nlocation data. During peak usage periods, the database is unable to maintain the performance \nthat is needed for reading and writing updates. The game's user base is increasing rapidly. \n \nWhat should a solutions architect do to improve the performance of the data tier?",
            options: [
                { id: 0, text: "Take a snapshot of the existing DB instance. Restore the snapshot with Multi-AZ enabled.", correct: false },
                { id: 1, text: "Migrate from Amazon RDS to Amazon OpenSearch Service with OpenSearch Dashboards.", correct: false },
                { id: 2, text: "Deploy Amazon DynamoDB Accelerator (DAX) in front of the existing DB instance. Modify the", correct: false },
                { id: 3, text: "Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company stores critical data in Amazon DynamoDB tables in the company's AWS account. An \nIT administrator accidentally deleted a DynamoDB table. The deletion caused a significant loss of \ndata and disrupted the company's operations. The company wants to prevent this type of \ndisruption in the future. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure a trail in AWS CloudTrail. Create an Amazon EventBridge rule for delete actions.", correct: false },
                { id: 1, text: "Create a backup and restore plan for the DynamoDB tables. Recover the DynamoDB tables", correct: false },
                { id: 2, text: "Configure deletion protection on the DynamoDB tables.", correct: true },
                { id: 3, text: "Enable point-in-time recovery on the DynamoDB tables.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A company has an on-premises data center that is running out of storage capacity. The company \nwants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution \nmust allow for immediate retrieval of data at no additional cost. \n \nHow can these requirements be met?",
            options: [
                { id: 0, text: "Deploy Amazon S3 Glacier Vault and enable expedited retrieval. Enable provisioned retrieval", correct: false },
                { id: 1, text: "Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in", correct: false },
                { id: 2, text: "Deploy AWS Storage Gateway using stored volumes to store data locally. Use Storage Gateway", correct: true },
                { id: 3, text: "Deploy AWS Direct Connect to connect with the on-premises data center. Configure AWS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 47,
            text: "A company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon \nEC2 instances run in an Auto Scaling group for the application tier. \n \nThe company needs to make an automated scaling plan that will analyze each resource's daily \nand weekly historical workload trends. The configuration must scale resources appropriately \naccording to both the forecast and live changes in utilization. \n \nWhich scaling strategy should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Implement dynamic scaling with step scaling based on average CPU utilization from the EC2", correct: false },
                { id: 1, text: "Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking", correct: true },
                { id: 2, text: "Create an automated scheduled scaling action based on the traffic patterns of the web", correct: false },
                { id: 3, text: "Set up a simple scaling policy. Increase the cooldown period based on the EC2 instance startup", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A package delivery company has an application that uses Amazon EC2 instances and an \nAmazon Aurora MySQL DB cluster. As the application becomes more popular, EC2 instance \nusage increases only slightly. DB cluster usage increases at a much faster rate. \n \nThe company adds a read replica, which reduces the DB cluster usage for a short period of time. \nHowever, the load continues to increase. The operations that cause the increase in DB cluster \nusage are all repeated read statements that are related to delivery details. The company needs to \nalleviate the effect of repeated reads on the DB cluster. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.", correct: true },
                { id: 1, text: "Add an additional read replica to the DB cluster.", correct: false },
                { id: 2, text: "Configure Aurora Auto Scaling for the Aurora read replicas.", correct: false },
                { id: 3, text: "Modify the DB cluster to have multiple writer instances.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A company has an application that uses an Amazon DynamoDB table for storage. A solutions \narchitect discovers that many requests to the table are not returning the latest data. The \ncompany's users have not reported any other issues with database performance. Latency is in an \nacceptable range. \n \nWhich design change should the solutions architect recommend?",
            options: [
                { id: 0, text: "Add read replicas to the table.", correct: false },
                { id: 1, text: "Use a global secondary index (GSI).", correct: false },
                { id: 2, text: "Request strongly consistent reads for the table.", correct: true },
                { id: 3, text: "Request eventually consistent reads for the table.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 50,
            text: "A company has deployed its application on Amazon EC2 instances with an Amazon RDS \ndatabase. The company used the principle of least privilege to configure the database access \ncredentials. The company's security team wants to protect the application and the database from \nSQL injection and other web-based attacks. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use security groups and network ACLs to secure the database and application servers.", correct: false },
                { id: 1, text: "Use AWS WAF to protect the application. Use RDS parameter groups to configure the security", correct: true },
                { id: 2, text: "Use AWS Network Firewall to protect the application and the database.", correct: false },
                { id: 3, text: "Use different database accounts in the application code for different functions. Avoid granting", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "An ecommerce company runs applications in AWS accounts that are part of an organization in \nAWS Organizations. The applications run on Amazon Aurora PostgreSQL databases across all \nthe accounts. The company needs to prevent malicious activity and must identify abnormal failed \nand incomplete login attempts to the databases. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
            options: [
                { id: 0, text: "Attach service control policies (SCPs) to the root of the organization to identity the failed login", correct: false },
                { id: 1, text: "Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of", correct: true },
                { id: 2, text: "Publish the Aurora general logs to a log group in Amazon CloudWatch Logs. Export the log data", correct: false },
                { id: 3, text: "Publish all the Aurora PostgreSQL database events in AWS CloudTrail to a central Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "A company has an AWS Direct Connect connection from its corporate data center to its VPC in \nthe us-east-1 Region. The company recently acquired a corporation that has several VPCs and a \nDirect Connect connection between its on-premises data center and the eu-west-2 Region. The \nCIDR blocks for the VPCs of the company and the corporation do not overlap. The company \nrequires connectivity between two Regions and the data centers. The company needs a solution \nthat is scalable while reducing operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Set up inter-Region VPC peering between the VPC in us-east-1 and the VPCs in eu-west-2.", correct: false },
                { id: 1, text: "Create private virtual interfaces from the Direct Connect connection in us-east-1 to the VPCs in", correct: false },
                { id: 2, text: "Establish VPN appliances in a fully meshed VPN network hosted by Amazon EC2. Use AWS", correct: false },
                { id: 3, text: "Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A company is developing a mobile game that streams score updates to a backend processor and \nthen posts results on a leaderboard. A solutions architect needs to design a solution that can \nhandle large traffic spikes, process the mobile game updates in order of receipt, and store the \nprocessed updates in a highly available database. The company also wants to minimize the \nmanagement overhead required to maintain the solution. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data", correct: true },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams. Process the updates with a fleet of", correct: false },
                { id: 2, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an", correct: false },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fleet of", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "A company has multiple AWS accounts with applications deployed in the us-west-2 Region. \nApplication logs are stored within Amazon S3 buckets in each account. The company wants to \nbuild a centralized log analysis solution that uses a single S3 bucket. Logs must not leave us-\nwest-2, and the company wants to incur minimal operational overhead. \n \nWhich solution meets these requirements and is MOST cost-effective?",
            options: [
                { id: 0, text: "Create an S3 Lifecycle policy that copies the objects from one of the application S3 buckets to the", correct: false },
                { id: 1, text: "Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in", correct: true },
                { id: 2, text: "Write a script that uses the PutObject API operation every day to copy the entire contents of the", correct: false },
                { id: 3, text: "Write AWS Lambda functions in these accounts that are triggered every time logs are delivered to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 55,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n393 \nA company has an application that delivers on-demand training videos to students around the \nworld. The application also allows authorized content developers to upload videos. The data is \nstored in an Amazon S3 bucket in the us-east-2 Region. \n \nThe company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-\nsoutheast-1 Region. The company wants to replicate the data to the new S3 buckets. The \ncompany needs to minimize latency for developers who upload videos and students who stream \nvideos near eu-west-2 and ap-southeast-1. \n \nWhich combination of steps will meet these requirements with the FEWEST changes to the \napplication? (Choose two.)",
            options: [
                { id: 0, text: "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket.", correct: false },
                { id: 1, text: "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket.", correct: false },
                { id: 2, text: "Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.", correct: true },
                { id: 3, text: "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource", correct: false },
                { id: 4, text: "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A company has a new mobile app. Anywhere in the world, users can see local news on topics \nthey choose. Users also can post photos and videos from inside the app. \n \nUsers access content often in the first minutes after the content is posted. New content quickly \nreplaces older content, and then the older content disappears. The local nature of the news \nmeans that users consume 90% of the content within the AWS Region where it is uploaded. \n \nWhich solution will optimize the user experience by providing the LOWEST latency for content \nuploads?",
            options: [
                { id: 0, text: "Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads.", correct: false },
                { id: 1, text: "Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.", correct: true },
                { id: 2, text: "Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data", correct: false },
                { id: 3, text: "Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A company is building a new application that uses serverless architecture. The architecture will \nconsist of an Amazon API Gateway REST API and AWS Lambda functions to manage incoming \nrequests. \n \nThe company wants to add a service that can send messages received from the API Gateway \nREST API to multiple target Lambda functions for processing. The service must offer message \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n394 \nfiltering that gives the target Lambda functions the ability to receive only the messages the \nfunctions need. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Send the requests from the API Gateway REST API to an Amazon Simple Notification Service", correct: true },
                { id: 1, text: "Send the requests from the API Gateway REST API to Amazon EventBridge. Configure", correct: false },
                { id: 2, text: "Send the requests from the API Gateway REST API to Amazon Managed Streaming for Apache", correct: false },
                { id: 3, text: "Send the requests from the API Gateway REST API to multiple Amazon Simple Queue Service", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A company migrated millions of archival files to Amazon S3. A solutions architect needs to \nimplement a solution that will encrypt all the archival data by using a customer-provided key. The \nsolution must encrypt existing unencrypted objects and future objects. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3", correct: true },
                { id: 1, text: "Use S3 Storage Lens metrics to identify unencrypted S3 buckets. Configure the S3 default", correct: false },
                { id: 2, text: "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure", correct: false },
                { id: 3, text: "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "The DNS provider that hosts a company's domain name records is experiencing outages that \ncause service disruption for a website running on AWS. The company needs to migrate to a more \nresilient managed DNS service and wants the service to run on AWS. \n \nWhat should a solutions architect do to rapidly migrate the DNS hosting service?",
            options: [
                { id: 0, text: "Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file", correct: true },
                { id: 1, text: "Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file", correct: false },
                { id: 2, text: "Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS", correct: false },
                { id: 3, text: "Create an Amazon Route 53 Resolver inbound endpoint in the VPC. Specify the IP addresses", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A company is building an application on AWS that connects to an Amazon RDS database. The \ncompany wants to manage the application configuration and to securely store and retrieve \ncredentials for the database and other services. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets", correct: true },
                { id: 1, text: "Use AWS Lambda to store and manage the application configuration. Use AWS Systems", correct: false },
                { id: 2, text: "Use an encrypted application configuration file. Store the file in Amazon S3 for the application", correct: false },
                { id: 3, text: "Use AWS AppConfig to store and manage the application configuration. Use Amazon RDS to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "To meet security requirements, a company needs to encrypt all of its application data in transit \nwhile communicating with an Amazon RDS MySQL DB instance. A recent security audit revealed \nthat encryption at rest is enabled using AWS Key Management Service (AWS KMS), but data in \ntransit is not enabled. \n \nWhat should a solutions architect do to satisfy the security requirements?",
            options: [
                { id: 0, text: "Enable IAM database authentication on the database.", correct: false },
                { id: 1, text: "Provide self-signed certificates. Use the certificates in all connections to the RDS instance.", correct: false },
                { id: 2, text: "Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption", correct: false },
                { id: 3, text: "Download AWS-provided root certificates. Provide the certificates in all connections to the RDS", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company is designing a new web service that will run on Amazon EC2 instances behind an \nElastic Load Balancing (ELB) load balancer. However, many of the web service clients can only \nreach IP addresses authorized on their firewalls. \n \nWhat should a solutions architect recommend to meet the clients' needs? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n396",
            options: [
                { id: 0, text: "A Network Load Balancer with an associated Elastic IP address.", correct: false },
                { id: 1, text: "An Application Load Balancer with an associated Elastic IP address.", correct: false },
                { id: 2, text: "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.", correct: true },
                { id: 3, text: "An EC2 instance with a public IP address running as a proxy in front of the load balancer.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company has established a new AWS account. The account is newly provisioned and no \nchanges have been made to the default settings. The company is concerned about the security of \nthe AWS account root user. \n \nWhat should be done to secure the root user?",
            options: [
                { id: 0, text: "Create IAM users for daily administrative tasks. Disable the root user.", correct: false },
                { id: 1, text: "Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root", correct: true },
                { id: 2, text: "Generate an access key for the root user. Use the access key for daily administration tasks", correct: false },
                { id: 3, text: "Provide the root user credentials to the most senior solutions architect. Have the solutions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company is deploying an application that processes streaming data in near-real time. The \ncompany plans to use Amazon EC2 instances for the workload. The network architecture must be \nconfigurable to provide the lowest possible latency between nodes. \n \nWhich combination of network solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Enable and configure enhanced networking on each EC2 instance.", correct: true },
                { id: 1, text: "Group the EC2 instances in separate accounts.", correct: false },
                { id: 2, text: "Run the EC2 instances in a cluster placement group.", correct: false },
                { id: 3, text: "Attach multiple elastic network interfaces to each EC2 instance.", correct: false },
                { id: 4, text: "Use Amazon Elastic Block Store (Amazon EBS) optimized instance types.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test22: [
        {
            id: 0,
            text: "A financial services company wants to shut down two data centers and migrate more than 100 TB \nof data to AWS. The data has an intricate directory structure with millions of small files stored in \ndeep hierarchies of subfolders. Most of the data is unstructured, and the company's file storage \nconsists of SMB-based storage types from multiple vendors. The company does not want to \nchange its applications to access the data after migration. \n \nWhat should a solutions architect do to meet these requirements with the LEAST operational \noverhead?",
            options: [
                { id: 0, text: "Use AWS Direct Connect to migrate the data to Amazon S3.", correct: false },
                { id: 1, text: "Use AWS DataSync to migrate the data to Amazon FSx for Lustre.", correct: false },
                { id: 2, text: "Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.", correct: true },
                { id: 3, text: "Use AWS Direct Connect to migrate the data on-premises file storage to an AWS Storage", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company uses an organization in AWS Organizations to manage AWS accounts that contain \napplications. The company sets up a dedicated monitoring member account in the organization. \nThe company wants to query and visualize observability data across the accounts by using \nAmazon CloudWatch. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS", correct: true },
                { id: 1, text: "Set up service control policies (SCPs) to provide access to CloudWatch in the monitoring account", correct: false },
                { id: 2, text: "Configure a new IAM user in the monitoring account. In each AWS account, configure an IAM", correct: false },
                { id: 3, text: "Create a new IAM user in the monitoring account. Create cross-account IAM policies in each", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company's website is used to sell products to the public. The site runs on Amazon EC2 \ninstances in an Auto Scaling group behind an Application Load Balancer (ALB). There is also an \nAmazon CloudFront distribution, and AWS WAF is being used to protect against SQL injection \nattacks. The ALB is the origin for the CloudFront distribution. A recent review of security logs \nrevealed an external malicious IP that needs to be blocked from accessing the website. \n \nWhat should a solutions architect do to protect the application?",
            options: [
                { id: 0, text: "Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP", correct: false },
                { id: 1, text: "Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP", correct: true },
                { id: 2, text: "Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the", correct: false },
                { id: 3, text: "Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A \nsolutions architect must design a solution to provide access to the accounts for several thousand \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n398 \nemployees. The company has an existing identity provider (IdP). The company wants to use the \nexisting IdP for authentication to AWS. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create IAM users for the employees in the required AWS accounts. Connect IAM users to the", correct: false },
                { id: 1, text: "Set up AWS account root users with user email addresses and passwords that are synchronized", correct: false },
                { id: 2, text: "Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the", correct: true },
                { id: 3, text: "Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A solutions architect is designing an AWS Identity and Access Management (IAM) authorization \nmodel for a company's AWS account. The company has designated five specific employees to \nhave full access to AWS services and resources in the AWS account. \n \nThe solutions architect has created an IAM user for each of the five designated employees and \nhas created an IAM user group. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Attach the AdministratorAccess resource-based policy to the IAM user group. Place each of the", correct: false },
                { id: 1, text: "Attach the SystemAdministrator identity-based policy to the IAM user group. Place each of the", correct: false },
                { id: 2, text: "Attach the AdministratorAccess identity-based policy to the IAM user group. Place each of the five", correct: true },
                { id: 3, text: "Attach the SystemAdministrator resource-based policy to the IAM user group. Place each of the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company has a multi-tier payment processing application that is based on virtual machines \n(VMs). The communication between the tiers occurs asynchronously through a third-party \nmiddleware solution that guarantees exactly-once delivery. \n \nThe company needs a solution that requires the least amount of infrastructure management. The \nsolution must guarantee exactly-once delivery for application messaging. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Lambda for the compute layers in the architecture.", correct: true },
                { id: 1, text: "Use Amazon EC2 instances for the compute layers in the architecture.", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) as the messaging component between", correct: false },
                { id: 3, text: "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the messaging component", correct: false },
                { id: 4, text: "Use containers that are based on Amazon Elastic Kubernetes Service (Amazon EKS) for the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A company has a nightly batch processing routine that analyzes report files that an on-premises \nfile system receives daily through SFTP. The company wants to move the solution to the AWS \nCloud. The solution must be highly available and resilient. The solution also must minimize \noperational effort. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Deploy AWS Transfer for SFTP and an Amazon Elastic File System (Amazon EFS) file system for", correct: false },
                { id: 1, text: "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic", correct: false },
                { id: 2, text: "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic", correct: false },
                { id: 3, text: "Deploy AWS Transfer for SFTP and an Amazon S3 bucket for storage. Modify the application to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "A company has users all around the world accessing its HTTP-based application deployed on \nAmazon EC2 instances in multiple AWS Regions. The company wants to improve the availability \nand performance of the application. The company also wants to protect the application against \ncommon web exploits that may affect availability, compromise security, or consume excessive \nresources. Static IP addresses are required. \n \nWhat should a solutions architect recommend to accomplish this?",
            options: [
                { id: 0, text: "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS", correct: false },
                { id: 1, text: "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Deploy AWS", correct: true },
                { id: 2, text: "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS", correct: false },
                { id: 3, text: "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company's data platform uses an Amazon Aurora MySQL database. The database has multiple \nread replicas and multiple DB instances across different Availability Zones. Users have recently \nreported errors from the database that indicate that there are too many connections. The \ncompany wants to reduce the failover time by 20% when a read replica is promoted to primary \nwriter. \n \nWhich solution will meet this requirement?",
            options: [
                { id: 0, text: "Switch from Aurora to Amazon RDS with Multi-AZ cluster deployment.", correct: false },
                { id: 1, text: "Use Amazon RDS Proxy in front of the Aurora database.", correct: true },
                { id: 2, text: "Switch to Amazon DynamoDB with DynamoDB Accelerator (DAX) for read connections.", correct: false },
                { id: 3, text: "Switch to Amazon Redshift with relocation capability.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A company stores text files in Amazon S3. The text files include customer chat messages, date \nand time information, and customer personally identifiable information (PII). \n \nThe company needs a solution to provide samples of the conversations to an external service \nprovider for quality control. The external service provider needs to randomly pick sample \nconversations up to the most recent conversation. The company must not share the customer PII \nwith the external service provider. The solution must scale when the number of customer \nconversations increases. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Object Lambda Access Point. Create an AWS Lambda function that redacts the PII", correct: true },
                { id: 1, text: "Create a batch process on an Amazon EC2 instance that regularly reads all new files, redacts the", correct: false },
                { id: 2, text: "Create a web application on an Amazon EC2 instance that presents a list of the files, redacts the", correct: false },
                { id: 3, text: "Create an Amazon DynamoDB table. Create an AWS Lambda function that reads only the data in", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A company is running a legacy system on an Amazon EC2 instance. The application code cannot \nbe modified, and the system cannot run on more than one instance. A solutions architect must \ndesign a resilient solution that can improve the recovery time for the system. \n \nWhat should the solutions architect recommend to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n401",
            options: [
                { id: 0, text: "Enable termination protection for the EC2 instance.", correct: false },
                { id: 1, text: "Configure the EC2 instance for Multi-AZ deployment.", correct: false },
                { id: 2, text: "Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure.", correct: true },
                { id: 3, text: "Launch the EC2 instance with two Amazon Elastic Block Store (Amazon EBS) volumes that use", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A company wants to deploy its containerized application workloads to a VPC across three \nAvailability Zones. The company needs a solution that is highly available across Availability \nZones. The solution must require minimal changes to the application. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS Service Auto", correct: true },
                { id: 1, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) self-managed nodes. Configure", correct: false },
                { id: 2, text: "Use Amazon EC2 Reserved Instances. Launch three EC2 instances in a spread placement", correct: false },
                { id: 3, text: "Use an AWS Lambda function. Configure the Lambda function to connect to a VPC. Configure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A media company stores movies in Amazon S3. Each movie is stored in a single video file that \nranges from 1 GB to 10 GB in size. \n \nThe company must be able to provide the streaming content of a movie within 5 minutes of a user \npurchase. There is higher demand for movies that are less than 20 years old than for movies that \nare more than 20 years old. The company wants to minimize hosting service costs based on \ndemand. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Store all media content in Amazon S3. Use S3 Lifecycle policies to move media data into the", correct: false },
                { id: 1, text: "Store newer movie video files in S3 Standard. Store older movie video files in S3 Standard-", correct: false },
                { id: 2, text: "Store newer movie video files in S3 Intelligent-Tiering. Store older movie video files in S3 Glacier", correct: true },
                { id: 3, text: "Store newer movie video files in S3 Standard. Store older movie video files in S3 Glacier Flexible", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 13,
            text: "A solutions architect needs to design the architecture for an application that a vendor provides as \na Docker container image. The container needs 50 GB of storage available for temporary files. \nThe infrastructure must be serverless. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS Lambda function that uses the Docker container image with an Amazon S3", correct: false },
                { id: 1, text: "Create an AWS Lambda function that uses the Docker container image with an Amazon Elastic", correct: false },
                { id: 2, text: "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the AWS Fargate", correct: true },
                { id: 3, text: "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the Amazon EC2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company needs to use its on-premises LDAP directory service to authenticate its users to the \nAWS Management Console. The directory service is not compatible with Security Assertion \nMarkup Language (SAML). \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Enable AWS IAM Identity Center (AWS Single Sign-On) between AWS and the on-premises", correct: false },
                { id: 1, text: "Create an IAM policy that uses AWS credentials, and integrate the policy into LDAP.", correct: false },
                { id: 2, text: "Set up a process that rotates the IAM credentials whenever LDAP credentials are updated.", correct: false },
                { id: 3, text: "Develop an on-premises custom identity broker application or process that uses AWS Security", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company stores multiple Amazon Machine Images (AMIs) in an AWS account to launch its \nAmazon EC2 instances. The AMIs contain critical data and configurations that are necessary for \nthe company's operations. The company wants to implement a solution that will recover \naccidentally deleted AMIs quickly and efficiently. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create Amazon Elastic Block Store (Amazon EBS) snapshots of the AMIs. Store the snapshots in", correct: false },
                { id: 1, text: "Copy all AMIs to another AWS account periodically.", correct: false },
                { id: 2, text: "Create a retention rule in Recycle Bin.", correct: true },
                { id: 3, text: "Upload the AMIs to an Amazon S3 bucket that has Cross-Region Replication.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2022/02/amazon-ec2-recycle-bin-machine- images/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A company has 150 TB of archived image data stored on-premises that needs to be moved to the \nAWS Cloud within the next month. The company's current network connection allows up to 100 \nMbps uploads for this purpose during the night only. \n \nWhat is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
            options: [
                { id: 0, text: "Use AWS Snowmobile to ship the data to AWS.", correct: false },
                { id: 1, text: "Order multiple AWS Snowball devices to ship the data to AWS.", correct: true },
                { id: 2, text: "Enable Amazon S3 Transfer Acceleration and securely upload the data.", correct: false },
                { id: 3, text: "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 17,
            text: "A company wants to migrate its three-tier application from on premises to AWS. The web tier and \nthe application tier are running on third-party virtual machines (VMs). The database tier is running \non MySQL. \n \nThe company needs to migrate the application by making the fewest possible changes to the \narchitecture. The company also needs a database solution that can restore data to a specific \npoint in time. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Migrate the web tier and the application tier to Amazon EC2 instances in private subnets. Migrate", correct: false },
                { id: 1, text: "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to", correct: true },
                { id: 2, text: "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to", correct: false },
                { id: 3, text: "Migrate the web tier and the application tier to Amazon EC2 instances in public subnets. Migrate", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A development team is collaborating with another company to create an integrated product. The \nother company needs to access an Amazon Simple Queue Service (Amazon SQS) queue that is \ncontained in the development team's account. The other company wants to poll the queue \nwithout giving up its own account permissions to do so. \n \nHow should a solutions architect provide access to the SQS queue? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n404",
            options: [
                { id: 0, text: "Create an instance profile that provides the other company access to the SQS queue.", correct: false },
                { id: 1, text: "Create an IAM policy that provides the other company access to the SQS queue.", correct: false },
                { id: 2, text: "Create an SQS access policy that provides the other company access to the SQS queue.", correct: true },
                { id: 3, text: "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company's developers want a secure way to gain SSH access on the company's Amazon EC2 \ninstances that run the latest version of Amazon Linux. The developers work remotely and in the \ncorporate office. \n \nThe company wants to use AWS services as a part of the solution. The EC2 instances are hosted \nin a VPC private subnet and access the internet through a NAT gateway that is deployed in a \npublic subnet. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a bastion host in the same subnet as the EC2 instances. Grant the", correct: false },
                { id: 1, text: "Create an AWS Site-to-Site VPN connection between the corporate network and the VPC.", correct: false },
                { id: 2, text: "Create a bastion host in the public subnet of the VPConfigure the security groups and SSH keys", correct: false },
                { id: 3, text: "Attach the AmazonSSMManagedInstanceCore IAM policy to an IAM role that is associated with", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A pharmaceutical company is developing a new drug. The volume of data that the company \ngenerates has grown exponentially over the past few months. The company's researchers \nregularly require a subset of the entire dataset to be immediately available with minimal lag. \nHowever, the entire dataset does not need to be accessed on a daily basis. All the data currently \nresides in on-premises storage arrays, and the company wants to reduce ongoing capital \nexpenses. \n \nWhich storage solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Run AWS DataSync as a scheduled cron job to migrate the data to an Amazon S3 bucket on an", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage.", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3", correct: true },
                { id: 3, text: "Configure an AWS Site-to-Site VPN connection from the on-premises environment to AWS.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A company has a business-critical application that runs on Amazon EC2 instances. The \napplication stores data in an Amazon DynamoDB table. The company must be able to revert the \ntable to any point within the last 24 hours. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure point-in-time recovery for the table.", correct: true },
                { id: 1, text: "Use AWS Backup for the table.", correct: false },
                { id: 2, text: "Use an AWS Lambda function to make an on-demand backup of the table every hour.", correct: false },
                { id: 3, text: "Turn on streams on the table to capture a log of all changes to the table in the last 24 hours.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, \nthe files are processed to extract metadata, which takes less than 5 seconds. The volume and \nfrequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. \nThe company has asked a solutions architect to design a cost-effective architecture that will meet \nthese requirements. \n \nWhat should the solutions architect recommend?",
            options: [
                { id: 0, text: "Configure AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the files.", correct: false },
                { id: 1, text: "Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda", correct: false },
                { id: 2, text: "Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an", correct: false },
                { id: 3, text: "Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A company's application is deployed on Amazon EC2 instances and uses AWS Lambda \nfunctions for an event-driven architecture. The company uses nonproduction development \nenvironments in a different AWS account to test new features before the company deploys the \nfeatures to production. \n \nThe production instances show constant usage because of customers in different time zones. \nThe company uses nonproduction instances only during business hours on weekdays. The \ncompany does not use the nonproduction instances on the weekends. The company wants to \noptimize the costs to run its application on AWS. \n \nWhich solution will meet these requirements MOST cost-effectively? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n406",
            options: [
                { id: 0, text: "Use On-Demand Instances for the production instances. Use Dedicated Hosts for the", correct: false },
                { id: 1, text: "Use Reserved Instances for the production instances and the nonproduction instances. Shut", correct: false },
                { id: 2, text: "Use Compute Savings Plans for the production instances. Use On-Demand Instances for the", correct: true },
                { id: 3, text: "Use Dedicated Hosts for the production instances. Use EC2 Instance Savings Plans for the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A company stores data in an on-premises Oracle relational database. The company needs to \nmake the data available in Amazon Aurora PostgreSQL for analysis. The company uses an AWS \nSite-to-Site VPN connection to connect its on-premises network to AWS. \n \nThe company must capture the changes that occur to the source database during the migration to \nAurora PostgreSQL. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora", correct: false },
                { id: 1, text: "Use AWS DataSync to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora", correct: false },
                { id: 2, text: "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora", correct: true },
                { id: 3, text: "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Import the S3 data to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A company built an application with Docker containers and needs to run the application in the \nAWS Cloud. The company wants to use a managed service to host the application. \n \nThe solution must scale in and out appropriately according to demand on the individual container \nservices. The solution also must not result in additional operational overhead or infrastructure to \nmanage. \n \nWhich solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.", correct: true },
                { id: 1, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.", correct: false },
                { id: 2, text: "Provision an Amazon API Gateway API. Connect the API to AWS Lambda to run the containers.", correct: false },
                { id: 3, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes.", correct: false },
                { id: 4, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "An ecommerce company is running a seasonal online sale. The company hosts its website on \nAmazon EC2 instances spanning multiple Availability Zones. The company wants its website to \nmanage sudden traffic increases during the sale. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create an Auto Scaling group that is large enough to handle peak traffic load. Stop half of the", correct: false },
                { id: 1, text: "Create an Auto Scaling group for the website. Set the minimum size of the Auto Scaling group so", correct: false },
                { id: 2, text: "Use Amazon CloudFront and Amazon ElastiCache to cache dynamic content with an Auto", correct: false },
                { id: 3, text: "Configure an Auto Scaling group to scale out as traffic increases. Create a launch template to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 27,
            text: "A solutions architect must provide an automated solution for a company's compliance policy that \nstates security groups cannot include a rule that allows SSH from 0.0.0.0/0. The company needs \nto be notified if there is any breach in the policy. A solution is needed as soon as possible. \n \nWhat should the solutions architect do to meet these requirements with the LEAST operational \noverhead?",
            options: [
                { id: 0, text: "Write an AWS Lambda script that monitors security groups for SSH being open to 0.0.0.0/0", correct: false },
                { id: 1, text: "Enable the restricted-ssh AWS Config managed rule and generate an Amazon Simple Notification", correct: true },
                { id: 2, text: "Create an IAM role with permissions to globally open security groups and network ACLs. Create", correct: false },
                { id: 3, text: "Configure a service control policy (SCP) that prevents non-administrative users from creating or", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes. \n \nA company has deployed an application in an AWS account. The application consists of \nmicroservices that run on AWS Lambda and Amazon Elastic Kubernetes Service (Amazon EKS). \nA separate team supports each microservice. The company has multiple AWS accounts and \nwants to give each team its own account for its microservices. \n \nA solutions architect needs to design a solution that will provide service-to-service communication \nover HTTPS (port 443). The solution also must provide a service registry for service discovery. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n408 \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Create an inspection VPC. Deploy an AWS Network Firewall firewall to the inspection VPC.", correct: false },
                { id: 1, text: "Create a VPC Lattice service network. Associate the microservices with the service network.", correct: true },
                { id: 2, text: "Create a Network Load Balancer (NLB) with an HTTPS listener and target groups for each", correct: false },
                { id: 3, text: "Create peering connections between VPCs that contain microservices. Create a prefix list for", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A company has a mobile game that reads most of its metadata from an Amazon RDS DB \ninstance. As the game increased in popularity, developers noticed slowdowns related to the \ngame's metadata load times. Performance metrics indicate that simply scaling the database will \nnot help. A solutions architect must explore all options that include capabilities for snapshots, \nreplication, and sub-millisecond response times. \n \nWhat should the solutions architect recommend to solve these issues?",
            options: [
                { id: 0, text: "Migrate the database to Amazon Aurora with Aurora Replicas.", correct: false },
                { id: 1, text: "Migrate the database to Amazon DynamoDB with global tables.", correct: false },
                { id: 2, text: "Add an Amazon ElastiCache for Redis layer in front of the database.", correct: true },
                { id: 3, text: "Add an Amazon ElastiCache for Memcached layer in front of the database.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "A company uses AWS Organizations for its multi-account AWS setup. The security organizational \nunit (OU) of the company needs to share approved Amazon Machine Images (AMIs) with the \ndevelopment OU. The AMIs are created by using AWS Key Management Service (AWS KMS) \nencrypted snapshots. \n \nWhich solution will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Add the development team's OU Amazon Resource Name (ARN) to the launch permission list for", correct: true },
                { id: 1, text: "Add the Organizations root Amazon Resource Name (ARN) to the launch permission list for the", correct: false },
                { id: 2, text: "Update the key policy to allow the development team's OU to use the AWS KMS keys that are", correct: false },
                { id: 3, text: "Add the development team's account Amazon Resource Name (ARN) to the launch permission", correct: false },
                { id: 4, text: "Recreate the AWS KMS key. Add a key policy to allow the Organizations root Amazon Resource", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A data analytics company has 80 offices that are distributed globally. Each office hosts 1 PB of \ndata and has between 1 and 2 Gbps of internet bandwidth. \n \nThe company needs to perform a one-time migration of a large amount of data from its offices to \nAmazon S3. The company must complete the migration within 4 weeks. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Establish a new 10 Gbps AWS Direct Connect connection to each office. Transfer the data to", correct: false },
                { id: 1, text: "Use multiple AWS Snowball Edge storage-optimized devices to store and transfer the data to", correct: true },
                { id: 2, text: "Use an AWS Snowmobile to store and transfer the data to Amazon S3.", correct: false },
                { id: 3, text: "Set up an AWS Storage Gateway Volume Gateway to transfer the data to Amazon S3.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "A company has an Amazon Elastic File System (Amazon EFS) file system that contains a \nreference dataset. The company has applications on Amazon EC2 instances that need to read \nthe dataset. However, the applications must not be able to change the dataset. The company \nwants to use IAM access control to prevent the applications from being able to modify or delete \nthe dataset. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Mount the EFS file system in read-only mode from within the EC2 instances.", correct: false },
                { id: 1, text: "Create a resource policy for the EFS file system that denies the elasticfilesystem:ClientWrite", correct: true },
                { id: 2, text: "Create an identity policy for the EFS file system that denies the elasticfilesystem:ClientWrite", correct: false },
                { id: 3, text: "Create an EFS access point for each application. Use Portable Operating System Interface", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 33,
            text: "A company has hired an external vendor to perform work in the company's AWS account. The \nvendor uses an automated tool that is hosted in an AWS account that the vendor owns. The \nvendor does not have IAM access to the company's AWS account. The company needs to grant \nthe vendor access to the company's AWS account. \n \nWhich solution will meet these requirements MOST securely?",
            options: [
                { id: 0, text: "Create an IAM role in the company's account to delegate access to the vendor's IAM role. Attach", correct: true },
                { id: 1, text: "Create an IAM user in the company's account with a password that meets the password", correct: false },
                { id: 2, text: "Create an IAM group in the company's account. Add the automated tool's IAM user from the", correct: false },
                { id: 3, text: "Create an IAM user in the company's account that has a permission boundary that allows the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A company wants to run its experimental workloads in the AWS Cloud. The company has a \nbudget for cloud spending. The company's CFO is concerned about cloud spending \naccountability for each department. The CFO wants to receive notification when the spending \nthreshold reaches 60% of the budget. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use cost allocation tags on AWS resources to label owners. Create usage budgets in AWS", correct: true },
                { id: 1, text: "Use AWS Cost Explorer forecasts to determine resource owners. Use AWS Cost Anomaly", correct: false },
                { id: 2, text: "Use cost allocation tags on AWS resources to label owners. Use AWS Support API on AWS", correct: false },
                { id: 3, text: "Use AWS Cost Explorer forecasts to determine resource owners. Create usage budgets in AWS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A company wants to deploy an internal web application on AWS. The web application must be \naccessible only from the company's office. The company needs to download security patches for \nthe web application from the internet. \n \nThe company has created a VPC and has configured an AWS Site-to-Site VPN connection to the \ncompany's office. A solutions architect must design a secure architecture for the web application. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the web application on Amazon EC2 instances in public subnets behind a public", correct: false },
                { id: 1, text: "Deploy the web application on Amazon EC2 instances in private subnets behind an internal", correct: true },
                { id: 2, text: "Deploy the web application on Amazon EC2 instances in public subnets behind an internal", correct: false },
                { id: 3, text: "Deploy the web application on Amazon EC2 instances in private subnets behind a public", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company maintains its accounting records in a custom application that runs on Amazon EC2 \ninstances. The company needs to migrate the data to an AWS managed service for development \nand maintenance of the application data. The solution must require minimal operational support \nand provide immutable, cryptographically verifiable logs of data changes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Copy the records from the application into an Amazon Redshift cluster.", correct: false },
                { id: 1, text: "Copy the records from the application into an Amazon Neptune cluster.", correct: false },
                { id: 2, text: "Copy the records from the application into an Amazon Timestream database.", correct: false },
                { id: 3, text: "Copy the records from the application into an Amazon Quantum Ledger Database (Amazon", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 37,
            text: "A company's marketing data is uploaded from multiple sources to an Amazon S3 bucket. A series \nof data preparation jobs aggregate the data for reporting. The data preparation jobs need to run \nat regular intervals in parallel. A few jobs need to run in a specific order later. \n \nThe company wants to remove the operational overhead of job error handling, retry logic, and \nstate management. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an AWS Lambda function to process the data as soon as the data is uploaded to the S3", correct: false },
                { id: 1, text: "Use Amazon Athena to process the data. Use Amazon EventBridge Scheduler to invoke Athena", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to process the data. Use an AWS Step Functions state machine to run", correct: true },
                { id: 3, text: "Use AWS Data Pipeline to process the data. Schedule Data Pipeline to process the data once at", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A solutions architect is designing a payment processing application that runs on AWS Lambda in \nprivate subnets across multiple Availability Zones. The application uses multiple Lambda \nfunctions and processes millions of transactions each day. \n \nThe architecture must ensure that the application does not process duplicate payments. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n412 \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon S3 bucket.", correct: false },
                { id: 1, text: "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple", correct: false },
                { id: 2, text: "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple", correct: false },
                { id: 3, text: "Use Lambda to retrieve all due payments. Store the due payments in an Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company runs multiple workloads in its on-premises data center. The company's data center \ncannot scale fast enough to meet the company's expanding business needs. The company wants \nto collect usage and configuration data about the on-premises servers and workloads to plan a \nmigration to AWS. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set the home AWS Region in AWS Migration Hub. Use AWS Systems Manager to collect data", correct: false },
                { id: 1, text: "Set the home AWS Region in AWS Migration Hub. Use AWS Application Discovery Service to", correct: true },
                { id: 2, text: "Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS", correct: false },
                { id: 3, text: "Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A company has an organization in AWS Organizations that has all features enabled. The \ncompany requires that all API calls and logins in any existing or new AWS account must be \naudited. The company needs a managed solution to prevent additional work and to minimize \ncosts. The company also needs to know when any AWS account is not compliant with the AWS \nFoundational Security Best Practices (FSBP) standard. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Deploy an AWS Control Tower environment in the Organizations management account. Enable", correct: true },
                { id: 1, text: "Deploy an AWS Control Tower environment in a dedicated Organizations member account.", correct: false },
                { id: 2, text: "Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ).", correct: false },
                { id: 3, text: "Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ).", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company has stored 10 TB of log files in Apache Parquet format in an Amazon S3 bucket. The \ncompany occasionally needs to use SQL to analyze the log files. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create an Amazon Aurora MySQL database. Migrate the data from the S3 bucket into Aurora by", correct: false },
                { id: 1, text: "Create an Amazon Redshift cluster. Use Redshift Spectrum to run SQL statements directly on the", correct: false },
                { id: 2, text: "Create an AWS Glue crawler to store and retrieve table metadata from the S3 bucket. Use", correct: true },
                { id: 3, text: "Create an Amazon EMR cluster. Use Apache Spark SQL to run SQL statements directly on the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 42,
            text: "A company needs a solution to prevent AWS CloudFormation stacks from deploying AWS \nIdentity and Access Management (IAM) resources that include an inline policy or \"*\" in the \nstatement. The solution must also prohibit deployment of Amazon EC2 instances with public IP \naddresses. The company has AWS Control Tower enabled in its organization in AWS \nOrganizations. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Control Tower proactive controls to block deployment of EC2 instances with public IP", correct: false },
                { id: 1, text: "Use AWS Control Tower detective controls to block deployment of EC2 instances with public IP", correct: false },
                { id: 2, text: "Use AWS Config to create rules for EC2 and IAM compliance. Configure the rules to run an AWS", correct: false },
                { id: 3, text: "Use a service control policy (SCP) to block actions for the EC2 instances and IAM resources if", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 43,
            text: "A company's web application that is hosted in the AWS Cloud recently increased in popularity. \nThe web application currently exists on a single Amazon EC2 instance in a single public subnet. \nThe web application has not been able to meet the demand of the increased web traffic. \n \nThe company needs a solution that will provide high availability and scalability to meet the \nincreased user demand without rewriting the web application. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n414 \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Replace the EC2 instance with a larger compute optimized instance.", correct: false },
                { id: 1, text: "Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private subnets.", correct: true },
                { id: 2, text: "Configure a NAT gateway in a public subnet to handle web requests.", correct: false },
                { id: 3, text: "Replace the EC2 instance with a larger memory optimized instance.", correct: false },
                { id: 4, text: "Configure an Application Load Balancer in a public subnet to distribute web traffic.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A company has AWS Lambda functions that use environment variables. The company does not \nwant its developers to see environment variables in plaintext. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy code to Amazon EC2 instances instead of using Lambda functions.", correct: false },
                { id: 1, text: "Configure SSL encryption on the Lambda functions to use AWS CloudHSM to store and encrypt", correct: false },
                { id: 2, text: "Create a certificate in AWS Certificate Manager (ACM). Configure the Lambda functions to use", correct: false },
                { id: 3, text: "Create an AWS Key Management Service (AWS KMS) key. Enable encryption helpers on the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 45,
            text: "An analytics company uses Amazon VPC to run its multi-tier services. The company wants to use \nRESTful APIs to offer a web analytics service to millions of users. Users must be verified by using \nan authentication service to access the APIs. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Configure an Amazon Cognito user pool for user authentication. Implement Amazon API Gateway", correct: true },
                { id: 1, text: "Configure an Amazon Cognito identity pool for user authentication. Implement Amazon API", correct: false },
                { id: 2, text: "Configure an AWS Lambda function to handle user authentication. Implement Amazon API", correct: false },
                { id: 3, text: "Configure an IAM user to handle user authentication. Implement Amazon API Gateway HTTP", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company has a mobile app for customers. The app's data is sensitive and must be encrypted at \nrest. The company uses AWS Key Management Service (AWS KMS). \n \nThe company needs a solution that prevents the accidental deletion of KMS keys. The solution \nmust use Amazon Simple Notification Service (Amazon SNS) to send an email notification to \nadministrators when a user attempts to delete a KMS key. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n415 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon EventBridge rule that reacts when a user tries to delete a KMS key. Configure", correct: false },
                { id: 1, text: "Create an AWS Lambda function that has custom logic to prevent KMS key deletion. Create an", correct: false },
                { id: 2, text: "Create an Amazon EventBridge rule that reacts when the KMS DeleteKey operation is performed.", correct: true },
                { id: 3, text: "Create an AWS CloudTrail trail. Configure the trail to deliver logs to a new Amazon CloudWatch", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A company wants to analyze and generate reports to track the usage of its mobile app. The app \nis popular and has a global user base. The company uses a custom report building program to \nanalyze application usage. \n \nThe program generates multiple reports during the last week of each month. The program takes \nless than 10 minutes to produce each report. The company rarely uses the program to generate \nreports outside of the last week of each month The company wants to generate reports in the \nleast amount of time when the reports are requested. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Run the program by using Amazon EC2 On-Demand Instances. Create an Amazon EventBridge", correct: false },
                { id: 1, text: "Run the program in AWS Lambda. Create an Amazon EventBridge rule to run a Lambda function", correct: true },
                { id: 2, text: "Run the program in Amazon Elastic Container Service (Amazon ECS). Schedule Amazon ECS to", correct: false },
                { id: 3, text: "Run the program by using Amazon EC2 Spot Instances. Create an Amazon EventBndge rule to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 48,
            text: "A company is designing a tightly coupled high performance computing (HPC) environment in the \nAWS Cloud. The company needs to include features that will optimize the HPC environment for \nnetworking and storage. \n \nWhich combination of solutions will meet these requirements? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n416",
            options: [
                { id: 0, text: "Create an accelerator in AWS Global Accelerator. Configure custom routing for the accelerator.", correct: false },
                { id: 1, text: "Create an Amazon FSx for Lustre file system. Configure the file system with scratch storage.", correct: true },
                { id: 2, text: "Create an Amazon CloudFront distribution. Configure the viewer protocol policy to be HTTP and", correct: false },
                { id: 3, text: "Launch Amazon EC2 instances. Attach an Elastic Fabric Adapter (EFA) to the instances.", correct: false },
                { id: 4, text: "Create an AWS Elastic Beanstalk deployment to manage the environment.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company needs a solution to prevent photos with unwanted content from being uploaded to the \ncompany's web application. The solution must not involve training a machine learning (ML) \nmodel. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create and deploy a model by using Amazon SageMaker Autopilot. Create a real-time endpoint", correct: false },
                { id: 1, text: "Create an AWS Lambda function that uses Amazon Rekognition to detect unwanted content.", correct: true },
                { id: 2, text: "Create an Amazon CloudFront function that uses Amazon Comprehend to detect unwanted", correct: false },
                { id: 3, text: "Create an AWS Lambda function that uses Amazon Rekognition Video to detect unwanted", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A company uses AWS to run its ecommerce platform. The platform is critical to the company's \noperations and has a high volume of traffic and transactions. The company configures a multi-\nfactor authentication (MFA) device to secure its AWS account root user credentials. The company \nwants to ensure that it will not lose access to the root user account if the MFA device is lost. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up a backup administrator account that the company can use to log in if the company loses", correct: false },
                { id: 1, text: "Add multiple MFA devices for the root user account to handle the disaster scenario.", correct: true },
                { id: 2, text: "Create a new administrator account when the company cannot access the root account.", correct: false },
                { id: 3, text: "Attach the administrator policy to another IAM user when the company cannot access the root", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A social media company is creating a rewards program website for its users. The company gives \nusers points when users create and upload videos to the website. Users redeem their points for \ngifts or discounts from the company's affiliated partners. A unique ID identifies users. The \npartners refer to this ID to verify user eligibility for rewards. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n417 \n \nThe partners want to receive notification of user IDs through an HTTP endpoint when the \ncompany gives users points. Hundreds of vendors are interested in becoming affiliated partners \nevery day. The company wants to design an architecture that gives the website the ability to add \npartners rapidly in a scalable way. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
            options: [
                { id: 0, text: "Create an Amazon Timestream database to keep a list of affiliated partners. Implement an AWS", correct: false },
                { id: 1, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an endpoint", correct: true },
                { id: 2, text: "Create an AWS Step Functions state machine. Create a task for every affiliated partner. Invoke", correct: false },
                { id: 3, text: "Create a data stream in Amazon Kinesis Data Streams. Implement producer and consumer", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A company needs to extract the names of ingredients from recipe records that are stored as text \nfiles in an Amazon S3 bucket. A web application will use the ingredient names to query an \nAmazon DynamoDB table and determine a nutrition score. \n \nThe application can handle non-food records and errors. The company does not have any \nemployees who have machine learning knowledge to develop this solution. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur.", correct: true },
                { id: 1, text: "Use an Amazon EventBridge rule to invoke an AWS Lambda function when PutObject requests", correct: false },
                { id: 2, text: "Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur.", correct: false },
                { id: 3, text: "Use an Amazon EventBridge rule to invoke an AWS Lambda function when a PutObject request", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "A company needs to create an AWS Lambda function that will run in a VPC in the company's \nprimary AWS account. The Lambda function needs to access files that the company stores in an \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n418 \nAmazon Elastic File System (Amazon EFS) file system. The EFS file system is located in a \nsecondary AWS account. As the company adds files to the file system, the solution must scale to \nmeet the demand. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a new EFS file system in the primary account. Use AWS DataSync to copy the contents of", correct: false },
                { id: 1, text: "Create a VPC peering connection between the VPCs that are in the primary account and the", correct: true },
                { id: 2, text: "Create a second Lambda function in the secondary account that has a mount that is configured", correct: false },
                { id: 3, text: "Move the contents of the file system to a Lambda layer. Configure the Lambda layer's", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-different-vpc.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "A financial company needs to handle highly sensitive data. The company will store the data in an \nAmazon S3 bucket. The company needs to ensure that the data is encrypted in transit and at \nrest. The company must manage the encryption keys outside the AWS Cloud. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key", correct: false },
                { id: 1, text: "Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key", correct: false },
                { id: 2, text: "Encrypt the data in the S3 bucket with the default server-side encryption (SSE).", correct: false },
                { id: 3, text: "Encrypt the data at the company's data center before storing the data in the S3 bucket.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A company wants to run its payment application on AWS. The application receives payment \nnotifications from mobile devices. Payment notifications require a basic validation before they are \nsent for further processing. \n \nThe backend processing application is long running and requires compute and memory to be \nadjusted. The company does not want to manage the infrastructure. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an", correct: false },
                { id: 1, text: "Create an Amazon API Gateway API. Integrate the API with an AWS Step Functions state", correct: false },
                { id: 2, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an", correct: false },
                { id: 3, text: "Create an Amazon API Gateway API. Integrate the API with AWS Lambda to receive payment", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 56,
            text: "A solutions architect is designing a user authentication solution for a company. The solution must \ninvoke two-factor authentication for users that log in from inconsistent geographical locations, IP \naddresses, or devices. The solution must also be able to scale up to accommodate millions of \nusers. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive", correct: true },
                { id: 1, text: "Configure Amazon Cognito identity pools for user authentication. Enable multi-factor", correct: false },
                { id: 2, text: "Configure AWS Identity and Access Management (IAM) users for user authentication. Attach an", correct: false },
                { id: 3, text: "Configure AWS IAM Identity Center (AWS Single Sign-On) authentication for user authentication.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "A company has an Amazon S3 data lake. The company needs a solution that transforms the data \nfrom the data lake and loads the data into a data warehouse every day. The data warehouse \nmust have massively parallel processing (MPP) capabilities. \n \nData analysts then need to create and train machine learning (ML) models by using SQL \ncommands on the data. The solution must use serverless AWS services wherever possible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Run a daily Amazon EMR job to transform the data and load the data into Amazon Redshift. Use", correct: false },
                { id: 1, text: "Run a daily Amazon EMR job to transform the data and load the data into Amazon Aurora", correct: false },
                { id: 2, text: "Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift", correct: true },
                { id: 3, text: "Run a daily AWS Glue job to transform the data and load the data into Amazon Athena tables.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company runs containers in a Kubernetes environment in the company's local data center. The \ncompany wants to use Amazon Elastic Kubernetes Service (Amazon EKS) and other AWS \nmanaged services. Data must remain locally in the company's data center and cannot be stored \nin any remote site or cloud to maintain compliance. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy AWS Local Zones in the company's data center.", correct: false },
                { id: 1, text: "Use an AWS Snowmobile in the company's data center.", correct: false },
                { id: 2, text: "Install an AWS Outposts rack in the company's data center.", correct: true },
                { id: 3, text: "Install an AWS Snowball Edge Storage Optimized node in the data center.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A social media company has workloads that collect and process data. The workloads store the \ndata in on-premises NFS storage. The data store cannot scale fast enough to meet the \ncompany's expanding business needs. The company wants to migrate the current data store to \nAWS. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Set up an AWS Storage Gateway Volume Gateway. Use an Amazon S3 Lifecycle policy to", correct: false },
                { id: 1, text: "Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy", correct: true },
                { id: 2, text: "Use the Amazon Elastic File System (Amazon EFS) Standard-Infrequent Access (Standard-IA)", correct: false },
                { id: 3, text: "Use the Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A company uses high concurrency AWS Lambda functions to process a constantly increasing \nnumber of messages in a message queue during marketing events. The Lambda functions use \nCPU intensive code to process the messages. The company wants to reduce the compute costs \nand to maintain service latency for its customers. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure reserved concurrency for the Lambda functions. Decrease the memory allocated to the", correct: false },
                { id: 1, text: "Configure reserved concurrency for the Lambda functions. Increase the memory according to", correct: false },
                { id: 2, text: "Configure provisioned concurrency for the Lambda functions. Decrease the memory allocated to", correct: false },
                { id: 3, text: "Configure provisioned concurrency for the Lambda functions. Increase the memory according to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 61,
            text: "A company runs its workloads on Amazon Elastic Container Service (Amazon ECS). The \ncontainer images that the ECS task definition uses need to be scanned for Common \nVulnerabilities and Exposures (CVEs). New container images that are created also need to be \nscanned. \n \nWhich solution will meet these requirements with the FEWEST changes to the workloads?",
            options: [
                { id: 0, text: "Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository to store the", correct: true },
                { id: 1, text: "Store the container images in an Amazon S3 bucket. Use Amazon Macie to scan the images.", correct: false },
                { id: 2, text: "Deploy the workloads to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic", correct: false },
                { id: 3, text: "Store the container images in an Amazon S3 bucket that has versioning enabled. Configure an", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "A company uses an AWS Batch job to run its end-of-day sales process. The company needs a \nserverless solution that will invoke a third-party reporting application when the AWS Batch job is \nsuccessful. The reporting application has an HTTP API interface that uses username and \npassword authentication. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an Amazon EventBridge rule to match incoming AWS Batch job SUCCEEDED events.", correct: false },
                { id: 1, text: "Configure Amazon EventBridge Scheduler to match incoming AWS Batch job SUCCEEDED", correct: false },
                { id: 2, text: "Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway", correct: false },
                { id: 3, text: "Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company collects and processes data from a vendor. The vendor stores its data in an Amazon \nRDS for MySQL database in the vendor's own AWS account. The company's VPC does not have \nan internet gateway, an AWS Direct Connect connection, or an AWS Site-to-Site VPN \nconnection. The company needs to access the data that is in the vendor database. \n \nWhich solution will meet this requirement?",
            options: [
                { id: 0, text: "Instruct the vendor to sign up for the AWS Hosted Connection Direct Connect Program. Use VPC", correct: false },
                { id: 1, text: "Configure a client VPN connection between the company's VPC and the vendor's VPC. Use VPC", correct: false },
                { id: 2, text: "Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the", correct: true },
                { id: 3, text: "Use AWS Transit Gateway to integrate the company's VPC and the vendor's VPC. Use VPC", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/how-to-securely-publish-internet- applications-at-scale-using-application-load-balancer-and-aws-privatelink/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A company wants to set up Amazon Managed Grafana as its visualization tool. The company \nwants to visualize data from its Amazon RDS database as one data source. The company needs \na secure solution that will not expose the data over the internet. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon Managed Grafana workspace without a VPC. Create a public endpoint for the", correct: false },
                { id: 1, text: "Create an Amazon Managed Grafana workspace in a VPC. Create a private endpoint for the RDS", correct: true },
                { id: 2, text: "Create an Amazon Managed Grafana workspace without a VPCreate an AWS PrivateLink", correct: false },
                { id: 3, text: "Create an Amazon Managed Grafana workspace in a VPC. Create a public endpoint for the RDS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test23: [
        {
            id: 0,
            text: "A company hosts a data lake on Amazon S3. The data lake ingests data in Apache Parquet \nformat from various data sources. The company uses multiple transformation steps to prepare the \ningested data. The steps include filtering of anomalies, normalizing of data to standard date and \ntime values, and generation of aggregates for analyses. \n \nThe company must store the transformed data in S3 buckets that data analysts access. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n423 \ncompany needs a prebuilt solution for data transformation that does not require code. The \nsolution must provide data lineage and data profiling. The company needs to share the data \ntransformation steps with employees throughout the company. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure an AWS Glue Studio visual canvas to transform the data. Share the transformation", correct: false },
                { id: 1, text: "Configure Amazon EMR Serverless to transform the data. Share the transformation steps with", correct: false },
                { id: 2, text: "Configure AWS Glue DataBrew to transform the data. Share the transformation steps with", correct: true },
                { id: 3, text: "Create Amazon Athena tables for the data. Write Athena SQL queries to transform the data.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A solutions architect runs a web application on multiple Amazon EC2 instances that are in \nindividual target groups behind an Application Load Balancer (ALB). Users can reach the \napplication through a public website. \n \nThe solutions architect wants to allow engineers to use a development version of the website to \naccess one specific development EC2 instance to test new features for the application. The \nsolutions architect wants to use an Amazon Route 53 hosted zone to give the engineers access \nto the development instance. The solution must automatically route to the development instance \neven if the development instance is replaced. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an A Record for the development website that has the value set to the ALB. Create a", correct: true },
                { id: 1, text: "Recreate the development instance with a public IP address. Create an A Record for the", correct: false },
                { id: 2, text: "Create an A Record for the development website that has the value set to the ALB. Create a", correct: false },
                { id: 3, text: "Place all the instances in the same target group. Create an A Record for the development", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A company runs a container application on a Kubernetes cluster in the company's data center. \nThe application uses Advanced Message Queuing Protocol (AMQP) to communicate with a \nmessage queue. The data center cannot scale fast enough to meet the company's expanding \nbusiness needs. The company wants to migrate the workloads to AWS. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n424",
            options: [
                { id: 0, text: "Migrate the container application to Amazon Elastic Container Service (Amazon ECS). Use", correct: false },
                { id: 1, text: "Migrate the container application to Amazon Elastic Kubernetes Service (Amazon EKS). Use", correct: true },
                { id: 2, text: "Use highly available Amazon EC2 instances to run the application. Use Amazon MQ to retrieve", correct: false },
                { id: 3, text: "Use AWS Lambda functions to run the application. Use Amazon Simple Queue Service (Amazon", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "An online gaming company hosts its platform on Amazon EC2 instances behind Network Load \nBalancers (NLBs) across multiple AWS Regions. The NLBs can route requests to targets over the \ninternet. The company wants to improve the customer playing experience by reducing end-to-end \nload time for its global customer base. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register", correct: false },
                { id: 1, text: "Configure Amazon Route 53 to route equally weighted traffic to the NLBs in each Region.", correct: false },
                { id: 2, text: "Create additional NLBs and EC2 instances in other Regions where the company has large", correct: false },
                { id: 3, text: "Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 4,
            text: "A company has an on-premises application that uses SFTP to collect financial data from multiple \nvendors. The company is migrating to the AWS Cloud. The company has created an application \nthat uses Amazon S3 APIs to upload files from vendors. \n \nSome vendors run their systems on legacy applications that do not support S3 APIs. The vendors \nwant to continue to use SFTP-based applications to upload data. The company wants to use \nmanaged services for the needs of the vendors that use legacy applications. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the", correct: false },
                { id: 1, text: "Create an AWS Transfer Family endpoint for vendors that use legacy applications.", correct: true },
                { id: 2, text: "Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy", correct: false },
                { id: 3, text: "Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A marketing team wants to build a campaign for an upcoming multi-sport event. The team has \nnews reports from the past five years in PDF format. The team needs a solution to extract \ninsights about the content and the sentiment of the news reports. The solution must use Amazon \nTextract to process the news reports. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Provide the extracted insights to Amazon Athena for analysis. Store the extracted insights and", correct: false },
                { id: 1, text: "Store the extracted insights in an Amazon DynamoDB table. Use Amazon SageMaker to build a", correct: false },
                { id: 2, text: "Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an", correct: true },
                { id: 3, text: "Store the extracted insights in an Amazon S3 bucket. Use Amazon QuickSight to visualize and", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A company's application runs on Amazon EC2 instances that are in multiple Availability Zones. \nThe application needs to ingest real-time data from third-party applications. \n \nThe company needs a data ingestion solution that places the ingested raw data in an Amazon S3 \nbucket. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose", correct: true },
                { id: 1, text: "Create database migration tasks in AWS Database Migration Service (AWS DMS). Specify", correct: false },
                { id: 2, text: "Create and configure AWS DataSync agents on the EC2 instances. Configure DataSync tasks to", correct: false },
                { id: 3, text: "Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "A company's application is receiving data from multiple data sources. The size of the data varies \nand is expected to increase over time. The current maximum size is 700 KB. The data volume \nand data size continue to grow as more data sources are added. \n \nThe company decides to use Amazon DynamoDB as the primary database for the application. A \nsolutions architect needs to identify a solution that handles the large data sizes. \n \nWhich solution will meet these requirements in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n426",
            options: [
                { id: 0, text: "Create an AWS Lambda function to filter the data that exceeds DynamoDB item size limits. Store", correct: false },
                { id: 1, text: "Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table, create an item", correct: true },
                { id: 2, text: "Split all incoming large data into a collection of items that have the same partition key. Write the", correct: false },
                { id: 3, text: "Create an AWS Lambda function that uses gzip compression to compress the large objects as", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company is migrating a legacy application from an on-premises data center to AWS. The \napplication relies on hundreds of cron jobs that run between 1 and 20 minutes on different \nrecurring schedules throughout the day. \n \nThe company wants a solution to schedule and run the cron jobs on AWS with minimal \nrefactoring. The solution must support running the cron jobs in response to an event in the future. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a", correct: false },
                { id: 1, text: "Create a container image for the cron jobs. Use AWS Batch on Amazon Elastic Container Service", correct: false },
                { id: 2, text: "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a", correct: true },
                { id: 3, text: "Create a container image for the cron jobs. Create a workflow in AWS Step Functions that uses a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/containers/migrate-cron-jobs-to-event-driven-architectures-using- amazon-elastic-container-service-and-amazon-eventbridge/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "A company uses Salesforce. The company needs to load existing data and ongoing data \nchanges from Salesforce to Amazon Redshift for analysis. The company does not want the data \nto travel over the public internet. \n \nWhich solution will meet these requirements with the LEAST development effort?",
            options: [
                { id: 0, text: "Establish a VPN connection from the VPC to Salesforce. Use AWS Glue DataBrew to transfer", correct: false },
                { id: 1, text: "Establish an AWS Direct Connect connection from the VPC to Salesforce. Use AWS Glue", correct: false },
                { id: 2, text: "Create an AWS PrivateLink connection in the VPC to Salesforce. Use Amazon AppFlow to", correct: true },
                { id: 3, text: "Create a VPC peering connection to Salesforce. Use Amazon AppFlow to transfer data.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/connect/latest/adminguide/integrate-salesforce-tasks.html https://docs.aws.amazon.com/connect/latest/adminguide/vpc-interface-endpoints.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A company recently migrated its application to AWS. The application runs on Amazon EC2 Linux \ninstances in an Auto Scaling group across multiple Availability Zones. The application stores data \nin an Amazon Elastic File System (Amazon EFS) file system that uses EFS Standard-Infrequent \nAccess storage. The application indexes the company's files. The index is stored in an Amazon \nRDS database. \n \nThe company needs to optimize storage costs with some application and services changes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket that uses an Intelligent-Tiering lifecycle policy. Copy all files to the", correct: true },
                { id: 1, text: "Deploy Amazon FSx for Windows File Server file shares. Update the application to use CIFS", correct: false },
                { id: 2, text: "Deploy Amazon FSx for OpenZFS file system shares. Update the application to use the new", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket that uses S3 Glacier Flexible Retrieval. Copy all files to the S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 11,
            text: "A robotics company is designing a solution for medical surgery. The robots will use advanced \nsensors, cameras, and AI algorithms to perceive their environment and to complete surgeries. \n \nThe company needs a public load balancer in the AWS Cloud that will ensure seamless \ncommunication with backend services. The load balancer must be capable of routing traffic based \non the query strings to different target groups. The traffic must also be encrypted. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use a Network Load Balancer with a certificate attached from AWS Certificate Manager (ACM).", correct: false },
                { id: 1, text: "Use a Gateway Load Balancer. Import a generated certificate in AWS Identity and Access", correct: false },
                { id: 2, text: "Use an Application Load Balancer with a certificate attached from AWS Certificate Manager", correct: true },
                { id: 3, text: "Use a Network Load Balancer. Import a generated certificate in AWS Identity and Access", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company has an application that runs on a single Amazon EC2 instance. The application uses \na MySQL database that runs on the same EC2 instance. The company needs a highly available \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n428 \nand automatically scalable solution to handle increased traffic. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application", correct: false },
                { id: 1, text: "Deploy the application to EC2 instances that are configured as a target group behind an", correct: false },
                { id: 2, text: "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application", correct: true },
                { id: 3, text: "Deploy the application to EC2 instances that are configured as a target group behind an", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nTarget groups are just a group of Ec2 instances. Target groups are closely associated with ELB and not ASG. We can just use ELB and Target groups to route requests to EC2 instances. With this setup, there is no autoscaling which means instances cannot be added or removed when your load increases/decreases.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company is planning to migrate data to an Amazon S3 bucket. The data must be encrypted at \nrest within the S3 bucket. The encryption key must be rotated automatically every year. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Migrate the data to the S3 bucket. Use server-side encryption with Amazon S3 managed keys", correct: false },
                { id: 1, text: "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable", correct: true },
                { id: 2, text: "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3", correct: false },
                { id: 3, text: "Use customer key material to encrypt the data. Migrate the data to the S3 bucket. Create an AWS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A company is migrating applications from an on-premises Microsoft Active Directory that the \ncompany manages to AWS. The company deploys the applications in multiple AWS accounts. \nThe company uses AWS Organizations to manage the accounts centrally. \n \nThe company's security team needs a single sign-on solution across all the company's AWS \naccounts. The company must continue to manage users and groups that are in the on-premises \nActive Directory. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n429",
            options: [
                { id: 0, text: "Create an Enterprise Edition Active Directory in AWS Directory Service for Microsoft Active", correct: false },
                { id: 1, text: "Enable AWS IAM Identity Center. Configure a two-way forest trust relationship to connect the", correct: true },
                { id: 2, text: "Use AWS Directory Service and create a two-way trust relationship with the company's self-", correct: false },
                { id: 3, text: "Deploy an identity provider (IdP) on Amazon EC2. Link the IdP as an identity source within AWS", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company is planning to deploy its application on an Amazon Aurora PostgreSQL Serverless v2 \ncluster. The application will receive large amounts of traffic. The company wants to optimize the \nstorage performance of the cluster as the load on the application increases. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure the cluster to use the Aurora Standard storage configuration.", correct: false },
                { id: 1, text: "Configure the cluster storage type as Provisioned IOPS.", correct: false },
                { id: 2, text: "Configure the cluster storage type as General Purpose.", correct: false },
                { id: 3, text: "Configure the cluster to use the Aurora I/O-Optimized storage configuration.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 16,
            text: "A financial services company that runs on AWS has designed its security controls to meet \nindustry standards. The industry standards include the National Institute of Standards and \nTechnology (NIST) and the Payment Card Industry Data Security Standard (PCI DSS). \n \nThe company's third-party auditors need proof that the designed controls have been implemented \nand are functioning correctly. The company has hundreds of AWS accounts in a single \norganization in AWS Organizations. The company needs to monitor the current state of the \ncontrols across accounts. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Designate one account as the Amazon Inspector delegated administrator account from the", correct: false },
                { id: 1, text: "Designate one account as the Amazon GuardDuty delegated administrator account from the", correct: false },
                { id: 2, text: "Configure an AWS CloudTrail organization trail in the Organizations management account.", correct: false },
                { id: 3, text: "Designate one account as the AWS Security Hub delegated administrator account from the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A company uses an Amazon S3 bucket as its data lake storage platform. The S3 bucket contains \na massive amount of data that is accessed randomly by multiple teams and hundreds of \napplications. The company wants to reduce the S3 storage costs and provide immediate \navailability for frequently accessed objects. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
            options: [
                { id: 0, text: "Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering storage class.", correct: true },
                { id: 1, text: "Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the", correct: false },
                { id: 2, text: "Use data from S3 storage class analysis to create S3 Lifecycle rules to automatically transition", correct: false },
                { id: 3, text: "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-managing.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company has 5 TB of datasets. The datasets consist of 1 million user profiles and 10 million \nconnections. The user profiles have connections as many-to-many relationships. The company \nneeds a performance efficient way to find mutual connections up to five levels. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an Amazon S3 bucket to store the datasets. Use Amazon Athena to perform SQL JOIN", correct: false },
                { id: 1, text: "Use Amazon Neptune to store the datasets with edges and vertices. Query the data to find", correct: true },
                { id: 2, text: "Use an Amazon S3 bucket to store the datasets. Use Amazon QuickSight to visualize", correct: false },
                { id: 3, text: "Use Amazon RDS to store the datasets with multiple tables. Perform SQL JOIN queries to find", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/neptune/latest/userguide/notebooks-visualization.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 19,
            text: "A company needs a secure connection between its on-premises environment and AWS. This \nconnection does not need high bandwidth and will handle a small amount of traffic. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n431 \nconnection should be set up quickly. \n \nWhat is the MOST cost-effective method to establish this type of connection?",
            options: [
                { id: 0, text: "Implement a client VPN.", correct: false },
                { id: 1, text: "Implement AWS Direct Connect.", correct: false },
                { id: 2, text: "Implement a bastion host on Amazon EC2.", correct: false },
                { id: 3, text: "Implement an AWS Site-to-Site VPN connection.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A company has an on-premises SFTP file transfer solution. The company is migrating to the AWS \nCloud to scale the file transfer solution and to optimize costs by using Amazon S3. The \ncompany's employees will use their credentials for the on-premises Microsoft Active Directory \n(AD) to access the new solution. The company wants to keep the current authentication and file \naccess mechanisms. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure an S3 File Gateway. Create SMB file shares on the file gateway that use the existing", correct: false },
                { id: 1, text: "Configure an Auto Scaling group with Amazon EC2 instances to run an SFTP solution. Configure", correct: false },
                { id: 2, text: "Create an AWS Transfer Family server with SFTP endpoints. Choose the AWS Directory Service", correct: true },
                { id: 3, text: "Create an AWS Transfer Family SFTP endpoint. Configure the endpoint to use the AWS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A company is designing an event-driven order processing system. Each order requires multiple \nvalidation steps after the order is created. An idempotent AWS Lambda function performs each \nvalidation step. Each validation step is independent from the other validation steps. Individual \nvalidation steps need only a subset of the order event information. \n \nThe company wants to ensure that each validation step Lambda function has access to only the \ninformation from the order event that the function requires. The components of the order \nprocessing system should be loosely coupled to accommodate future business changes. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue for each validation step. Create", correct: false },
                { id: 1, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the validation step", correct: false },
                { id: 2, text: "Create an Amazon EventBridge event bus. Create an event rule for each validation step.", correct: true },
                { id: 3, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a new Lambda function", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A company is migrating a three-tier application to AWS. The application requires a MySQL \ndatabase. In the past, the application users reported poor application performance when creating \nnew entries. These performance issues were caused by users generating different real-time \nreports from the application during working hours. \n \nWhich solution will improve the performance of the application when it is moved to AWS?",
            options: [
                { id: 0, text: "Import the data into an Amazon DynamoDB table with provisioned capacity. Refactor the", correct: false },
                { id: 1, text: "Create the database on a compute optimized Amazon EC2 instance. Ensure compute resources", correct: false },
                { id: 2, text: "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas. Configure the", correct: true },
                { id: 3, text: "Create an Amazon Aurora MySQL Multi-AZ DB cluster. Configure the application to use the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A company is expanding a secure on-premises network to the AWS Cloud by using an AWS \nDirect Connect connection. The on-premises network has no direct internet access. An \napplication that runs on the on-premises network needs to use an Amazon S3 bucket. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a public virtual interface (VIF). Route the AWS traffic over the public VIF.", correct: false },
                { id: 1, text: "Create a VPC and a NAT gateway. Route the AWS traffic from the on-premises network to the", correct: false },
                { id: 2, text: "Create a VPC and an Amazon S3 interface endpoint. Route the AWS traffic from the on-premises", correct: true },
                { id: 3, text: "Create a VPC peering connection between the on-premises network and Direct Connect. Route", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A company serves its website by using an Auto Scaling group of Amazon EC2 instances in a \nsingle AWS Region. The website does not require a database. \n \nThe company is expanding, and the company's engineering team deploys the website to a \nsecond Region. The company wants to distribute traffic across both Regions to accommodate \ngrowth and for disaster recovery purposes. The solution should not serve traffic from a Region in \nwhich the website is unhealthy. \n \nWhich policy or resource should the company use to meet these requirements?",
            options: [
                { id: 0, text: "An Amazon Route 53 simple routing policy", correct: false },
                { id: 1, text: "An Amazon Route 53 multivalue answer routing policy", correct: true },
                { id: 2, text: "An Application Load Balancer in one Region with a target group that specifies the EC2 instance", correct: false },
                { id: 3, text: "An Application Load Balancer in one Region with a target group that specifies the IP addresses of", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A company runs its applications on Amazon EC2 instances that are backed by Amazon Elastic \nBlock Store (Amazon EBS). The EC2 instances run the most recent Amazon Linux release. The \napplications are experiencing availability issues when the company's employees store and \nretrieve files that are 25 GB or larger. The company needs a solution that does not require the \ncompany to transfer files between EC2 instances. The files must be available across many EC2 \ninstances and across multiple Availability Zones. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate all the files to an Amazon S3 bucket. Instruct the employees to access the files from the", correct: false },
                { id: 1, text: "Take a snapshot of the existing EBS volume. Mount the snapshot as an EBS volume across the", correct: false },
                { id: 2, text: "Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2 instances.", correct: true },
                { id: 3, text: "Create an Amazon Machine Image (AMI) from the EC2 instances. Configure new EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A company is running a highly sensitive application on Amazon EC2 backed by an Amazon RDS \ndatabase. Compliance regulations mandate that all personally identifiable information (PII) be \nencrypted at rest. \n \nWhich solution should a solutions architect recommend to meet this requirement with the LEAST \namount of changes to the infrastructure? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n434",
            options: [
                { id: 0, text: "Deploy AWS Certificate Manager to generate certificates. Use the certificates to encrypt the", correct: false },
                { id: 1, text: "Deploy AWS CloudHSM, generate encryption keys, and use the keys to encrypt database", correct: false },
                { id: 2, text: "Configure SSL encryption using AWS Key Management Service (AWS KMS) keys to encrypt", correct: false },
                { id: 3, text: "Configure Amazon Elastic Block Store (Amazon EBS) encryption and Amazon RDS encryption", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company runs an AWS Lambda function in private subnets in a VPC. The subnets have a \ndefault route to the internet through an Amazon EC2 NAT instance. The Lambda function \nprocesses input data and saves its output as an object to Amazon S3. \n \nIntermittently, the Lambda function times out while trying to upload the object because of \nsaturated traffic on the NAT instance's network. The company wants to access Amazon S3 \nwithout traversing the internet. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Replace the EC2 NAT instance with an AWS managed NAT gateway.", correct: false },
                { id: 1, text: "Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type.", correct: false },
                { id: 2, text: "Provision a gateway endpoint for Amazon S3 in the VPUpdate the route tables of the subnets", correct: true },
                { id: 3, text: "Provision a transit gateway. Place transit gateway attachments in the private subnets where the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A news company that has reporters all over the world is hosting its broadcast system on AWS. \nThe reporters send live broadcasts to the broadcast system. The reporters use software on their \nphones to send live streams through the Real Time Messaging Protocol (RTMP). \n \nA solutions architect must design a solution that gives the reporters the ability to send the highest \nquality streams. The solution must provide accelerated TCP connections back to the broadcast \nsystem. \n \nWhat should the solutions architect use to meet these requirements?",
            options: [
                { id: 0, text: "Amazon CloudFront", correct: false },
                { id: 1, text: "AWS Global Accelerator", correct: true },
                { id: 2, text: "AWS Client VPN", correct: false },
                { id: 3, text: "Amazon EC2 instances and AWS Elastic IP addresses", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n435 \nA company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) to run \nits self-managed database. The company has 350 TB of data spread across all EBS volumes. \nThe company takes daily EBS snapshots and keeps the snapshots for 1 month. The daily change \nrate is 5% of the EBS volumes. \n \nBecause of new regulations, the company needs to keep the monthly snapshots for 7 years. The \ncompany needs to change its backup strategy to comply with the new regulations and to ensure \nthat data is available with minimal administrative effort. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Copy the monthly", correct: true },
                { id: 1, text: "Continue with the current EBS snapshot policy. Add a new policy to move the monthly snapshot", correct: false },
                { id: 2, text: "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Keep the monthly", correct: false },
                { id: 3, text: "Keep the daily snapshot in the EBS snapshot standard tier. Use EBS direct APIs to take", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 30,
            text: "A company runs an application on several Amazon EC2 instances that store persistent data on \nan Amazon Elastic File System (Amazon EFS) file system. The company needs to replicate the \ndata to another AWS Region by using an AWS managed service solution. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use the EFS-to-EFS backup solution to replicate the data to an EFS file system in another", correct: false },
                { id: 1, text: "Run a nightly script to copy data from the EFS file system to an Amazon S3 bucket. Enable S3", correct: false },
                { id: 2, text: "Create a VPC in another Region. Establish a cross-Region VPC peer. Run a nightly rsync to copy", correct: false },
                { id: 3, text: "Use AWS Backup to create a backup plan with a rule that takes a daily backup and replicates it to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An ecommerce company is migrating its on-premises workload to the AWS Cloud. The workload \ncurrently consists of a web application and a backend Microsoft SQL database for storage. \n \nThe company expects a high volume of customers during a promotional event. The new \ninfrastructure in the AWS Cloud must be highly available and scalable. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Migrate the web application to two Amazon EC2 instances across two Availability Zones behind", correct: false },
                { id: 1, text: "Migrate the web application to an Amazon EC2 instance that runs in an Auto Scaling group", correct: false },
                { id: 2, text: "Migrate the web application to Amazon EC2 instances that run in an Auto Scaling group across", correct: true },
                { id: 3, text: "Migrate the web application to three Amazon EC2 instances across three Availability Zones", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 32,
            text: "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low-latency connection to the \napplication servers. A new company policy states all application-generated files must be copied to \nAWS. There is already a VPN connection to AWS. \n \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \n \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
            options: [
                { id: 0, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 1, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 2, text: "AWS Snowball", correct: false },
                { id: 3, text: "AWS Storage Gateway", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/storagegateway/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A company has 15 employees. The company stores employee start dates in an Amazon \nDynamoDB table. The company wants to send an email message to each employee on the day \nof the employee's work anniversary. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create a script that scans the DynamoDB table and uses Amazon Simple Notification Service", correct: false },
                { id: 1, text: "Create a script that scans the DynamoDB table and uses Amazon Simple Queue Service", correct: false },
                { id: 2, text: "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple", correct: true },
                { id: 3, text: "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nSNS for sending mails Lambda to scan the database + send the message to the SNS topic. Using a script on a EC2 will add maintenance on both the EC2 and the script + cronjobs are not reliable and can be hard to monitor properly.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "A company's application is running on Amazon EC2 instances within an Auto Scaling group \nbehind an Elastic Load Balancing (ELB) load balancer. Based on the application's history, the \ncompany anticipates a spike in traffic during a holiday each year. A solutions architect must \ndesign a strategy to ensure that the Auto Scaling group proactively increases capacity to \nminimize any performance impact on application users. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization", correct: false },
                { id: 1, text: "Create a recurring scheduled action to scale up the Auto Scaling group before the expected", correct: true },
                { id: 2, text: "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during", correct: false },
                { id: 3, text: "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A company uses Amazon RDS for PostgreSQL databases for its data tier. The company must \nimplement password rotation for the databases. \n \nWhich solution meets this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the password in AWS Secrets Manager. Enable automatic rotation on the secret.", correct: true },
                { id: 1, text: "Store the password in AWS Systems Manager Parameter Store. Enable automatic rotation on the", correct: false },
                { id: 2, text: "Store the password in AWS Systems Manager Parameter Store. Write an AWS Lambda function", correct: false },
                { id: 3, text: "Store the password in AWS Key Management Service (AWS KMS). Enable automatic rotation on", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "A company runs its application on Oracle Database Enterprise Edition. The company needs to \nmigrate the application and the database to AWS. The company can use the Bring Your Own \nLicense (BYOL) model while migrating to AWS. The application uses third-party database \nfeatures that require privileged access. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n438 \nA solutions architect must design a solution for the database migration. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Migrate the database to Amazon RDS for Oracle by using native tools. Replace the third-party", correct: false },
                { id: 1, text: "Migrate the database to Amazon RDS Custom for Oracle by using native tools. Customize the", correct: true },
                { id: 2, text: "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS", correct: false },
                { id: 3, text: "Migrate the database to Amazon RDS for PostgreSQL by using AWS Database Migration Service", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A large international university has deployed all of its compute services in the AWS Cloud. These \nservices include Amazon EC2, Amazon RDS, and Amazon DynamoDB. The university currently \nrelies on many custom scripts to back up its infrastructure. However, the university wants to \ncentralize management and automate data backups as much as possible by using AWS native \noptions. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use third-party backup software with an AWS Storage Gateway tape gateway virtual tape library.", correct: false },
                { id: 1, text: "Use AWS Backup to configure and monitor all backups for the services in use.", correct: true },
                { id: 2, text: "Use AWS Config to set lifecycle management to take snapshots of all data sources on a", correct: false },
                { id: 3, text: "Use AWS Systems Manager State Manager to manage the configuration and monitoring of", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "A company wants to build a map of its IT infrastructure to identify and enforce policies on \nresources that pose security risks. The company's security team must be able to query data in \nthe IT infrastructure map and quickly identify security risks. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon RDS to store the data. Use SQL to query the data to identify security risks.", correct: false },
                { id: 1, text: "Use Amazon Neptune to store the data. Use SPARQL to query the data to identify security risks.", correct: true },
                { id: 2, text: "Use Amazon Redshift to store the data. Use SQL to query the data to identify security risks.", correct: false },
                { id: 3, text: "Use Amazon DynamoDB to store the data. Use PartiQL to query the data to identify security", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nUsing Amazon Neptune with SPARQL, a query language for graph databases, allows the security team to easily query the data in the IT infrastructure map to identify security risks. SPARQL is specifically designed for querying graph data and allows for complex queries to traverse relationships between resources efficiently. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A large company wants to provide its globally located developers separate, limited size, managed \nPostgreSQL databases for development purposes. The databases will be low volume. The \ndevelopers need the databases only when they are actively working. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Give the developers the ability to launch separate Amazon Aurora instances. Set up a process to", correct: false },
                { id: 1, text: "Develop an AWS Service Catalog product that enforces size restrictions for launching Amazon", correct: true },
                { id: 2, text: "Create an Amazon Aurora Serverless cluster. Develop an AWS Service Catalog product to", correct: false },
                { id: 3, text: "Monitor AWS Trusted Advisor checks for idle Amazon RDS databases. Create a process to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWith AWS Service Catalog, you can meet your compliance requirements while making sure your customers can quickly deploy the cloud resources they need.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 40,
            text: "A company is building a web application that serves a content management system. The content \nmanagement system runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. Users \nare constantly adding and updating files, blogs, and other website assets in the content \nmanagement system. \n \nA solutions architect must implement a solution in which all the EC2 instances share up-to-date \nwebsite content with the least possible lag time. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Update the EC2 user data in the Auto Scaling group lifecycle policy to copy the website assets", correct: false },
                { id: 1, text: "Copy the website assets to an Amazon Elastic File System (Amazon EFS) file system. Configure", correct: true },
                { id: 2, text: "Copy the website assets to an Amazon S3 bucket. Ensure that each EC2 instance downloads the", correct: false },
                { id: 3, text: "Restore an Amazon Elastic Block Store (Amazon EBS) snapshot with the website assets. Attach", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A company's web application consists of multiple Amazon EC2 instances that run behind an \nApplication Load Balancer in a VPC. An Amazon RDS for MySQL DB instance contains the data. \nThe company needs the ability to automatically detect and respond to suspicious or unexpected \nbehavior in its AWS environment. The company already has added AWS WAF to its architecture. \n \nWhat should a solutions architect do next to protect against threats?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to perform threat detection. Configure Amazon EventBridge to filter for", correct: true },
                { id: 1, text: "Use AWS Firewall Manager to perform threat detection. Configure Amazon EventBridge to filter", correct: false },
                { id: 2, text: "Use Amazon Inspector to perform threat detection and to update the AWS WAF rules. Create a", correct: false },
                { id: 3, text: "Use Amazon Macie to perform threat detection and to update the AWS WAF rules. Create a VPC", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A company is planning to run a group of Amazon EC2 instances that connect to an Amazon \nAurora database. The company has built an AWS CloudFormation template to deploy the EC2 \ninstances and the Aurora DB cluster. The company wants to allow the instances to authenticate \nto the database in a secure way. The company does not want to maintain static database \ncredentials. \n \nWhich solution meets these requirements with the LEAST operational effort?",
            options: [
                { id: 0, text: "Create a database user with a user name and password. Add parameters for the database user", correct: false },
                { id: 1, text: "Create a database user with a user name and password. Store the user name and password in", correct: false },
                { id: 2, text: "Configure the DB cluster to use IAM database authentication. Create a database user to use with", correct: true },
                { id: 3, text: "Configure the DB cluster to use IAM database authentication with an IAM user. Create a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. \nThe company does not want to use the default domain name for the distribution. Instead, the \ncompany wants to use a different domain name for the distribution. \n \nWhich solution will deploy the certificate without incurring any additional costs? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n441",
            options: [
                { id: 0, text: "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-", correct: false },
                { id: 1, text: "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-", correct: false },
                { id: 2, text: "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-", correct: true },
                { id: 3, text: "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/certificate-manager/pricing/ AWS Certificate Manager Pricing Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources you create to run your application. If you manage AWS Private Certificate Authority (CA) through ACM, refer to the AWS Private CA Pricing page for more details and examples.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A company creates operations data and stores the data in an Amazon S3 bucket. For the \ncompany's annual audit, an external consultant needs to access an annual report that is stored in \nthe S3 bucket. The external consultant needs to access the report for 7 days. \n \nThe company must implement a solution to allow the external consultant access to only the \nreport. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create a new S3 bucket that is configured to host a public static website. Migrate the operations", correct: false },
                { id: 1, text: "Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when the", correct: false },
                { id: 2, text: "Create a new IAM user that has access to the report in the S3 bucket. Provide the access keys to", correct: false },
                { id: 3, text: "Generate a presigned URL that has the required access to the location of the report on the S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company plans to run a high performance computing (HPC) workload on Amazon EC2 \nInstances. The workload requires low-latency network performance and high network throughput \nwith tightly coupled node-to-node communication. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the EC2 instances to be part of a cluster placement group.", correct: true },
                { id: 1, text: "Launch the EC2 instances with Dedicated Instance tenancy.", correct: false },
                { id: 2, text: "Launch the EC2 instances as Spot Instances.", correct: false },
                { id: 3, text: "Configure an On-Demand Capacity Reservation when the EC2 instances are launched.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A company has primary and secondary data centers that are 500 miles (804.7 km) apart and \ninterconnected with high-speed fiber-optic cable. The company needs a highly available and \nsecure network connection between its data centers and a VPC on AWS for a mission-critical \nworkload. A solutions architect must choose a connection solution that provides maximum \nresiliency. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Two AWS Direct Connect connections from the primary data center terminating at two Direct", correct: false },
                { id: 1, text: "A single AWS Direct Connect connection from each of the primary and secondary data centers", correct: false },
                { id: 2, text: "Two AWS Direct Connect connections from each of the primary and secondary data centers", correct: true },
                { id: 3, text: "A single AWS Direct Connect connection from each of the primary and secondary data centers", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company runs several Amazon RDS for Oracle On-Demand DB instances that have high \nutilization. The RDS DB instances run in member accounts that are in an organization in AWS \nOrganizations. \n \nThe company's finance team has access to the organization's management account and member \naccounts. The finance team wants to find ways to optimize costs by using AWS Trusted Advisor. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use the Trusted Advisor recommendations in the management account.", correct: true },
                { id: 1, text: "Use the Trusted Advisor recommendations in the member accounts where the RDS DB instances", correct: false },
                { id: 2, text: "Review the Trusted Advisor checks for Amazon RDS Reserved Instance Optimization.", correct: false },
                { id: 3, text: "Review the Trusted Advisor checks for Amazon RDS Idle DB Instances.", correct: false },
                { id: 4, text: "Review the Trusted Advisor checks for compute optimization. Crosscheck the results by using", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/awssupport/latest/user/organizational-view.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 48,
            text: "A solutions architect is creating an application. The application will run on Amazon EC2 instances \nin private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently \naccess large files that contain confidential information. These files are stored in Amazon S3 \nbuckets for processing. The solutions architect must optimize the network architecture to \nminimize data transfer costs. \n \nWhat should the solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n443",
            options: [
                { id: 0, text: "Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets,", correct: true },
                { id: 1, text: "Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a", correct: false },
                { id: 2, text: "Create an AWS PrivateLink interface endpoint for Amazon S3 in the VPIn the route tables for the", correct: false },
                { id: 3, text: "Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company wants to relocate its on-premises MySQL database to AWS. The database accepts \nregular imports from a client-facing application, which causes a high volume of write operations. \nThe company is concerned that the amount of traffic might be causing performance issues within \nthe application. \n \nHow should a solutions architect design the architecture on AWS?",
            options: [
                { id: 0, text: "Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor", correct: true },
                { id: 1, text: "Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an", correct: false },
                { id: 2, text: "Provision an Amazon DocumentDB (with MongoDB compatibility) instance with a memory", correct: false },
                { id: 3, text: "Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 50,
            text: "A company runs an application in the AWS Cloud that generates sensitive archival data files. The \ncompany wants to rearchitect the application's data storage. The company wants to encrypt the \ndata files and to ensure that third parties do not have access to the data before the data is \nencrypted and sent to AWS. The company has already created an Amazon S3 bucket. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption", correct: false },
                { id: 1, text: "Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS).", correct: false },
                { id: 2, text: "Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-", correct: false },
                { id: 3, text: "Configure the application to use client-side encryption with a key stored in AWS Key Management", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company uses Amazon RDS with default backup settings for its database tier. The company \nneeds to make a daily backup of the database to meet regulatory requirements. The company \nmust retain the backups for 30 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Write an AWS Lambda function to create an RDS snapshot every day.", correct: false },
                { id: 1, text: "Modify the RDS database to have a retention period of 30 days for automated backups.", correct: true },
                { id: 2, text: "Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.", correct: false },
                { id: 3, text: "Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. \nDuring peak usage hours when multiple users access and read the data, the monitoring system \nshows degradation of database performance for the write queries. The company wants to \nincrease the scalability of the application to meet peak usage demands. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a second Aurora DB cluster. Configure a copy job to replicate the users' data to the new", correct: false },
                { id: 1, text: "Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the existing Aurora DB", correct: false },
                { id: 2, text: "Create an Aurora read replica in the existing Aurora DB cluster. Update the application to use the", correct: true },
                { id: 3, text: "Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "A company's near-real-time streaming application is running on AWS. As the data is ingested, a \njob runs on the data and takes 30 minutes to complete. The workload frequently experiences high \nlatency due to large amounts of incoming data. A solutions architect needs to design a scalable \nand serverless solution to enhance performance. \n \nWhich combination of steps should the solutions architect take? (Choose two.)",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Firehose to ingest the data.", correct: true },
                { id: 1, text: "Use AWS Lambda with AWS Step Functions to process the data.", correct: false },
                { id: 2, text: "Use AWS Database Migration Service (AWS DMS) to ingest the data.", correct: false },
                { id: 3, text: "Use Amazon EC2 instances in an Auto Scaling group to process the data.", correct: false },
                { id: 4, text: "Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A company runs a web application on multiple Amazon EC2 instances in a VPC. The application \nneeds to write sensitive data to an Amazon S3 bucket. The data cannot be sent over the public \ninternet. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a gateway VPC endpoint for Amazon S3. Create a route in the VPC route table to the", correct: true },
                { id: 1, text: "Create an internal Network Load Balancer that has the S3 bucket as the target.", correct: false },
                { id: 2, text: "Deploy the S3 bucket inside the VPCreate a route in the VPC route table to the bucket.", correct: false },
                { id: 3, text: "Create an AWS Direct Connect connection between the VPC and an S3 regional endpoint.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company runs its production workload on Amazon EC2 instances with Amazon Elastic Block \nStore (Amazon EBS) volumes. A solutions architect needs to analyze the current EBS volume \ncost and to recommend optimizations. The recommendations need to include estimated monthly \nsaving opportunities. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Inspector reporting to generate EBS volume recommendations for optimization.", correct: false },
                { id: 1, text: "Use AWS Systems Manager reporting to determine EBS volume recommendations for", correct: false },
                { id: 2, text: "Use Amazon CloudWatch metrics reporting to determine EBS volume recommendations for", correct: false },
                { id: 3, text: "Use AWS Compute Optimizer to generate EBS volume recommendations for optimization.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS Compute Optimizer helps avoid overprovisioning and underprovisioning four types of AWS resources - Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volumes, Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda functions - based on your utilization data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A global company runs its workloads on AWS. The company's application uses Amazon S3 \nbuckets across AWS Regions for sensitive data storage and analysis. The company stores \nmillions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that \nare not versioning-enabled. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across", correct: true },
                { id: 1, text: "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled", correct: false },
                { id: 2, text: "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://aws.amazon.com/blogs/aws/s3-storage-lens/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A company wants to enhance its ecommerce order-processing application that is deployed on \nAWS. The application must process each order exactly once without affecting the customer \nexperience during unpredictable traffic surges. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Put all the orders in the", correct: true },
                { id: 1, text: "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the", correct: false },
                { id: 2, text: "Create a flow by using Amazon AppFlow. Send the orders to the flow. Configure an AWS Lambda", correct: false },
                { id: 3, text: "Configure AWS X-Ray in the application to track the order requests. Configure the application to", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company has two AWS accounts: Production and Development. The company needs to push \ncode changes in the Development account to the Production account. In the alpha phase, only \ntwo senior developers on the development team need access to the Production account. In the \nbeta phase, more developers will need access to perform testing. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create two policy documents by using the AWS Management Console in each account. Assign", correct: false },
                { id: 1, text: "Create an IAM role in the Development account. Grant the IAM role access to the Production", correct: false },
                { id: 2, text: "Create an IAM role in the Production account. Define a trust policy that specifies the Development", correct: false },
                { id: 3, text: "Create an IAM group in the Production account. Add the group as a principal in a trust policy that", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A company wants to restrict access to the content of its web application. The company needs to \nprotect the content by using authorization techniques that are available on AWS. The company \nalso wants to implement a serverless architecture for authorization and authentication that has \nlow login latency. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n447 \nThe solution must integrate with the web application and serve web content globally. The \napplication currently has a small user base, but the company expects the application's user base \nto increase. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure Amazon Cognito for authentication. Implement Lambda@Edge for authorization.", correct: true },
                { id: 1, text: "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement", correct: false },
                { id: 2, text: "Configure Amazon Cognito for authentication. Implement AWS Lambda for authorization. Use", correct: false },
                { id: 3, text: "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A development team uses multiple AWS accounts for its development, staging, and production \nenvironments. Team members have been launching large Amazon EC2 instances that are \nunderutilized. A solutions architect must prevent large instances from being launched in all \naccounts. \n \nHow can the solutions architect meet this requirement with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Update the IAM policies to deny the launch of large EC2 instances. Apply the policies to all users.", correct: false },
                { id: 1, text: "Define a resource in AWS Resource Access Manager that prevents the launch of large EC2", correct: false },
                { id: 2, text: "Create an IAM role in each account that denies the launch of large EC2 instances. Grant the", correct: false },
                { id: 3, text: "Create an organization in AWS Organizations in the management account with the default policy.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 61,
            text: "A company has migrated a fleet of hundreds of on-premises virtual machines (VMs) to Amazon \nEC2 instances. The instances run a diverse fleet of Windows Server versions along with several \nLinux distributions. The company wants a solution that will automate inventory and updates of the \noperating systems. The company also needs a summary of common vulnerabilities of each \ninstance for regular monthly reviews. \n \nWhat should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Configure AWS", correct: false },
                { id: 1, text: "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Deploy Amazon", correct: true },
                { id: 2, text: "Set up AWS Shield Advanced, and configure monthly reports. Deploy AWS Config to automate", correct: false },
                { id: 3, text: "Set up Amazon GuardDuty in the account to monitor all EC2 instances. Deploy AWS Config to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 \ninstances in an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. The \napplication connects to an Amazon DynamoDB table. \n \nFor disaster recovery (DR) purposes, the company wants to ensure that the application is \navailable from another AWS Region with minimal downtime. \n \nWhich solution will meet these requirements with the LEAST downtime?",
            options: [
                { id: 0, text: "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a", correct: true },
                { id: 1, text: "Create an AWS CloudFormation template to create EC2 instances, ELBs, and DynamoDB tables", correct: false },
                { id: 2, text: "Create an AWS CloudFormation template to create EC2 instances and an ELB to be launched", correct: false },
                { id: 3, text: "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nCreate an Auto Scaling group and an ELB in the DR Region, configuring the DynamoDB table as a global table, and setting up DNS failover to the new ELB. This approach allows for quick failover since the infrastructure is already in place and only DNS needs to be updated to redirect traffic.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 63,
            text: "A company runs an application on Amazon EC2 instances in a private subnet. The application \nneeds to store and retrieve data in Amazon S3 buckets. According to regulatory requirements, \nthe data must not travel across the public internet. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Deploy a NAT gateway to access the S3 buckets.", correct: false },
                { id: 1, text: "Deploy AWS Storage Gateway to access the S3 buckets.", correct: false },
                { id: 2, text: "Deploy an S3 interface endpoint to access the S3 buckets.", correct: false },
                { id: 3, text: "Deploy an S3 gateway endpoint to access the S3 buckets.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n449 \nA company hosts an application on Amazon EC2 instances that run in a single Availability Zone. \nThe application is accessible by using the transport layer of the Open Systems Interconnection \n(OSI) model. The company needs the application architecture to have high availability. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
            options: [
                { id: 0, text: "Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route", correct: false },
                { id: 1, text: "Configure a Network Load Balancer in front of the EC2 instances.", correct: true },
                { id: 2, text: "Configure a Network Load Balancer for TCP traffic to the instances. Configure an Application", correct: false },
                { id: 3, text: "Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use", correct: false },
                { id: 4, text: "Create an Amazon CloudWatch alarm. Configure the alarm to restart EC2 instances that", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
    ],
    test24: [
        {
            id: 0,
            text: "A company uses Amazon S3 to host its static website. The company wants to add a contact form \nto the webpage. The contact form will have dynamic server-side components for users to input \ntheir name, email address, phone number, and user message. \n \nThe company expects fewer than 100 site visits each month. The contact form must notify the \ncompany by email when a customer fills out the form. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Host the dynamic contact form in Amazon Elastic Container Service (Amazon ECS). Set up", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint that returns the contact form from an AWS Lambda", correct: true },
                { id: 2, text: "Host the website by using AWS Amplify Hosting for static content and dynamic content. Use", correct: false },
                { id: 3, text: "Migrate the website from Amazon S3 to Amazon EC2 instances that run Windows Server. Use", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 1,
            text: "A company creates dedicated AWS accounts in AWS Organizations for its business units. \nRecently, an important notification was sent to the root user email address of a business unit \naccount instead of the assigned account owner. The company wants to ensure that all future \nnotifications can be sent to different employees based on the notification categories of billing, \noperations, or security. \n \nWhich solution will meet these requirements MOST securely? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n450",
            options: [
                { id: 0, text: "Configure each AWS account to use a single email address that the company manages. Ensure", correct: false },
                { id: 1, text: "Configure each AWS account to use a different email distribution list for each business unit that", correct: true },
                { id: 2, text: "Configure each AWS account root user email address to be the individual company managed", correct: false },
                { id: 3, text: "Configure each AWS account root user to use email aliases that go to a centralized mailbox.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A company runs an ecommerce application on AWS. Amazon EC2 instances process purchases \nand store the purchase details in an Amazon Aurora PostgreSQL DB cluster. \n \nCustomers are experiencing application timeouts during times of peak usage. A solutions \narchitect needs to rearchitect the application so that the application can scale to meet peak usage \ndemands. \n \nWhich combination of actions will meet these requirements MOST cost-effectively? (Choose two.)",
            options: [
                { id: 0, text: "Configure an Auto Scaling group of new EC2 instances to retry the purchases until the processing", correct: true },
                { id: 1, text: "Configure the application to use an Amazon ElastiCache cluster in front of the Aurora", correct: false },
                { id: 2, text: "Update the application to send the purchase requests to an Amazon Simple Queue Service", correct: false },
                { id: 3, text: "Configure an AWS Lambda function to retry the ticket purchases until the processing is complete.", correct: false },
                { id: 4, text: "Configure an Amazon AP! Gateway REST API with a usage plan.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 3,
            text: "A company that uses AWS Organizations runs 150 applications across 30 different AWS \naccounts. The company used AWS Cost and Usage Report to create a new report in the \nmanagement account. The report is delivered to an Amazon S3 bucket that is replicated to a \nbucket in the data collection account. \n \nThe company's senior leadership wants to view a custom dashboard that provides NAT gateway \ncosts each day starting at the beginning of the current month. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n451",
            options: [
                { id: 0, text: "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure", correct: false },
                { id: 1, text: "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure", correct: true },
                { id: 2, text: "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure", correct: false },
                { id: 3, text: "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company is hosting a high-traffic static website on Amazon S3 with an Amazon CloudFront \ndistribution that has a default TTL of 0 seconds. The company wants to implement caching to \nimprove performance for the website. However, the company also wants to ensure that stale \ncontent is not served for more than a few minutes after a deployment. \n \nWhich combination of caching methods should a solutions architect implement to meet these \nrequirements? (Choose two.)",
            options: [
                { id: 0, text: "Set the CloudFront default TTL to 2 minutes.", correct: true },
                { id: 1, text: "Set a default TTL of 2 minutes on the S3 bucket.", correct: false },
                { id: 2, text: "Add a Cache-Control private directive to the objects in Amazon S3.", correct: false },
                { id: 3, text: "Create an AWS Lambda@Edge function to add an Expires header to HTTP responses. Configure", correct: false },
                { id: 4, text: "Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment,", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A company runs its application by using Amazon EC2 instances and AWS Lambda functions. The \nEC2 instances run in private subnets of a VPC. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for 1 year. The number of Lambda functions that the application uses will \nincrease during the 1-year period. The company must minimize costs on all application \nresources. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to the private subnets", correct: false },
                { id: 1, text: "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to new public subnets in", correct: false },
                { id: 2, text: "Purchase a Compute Savings Plan. Connect the Lambda functions to the private subnets that", correct: true },
                { id: 3, text: "Purchase a Compute Savings Plan. Keep the Lambda functions in the Lambda service VPC.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nCompute Savings Plan: This plan offers significant discounts on Lambda functions compared to Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A company has deployed a multi-account strategy on AWS by using AWS Control Tower. The \ncompany has provided individual AWS accounts to each of its developers. The company wants to \nimplement controls to limit AWS resource costs that the developers incur. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Instruct each developer to tag all their resources with a tag that has a key of CostCenter and a", correct: false },
                { id: 1, text: "Use AWS Budgets to establish budgets for each developer account. Set up budget alerts for", correct: true },
                { id: 2, text: "Use AWS Cost Explorer to monitor and report on costs for each developer account. Configure", correct: false },
                { id: 3, text: "Use AWS Service Catalog to allow developers to launch resources within a limited cost range.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nTaking into consideration that AWS Budgets is allowing to will inform you that you exceeded budged and execute actions like for example IAM actions to prevent running new resources in cloud, I think this option is a good and resonable move. In case of need budged can be always increased and \"chains\" disabled. https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A solutions architect is designing a three-tier web application. The architecture consists of an \ninternet-facing Application Load Balancer (ALB) and a web tier that is hosted on Amazon EC2 \ninstances in private subnets. The application tier with the business logic runs on EC2 instances in \nprivate subnets. The database tier consists of Microsoft SQL Server that runs on EC2 instances \nin private subnets. Security is a high priority for the company. \n \nWhich combination of security group configurations should the solutions architect use? (Choose \nthree.)",
            options: [
                { id: 0, text: "Configure the security group for the web tier to allow inbound HTTPS traffic from the security", correct: true },
                { id: 1, text: "Configure the security group for the web tier to allow outbound HTTPS traffic to 0.0.0.0/0.", correct: false },
                { id: 2, text: "Configure the security group for the database tier to allow inbound Microsoft SQL Server traffic", correct: false },
                { id: 3, text: "Configure the security group for the database tier to allow outbound HTTPS traffic and Microsoft", correct: false },
                { id: 4, text: "Configure the security group for the application tier to allow inbound HTTPS traffic from the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSecurity Group is protecting instances, it's statefull. by defoult is allowing for outgoing traffic but not incomming. hence we need to allow for inboud traffic. path looks like below ALB >>HTTPS>> WEB tier >>HTTPS>> Application >>SQL traffic>> SQL DB hence we need allow for incoming https traffic on web tier then incomming http on app tier and on the end for incomming sql traffic on DB tier\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company has released a new version of its production application. The company's workload \nuses Amazon EC2, AWS Lambda, AWS Fargate, and Amazon SageMaker. \n \nThe company wants to cost optimize the workload now that usage is at a steady state. The \ncompany wants to cover the most services with the fewest savings plans. \n \nWhich combination of savings plans will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Purchase an EC2 Instance Savings Plan for Amazon EC2 and SageMaker.", correct: false },
                { id: 1, text: "Purchase a Compute Savings Plan for Amazon EC2, Lambda, and SageMaker.", correct: false },
                { id: 2, text: "Purchase a SageMaker Savings Plan.", correct: true },
                { id: 3, text: "Purchase a Compute Savings Plan for Lambda, Fargate, and Amazon EC2.", correct: false },
                { id: 4, text: "Purchase an EC2 Instance Savings Plan for Amazon EC2 and Fargate.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/savingsplans/ml-pricing/ https://aws.amazon.com/savingsplans/compute-pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 9,
            text: "A company uses a Microsoft SQL Server database. The company's applications are connected to \nthe database. The company wants to migrate to an Amazon Aurora PostgreSQL database with \nminimal changes to the application code. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the", correct: false },
                { id: 1, text: "Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.", correct: true },
                { id: 2, text: "Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT)", correct: false },
                { id: 3, text: "Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL.", correct: false },
                { id: 4, text: "Use AWS Database Migration Service (AWS DMS) to rewrite the SQL queries in the applications.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDMS will allow for DATABASE migration and use AWS Schema Conversion Tool (AWS SCT) to create some or all of the target tables, indexes, views, triggers, and so on. https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html To minimalize amount of code which need to me changes we need to use babelfish https://aws.amazon.com/rds/aurora/babelfish/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic \nBlock Store (Amazon EBS) as the attached storage. \n \nA solutions architect must design a solution to ensure that all newly created Amazon EBS \nvolumes are encrypted by default. The solution must also prevent the creation of unencrypted \nEBS volumes. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the EC2 account attributes to always encrypt new EBS volumes.", correct: true },
                { id: 1, text: "Use AWS Config. Configure the encrypted-volumes identifier. Apply the default AWS Key", correct: false },
                { id: 2, text: "Configure AWS Systems Manager to create encrypted copies of the EBS volumes. Reconfigure", correct: false },
                { id: 3, text: "Create a customer managed key in AWS Key Management Service (AWS KMS). Configure AWS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://repost.aws/knowledge-center/ebs-automatic-encryption\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An ecommerce company wants to collect user clickstream data from the company's website for \nreal-time analysis. The website experiences fluctuating traffic patterns throughout the day. The \ncompany needs a scalable solution that can adapt to varying levels of traffic. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process", correct: false },
                { id: 2, text: "Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process", correct: false },
                { id: 3, text: "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nApache Flink (previously known as Amazon Kinesis Data Analytics) seems to not allowing sent data directly to Lambda... Glue is allowing to integrate data from couple of sources in to one. https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A global company runs its workloads on AWS. The company's application uses Amazon S3 \nbuckets across AWS Regions for sensitive data storage and analysis. The company stores \nmillions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that \nare not versioning-enabled. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up an AWS CloudTrail event that has a rule to identify all S3 buckets that are not versioning-", correct: false },
                { id: 1, text: "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across", correct: true },
                { id: 2, text: "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled", correct: false },
                { id: 3, text: "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 Sorage Lens \"can also identify buckets that aren't following data-protection best practices, such as using S3 Replication or S3 Versioning. \" https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_basics_metrics_recomm endations.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company needs to optimize its Amazon S3 storage costs for an application that generates \nmany files that cannot be recreated. Each file is approximately 5 MB and is stored in Amazon S3 \nStandard storage. \n \nThe company must store the files for 4 years before the files can be deleted. The files must be \nimmediately accessible. The files are frequently accessed in the first 30 days of object creation, \nbut they are rarely accessed after the first 30 days. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create an S3 Lifecycle policy to move the files to S3 Glacier Instant Retrieval 30 days after object", correct: false },
                { id: 1, text: "Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-", correct: false },
                { id: 2, text: "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-", correct: true },
                { id: 3, text: "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nRequirements: - frequently accessed for 30 days - lower cost Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 14,
            text: "A company runs its critical storage application in the AWS Cloud. The application uses Amazon \nS3 in two AWS Regions. The company wants the application to send remote user data to the \nnearest S3 bucket with no public network congestion. The company also wants the application to \nfail over with the least amount of management of Amazon S3. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Implement an active-active design between the two Regions. Configure the application to use the", correct: false },
                { id: 1, text: "Use an active-passive configuration with S3 Multi-Region Access Points. Create a global endpoint", correct: false },
                { id: 2, text: "Send user data to the regional S3 endpoints closest to the user. Configure an S3 cross-account", correct: false },
                { id: 3, text: "Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nUsing a Multi-region Accesspoint in an Active-Active setup will send data to the closest Region, without accessing the internet: \"send remote user data to the nearest S3 bucket with no public network congestion\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company is migrating a data center from its on-premises location to AWS. The company has \nseveral legacy applications that are hosted on individual virtual servers. Changes to the \napplication designs cannot be made. \n \nEach individual virtual server currently runs as its own EC2 instance. A solutions architect needs \nto ensure that the applications are reliable and fault tolerant after migration to AWS. The \napplications will run on Amazon EC2 instances. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Auto Scaling group that has a minimum of one and a maximum of one. Create an", correct: false },
                { id: 1, text: "Use AWS Backup to create an hourly backup of the EC2 instance that hosts each application.", correct: false },
                { id: 2, text: "Create an Amazon Machine Image (AMI) of each application instance. Launch two new EC2", correct: true },
                { id: 3, text: "Use AWS Mitigation Hub Refactor Spaces to migrate each application off the EC2 instance.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A company wants to isolate its workloads by creating an AWS account for each workload. The \ncompany needs a solution that centrally manages networking components for the workloads. The \nsolution also must create accounts with automatic security controls (guardrails). \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS Control Tower to deploy accounts. Create a networking account that has a VPC with", correct: true },
                { id: 1, text: "Use AWS Organizations to deploy accounts. Create a networking account that has a VPC with", correct: false },
                { id: 2, text: "Use AWS Control Tower to deploy accounts. Deploy a VPC in each workload account. Configure", correct: false },
                { id: 3, text: "Use AWS Organizations to deploy accounts. Deploy a VPC in each workload account. Configure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Control Tower provides a pre-packaged set of guardrails (policies) and blueprints (best- practice configurations) to ensure that the environment complies with security and compliance standards. It’s designed to simplify the process of creating and managing a multi-account AWS environment while maintaining security and compliance. https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The website serves static content. Website traffic is increasing. The company wants to \nminimize the website hosting costs. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Move the website to an Amazon S3 bucket. Configure an Amazon CloudFront distribution for the", correct: true },
                { id: 1, text: "Move the website to an Amazon S3 bucket. Configure an Amazon ElastiCache cluster for the S3", correct: false },
                { id: 2, text: "Move the website to AWS Amplify. Configure an ALB to resolve to the Amplify website.", correct: false },
                { id: 3, text: "Move the website to AWS Amplify. Configure EC2 instances to cache the website.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAmazon CloudFront: Uses the durable storage of Amazon Simple Storage Service (Amazon S3) - This solution creates an Amazon S3 bucket to host your static website’s content. To update your website, just upload your new files to the S3 bucket. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/getting-started-secure- static-website-cloudformation-template.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 18,
            text: "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n458 \nA company is implementing a shared storage solution for a media application that the company \nhosts on AWS. The company needs the ability to use SMB clients to access stored data. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Create an AWS Storage Gateway Volume Gateway. Create a file share that uses the required", correct: false },
                { id: 1, text: "Create an AWS Storage Gateway Tape Gateway. Configure tapes to use Amazon S3. Connect", correct: false },
                { id: 2, text: "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the", correct: false },
                { id: 3, text: "Create an Amazon FSx for Windows File Server file system. Connect the application server to the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nhttps://aws.amazon.com/fsx/windows/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company is designing its production application's disaster recovery (DR) strategy. The \napplication is backed by a MySQL database on an Amazon Aurora cluster in the us-east-1 \nRegion. The company has chosen the us-west-1 Region as its DR Region. \n \nThe company's target recovery point objective (RPO) is 5 minutes and the target recovery time \nobjective (RTO) is 20 minutes. The company wants to minimize configuration changes. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create an Aurora read replica in us-west-1 similar in size to the production application's Aurora", correct: false },
                { id: 1, text: "Convert the Aurora cluster to an Aurora global database. Configure managed failover.", correct: true },
                { id: 2, text: "Create a new Aurora cluster in us-west-1 that has Cross-Region Replication.", correct: false },
                { id: 3, text: "Create a new Aurora cluster in us-west-1. Use AWS Database Migration Service (AWS DMS) to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCross-Region disaster recovery If your primary Region suffers a performance degradation or outage, you can promote one of the secondary Regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute, even in the event of a complete Regional outage. This provides your application with an effective recovery point objective (RPO) of 1 second and a recovery time objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan. https://aws.amazon.com/rds/aurora/global-database/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 20,
            text: "A company runs a critical data analysis job each week before the first day of the work week. The \njob requires at least 1 hour to complete the analysis. The job is stateful and cannot tolerate \ninterruptions. The company needs a solution to run the job on AWS. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n459",
            options: [
                { id: 0, text: "Create a container for the job. Schedule the job to run as an AWS Fargate task on an Amazon", correct: true },
                { id: 1, text: "Configure the job to run in an AWS Lambda function. Create a scheduled rule in Amazon", correct: false },
                { id: 2, text: "Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux.", correct: false },
                { id: 3, text: "Configure an AWS DataSync task to run the job. Configure a cron expression to run the task on a", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A company runs workloads in the AWS Cloud. The company wants to centrally collect security \ndata to assess security across the entire company and to improve workload protection. \n \nWhich solution will meet these requirements with the LEAST development effort?",
            options: [
                { id: 0, text: "Configure a data lake in AWS Lake Formation. Use AWS Glue crawlers to ingest the security", correct: false },
                { id: 1, text: "Configure an AWS Lambda function to collect the security data in .csv format. Upload the data to", correct: false },
                { id: 2, text: "Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an", correct: true },
                { id: 3, text: "Configure an AWS Database Migration Service (AWS DMS) replication instance to load the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon Security Lake automatically centralizes security data from AWS environments, you can get a more complete understanding of your security data across your entire organization. You can also improve the protection.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A company is migrating five on-premises applications to VPCs in the AWS Cloud. Each \napplication is currently deployed in isolated virtual networks on premises and should be deployed \nsimilarly in the AWS Cloud. The applications need to reach a shared services VPC. All the \napplications must be able to communicate with each other. \n \nIf the migration is successful, the company will repeat the migration process for more than 100 \napplications. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Deploy software VPN tunnels between the application VPCs and the shared services VPC. Add", correct: false },
                { id: 1, text: "Deploy VPC peering connections between the application VPCs and the shared services VPC.", correct: false },
                { id: 2, text: "Deploy an AWS Direct Connect connection between the application VPCs and the shared", correct: false },
                { id: 3, text: "Deploy a transit gateway with associations between the transit gateway and the application VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nIt will allow for inter-VPC communication for all 5 applications/VPC, reach shared resource/VPC and in the future it will be easy to allow for inter-communication between even 100 VPCs (applications). https://aws.amazon.com/transit-gateway/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A company wants to use Amazon Elastic Container Service (Amazon ECS) to run its on-premises \napplication in a hybrid environment. The application currently runs on containers on premises. \n \nThe company needs a single container solution that can scale in an on-premises, hybrid, or cloud \nenvironment. The company must run new application containers in the AWS Cloud and must use \na load balancer for HTTP traffic. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application", correct: true },
                { id: 1, text: "Set up an Application Load Balancer for cloud ECS services.", correct: false },
                { id: 2, text: "Set up a Network Load Balancer for cloud ECS services.", correct: false },
                { id: 3, text: "Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud", correct: false },
                { id: 4, text: "Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nWe need to load-balance HTTP traffic hence Application Load Balancer is needed. Because Customer want to use container solution we need to use ECS with Fargate which will lunch cloud applications. To run on-premises applications in containers we need to use ECS Anyware. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "A company is migrating its workloads to AWS. The company has sensitive and critical data in on-\npremises relational databases that run on SQL Server instances. \n \nThe company wants to use the AWS Cloud to increase security and reduce operational overhead \nfor the databases. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the databases to Amazon EC2 instances. Use an AWS Key Management Service (AWS", correct: false },
                { id: 1, text: "Migrate the databases to a Multi-AZ Amazon RDS for SQL Server DB instance. Use an AWS Key", correct: true },
                { id: 2, text: "Migrate the data to an Amazon S3 bucket. Use Amazon Macie to ensure data security.", correct: false },
                { id: 3, text: "Migrate the databases to an Amazon DynamoDB table. Use Amazon CloudWatch Logs to ensure", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "A company wants to migrate an application to AWS. The company wants to increase the \napplication's current availability. The company wants to use AWS WAF in the application's \narchitecture. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the", correct: true },
                { id: 1, text: "Create a cluster placement group that contains multiple Amazon EC2 instances that hosts the", correct: false },
                { id: 2, text: "Create two Amazon EC2 instances that host the application across two Availability Zones.", correct: false },
                { id: 3, text: "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A company manages a data lake in an Amazon S3 bucket that numerous applications access. \nThe S3 bucket contains a unique prefix for each application. The company wants to restrict each \napplication to its specific prefix and to have granular control of the objects under each prefix. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create dedicated S3 access points and access point policies for each application.", correct: true },
                { id: 1, text: "Create an S3 Batch Operations job to set the ACL permissions for each object in the S3 bucket.", correct: false },
                { id: 2, text: "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create replication", correct: false },
                { id: 3, text: "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create dedicated", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-policies.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A company has an application that customers use to upload images to an Amazon S3 bucket. \nEach night, the company launches an Amazon EC2 Spot Fleet that processes all the images that \nthe company received that day. The processing for each image takes 2 minutes and requires 512 \nMB of memory. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n462 \n \nA solutions architect needs to change the application to process the images when the images are \nuploaded. \n \nWhich change will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue", correct: true },
                { id: 1, text: "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue", correct: false },
                { id: 2, text: "Use S3 Event Notifications to publish a message with image details to an Amazon Simple", correct: false },
                { id: 3, text: "Use S3 Event Notifications to publish a message with image details to an Amazon Simple", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nWhen using SQS we will be sure that all images will be processed and hence to process we need 2 min and 512 MB of memory (Lambda is allowing upto 15 min and upto10K MB) Lambda should be perfect scalable solution which will allow for almost in real time image processing.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 28,
            text: "A company wants to improve the availability and performance of its hybrid application. The \napplication consists of a stateful TCP-based workload hosted on Amazon EC2 instances in \ndifferent AWS Regions and a stateless UDP-based workload hosted on premises. \n \nWhich combination of actions should a solutions architect take to improve availability and \nperformance? (Choose two.)",
            options: [
                { id: 0, text: "Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints.", correct: true },
                { id: 1, text: "Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-", correct: false },
                { id: 2, text: "Configure two Application Load Balancers in each Region. The first will route to the EC2", correct: false },
                { id: 3, text: "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a", correct: false },
                { id: 4, text: "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure an", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A company runs a self-managed Microsoft SQL Server on Amazon EC2 instances and Amazon \nElastic Block Store (Amazon EBS). Daily snapshots are taken of the EBS volumes. \n \nRecently, all the company's EBS snapshots were accidentally deleted while running a snapshot \ncleaning script that deletes all expired EBS snapshots. A solutions architect needs to update the \narchitecture to prevent data loss without retaining EBS snapshots indefinitely. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n463 \n \nWhich solution will meet these requirements with the LEAST development effort?",
            options: [
                { id: 0, text: "Change the IAM policy of the user to deny EBS snapshot deletion.", correct: false },
                { id: 1, text: "Copy the EBS snapshots to another AWS Region after completing the snapshots daily.", correct: false },
                { id: 2, text: "Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots.", correct: true },
                { id: 3, text: "Copy EBS snapshots to Amazon S3 Standard-Infrequent Access (S3 Standard-IA).", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/aws/new-recycle-bin-for-ebs-snapshots/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company wants to use an AWS CloudFormation stack for its application in a test environment. \nThe company stores the CloudFormation template in an Amazon S3 bucket that blocks public \naccess. The company wants to grant CloudFormation access to the template in the S3 bucket \nbased on specific user requests to create the test environment. The solution must follow security \nbest practices. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a gateway VPC endpoint for Amazon S3. Configure the CloudFormation stack to use the", correct: false },
                { id: 1, text: "Create an Amazon API Gateway REST API that has the S3 bucket as the target. Configure the", correct: false },
                { id: 2, text: "Create a presigned URL for the template object. Configure the CloudFormation stack to use the", correct: true },
                { id: 3, text: "Allow public access to the template object in the S3 bucket. Block the public access after the test", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company has applications that run in an organization in AWS Organizations. The company \noutsources operational support of the applications. The company needs to provide access for the \nexternal support engineers without compromising security. \n \nThe external support engineers need access to the AWS Management Console. The external \nsupport engineers also need operating system access to the company's fleet ofAmazon EC2 \ninstances that run Amazon Linux in private subnets. \n \nWhich solution will meet these requirements MOST securely?",
            options: [
                { id: 0, text: "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an", correct: true },
                { id: 1, text: "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an", correct: false },
                { id: 2, text: "Confirm that all instances have a security group that allows SSH access only from the external", correct: false },
                { id: 3, text: "Create a bastion host in a public subnet. Set up the bastion host security group to allow access", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nSystems Manager Session Manager allows secure, auditable, and controlled access to your EC2 instances without needing to open SSH ports or manage SSH keys, reducing the attack surface. Local IAM user credentials are less secure and harder to manage at scale compared to using IAM Identity Center.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A company uses Amazon RDS for PostgreSQL to run its applications in the us-east-1 Region. \nThe company also uses machine learning (ML) models to forecast annual revenue based on near \nreal-time reports. The reports are generated by using the same RDS for PostgreSQL database. \nThe database performance slows during business hours. The company needs to improve \ndatabase performance. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create a cross-Region read replica. Configure the reports to be generated from the read replica.", correct: false },
                { id: 1, text: "Activate Multi-AZ DB instance deployment for RDS for PostgreSQL. Configure the reports to be", correct: false },
                { id: 2, text: "Use AWS Data Migration Service (AWS DMS) to logically replicate data to a new database.", correct: false },
                { id: 3, text: "Create a read replica in us-east-1. Configure the reports to be generated from the read replica.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nRead replicas are typically less expensive than setting up a cross-Region replica or activating Multi-AZ deployments. You only pay for the additional read replica, without the overhead costs associated with cross-Region data transfer or maintaining a synchronous standby in Multi-AZ setups.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A company hosts its multi-tier, public web application in the AWS Cloud. The web application \nruns on Amazon EC2 instances, and its database runs on Amazon RDS. The company is \nanticipating a large increase in sales during an upcoming holiday weekend. A solutions architect \nneeds to build a solution to analyze the performance of the web application with a granularity of \nno more than 2 minutes. \n \nWhat should the solutions architect do to meet this requirement?",
            options: [
                { id: 0, text: "Send Amazon CloudWatch logs to Amazon Redshift. Use Amazon QuickS ght to perform further", correct: false },
                { id: 1, text: "Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform", correct: true },
                { id: 2, text: "Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon", correct: false },
                { id: 3, text: "Send EC2 logs to Amazon S3. Use Amazon Redshift to fetch logs from the S3 bucket to process", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A company runs an application that stores and shares photos. Users upload the photos to an \nAmazon S3 bucket. Every day, users upload approximately 150 photos. The company wants to \ndesign a solution that creates a thumbnail of each new photo and stores the thumbnail in a \nsecond S3 bucket. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a long-", correct: false },
                { id: 1, text: "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a memory-", correct: false },
                { id: 2, text: "Configure an S3 event notification to invoke an AWS Lambda function each time a user uploads a", correct: true },
                { id: 3, text: "Configure S3 Storage Lens to invoke an AWS Lambda function each time a user uploads a new", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A company has stored millions of objects across multiple prefixes in an Amazon S3 bucket by \nusing the Amazon S3 Glacier Deep Archive storage class. The company needs to delete all data \nolder than 3 years except for a subset of data that must be retained. The company has identified \nthe data that must be retained and wants to implement a serverless solution. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use S3 Inventory to list all objects. Use the AWS CLI to create a script that runs on an Amazon", correct: false },
                { id: 1, text: "Use AWS Batch to delete objects older than 3 years except for the data that must be retained.", correct: false },
                { id: 2, text: "Provision an AWS Glue crawler to query objects older than 3 years. Save the manifest file of old", correct: false },
                { id: 3, text: "Enable S3 Inventory. Create an AWS Lambda function to filter and delete objects. Invoke the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A company is building an application on AWS. The application uses multiple AWS Lambda \nfunctions to retrieve sensitive data from a single Amazon S3 bucket for processing. The company \nmust ensure that only authorized Lambda functions can access the data. The solution must \ncomply with the principle of least privilege. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Grant full S3 bucket access to all Lambda functions through a shared IAM role.", correct: false },
                { id: 1, text: "Configure the Lambda functions to run within a VPC. Configure a bucket policy to grant access", correct: false },
                { id: 2, text: "Create individual IAM roles for each Lambda function. Grant the IAM roles access to the S3", correct: true },
                { id: 3, text: "Configure a bucket policy granting access to the Lambda functions based on their function ARNs.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has developed a non-production application that is composed of multiple \nmicroservices for each of the company's business units. A single development team maintains all \nthe microservices. \n \nThe current architecture uses a static web frontend and a Java-based backend that contains the \napplication logic. The architecture also uses a MySQL database that the company hosts on an \nAmazon EC2 instance. \n \nThe company needs to ensure that the application is secure and available globally. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon CloudFront and AWS Amplify to host the static web frontend. Refactor the", correct: false },
                { id: 1, text: "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the", correct: true },
                { id: 2, text: "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the", correct: false },
                { id: 3, text: "Use Amazon S3 to host the static web frontend. Refactor the microservices to use AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A video game company is deploying a new gaming application to its global users. The company \nrequires a solution that will provide near real-time reviews and rankings of the players. \n \nA solutions architect must design a solution to provide fast access to the data. The solution must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n467 \nalso ensure the data persists on disks in the event that the company restarts the application. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. Store the", correct: false },
                { id: 1, text: "Create Amazon EC2 instances in multiple AWS Regions. Store the player data on the EC2", correct: false },
                { id: 2, text: "Deploy an Amazon ElastiCache for Redis duster. Store the player data in the ElastiCache cluster.", correct: true },
                { id: 3, text: "Deploy an Amazon ElastiCache for Memcached duster. Store the player data in the ElastiCache", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon ElastiCache for Redis provides in-memory caching which ensures low latency and high throughput, perfect for near real-time access to player reviews and rankings. Redis supports data persistence by snapshotting data to disk (RDB snapshots) and appending changes to a log (AOF), ensuring that the data is not lost even if the application restarts.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company is designing an application on AWS that processes sensitive data. The application \nstores and processes financial data for multiple customers. \n \nTo meet compliance requirements, the data for each customer must be encrypted separately at \nrest by using a secure, centralized key management solution. The company wants to use AWS \nKey Management Service (AWS KMS) to implement encryption. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Generate a unique encryption key for each customer. Store the keys in an Amazon S3 bucket.", correct: false },
                { id: 1, text: "Deploy a hardware security appliance in the AWS environment that securely stores customer-", correct: false },
                { id: 2, text: "Create a single AWS KMS key to encrypt all sensitive data across the application.", correct: false },
                { id: 3, text: "Create separate AWS KMS keys for each customer's data that have granular access control and", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nWhile enabling server-side encryption in S3 can manage encryption, it does not offer the same level of control and auditing as AWS KMS. Managing individual keys manually in S3 would also increase operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A company needs to design a resilient web application to process customer orders. The web \napplication must automatically handle increases in web traffic and application usage without \naffecting the customer experience or losing customer orders. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n468",
            options: [
                { id: 0, text: "Use a NAT gateway to manage web traffic. Use Amazon EC2 Auto Scaling groups to receive,", correct: false },
                { id: 1, text: "Use a Network Load Balancer (NLB) to manage web traffic. Use an Application Load Balancer to", correct: false },
                { id: 2, text: "Use a Gateway Load Balancer (GWLB) to manage web traffic. Use Amazon Elastic Container", correct: false },
                { id: 3, text: "Use an Application Load Balancer to manage web traffic. Use Amazon EC2 Auto Scaling groups", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company is using AWS DataSync to migrate millions of files from an on-premises system to \nAWS. The files are 10 KB in size on average. \n \nThe company wants to use Amazon S3 for file storage. For the first year after the migration, the \nfiles will be accessed once or twice and must be immediately available. After 1 year, the files \nmust be archived for at least 7 years. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use an archive tool to group the files into large objects. Use DataSync to migrate the objects.", correct: false },
                { id: 1, text: "Use an archive tool to group the files into large objects. Use DataSync to copy the objects to S3", correct: false },
                { id: 2, text: "Configure the destination storage class for the files as S3 Glacier Instant Retrieval. Use a lifecycle", correct: false },
                { id: 3, text: "Configure a DataSync task to transfer the files to S3 Standard-Infrequent Access (S3 Standard-", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 42,
            text: "A company recently performed a lift and shift migration of its on-premises Oracle database \nworkload to run on an Amazon EC2 memory optimized Linux instance. The EC2 Linux instance \nuses a 1 TB Provisioned IOPS SSD (io1) EBS volume with 64,000 IOPS. \n \nThe database storage performance after the migration is slower than the performance of the on-\npremises database. \n \nWhich solution will improve storage performance?",
            options: [
                { id: 0, text: "Add more Provisioned IOPS SSD (io1) EBS volumes. Use OS commands to create a Logical", correct: true },
                { id: 1, text: "Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS.", correct: false },
                { id: 2, text: "Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB.", correct: false },
                { id: 3, text: "Change the EC2 Linux instance to a storage optimized instance type. Do not change the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nThe maximum provisioned IOPS for io1 is 64000 and hence you can achieve higher aggregate performance by adding more io1 volumes.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A company is migrating from a monolithic architecture for a web application that is hosted on \nAmazon EC2 to a serverless microservices architecture. The company wants to use AWS \nservices that support an event-driven, loosely coupled architecture. The company wants to use \nthe publish/subscribe (pub/sub) pattern. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes", correct: false },
                { id: 1, text: "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes", correct: true },
                { id: 2, text: "Configure an Amazon API Gateway WebSocket API to write to a data stream in Amazon Kinesis", correct: false },
                { id: 3, text: "Configure an Amazon API Gateway HTTP API to invoke an AWS Lambda function that publishes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A company recently migrated a monolithic application to an Amazon EC2 instance and Amazon \nRDS. The application has tightly coupled modules. The existing design of the application gives \nthe application the ability to run on only a single EC2 instance. \n \nThe company has noticed high CPU utilization on the EC2 instance during peak usage times. The \nhigh CPU utilization corresponds to degraded performance on Amazon RDS for read requests. \nThe company wants to reduce the high CPU utilization and improve read request performance. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto", correct: false },
                { id: 1, text: "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto", correct: true },
                { id: 2, text: "Configure an Auto Scaling group with a minimum size of 1 and maximum size of 2. Resize the", correct: false },
                { id: 3, text: "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThis approach addresses both the high CPU utilization on the EC2 instance and the degraded read performance on the RDS instance effectively.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company needs to grant a team of developers access to the company's AWS resources. The \ncompany must maintain a high level of security for the resources. \n \nThe company requires an access control solution that will prevent unauthorized access to the \nsensitive data. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Share the IAM user credentials for each development team member with the rest of the team to", correct: false },
                { id: 1, text: "Define IAM roles that have fine-grained permissions based on the principle of least privilege.", correct: true },
                { id: 2, text: "Create IAM access keys to grant programmatic access to AWS resources. Allow only developers", correct: false },
                { id: 3, text: "Create an AWS Cognito user pool. Grant developers access to AWS resources by using the user", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company hosts a monolithic web application on an Amazon EC2 instance. Application users \nhave recently reported poor performance at specific times. Analysis of Amazon CloudWatch \nmetrics shows that CPU utilization is 100% during the periods of poor performance. \n \nThe company wants to resolve this performance issue and improve application availability. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale vertically.", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) from the web server. Reference the AMI in a new", correct: true },
                { id: 2, text: "Create an Auto Scaling group and an Application Load Balancer to scale vertically.", correct: false },
                { id: 3, text: "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale", correct: false },
                { id: 4, text: "Create an Auto Scaling group and an Application Load Balancer to scale horizontally.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 47,
            text: "A company runs all its business applications in the AWS Cloud. The company uses AWS \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n471 \nOrganizations to manage multiple AWS accounts. \n \nA solutions architect needs to review all permissions that are granted to IAM users to determine \nwhich IAM users have more permissions than required. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Use Network Access Analyzer to review all access permissions in the company's AWS accounts.", correct: false },
                { id: 1, text: "Create an AWS CloudWatch alarm that activates when an IAM user creates or modifies", correct: false },
                { id: 2, text: "Use AWS Identity and Access Management (IAM) Access Analyzer to review all the company's", correct: true },
                { id: 3, text: "Use Amazon Inspector to find vulnerabilities in existing IAM policies.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A company needs to implement a new data retention policy for regulatory compliance. As part of \nthis policy, sensitive documents that are stored in an Amazon S3 bucket must be protected from \ndeletion or modification for a fixed period of time. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Activate S3 Object Lock on the required objects and enable governance mode.", correct: false },
                { id: 1, text: "Activate S3 Object Lock on the required objects and enable compliance mode.", correct: true },
                { id: 2, text: "Enable versioning on the S3 bucket. Set a lifecycle policy to delete the objects after a specified", correct: false },
                { id: 3, text: "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Flexible Retrieval for the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company runs its customer-facing web application on containers. The workload uses Amazon \nElastic Container Service (Amazon ECS) on AWS Fargate. The web application is resource \nintensive. \n \nThe web application needs to be available 24 hours a day, 7 days a week for customers. The \ncompany expects the application to experience short bursts of high traffic. The workload must be \nhighly available. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure an ECS capacity provider with Fargate. Conduct load testing by using a third-party tool.", correct: false },
                { id: 1, text: "Configure an ECS capacity provider with Fargate for steady state and Fargate Spot for burst", correct: true },
                { id: 2, text: "Configure an ECS capacity provider with Fargate Spot for steady state and Fargate for burst", correct: false },
                { id: 3, text: "Configure an ECS capacity provider with Fargate. Use AWS Compute Optimizer to rightsize the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nThis combination leverages the cost benefits of Fargate Spot for burst traffic while ensuring steady performance with regular Fargate instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 50,
            text: "A company is building an application in the AWS Cloud. The application is hosted on Amazon \nEC2 instances behind an Application Load Balancer (ALB). The company uses Amazon Route 53 \nfor the DNS. \n \nThe company needs a managed solution with proactive engagement to detect against DDoS \nattacks. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Enable AWS Config. Configure an AWS Config managed rule that detects DDoS attacks.", correct: false },
                { id: 1, text: "Enable AWS WAF on the ALCreate an AWS WAF web ACL with rules to detect and prevent", correct: false },
                { id: 2, text: "Store the ALB access logs in an Amazon S3 bucket. Configure Amazon GuardDuty to detect and", correct: false },
                { id: 3, text: "Subscribe to AWS Shield Advanced. Configure hosted zones in Route 53. Add ALB resources as", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 51,
            text: "A company hosts a video streaming web application in a VPC. The company uses a Network \nLoad Balancer (NLB) to handle TCP traffic for real-time data processing. There have been \nunauthorized attempts to access the application. \n \nThe company wants to improve application security with minimal architectural change to prevent \nunauthorized attempts to access the application. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Implement a series of AWS WAF rules directly on the NLB to filter out unauthorized traffic.", correct: false },
                { id: 1, text: "Recreate the NLB with a security group to allow only trusted IP addresses.", correct: true },
                { id: 2, text: "Deploy a second NLB in parallel with the existing NLB configured with a strict IP address allow", correct: false },
                { id: 3, text: "Use AWS Shield Advanced to provide enhanced DDoS protection and prevent unauthorized", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A healthcare company is developing an AWS Lambda function that publishes notifications to an \nencrypted Amazon Simple Notification Service (Amazon SNS) topic. The notifications contain \nprotected health information (PHI). \n \nThe SNS topic uses AWS Key Management Service (AWS KMS) customer managed keys for \nencryption. The company must ensure that the application has the necessary permissions to \npublish messages securely to the SNS topic. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n473 \n \nWhich combination of steps will meet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Create a resource policy for the SNS topic that allows the Lambda function to publish messages", correct: true },
                { id: 1, text: "Use server-side encryption with AWS KMS keys (SSE-KMS) for the SNS topic instead of", correct: false },
                { id: 2, text: "Create a resource policy for the encryption key that the SNS topic uses that has the necessary", correct: false },
                { id: 3, text: "Specify the Lambda function's Amazon Resource Name (ARN) in the SNS topic's resource policy.", correct: false },
                { id: 4, text: "Associate an Amazon API Gateway HTTP API with the SNS topic to control access to the topic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A company has an employee web portal. Employees log in to the portal to view payroll details. \nThe company is developing a new system to give employees the ability to upload scanned \ndocuments for reimbursement. The company runs a program to extract text-based data from the \ndocuments and attach the extracted information to each employee's reimbursement IDs for \nprocessing. \n \nThe employee web portal requires 100% uptime. The document extract program runs infrequently \nthroughout the day on an on-demand basis. The company wants to build a scalable and cost-\neffective new system that will require minimal changes to the existing web portal. The company \ndoes not want to make any code changes. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
            options: [
                { id: 0, text: "Run Amazon EC2 On-Demand Instances in an Auto Scaling group for the web portal. Use an", correct: true },
                { id: 1, text: "Run Amazon EC2 Spot Instances in an Auto Scaling group for the web portal. Run the document", correct: false },
                { id: 2, text: "Purchase a Savings Plan to run the web portal and the document extract program. Run the web", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket to host the web portal. Use Amazon API Gateway and an AWS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 54,
            text: "A media company has a multi-account AWS environment in the us-east-1 Region. The company \nhas an Amazon Simple Notification Service (Amazon SNS) topic in a production account that \npublishes performance metrics. The company has an AWS Lambda function in an administrator \naccount to process and analyze log data. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n474 \nThe Lambda function that is in the administrator account must be invoked by messages from the \nSNS topic that is in the production account when significant metrics are reported. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Create an IAM resource policy for the Lambda function that allows Amazon SNS to invoke the", correct: true },
                { id: 1, text: "Implement an Amazon Simple Queue Service (Amazon SQS) queue in the administrator account", correct: false },
                { id: 2, text: "Create an IAM policy for the SNS topic that allows the Lambda function to subscribe to the topic.", correct: false },
                { id: 3, text: "Use an Amazon EventBridge rule in the production account to capture the SNS topic notifications.", correct: false },
                { id: 4, text: "Store performance metrics in an Amazon S3 bucket in the production account. Use Amazon", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A company is migrating an application from an on-premises location to Amazon Elastic \nKubernetes Service (Amazon EKS). The company must use a custom subnet for pods that are in \nthe company's VPC to comply with requirements. The company also needs to ensure that the \npods can communicate securely within the pods' VPC. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure AWS Transit Gateway to directly manage custom subnet configurations for the pods in", correct: false },
                { id: 1, text: "Create an AWS Direct Connect connection from the company's on-premises IP address ranges to", correct: false },
                { id: 2, text: "Use the Amazon VPC CNI plugin for Kubernetes. Define custom subnets in the VPC cluster for", correct: true },
                { id: 3, text: "Implement a Kubernetes network policy that has pod anti-affinity rules to restrict pod placement to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nThe Amazon VPC Container Network Interface (CNI) plugin is the default network plugin for Amazon EKS. It allows Kubernetes pods to receive IP addresses from a VPC's subnet and enables pods to communicate securely within the VPC as if they were native VPC resources.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 56,
            text: "A company hosts an ecommerce application that stores all data in a single Amazon RDS for \nMySQL DB instance that is fully managed by AWS. The company needs to mitigate the risk of a \nsingle point of failure. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
            options: [
                { id: 0, text: "Modify the RDS DB instance to use a Multi-AZ deployment. Apply the changes during the next", correct: true },
                { id: 1, text: "Migrate the current database to a new Amazon DynamoDB Multi-AZ deployment. Use AWS", correct: false },
                { id: 2, text: "Create a new RDS DB instance in a Multi-AZ deployment. Manually restore the data from the", correct: false },
                { id: 3, text: "Configure the DB instance in an Amazon EC2 Auto Scaling group with a minimum group size of", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 57,
            text: "A company has multiple Microsoft Windows SMB file servers and Linux NFS file servers for file \nsharing in an on-premises environment. As part of the company's AWS migration plan, the \ncompany wants to consolidate the file servers in the AWS Cloud. \n \nThe company needs a managed AWS storage service that supports both NFS and SMB access. \nThe solution must be able to share between protocols. The solution must have redundancy at the \nAvailability Zone level. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon FSx for NetApp ONTAP for storage. Configure multi-protocol access.", correct: true },
                { id: 1, text: "Create two Amazon EC2 instances. Use one EC2 instance for Windows SMB file server access", correct: false },
                { id: 2, text: "Use Amazon FSx for NetApp ONTAP for SMB access. Use Amazon FSx for Lustre for NFS", correct: false },
                { id: 3, text: "Use Amazon S3 storage. Access Amazon S3 through an Amazon S3 File Gateway.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A software company needs to upgrade a critical web application. The application currently runs \non a single Amazon EC2 instance that the company hosts in a public subnet. The EC2 instance \nruns a MySQL database. The application's DNS records are published in an Amazon Route 53 \nzone. \n \nA solutions architect must reconfigure the application to be scalable and highly available. The \nsolutions architect must also reduce MySQL read latency. \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Launch a second EC2 instance in a second AWS Region. Use a Route 53 failover routing policy", correct: false },
                { id: 1, text: "Create and configure an Auto Scaling group to launch private EC2 instances in multiple", correct: true },
                { id: 2, text: "Migrate the database to an Amazon Aurora MySQL cluster. Create the primary DB instance and", correct: false },
                { id: 3, text: "Create and configure an Auto Scaling group to launch private EC2 instances in multiple AWS", correct: false },
                { id: 4, text: "Migrate the database to an Amazon Aurora MySQL cluster with cross-Region read replicas.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company runs thousands of AWS Lambda functions. The company needs a solution to \nsecurely store sensitive information that all the Lambda functions use. The solution must also \nmanage the automatic rotation of the sensitive information. \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
            options: [
                { id: 0, text: "Create HTTP security headers by using Lambda@Edge to retrieve and create sensitive", correct: false },
                { id: 1, text: "Create a Lambda layer that retrieves sensitive information", correct: true },
                { id: 2, text: "Store sensitive information in AWS Secrets Manager", correct: false },
                { id: 3, text: "Store sensitive information in AWS Systems Manager Parameter Store", correct: false },
                { id: 4, text: "Create a Lambda consumer with dedicated throughput to retrieve sensitive information and create", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS Secrets Manager securely stores sensitive information and provides automatic rotation of secrets, reducing the need for manual management. Using a Lambda layer allows multiple Lambda functions to access the sensitive information stored in Secrets Manager without needing to duplicate retrieval logic in each function. This approach centralizes the retrieval process and reduces operational complexity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 60,
            text: "A company has an internal application that runs on Amazon EC2 instances in an Auto Scaling \ngroup. The EC2 instances are compute optimized and use Amazon Elastic Block Store (Amazon \nEBS) volumes. \n \nThe company wants to identify cost optimizations across the EC2 instances, the Auto Scaling \ngroup, and the EBS volumes. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the", correct: false },
                { id: 1, text: "Create new Amazon CloudWatch billing alerts. Check the alert statuses for cost", correct: false },
                { id: 2, text: "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances, the Auto", correct: true },
                { id: 3, text: "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances. Create a", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 61,
            text: "A company is running a media store across multiple Amazon EC2 instances distributed across \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n477 \nmultiple Availability Zones in a single VPC. The company wants a high-performing solution to \nshare data between all the EC2 instances, and prefers to keep the data within the VPC only. \n \nWhat should a solutions architect recommend?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket and call the service APIs from each instance's application", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket and configure all instances to access it as a mounted volume", correct: false },
                { id: 2, text: "Configure an Amazon Elastic Block Store (Amazon EBS) volume and mount it across all", correct: false },
                { id: 3, text: "Configure an Amazon Elastic File System (Amazon EFS) file system and mount it across all", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A company uses an Amazon RDS for MySQL instance. To prepare for end-of-year processing, \nthe company added a read replica to accommodate extra read-only queries from the company's \nreporting tool. The read replica CPU usage was 60% and the primary instance CPU usage was \n60%. \n \nAfter end-of-year activities are complete, the read replica has a constant 25% CPU usage. The \nprimary instance still has a constant 60% CPU usage. The company wants to rightsize the \ndatabase and still provide enough performance for future growth. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Delete the read replica Do not make changes to the primary instance", correct: false },
                { id: 1, text: "Resize the read replica to a smaller instance size Do not make changes to the primary instance", correct: true },
                { id: 2, text: "Resize the read replica to a larger instance size Resize the primary instance to a smaller instance", correct: false },
                { id: 3, text: "Delete the read replica Resize the primary instance to a larger instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A company is migrating its databases to Amazon RDS for PostgreSQL. The company is migrating \nits applications to Amazon EC2 instances. The company wants to optimize costs for long-running \nworkloads. \n \nWhich solution will meet this requirement MOST cost-effectively?",
            options: [
                { id: 0, text: "Use On-Demand Instances for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year", correct: false },
                { id: 1, text: "Purchase Reserved Instances for a 1 year term with the No Upfront option for the Amazon RDS", correct: false },
                { id: 2, text: "Purchase Reserved Instances for a 1 year term with the Partial Upfront option for the Amazon", correct: false },
                { id: 3, text: "Purchase Reserved Instances for a 3 year term with the All Upfront option for the Amazon RDS", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A company is using an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company \nmust ensure that Kubernetes service accounts in the EKS cluster have secure and granular \naccess to specific AWS resources by using IAM roles for service accounts (IRSA). \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Create an IAM policy that defines the required permissions Attach the policy directly to the IAM", correct: false },
                { id: 1, text: "Implement network policies within the EKS cluster to prevent Kubernetes service accounts from", correct: false },
                { id: 2, text: "Modify the EKS cluster's IAM role to include permissions for each Kubernetes service account.", correct: false },
                { id: 3, text: "Define an IAM role that includes the necessary permissions. Annotate the Kubernetes service", correct: true },
                { id: 4, text: "Set up a trust relationship between the IAM roles for the service accounts and an OpenID", correct: false },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
    ],
    test25: [
        {
            id: 0,
            text: "A company regularly uploads confidential data to Amazon S3 buckets for analysis. \n \nThe company's security policies mandate that the objects must be encrypted at rest. The \ncompany must automatically rotate the encryption key every year. The company must be able to \ntrack key rotation by using AWS CloudTrail. The company also must minimize costs for the \nencryption key. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use server-side encryption with customer-provided keys (SSE-C)", correct: false },
                { id: 1, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
                { id: 2, text: "Use server-side encryption with AWS KMS keys (SSE-KMS)", correct: true },
                { id: 3, text: "Use server-side encryption with customer managed AWS KMS keys", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company has migrated several applications to AWS in the past 3 months. The company wants \nto know the breakdown of costs for each of these applications. The company wants to receive a \nregular report that includes this information. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use AWS Budgets to download data for the past 3 months into a .csv file. Look up the desired", correct: false },
                { id: 1, text: "Load AWS Cost and Usage Reports into an Amazon RDS DB instance. Run SQL queries to get", correct: false },
                { id: 2, text: "Tag all the AWS resources with a key for cost and a value of the application's name. Activate cost", correct: true },
                { id: 3, text: "Tag all the AWS resources with a key for cost and a value of the application's name. Use the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "An ecommerce company is preparing to deploy a web application on AWS to ensure continuous \nservice for customers. The architecture includes a web application that the company hosts on \nAmazon EC2 instances, a relational database in Amazon RDS, and static assets that the \ncompany stores in Amazon S3. \n \nThe company wants to design a robust and resilient architecture for the application. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy Amazon EC2 instances in a single Availability Zone. Deploy an RDS DB instance in the", correct: false },
                { id: 1, text: "Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones.", correct: true },
                { id: 2, text: "Deploy Amazon EC2 instances in a single Availability Zone. Deploy an RDS DB instance in a", correct: false },
                { id: 3, text: "Use AWS Lambda functions to serve the web application. Use Amazon Aurora Serverless v2 for", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "An ecommerce company runs several internal applications in multiple AWS accounts. The \ncompany uses AWS Organizations to manage its AWS accounts. \n \nA security appliance in the company's networking account must inspect interactions between \napplications across AWS accounts. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy a Network Load Balancer (NLB) in the networking account to send traffic to the security", correct: false },
                { id: 1, text: "Deploy an Application Load Balancer (ALB) in the application accounts to send traffic directly to", correct: false },
                { id: 2, text: "Deploy a Gateway Load Balancer (GWLB) in the networking account to send traffic to the security", correct: true },
                { id: 3, text: "Deploy an interface VPC endpoint in the application accounts to send traffic directly to the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company runs its production workload on an Amazon Aurora MySQL DB cluster that includes \nsix Aurora Replicas. The company wants near-real-time reporting queries from one of its \ndepartments to be automatically distributed across three of the Aurora Replicas. Those three \nreplicas have a different compute and memory specification from the rest of the DB cluster. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Create and use a custom endpoint for the workload", correct: true },
                { id: 1, text: "Create a three-node cluster clone and use the reader endpoint", correct: false },
                { id: 2, text: "Use any of the instance endpoints for the selected three nodes", correct: false },
                { id: 3, text: "Use the reader endpoint to automatically distribute the read-only workload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A company runs a Node js function on a server in its on-premises data center. The data center \nstores data in a PostgreSQL database. The company stores the credentials in a connection string \nin an environment variable on the server. The company wants to migrate its application to AWS \nand to replace the Node.js application server with AWS Lambda. The company also wants to \nmigrate to Amazon RDS for PostgreSQL and to ensure that the database credentials are securely \nmanaged. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the database credentials as a parameter in AWS Systems Manager Parameter Store", correct: false },
                { id: 1, text: "Store the database credentials as a secret in AWS Secrets Manager. Configure Secrets Manager", correct: true },
                { id: 2, text: "Store the database credentials as an encrypted Lambda environment variable. Write a custom", correct: false },
                { id: 3, text: "Store the database credentials as a key in AWS Key Management Service (AWS KMS).", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A company wants to replicate existing and ongoing data changes from an on-premises Oracle \ndatabase to Amazon RDS for Oracle. The amount of data to replicate varies throughout each \nday. The company wants to use AWS Database Migration Service (AWS DMS) for data \nreplication. The solution must allocate only the capacity that the replication instance requires. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the AWS DMS replication instance with a Multi-AZ deployment to provision instances", correct: false },
                { id: 1, text: "Create an AWS DMS Serverless replication task to analyze and replicate the data while", correct: true },
                { id: 2, text: "Use Amazon EC2 Auto Scaling to scale the size of the AWS DMS replication instance up or down", correct: false },
                { id: 3, text: "Provision AWS DMS replication capacity by using Amazon Elastic Container Service (Amazon", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DMS Serverless is designed to automatically allocate and manage the necessary compute and memory resources based on the demand of the data replication workload. It scales capacity up or down according to the data replication requirements without manual intervention. This approach ensures that the replication task uses only the required capacity at any given time, optimizing costs and resources, especially given that the amount of data to replicate varies throughout the day.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A company has a multi-tier web application. The application's internal service components are \ndeployed on Amazon EC2 instances. The internal service components need to access third-party \nsoftware as a service (SaaS) APIs that are hosted on AWS. \n \nThe company needs to provide secure and private connectivity from the application's internal \nservices to the third-party SaaS application. The company needs to ensure that there is minimal \npublic internet exposure. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Implement an AWS Site-to-Site VPN to establish a secure connection with the third-party SaaS", correct: false },
                { id: 1, text: "Deploy AWS Transit Gateway to manage and route traffic between the application's VPC and the", correct: false },
                { id: 2, text: "Configure AWS PrivateLink to allow only outbound traffic from the VPC without enabling the third-", correct: false },
                { id: 3, text: "Use AWS PrivateLink to create a private connection between the application's VPC and the third-", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A solutions architect needs to connect a company's corporate network to its VPC to allow on-\npremises access to its AWS resources. The solution must provide encryption of all traffic between \nthe corporate network and the VPC at the network layer and the session layer. The solution also \nmust provide security controls to prevent unrestricted access between AWS and the on-premises \nsystems. \n \nWhich solution meets these requirements?",
            options: [
                { id: 0, text: "Configure AWS Direct Connect to connect to the VPC. Configure the VPC route tables to allow", correct: false },
                { id: 1, text: "Create an IAM policy to allow access to the AWS Management Console only from a defined set of", correct: false },
                { id: 2, text: "Configure AWS Site-to-Site VPN to connect to the VPConfigure route table entries to direct traffic", correct: true },
                { id: 3, text: "Configure AWS Transit Gateway to connect to the VPC. Configure route table entries to direct", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Direct Connect does not provide encryption by itself; it is often used in conjunction with VPN for encrypted traffic. Direct Connect primarily offers a dedicated connection and does not inherently satisfy the encryption requirement.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A company has a custom application with embedded credentials that retrieves information from a \ndatabase in an Amazon RDS for MySQL DB cluster. The company needs to make the application \nmore secure with minimal programming effort. The company has created credentials on the RDS \nfor MySQL database for the application user. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Store the credentials in AWS Key Management Service (AWS KMS). Create keys in AWS KMS.", correct: false },
                { id: 1, text: "Store the credentials in encrypted local storage. Configure the application to load the database", correct: false },
                { id: 2, text: "Store the credentials in AWS Secrets Manager. Configure the application to load the database", correct: true },
                { id: 3, text: "Store the credentials in AWS Systems Manager Parameter Store. Configure the application to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company wants to move its application to a serverless solution. The serverless solution needs \nto analyze existing data and new data by using SQL. The company stores the data in an Amazon \nS3 bucket. The data must be encrypted at rest and replicated to a different AWS Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a new S3 bucket that uses server-side encryption with AWS KMS multi-Region keys", correct: true },
                { id: 1, text: "Create a new S3 bucket that uses server-side encryption with Amazon S3 managed keys (SSE-", correct: false },
                { id: 2, text: "Configure Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side encryption", correct: false },
                { id: 3, text: "Configure S3 Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A company has a web application that has thousands of users. The application uses 8-10 user-\nuploaded images to generate AI images. Users can download the generated AI images once \nevery 6 hours. The company also has a premium user option that gives users the ability to \ndownload the generated AI images anytime. \n \nThe company uses the user-uploaded images to run AI model training twice a year. The company \nneeds a storage solution to store the images. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Move uploaded images to Amazon S3 Glacier Deep Archive. Move premium user-generated AI", correct: true },
                { id: 1, text: "Move uploaded images to Amazon S3 Glacier Deep Archive Move all generated AI images to S3", correct: false },
                { id: 2, text: "Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move", correct: false },
                { id: 3, text: "Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move all", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 12,
            text: "A company is developing machine learning (ML) models on AWS. The company is developing the \nML models as independent microservices. The microservices fetch approximately 1 GB of model \ndata from Amazon S3 at startup and load the data into memory. Users access the ML models \nthrough an asynchronous API. Users can send a request or a batch of requests. \n \nThe company provides the ML models to hundreds of users. The usage patterns for the models \nare irregular. Some models are not used for days or weeks. Other models receive batches of \nthousands of requests at a time. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the ML models as", correct: false },
                { id: 1, text: "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the ML models", correct: false },
                { id: 2, text: "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.", correct: false },
                { id: 3, text: "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an \nApplication Load Balancer (ALB). The application stores data in an Amazon Aurora MySQL DB \ncluster. \n \nThe company needs to create a disaster recovery (DR) solution. The acceptable recovery time for \nthe DR solution is up to 30 minutes. The DR solution does not need to support customer usage \nwhen the primary infrastructure is healthy. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Deploy the DR infrastructure in a second AWS Region with an ALB and an Auto Scaling group.", correct: true },
                { id: 1, text: "Deploy the DR infrastructure in a second AWS Region with an ALUpdate the Auto Scaling group", correct: false },
                { id: 2, text: "Back up the Aurora MySQL DB cluster data by using AWS Backup. Deploy the DR infrastructure", correct: false },
                { id: 3, text: "Back up the infrastructure configuration by using AWS Backup. Use the backup to create the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company is migrating its data processing application to the AWS Cloud. The application \nprocesses several short-lived batch jobs that cannot be disrupted. Data is generated after each \nbatch job is completed. The data is accessed for 30 days and retained for 2 years. \n \nThe company wants to keep the cost of running the application in the AWS Cloud as low as \npossible. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Migrate the data processing application to Amazon EC2 Spot Instances. Store the data in", correct: false },
                { id: 1, text: "Migrate the data processing application to Amazon EC2 On-Demand Instances. Store the data in", correct: false },
                { id: 2, text: "Deploy Amazon EC2 Spot Instances to run the batch jobs. Store the data in Amazon S3", correct: false },
                { id: 3, text: "Deploy Amazon EC2 On-Demand Instances to run the batch jobs. Store the data in Amazon S3", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company needs to design a hybrid network architecture. The company's workloads are \ncurrently stored in the AWS Cloud and in on-premises data centers. The workloads require \nsingle-digit latencies to communicate. The company uses an AWS Transit Gateway transit \ngateway to connect multiple VPCs. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
            options: [
                { id: 0, text: "Establish an AWS Site-to-Site VPN connection to each VPC.", correct: false },
                { id: 1, text: "Associate an AWS Direct Connect gateway with the transit gateway that is attached to the VPCs.", correct: true },
                { id: 2, text: "Establish an AWS Site-to-Site VPN connection to an AWS Direct Connect gateway.", correct: false },
                { id: 3, text: "Establish an AWS Direct Connect connection. Create a transit virtual interface (VIF) to a Direct", correct: false },
                { id: 4, text: "Associate AWS Site-to-Site VPN connections with the transit gateway that is attached to the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 16,
            text: "A global ecommerce company runs its critical workloads on AWS. The workloads use an Amazon \nRDS for PostgreSQL DB instance that is configured for a Multi-AZ deployment. \n \nCustomers have reported application timeouts when the company undergoes database failovers. \nThe company needs a resilient solution to reduce failover time. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon RDS Proxy. Assign the proxy to the DB instance.", correct: true },
                { id: 1, text: "Create a read replica for the DB instance. Move the read traffic to the read replica.", correct: false },
                { id: 2, text: "Enable Performance Insights. Monitor the CPU load to identify the timeouts.", correct: false },
                { id: 3, text: "Take regular automatic snapshots. Copy the automatic snapshots to multiple AWS Regions.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A company has multiple Amazon RDS DB instances that run in a development AWS account. All \nthe instances have tags to identify them as development resources. The company needs the \ndevelopment DB instances to run on a schedule only during business hours. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch alarm to identify RDS instances that need to be stopped. Create", correct: false },
                { id: 1, text: "Create an AWS Trusted Advisor report to identify RDS instances to be started and stopped.", correct: false },
                { id: 2, text: "Create AWS Systems Manager State Manager associations to start and stop the RDS instances.", correct: true },
                { id: 3, text: "Create an Amazon EventBridge rule that invokes AWS Lambda functions to start and stop the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAWS Systems Manager State Manager allows you to automate the process of starting and stopping RDS instances based on a defined schedule.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A consumer survey company has gathered data for several years from a specific geographic \nregion. The company stores this data in an Amazon S3 bucket in an AWS Region. \n \nThe company has started to share this data with a marketing firm in a new geographic region. \nThe company has granted the firm's AWS account access to the S3 bucket. The company wants \nto minimize the data transfer costs when the marketing firm requests data from the S3 bucket. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure the Requester Pays feature on the company's S3 bucket.", correct: true },
                { id: 1, text: "Configure S3 Cross-Region Replication (CRR) from the company's S3 bucket to one of the", correct: false },
                { id: 2, text: "Configure AWS Resource Access Manager to share the S3 bucket with the marketing firm AWS", correct: false },
                { id: 3, text: "Configure the company's S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of the", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company uses AWS to host its public ecommerce website. The website uses an AWS Global \nAccelerator accelerator for traffic from the internet. The Global Accelerator accelerator forwards \nthe traffic to an Application Load Balancer (ALB) that is the entry point for an Auto Scaling group. \n \nThe company recently identified a DDoS attack on the website. The company needs a solution to \nmitigate future attacks. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
            options: [
                { id: 0, text: "Configure an AWS WAF web ACL for the Global Accelerator accelerator to block traffic by using", correct: false },
                { id: 1, text: "Configure an AWS Lambda function to read the ALB metrics to block attacks by updating a VPC", correct: false },
                { id: 2, text: "Configure an AWS WAF web ACL on the ALB to block traffic by using rate-based rules", correct: true },
                { id: 3, text: "Configure an Amazon CloudFront distribution in front of the Global Accelerator accelerator", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nhttps://repost.aws/knowledge-center/globalaccelerator-aws-waf-filter-layer7-traffic Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A company uses an Amazon DynamoDB table to store data that the company receives from \ndevices. The DynamoDB table supports a customer-facing website to display recent activity on \ncustomer devices. The company configured the table with provisioned throughput for writes and \nreads. \n \nThe company wants to calculate performance metrics for customer device data on a daily basis. \nThe solution must have minimal effect on the table's provisioned read and write capacity. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an Amazon Athena SQL query with the Amazon Athena DynamoDB connector to calculate", correct: false },
                { id: 1, text: "Use an AWS Glue job with the AWS Glue DynamoDB export connector to calculate performance", correct: true },
                { id: 2, text: "Use an Amazon Redshift COPY command to calculate performance metrics on a recurring", correct: false },
                { id: 3, text: "Use an Amazon EMR job with an Apache Hive external table to calculate performance metrics on", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDynamoDB export connector literally \"exports\" table snapshot to s3 as dynamoDB-json object, then process on it. So it does not affect on read / write capacity on dynamoDB itself. But Athena query directly on dynamoDB so affects on read / write capacity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A solutions architect is designing the cloud architecture for a new stateless application that will be \ndeployed on AWS. The solutions architect created an Amazon Machine Image (AMI) and launch \ntemplate for the application. \n \nBased on the number of jobs that need to be processed, the processing must run in parallel while \nadding and removing application Amazon EC2 instances as needed. The application must be \nloosely coupled. The job items must be durably stored. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic to send the jobs that need to", correct: false },
                { id: 1, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs that need to be", correct: false },
                { id: 2, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs that need to be", correct: true },
                { id: 3, text: "Create an Amazon Simple Notification Service (Amazon SNS) topic to send the jobs that need to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 22,
            text: "A global ecommerce company uses a monolithic architecture. The company needs a solution to \nmanage the increasing volume of product data. The solution must be scalable and have a \nmodular service architecture. The company needs to maintain its structured database schemas. \nThe company also needs a storage solution to store product data and product images. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use an Amazon EC2 instance in an Auto Scaling group to deploy a containerized application.", correct: false },
                { id: 1, text: "Use AWS Lambda functions to manage the existing monolithic application. Use Amazon", correct: false },
                { id: 2, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with an Amazon EC2 deployment to", correct: false },
                { id: 3, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to deploy a", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A company is migrating an application from an on-premises environment to AWS. The application \nwill store sensitive data in Amazon S3. The company must encrypt the data before storing the \ndata in Amazon S3. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Encrypt the data by using client-side encryption with customer managed keys.", correct: true },
                { id: 1, text: "Encrypt the data by using server-side encryption with AWS KMS keys (SSE-KMS).", correct: false },
                { id: 2, text: "Encrypt the data by using server-side encryption with customer-provided keys (SSE-C).", correct: false },
                { id: 3, text: "Encrypt the data by using client-side encryption with Amazon S3 managed keys.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nIf client-side encryption is used, the keys must be managed by the customer.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "A company wants to create an Amazon EMR cluster that multiple teams will use. The company \nwants to ensure that each team's big data workloads can access only the AWS services that \neach team needs to interact with. The company does not want the workloads to have access to \nInstance Metadata Service Version 2 (IMDSv2) on the cluster's underlying EC2 instances. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Configure interface VPC endpoints for each AWS service that the teams need. Use the required", correct: false },
                { id: 1, text: "Create EMR runtime roles. Configure the cluster to use the runtime roles. Use the runtime roles to", correct: true },
                { id: 2, text: "Create an EC2 IAM instance profile that has the required permissions for each team. Use the", correct: false },
                { id: 3, text: "Create an EMR security configuration that has the EnableApplicationScopedIAMRole option set", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "A solutions architect is designing an application that helps users fill out and submit registration \nforms. The solutions architect plans to use a two-tier architecture that includes a web application \nserver tier and a worker tier. \n \nThe application needs to process submitted forms quickly. The application needs to process each \nform exactly once. The solution must ensure that no data is lost. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO queue between the web application", correct: true },
                { id: 1, text: "Use an Amazon API Gateway HTTP API between the web application server tier and the worker", correct: false },
                { id: 2, text: "Use an Amazon Simple Queue Service (Amazon SQS) standard queue between the web", correct: false },
                { id: 3, text: "Use an AWS Step Functions workflow. Create a synchronous workflow between the web", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 26,
            text: "A finance company uses an on-premises search application to collect streaming data from \nvarious producers. The application provides real-time updates to search and visualization \nfeatures. \n \nThe company is planning to migrate to AWS and wants to use an AWS native solution. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 instances to ingest and process the data streams to Amazon S3 buckets tor", correct: false },
                { id: 1, text: "Use Amazon EMR to ingest and process the data streams to Amazon Redshift for storage. Use", correct: false },
                { id: 2, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) to ingest and process the data streams", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to ingest and process the data streams to Amazon", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "A company currently runs an on-premises application that usesASP.NET on Linux machines. The \napplication is resource-intensive and serves customers directly. \n \nThe company wants to modernize the application to .NET. The company wants to run the \napplication on containers and to scale based on Amazon CloudWatch metrics. The company also \nwants to reduce the time spent on operational maintenance activities. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use AWS App2Container to containerize the application. Use an AWS CloudFormation template", correct: true },
                { id: 1, text: "Use AWS App2Container to containerize the application. Use an AWS CloudFormation template", correct: false },
                { id: 2, text: "Use AWS App Runner to containerize the application. Use App Runner to deploy the application", correct: false },
                { id: 3, text: "Use AWS App Runner to containerize the application. Use App Runner to deploy the application", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company is designing a new internal web application in the AWS Cloud. The new application \nmust securely retrieve and store multiple employee usernames and passwords from an AWS \nmanaged service. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Store the employee credentials in AWS Systems Manager Parameter Store. Use AWS", correct: false },
                { id: 1, text: "Store the employee credentials in AWS Secrets Manager. Use AWS CloudFormation and AWS", correct: false },
                { id: 2, text: "Store the employee credentials in AWS Systems Manager Parameter Store. Use AWS", correct: false },
                { id: 3, text: "Store the employee credentials in AWS Secrets Manager. Use AWS CloudFormation and the", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A company that is in the ap-northeast-1 Region has a fleet of thousands of AWS Outposts \nservers. The company has deployed the servers at remote locations around the world. All the \nservers regularly download new software versions that consist of 100 files. There is significant \nlatency before all servers run the new software versions. \n \nThe company must reduce the deployment latency for new software versions. \n \nWhich solution will meet this requirement with the LEAST operational overhead? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n491",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket in ap-northeast-1. Set up an Amazon CloudFront distribution in ap-", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket in ap-northeast-1. Create a second S3 bucket in the us-east-1", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket in ap-northeast-1. Configure Amazon S3 Transfer Acceleration.", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket in ap-northeast-1. Set up an Amazon CloudFront distribution.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A company currently runs an on-premises stock trading application by using Microsoft Windows \nServer. The company wants to migrate the application to the AWS Cloud. \n \nThe company needs to design a highly available solution that provides low-latency access to \nblock storage across multiple Availability Zones. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
            options: [
                { id: 0, text: "Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances.", correct: true },
                { id: 1, text: "Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances.", correct: false },
                { id: 2, text: "Deploy the application on Amazon EC2 instances in two Availability Zones. Configure one EC2", correct: false },
                { id: 3, text: "Deploy the application on Amazon EC2 instances in two Availability Zones. Configure one EC2", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A company is designing a web application with an internet-facing Application Load Balancer \n(ALB). \n \nThe company needs the ALB to receive HTTPS web traffic from the public internet. The ALB \nmust send only HTTPS traffic to the web application servers hosted on the Amazon EC2 \ninstances on port 443. The ALB must perform a health check of the web application servers over \nHTTPS on port 8443. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n492 \nWhich combination of configurations of the security group that is associated with the ALB will \nmeet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Allow HTTPS inbound traffic from 0.0.0.0/0 for port 443.", correct: true },
                { id: 1, text: "Allow all outbound traffic to 0.0.0.0/0 for port 443.", correct: false },
                { id: 2, text: "Allow HTTPS outbound traffic to the web application instances for port 443.", correct: false },
                { id: 3, text: "Allow HTTPS inbound traffic from the web application instances for port 443.", correct: false },
                { id: 4, text: "Allow HTTPS outbound traffic to the web application instances for the health check on port 8443.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A company hosts an application on AWS. The application gives users the ability to upload photos \nand store the photos in an Amazon S3 bucket. The company wants to use Amazon CloudFront \nand a custom domain name to upload the photo files to the S3 bucket in the eu-west-1 Region. \n \nWhich solution will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "Use AWS Certificate Manager (ACM) to create a public certificate in the us-east-1 Region. Use", correct: true },
                { id: 1, text: "Use AWS Certificate Manager (ACM) to create a public certificate in eu-west-1. Use the certificate", correct: false },
                { id: 2, text: "Configure Amazon S3 to allow uploads from CloudFront. Configure S3 Transfer Acceleration.", correct: false },
                { id: 3, text: "Configure Amazon S3 to allow uploads from CloudFront origin access control (OAC).", correct: false },
                { id: 4, text: "Configure Amazon S3 to allow uploads from CloudFront. Configure an Amazon S3 website", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A weather forecasting company collects temperature readings from various sensors on a \ncontinuous basis. An existing data ingestion process collects the readings and aggregates the \nreadings into larger Apache Parquet files. Then the process encrypts the files by using client-side \nencryption with KMS managed keys (CSE-KMS). Finally, the process writes the files to an \nAmazon S3 bucket with separate prefixes for each calendar day. \n \nThe company wants to run occasional SQL queries on the data to take sample moving averages \nfor a specific calendar day. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure Amazon Athena to read the encrypted files. Run SQL queries on the data directly in", correct: true },
                { id: 1, text: "Use Amazon S3 Select to run SQL queries on the data directly in Amazon S3.", correct: false },
                { id: 2, text: "Configure Amazon Redshift to read the encrypted files. Use Redshift Spectrum and Redshift", correct: false },
                { id: 3, text: "Configure Amazon EMR Serverless to read the encrypted files. Use Apache SparkSQL to run", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A company is implementing a new application on AWS. The company will run the application on \nmultiple Amazon EC2 instances across multiple Availability Zones within multiple AWS Regions. \nThe application will be available through the internet. Users will access the application from \naround the world. \n \nThe company wants to ensure that each user who accesses the application is sent to the EC2 \ninstances that are closest to the user's location. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Implement an Amazon Route 53 geolocation routing policy. Use an internet-facing Application", correct: false },
                { id: 1, text: "Implement an Amazon Route 53 geoproximity routing policy. Use an internet-facing Network Load", correct: true },
                { id: 2, text: "Implement an Amazon Route 53 multivalue answer routing policy. Use an internet-facing", correct: false },
                { id: 3, text: "Implement an Amazon Route 53 weighted routing policy. Use an internet-facing Network Load", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A financial services company plans to launch a new application on AWS to handle sensitive \nfinancial transactions. The company will deploy the application on Amazon EC2 instances. The \ncompany will use Amazon RDS for MySQL as the database. The company's security policies \nmandate that data must be encrypted at rest and in transit. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.", correct: true },
                { id: 1, text: "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.", correct: false },
                { id: 2, text: "Implement third-party application-level data encryption before storing data in Amazon RDS for", correct: false },
                { id: 3, text: "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company is migrating its on-premises Oracle database to an Amazon RDS for Oracle \ndatabase. The company needs to retain data for 90 days to meet regulatory requirements. The \ncompany must also be able to restore the database to a specific point in time for up to 14 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create Amazon RDS automated backups. Set the retention period to 90 days.", correct: false },
                { id: 1, text: "Create an Amazon RDS manual snapshot every day. Delete manual snapshots that are older", correct: false },
                { id: 2, text: "Use the Amazon Aurora Clone feature for Oracle to create a point-in-time restore. Delete clones", correct: false },
                { id: 3, text: "Create a backup plan that has a retention period of 90 days by using AWS Backup for Amazon", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A company is developing a new application that uses a relational database to store user data and \napplication configurations. The company expects the application to have steady user growth. The \ncompany expects the database usage to be variable and read-heavy, with occasional writes. \n \nThe company wants to cost-optimize the database solution. The company wants to use an AWS \nmanaged database solution that will provide the necessary performance. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Deploy the database on Amazon RDS. Use Provisioned IOPS SSD storage to ensure consistent", correct: false },
                { id: 1, text: "Deploy the database on Amazon Aurora Serverless to automatically scale the database capacity", correct: true },
                { id: 2, text: "Deploy the database on Amazon DynamoDB. Use on-demand capacity mode to automatically", correct: false },
                { id: 3, text: "Deploy the database on Amazon RDS. Use magnetic storage and use read replicas to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A company hosts its application on several Amazon EC2 instances inside a VPC. The company \ncreates a dedicated Amazon S3 bucket for each customer to store their relevant information in \nAmazon S3. \n \nThe company wants to ensure that the application running on EC2 instances can securely access \nonly the S3 buckets that belong to the company's AWS account. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Create a gateway endpoint for Amazon S3 that is attached to the VPC. Update the IAM instance", correct: false },
                { id: 1, text: "Create a NAT gateway in a public subnet with a security group that allows access to only Amazon", correct: false },
                { id: 2, text: "Create a gateway endpoint for Amazon S3 that is attached to the VPUpdate the IAM instance", correct: true },
                { id: 3, text: "Create a NAT Gateway in a public subnet. Update route tables to use the NAT Gateway. Assign", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A company is building a cloud-based application on AWS that will handle sensitive customer \ndata. The application uses Amazon RDS for the database, Amazon S3 for object storage, and S3 \nEvent Notifications that invoke AWS Lambda for serverless processing. \n \nThe company uses AWS IAM Identity Center to manage user credentials. The development, \ntesting, and operations teams need secure access to Amazon RDS and Amazon S3 while \nensuring the confidentiality of sensitive customer data. The solution must comply with the \nprinciple of least privilege. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use IAM roles with least privilege to grant all the teams access. Assign IAM roles to each team", correct: false },
                { id: 1, text: "Enable IAM Identity Center with an Identity Center directory. Create and configure permission", correct: true },
                { id: 2, text: "Create individual IAM users for each member in all the teams with role-based permissions.", correct: false },
                { id: 3, text: "Use AWS Organizations to create separate accounts for each team. Implement cross-account", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIAM Identity Center: This service simplifies user management by centralizing credentials and access control. Permission Sets: You can create granular permission sets that align with the principle of least privilege, ensuring that each team has only the access they need. Group Assignments: By assigning teams to groups with specific permission sets, you streamline access management and reduce the complexity of individual user permissions. This approach minimizes operational overhead while maintaining secure and compliant access to sensitive customer data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A company has an Amazon S3 bucket that contains sensitive data files. The company has an \napplication that runs on virtual machines in an on-premises data center. The company currently \nuses AWS IAM Identity Center. \n \nThe application requires temporary access to files in the S3 bucket. The company wants to grant \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n496 \nthe application secure access to the files in the S3 bucket. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an S3 bucket policy that permits access to the bucket from the public IP address range of", correct: false },
                { id: 1, text: "Use IAM Roles Anywhere to obtain security credentials in IAM Identity Center that grant access to", correct: true },
                { id: 2, text: "Install the AWS CLI on the virtual machine. Configure the AWS CLI with access keys from an IAM", correct: false },
                { id: 3, text: "Create an IAM user and policy that grants access to the bucket. Store the access key and secret", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A company hosts its core network services, including directory services and DNS, in its on-\npremises data center. The data center is connected to the AWS Cloud using AWS Direct Connect \n(DX). Additional AWS accounts are planned that will require quick, cost-effective, and consistent \naccess to these network services. \n \nWhat should a solutions architect implement to meet these requirements with the LEAST amount \nof operational overhead?",
            options: [
                { id: 0, text: "Create a DX connection in each new account. Route the network traffic to the on-premises", correct: false },
                { id: 1, text: "Configure VPC endpoints in the DX VPC for all required services. Route the network traffic to the", correct: false },
                { id: 2, text: "Create a VPN connection between each new account and the DX VPRoute the network traffic to", correct: false },
                { id: 3, text: "Configure AWS Transit Gateway between the accounts. Assign DX to the transit gateway and", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 42,
            text: "A company hosts its main public web application in one AWS Region across multiple Availability \nZones. The application uses an Amazon EC2 Auto Scaling group and an Application Load \nBalancer (ALB). \n \nA web development team needs a cost-optimized compute solution to improve the company's \nability to serve dynamic content globally to millions of customers. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon CloudFront distribution. Configure the existing ALB as the origin.", correct: true },
                { id: 1, text: "Use Amazon Route 53 to serve traffic to the ALB and EC2 instances based on the geographic", correct: false },
                { id: 2, text: "Create an Amazon S3 bucket with public read access enabled. Migrate the web application to the", correct: false },
                { id: 3, text: "Use AWS Direct Connect to directly serve content from the web application to the location of each", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A company stores user data in AWS. The data is used continuously with peak usage during \nbusiness hours. Access patterns vary, with some data not being used for months at a time. A \nsolutions architect must choose a cost-effective solution that maintains the highest level of \ndurability while maintaining high availability. \n \nWhich storage solution meets these requirements?",
            options: [
                { id: 0, text: "Amazon S3 Standard", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering", correct: true },
                { id: 2, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 3, text: "Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A company is testing an application that runs on an Amazon EC2 Linux instance. A single 500 \nGB Amazon Elastic Block Store (Amazon EBS) General Purpose SSO (gp2) volume is attached \nto the EC2 instance. \n \nThe company will deploy the application on multiple EC2 instances in an Auto Scaling group. All \ninstances require access to the data that is stored in the EBS volume. The company needs a \nhighly available and resilient solution that does not introduce significant changes to the \napplication's code. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Provision an EC2 instance that uses NFS server software. Attach a single 500 GB gp2 EBS", correct: false },
                { id: 1, text: "Provision an Amazon FSx for Windows File Server file system. Configure the file system as an", correct: false },
                { id: 2, text: "Provision an EC2 instance with two 250 GB Provisioned IOPS SSD EBS volumes.", correct: false },
                { id: 3, text: "Provision an Amazon Elastic File System (Amazon EFS) file system. Configure the file system to", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company recently launched a new application for its customers. The application runs on \nmultiple Amazon EC2 instances across two Availability Zones. End users use TCP to \ncommunicate with the application. \n \nThe application must be highly available and must automatically scale as the number of users \nincreases. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n498",
            options: [
                { id: 0, text: "Add a Network Load Balancer in front of the EC2 instances.", correct: true },
                { id: 1, text: "Configure an Auto Scaling group for the EC2 instances.", correct: false },
                { id: 2, text: "Add an Application Load Balancer in front of the EC2 instances.", correct: false },
                { id: 3, text: "Manually add more EC2 instances for the application.", correct: false },
                { id: 4, text: "Add a Gateway Load Balancer in front of the EC2 instances.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "A company is designing the architecture for a new mobile app that uses the AWS Cloud. The \ncompany uses organizational units (OUs) in AWS Organizations to manage its accounts. The \ncompany wants to tag Amazon EC2 instances with data sensitivity by using values of sensitive \nand nonsensitive. IAM identities must not be able to delete a tag or create instances without a \ntag. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
            options: [
                { id: 0, text: "In Organizations, create a new tag policy that specifies the data sensitivity tag key and the", correct: true },
                { id: 1, text: "In Organizations, create a new service control policy (SCP) that specifies the data sensitivity tag", correct: false },
                { id: 2, text: "Create a tag policy to deny running instances when a tag key is not specified. Create another tag", correct: false },
                { id: 3, text: "Create a service control policy (SCP) to deny creating instances when a tag key is not specified.", correct: false },
                { id: 4, text: "Create an AWS Config rule to check if EC2 instances use the data sensitivity tag and the specified", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A company runs database workloads on AWS that are the backend for the company's customer \nportals. The company runs a Multi-AZ database cluster on Amazon RDS for PostgreSQL. \n \nThe company needs to implement a 30-day backup retention policy. The company currently has \nboth automated RDS backups and manual RDS backups. The company wants to maintain both \ntypes of existing RDS backups that are less than 30 days old. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure the RDS backup retention policy to 30 days for automated backups by using AWS", correct: false },
                { id: 1, text: "Disable RDS automated backups. Delete automated backups and manual backups that are older", correct: false },
                { id: 2, text: "Configure the RDS backup retention policy to 30 days for automated backups. Manually delete", correct: true },
                { id: 3, text: "Disable RDS automated backups. Delete automated backups and manual backups that are older", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 48,
            text: "A company is planning to migrate a legacy application to AWS. The application currently uses \nNFS to communicate to an on-premises storage solution to store application data. The application \ncannot be modified to use any other communication protocols other than NFS for this purpose. \n \nWhich storage solution should a solutions architect recommend for use after the migration?",
            options: [
                { id: 0, text: "AWS DataSync", correct: false },
                { id: 1, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
                { id: 2, text: "Amazon Elastic File System (Amazon EFS)", correct: true },
                { id: 3, text: "Amazon EMR File System (Amazon EMRFS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 49,
            text: "A company uses GPS trackers to document the migration patterns of thousands of sea turtles. \nThe trackers check every 5 minutes to see if a turtle has moved more than 100 yards (91.4 \nmeters). If a turtle has moved, its tracker sends the new coordinates to a web application running \non three Amazon EC2 instances that are in multiple Availability Zones in one AWS Region. \n \nRecently, the web application was overwhelmed while processing an unexpected volume of \ntracker data. Data was lost with no way to replay the events. A solutions architect must prevent \nthis problem from happening again and needs a solution with the least operational overhead. \n \nWhat should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket to store the data. Configure the application to scan for new data in", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to handle transmitted location coordinates. Use an AWS", correct: false },
                { id: 2, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming data.", correct: true },
                { id: 3, text: "Create an Amazon DynamoDB table to store transmitted location coordinates. Configure the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 50,
            text: "A company's software development team needs an Amazon RDS Multi-AZ cluster. The RDS \ncluster will serve as a backend for a desktop client that is deployed on premises. The desktop \nclient requires direct connectivity to the RDS cluster. \n \nThe company must give the development team the ability to connect to the cluster by using the \nclient when the team is in the office. \n \nWhich solution provides the required connectivity MOST securely? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n500",
            options: [
                { id: 0, text: "Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Use AWS", correct: false },
                { id: 1, text: "Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use AWS", correct: true },
                { id: 2, text: "Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use RDS", correct: false },
                { id: 3, text: "Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Create a", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A solutions architect is creating an application that will handle batch processing of large amounts \nof data. The input data will be held in Amazon S3 and the output data will be stored in a different \nS3 bucket. For processing, the application will transfer the data over the network between \nmultiple Amazon EC2 instances. \n \nWhat should the solutions architect do to reduce the overall data transfer costs?",
            options: [
                { id: 0, text: "Place all the EC2 instances in an Auto Scaling group.", correct: false },
                { id: 1, text: "Place all the EC2 instances in the same AWS Region.", correct: false },
                { id: 2, text: "Place all the EC2 instances in the same Availability Zone.", correct: true },
                { id: 3, text: "Place all the EC2 instances in private subnets in multiple Availability Zones.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 52,
            text: "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for \nstorage. The application tier is hosted on Amazon EC2 instances. The company's IT security \nguidelines mandate that the database credentials be encrypted and rotated every 14 days. \n \nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
            options: [
                { id: 0, text: "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets", correct: true },
                { id: 1, text: "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a", correct: false },
                { id: 2, text: "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)", correct: false },
                { id: 3, text: "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A streaming media company is rebuilding its infrastructure to accommodate increasing demand \nfor video content that users consume daily. \n \nThe company needs to process terabyte-sized videos to block some content in the videos. Video \nprocessing can take up to 20 minutes. \n \nThe company needs a solution that will scale with demand and remain cost-effective. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Lambda functions to process videos. Store video metadata in Amazon DynamoDB.", correct: false },
                { id: 1, text: "Use Amazon Elastic Container Service (Amazon ECS) and AWS Fargate to implement", correct: true },
                { id: 2, text: "Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB)", correct: false },
                { id: 3, text: "Deploy a containerized video processing application on Amazon Elastic Kubernetes Service", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 54,
            text: "A company runs an on-premises application on a Kubernetes cluster. The company recently \nadded millions of new customers. The company's existing on-premises infrastructure is unable to \nhandle the large number of new customers. The company needs to migrate the on-premises \napplication to the AWS Cloud. \n \nThe company will migrate to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The \ncompany does not want to manage the underlying compute infrastructure for the new architecture \non AWS. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use a self-managed node to supply compute capacity. Deploy the application to the new EKS", correct: false },
                { id: 1, text: "Use managed node groups to supply compute capacity. Deploy the application to the new EKS", correct: false },
                { id: 2, text: "Use AWS Fargate to supply compute capacity. Create a Fargate profile. Use the Fargate profile to", correct: true },
                { id: 3, text: "Use managed node groups with Karpenter to supply compute capacity. Deploy the application to", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A company is launching a new application that requires a structured database to store user \nprofiles, application settings, and transactional data. The database must be scalable with \napplication traffic and must offer backups. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Deploy a self-managed database on Amazon EC2 instances by using open source software. Use", correct: false },
                { id: 1, text: "Use Amazon RDS. Use on-demand capacity mode for the database with General Purpose SSD", correct: false },
                { id: 2, text: "Use Amazon Aurora Serverless for the database. Use serverless capacity scaling. Configure", correct: true },
                { id: 3, text: "Deploy a self-managed NoSQL database on Amazon EC2 instances. Use Reserved Instances for", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A company runs its legacy web application on AWS. The web application server runs on an \nAmazon EC2 instance in the public subnet of a VPC. The web application server collects images \nfrom customers and stores the image files in a locally attached Amazon Elastic Block Store \n(Amazon EBS) volume. The image files are uploaded every night to an Amazon S3 bucket for \nbackup. \n \nA solutions architect discovers that the image files are being uploaded to Amazon S3 through the \npublic endpoint. The solutions architect needs to ensure that traffic to Amazon S3 does not use \nthe public endpoint. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Create a gateway VPC endpoint for the S3 bucket that has the necessary permissions for the", correct: true },
                { id: 1, text: "Move the S3 bucket inside the VPC. Configure the subnet route table to access the S3 bucket", correct: false },
                { id: 2, text: "Create an Amazon S3 access point for the Amazon EC2 instance inside the VPConfigure the web", correct: false },
                { id: 3, text: "Configure an AWS Direct Connect connection between the VPC that has the Amazon EC2", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A company is creating a prototype of an ecommerce website on AWS. The website consists of an \nApplication Load Balancer, an Auto Scaling group of Amazon EC2 instances for web servers, and \nan Amazon RDS for MySQL DB instance that runs with the Single-AZ configuration. \n \nThe website is slow to respond during searches of the product catalog. The product catalog is a \ngroup of tables in the MySQL database that the company does not update frequently. A solutions \narchitect has determined that the CPU utilization on the DB instance is high when product catalog \nsearches occur. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n503 \nWhat should the solutions architect recommend to improve the performance of the website during \nsearches of the product catalog?",
            options: [
                { id: 0, text: "Migrate the product catalog to an Amazon Redshift database. Use the COPY command to load the", correct: false },
                { id: 1, text: "Implement an Amazon ElastiCache for Redis cluster to cache the product catalog. Use lazy", correct: true },
                { id: 2, text: "Add an additional scaling policy to the Auto Scaling group to launch additional EC2 instances", correct: false },
                { id: 3, text: "Turn on the Multi-AZ configuration for the DB instance. Configure the EC2 instances to throttle the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A company currently stores 5 TB of data in on-premises block storage systems. The company's \ncurrent storage solution provides limited space for additional data. The company runs \napplications on premises that must be able to retrieve frequently accessed data with low latency. \nThe company requires a cloud-based storage solution. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
            options: [
                { id: 0, text: "Use Amazon S3 File Gateway. Integrate S3 File Gateway with the on-premises applications to", correct: false },
                { id: 1, text: "Use an AWS Storage Gateway Volume Gateway with cached volumes as iSCSI targets.", correct: true },
                { id: 2, text: "Use an AWS Storage Gateway Volume Gateway with stored volumes as iSCSI targets.", correct: false },
                { id: 3, text: "Use an AWS Storage Gateway Tape Gateway. Integrate Tape Gateway with the on-premises", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A company operates a food delivery service. Because of recent growth, the company's order \nprocessing system is experiencing scaling problems during peak traffic hours. The current \narchitecture includes Amazon EC2 instances in an Auto Scaling group that collect orders from an \napplication. A second group of EC2 instances in an Auto Scaling group fulfills the orders. \n \nThe order collection process occurs quickly, but the order fulfillment process can take longer. \nData must not be lost because of a scaling event. \n \nA solutions architect must ensure that the order collection process and the order fulfillment \nprocess can both scale adequately during peak traffic hours. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto", correct: false },
                { id: 1, text: "Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto", correct: false },
                { id: 2, text: "Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for", correct: false },
                { id: 3, text: "Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 60,
            text: "An online gaming company is transitioning user data storage to Amazon DynamoDB to support \nthe company's growing user base. The current architecture includes DynamoDB tables that \ncontain user profiles, achievements, and in-game transactions. \n \nThe company needs to design a robust, continuously available, and resilient DynamoDB \narchitecture to maintain a seamless gaming experience for users. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create DynamoDB tables in a single AWS Region. Use on-demand capacity mode. Use global", correct: false },
                { id: 1, text: "Use DynamoDB Accelerator (DAX) to cache frequently accessed data. Deploy tables in a single", correct: false },
                { id: 2, text: "Create DynamoDB tables in multiple AWS Regions. Use on-demand capacity mode. Use", correct: false },
                { id: 3, text: "Use DynamoDB global tables for automatic multi-Region replication. Deploy tables in multiple", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 61,
            text: "A company runs its media rendering application on premises. The company wants to reduce \nstorage costs and has moved all data to Amazon S3. The on-premises rendering application \nneeds low-latency access to storage. \n \nThe company needs to design a storage solution for the application. The storage solution must \nmaintain the desired application performance. \n \nWhich storage solution will meet these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 to access the data in Amazon S3 for the on-premises application.", correct: false },
                { id: 1, text: "Configure an Amazon S3 File Gateway to provide storage for the on-premises application.", correct: true },
                { id: 2, text: "Copy the data from Amazon S3 to Amazon FSx for Windows File Server. Configure an Amazon", correct: false },
                { id: 3, text: "Configure an on-premises file server. Use the Amazon S3 API to connect to S3 storage. Configure", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 62,
            text: "A company hosts its enterprise resource planning (ERP) system in the us-east-1 Region. The \nsystem runs on Amazon EC2 instances. Customers use a public API that is hosted on the EC2 \ninstances to exchange information with the ERP system. International customers report slow API \nresponse times from their data centers. \n \nWhich solution will improve response times for the international customers MOST cost-\neffectively?",
            options: [
                { id: 0, text: "Create an AWS Direct Connect connection that has a public virtual interface (VIF) to provide", correct: false },
                { id: 1, text: "Set up an Amazon CloudFront distribution in front of the API. Configure the CachingOptimized", correct: true },
                { id: 2, text: "Set up AWS Global Accelerator. Configure listeners for the necessary ports. Configure endpoint", correct: false },
                { id: 3, text: "Use AWS Site-to-Site VPN to establish dedicated VPN tunnels between Regions and customer", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 63,
            text: "A company tracks customer satisfaction by using surveys that the company hosts on its website. \nThe surveys sometimes reach thousands of customers every hour. Survey results are currently \nsent in email messages to the company so company employees can manually review results and \nassess customer sentiment. \n \nThe company wants to automate the customer survey process. Survey results must be available \nfor the previous 12 months. \n \nWhich solution will meet these requirements in the MOST scalable way?",
            options: [
                { id: 0, text: "Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon", correct: true },
                { id: 1, text: "Send the survey results data to an API that is running on an Amazon EC2 instance. Configure the", correct: false },
                { id: 2, text: "Write the survey results data to an Amazon S3 bucket. Use S3 Event Notifications to invoke an", correct: false },
                { id: 3, text: "Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 64,
            text: "A company uses AWS Systems Manager for routine management and patching of Amazon EC2 \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n506 \ninstances. The EC2 instances are in an IP address type target group behind an Application Load \nBalancer (ALB). \n \nNew security protocols require the company to remove EC2 instances from service during a \npatch. When the company attempts to follow the security protocol during the next patch, the \ncompany receives errors during the patching window. \n \nWhich combination of solutions will resolve the errors? (Choose two.)",
            options: [
                { id: 0, text: "Change the target type of the target group from IP address type to instance type.", correct: false },
                { id: 1, text: "Continue to use the existing Systems Manager document without changes because it is already", correct: false },
                { id: 2, text: "Implement the AWSEC2-PatchLoadBalanacerInstance Systems Manager Automation document to", correct: true },
                { id: 3, text: "Use Systems Manager Maintenance Windows to automatically remove the instances from service", correct: false },
                { id: 4, text: "Configure Systems Manager State Manager to remove the instances from service and manage the", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
    ],
    test26: [
        {
            id: 0,
            text: "A medical company wants to perform transformations on a large amount of clinical trial data that \ncomes from several customers. The company must extract the data from a relational database \nthat contains the customer data. Then the company will transform the data by using a series of \ncomplex rules. The company will load the data to Amazon S3 when the transformations are \ncomplete. \n \nAll data must be encrypted where it is processed before the company stores the data in Amazon \nS3. All data must be encrypted by using customer-specific keys. \n \nWhich solution will meet these requirements with the LEAST amount of operational effort?",
            options: [
                { id: 0, text: "Create one AWS Glue job for each customer. Attach a security configuration to each job that uses", correct: false },
                { id: 1, text: "Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster", correct: false },
                { id: 2, text: "Create one AWS Glue job for each customer. Attach a security configuration to each job that uses", correct: true },
                { id: 3, text: "Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 1,
            text: "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. \nThe analytics application is highly resilient and is designed to run in stateless mode. \n \nThe company notices that the application is showing signs of performance degradation during \nbusy times and is presenting 5xx errors. The company needs to make the application scale \nseamlessly. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n507 \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second", correct: false },
                { id: 2, text: "Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create", correct: false },
                { id: 3, text: "Create an Amazon Machine Image (AMI) of the web application. Apply the AMI to a launch", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "A company runs an environment where data is stored in an Amazon S3 bucket. The objects are \naccessed frequently throughout the day. The company has strict da ta encryption requirements \nfor data that is stored in the S3 bucket. The company currently uses AWS Key Management \nService (AWS KMS) for encryption. \n \nThe company wants to optimize costs associated with encrypting S3 objects without making \nadditional calls to AWS KMS. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3).", correct: false },
                { id: 1, text: "Use an S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the new", correct: true },
                { id: 2, text: "Use client-side encryption with AWS KMS customer managed keys.", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) stored in AWS KMS.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A company runs multiple workloads on virtual machines (VMs) in an on-premises data center. \nThe company is expanding rapidly. The on-premises data center is not able to scale fast enough \nto meet business needs. The company wants to migrate the workloads to AWS. \n \nThe migration is time sensitive. The company wants to use a lift-and-shift strategy for non-critical \nworkloads. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
            options: [
                { id: 0, text: "Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs.", correct: false },
                { id: 1, text: "Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs.", correct: true },
                { id: 2, text: "Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on", correct: false },
                { id: 3, text: "Stop all operations on the VMs. Launch a cutover instance.", correct: false },
                { id: 4, text: "Use AWS App2Container (A2C) to collect data about the VMs.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company hosts an application in a private subnet. The company has already integrated the \napplication with Amazon Cognito. The company uses an Amazon Cognito user pool to \nauthenticate users. \n \nThe company needs to modify the application so the application can securely store user \ndocuments in an Amazon S3 bucket. \n \nWhich combination of steps will securely integrate Amazon S3 with the application? (Choose \ntwo.)",
            options: [
                { id: 0, text: "Create an Amazon Cognito identity pool to generate secure Amazon S3 access tokens for users", correct: true },
                { id: 1, text: "Use the existing Amazon Cognito user pool to generate Amazon S3 access tokens for users when", correct: false },
                { id: 2, text: "Create an Amazon S3 VPC endpoint in the same VPC where the company hosts the application.", correct: false },
                { id: 3, text: "Create a NAT gateway in the VPC where the company hosts the application. Assign a policy to the", correct: false },
                { id: 4, text: "Attach a policy to the S3 bucket that allows access only from the users' IP addresses.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company has a three-tier web application that processes orders from customers. The web tier \nconsists of Amazon EC2 instances behind an Application Load Balancer. The processing tier \nconsists of EC2 instances. The company decoupled the web tier and processing tier by using \nAmazon Simple Queue Service (Amazon SQS). The storage layer uses Amazon DynamoDB. \n \nAt peak times, some users report order processing delays and halls. The company has noticed \nthat during these delays, the EC2 instances are running at 100% CPU usage, and the SQS \nqueue fills up. The peak times are variable and unpredictable. \n \nThe company needs to improve the performance of the application. \n \nWhich solution will meet these requirements?",
            options: [
                { id: 0, text: "Use scheduled scaling for Amazon EC2 Auto Scaling to scale out the processing tier instances for", correct: false },
                { id: 1, text: "Use Amazon ElastiCache for Redis in front of the DynamoDB backend tier. Use target utilization", correct: false },
                { id: 2, text: "Add an Amazon CloudFront distribution to cache the responses for the web tier. Use HTTP latency", correct: false },
                { id: 3, text: "Use an Amazon EC2 Auto Scaling target tracking policy to scale out the processing tier instances.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A company's production environment consists of Amazon EC2 On-Demand Instances that run \nconstantly between Monday and Saturday. The instances must run for only 12 hours on Sunday \nand cannot tolerate interruptions. The company wants to cost-optimize the production \nenvironment. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Purchase Scheduled Reserved Instances for the EC2 instances that run for only 12 hours on", correct: true },
                { id: 1, text: "Purchase Convertible Reserved Instances for the EC2 instances that run for only 12 hours on", correct: false },
                { id: 2, text: "Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase", correct: false },
                { id: 3, text: "Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A digital image processing company wants to migrate its on-premises monolithic application to \nthe AWS Cloud. The company processes thousands of images and generates large files as part \nof the processing workflow. \n \nThe company needs a solution to manage the growing number of image processing jobs. The \nsolution must also reduce the manual tasks in the image processing workflow. The company \ndoes not want to manage the underlying infrastructure of the solution. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 Spot Instances to", correct: false },
                { id: 1, text: "Use AWS Batch jobs to process the images. Use AWS Step Functions to orchestrate the workflow.", correct: true },
                { id: 2, text: "Use AWS Lambda functions and Amazon EC2 Spot Instances to process the images. Store the", correct: false },
                { id: 3, text: "Deploy a group of Amazon EC2 instances to process the images. Use AWS Step Functions to", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A company's image-hosting website gives users around the world the ability to up load, view, and \ndownload images from their mobile devices. The company currently hosts the static website in an \nAmazon S3 bucket. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n510 \nBecause of the website's growing popularity, the website's performance has decreased. Users \nhave reported latency issues when they upload and download images. \n \nThe company must improve the performance of the website. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
            options: [
                { id: 0, text: "Configure an Amazon CloudFront distribution for the S3 bucket to improve the download", correct: true },
                { id: 1, text: "Configure Amazon EC2 instances of the right sizes in multiple AWS Regions. Migrate the", correct: false },
                { id: 2, text: "Configure an Amazon CloudFront distribution that uses the S3 bucket as an origin to improve the", correct: false },
                { id: 3, text: "Configure AWS Global Accelerator for the S3 bucket to improve network performance. Create an", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Explanation not available.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A company runs an application in a private subnet behind an Application Load Balancer (ALB) in \na VPC. The VPC has a NAT gateway and an internet gateway. The application calls the Amazon \nS3 API to store objects. \n \nAccording to the company's security policy, traffic from the application must not travel across the \ninternet. \n \nWhich solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Configure an S3 interface endpoint. Create a security group that allows outbound traffic to Amazon", correct: false },
                { id: 1, text: "Configure an S3 gateway endpoint. Update the VPC route table to use the endpoint.", correct: true },
                { id: 2, text: "Configure an S3 bucket policy to allow traffic from the Elastic IP address that is assigned to the", correct: false },
                { id: 3, text: "Create a second NAT gateway in the same subnet where the legacy application is deployed.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has an application that runs on an Amazon Elastic Kubernetes Service (Amazon \nEKS) cluster on Amazon EC2 instances. The application has a UI that uses Amazon DynamoDB \nand data services that use Amazon S3 as part of the application deployment. \n \nThe company must ensure that the EKS Pods for the UI can access only Amazon DynamoDB \nand that the EKS Pods for the data services can access only Amazon S3. The company uses \nAWS Identity and Access Management (IAM). \n \nWhich solution meals these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n511",
            options: [
                { id: 0, text: "Create separate IAM policies for Amazon S3 and DynamoDB access with the required", correct: false },
                { id: 1, text: "Create separate IAM policies for Amazon S3 and DynamoDB access with the required", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services to assume an IAM role.", correct: true },
                { id: 3, text: "Create separate Kubernetes service accounts for the UI and data services to assume an IAM role.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A company needs to give a globally distributed development team secure access to the \ncompany's AWS resources in a way that complies with security policies. \n \nThe company currently uses an on-premises Active Directory for internal authentication. The \ncompany uses AWS Organizations to manage multiple AWS accounts that support multiple \nprojects. \n \nThe company needs a solution to integrate with the existing infrastructure to provide centralized \nidentity management and access control. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Set up AWS Directory Service to create an AWS managed Microsoft Active Directory on AWS.", correct: false },
                { id: 1, text: "Create an IAM user for each developer. Manually manage permissions for each IAM user based", correct: false },
                { id: 2, text: "Use AD Connector in AWS Directory Service to connect to the on-premises Active Directory.", correct: true },
                { id: 3, text: "Use Amazon Cognito to deploy an identity federation solution. Integrate the identity federation", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company is developing an application in the AWS Cloud. The application's HTTP API contains \ncritical information that is published in Amazon API Gateway. The critical information must be \naccessible from only a limited set of trusted IP addresses that belong to the company's internal \nnetwork. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n512",
            options: [
                { id: 0, text: "Set up an API Gateway private integration to restrict access to a predefined set of IP addresses.", correct: false },
                { id: 1, text: "Create a resource policy for the API that denies access to any IP address that is not specifically", correct: true },
                { id: 2, text: "Directly deploy the API in a private subnet. Create a network ACL. Set up rules to allow the traffic", correct: false },
                { id: 3, text: "Modify the security group that is attached to API Gateway to allow inbound traffic from only the", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Explanation not available.",
            domain: "Design Secure Architectures",
        },
    ],
};

// Function to get all questions for a test
function getTestQuestions(testNumber) {
    const testKey = `test${testNumber}`;
    return examQuestions[testKey] || [];
}
