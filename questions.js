// AWS SAA-C03 Exam Questions
// Auto-generated from files in questions directory

const examQuestions = {
    test1: [
        {
            id: 1,
            text: "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 2, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: true },
                { id: 3, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3", correct: false },
                { id: 1, text: "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3", correct: true },
                { id: 2, text: "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
            options: [
                { id: 0, text: "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 1, text: "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 2, text: "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 3, text: "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 4, text: "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 1, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
            options: [
                { id: 0, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS", correct: true },
                { id: 1, text: "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS", correct: false },
                { id: 2, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS", correct: false },
                { id: 3, text: "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
            options: [
                { id: 0, text: "Different versions of a single object can have different retention modes and periods", correct: true },
                { id: 1, text: "The bucket default settings will override any explicit retention mode or period you request on an object version", correct: false },
                { id: 2, text: "You cannot place a retention period on an object version through a bucket default setting", correct: false },
                { id: 3, text: "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", correct: true },
                { id: 4, text: "When you use bucket default settings, you specify a Retain Until Date for the object version", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 1, text: "AWS Glue", correct: false },
                { id: 2, text: "Amazon EMR", correct: false },
                { id: 3, text: "Amazon FSx for Lustre", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
            options: [
                { id: 0, text: "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering => Amazon S3 Standard", correct: true },
                { id: 2, text: "Amazon S3 Standard-IA => Amazon S3 One Zone-IA", correct: false },
                { id: 3, text: "Amazon S3 One Zone-IA => Amazon S3 Standard-IA", correct: true },
                { id: 4, text: "Amazon S3 Standard => Amazon S3 Intelligent-Tiering", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
            options: [
                { id: 0, text: "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources", correct: false },
                { id: 1, text: "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI", correct: false },
                { id: 2, text: "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions", correct: false },
                { id: 3, text: "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
            options: [
                { id: 0, text: "Configure the security group for the Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)", correct: false },
                { id: 2, text: "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 3, text: "Configure the security group on the Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway with AWS Lambda", correct: false },
                { id: 1, text: "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Leverage Amazon QuickSight with Amazon Redshift", correct: false },
                { id: 3, text: "Leverage Amazon Athena with Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor", correct: false },
                { id: 1, text: "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets", correct: true },
                { id: 3, text: "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
            options: [
                { id: 0, text: "Encrypt the access keys and save them on Amazon S3", correct: false },
                { id: 1, text: "Create a strong password for the AWS account root user", correct: true },
                { id: 2, text: "Enable Multi Factor Authentication (MFA) for the AWS account root user account", correct: true },
                { id: 3, text: "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future", correct: false },
                { id: 4, text: "Create AWS account root user access keys and share those keys only with the business owner", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
            options: [
                { id: 0, text: "Grant maximum privileges to avoid assigning privileges again", correct: false },
                { id: 1, text: "Use user credentials to provide access specific permissions for Amazon EC2 instances", correct: false },
                { id: 2, text: "Create a minimum number of accounts and share these account credentials among employees", correct: false },
                { id: 3, text: "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users", correct: true },
                { id: 4, text: "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 1, text: "Only root user should have full database access in the organization", correct: false },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Remove full database access for all IAM users in the organization", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance", correct: true },
                { id: 1, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched", correct: false },
                { id: 2, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application", correct: true },
                { id: 3, text: "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously", correct: false },
                { id: 4, text: "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time", correct: false },
                { id: 1, text: "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly", correct: false },
                { id: 2, text: "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores", correct: true },
                { id: 3, text: "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
            options: [
                { id: 0, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate", correct: true },
                { id: 1, text: "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages", correct: false },
                { id: 2, text: "Use Amazon SQS standard queue to process the messages", correct: false },
                { id: 3, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
            options: [
                { id: 0, text: "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs", correct: true },
                { id: 1, text: "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches", correct: false },
                { id: 2, text: "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share", correct: false },
                { id: 3, text: "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
            options: [
                { id: 0, text: "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket", correct: false },
                { id: 1, text: "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)", correct: true },
                { id: 2, text: "Upload the compressed file using multipart upload", correct: false },
                { id: 3, text: "Upload the compressed file in a single operation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
            options: [
                { id: 0, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance", correct: false },
                { id: 1, text: "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance", correct: true },
                { id: 2, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance", correct: false },
                { id: 3, text: "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
            options: [
                { id: 0, text: "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution", correct: true },
                { id: 1, text: "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: true },
                { id: 2, text: "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 3, text: "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 4, text: "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection", correct: true },
                { id: 1, text: "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region", correct: false },
                { id: 2, text: "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region", correct: false },
                { id: 3, text: "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes", correct: true },
                { id: 3, text: "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
            options: [
                { id: 0, text: "The engineering team needs to provision more servers running the Amazon SNS service", correct: false },
                { id: 1, text: "The engineering team needs to provision more servers running the AWS Lambda service", correct: false },
                { id: 2, text: "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit", correct: true },
                { id: 3, text: "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
            options: [
                { id: 0, text: "General Purpose Solid State Drive (gp2)", correct: false },
                { id: 1, text: "Throughput Optimized Hard disk drive (st1)", correct: true },
                { id: 2, text: "Instance Store", correct: false },
                { id: 3, text: "Cold Hard disk drive (sc1)", correct: true },
                { id: 4, text: "Provisioned IOPS Solid state drive (io1)", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
            options: [
                { id: 0, text: "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 1, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 2, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: true },
                { id: 3, text: "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
            options: [
                { id: 0, text: "Host-based Routing", correct: false },
                { id: 1, text: "Path-based Routing", correct: true },
                { id: 2, text: "HTTP header-based routing", correct: false },
                { id: 3, text: "Query string parameter-based routing", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time", correct: false },
                { id: 1, text: "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: true },
                { id: 2, text: "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling", correct: false },
                { id: 3, text: "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
            options: [
                { id: 0, text: "Use AWS Lambda function to run the workflow processes", correct: false },
                { id: 1, text: "Use Amazon EC2 on-demand instances to run the workflow processes", correct: false },
                { id: 2, text: "Use Amazon EC2 reserved instances to run the workflow processes", correct: false },
                { id: 3, text: "Use Amazon EC2 spot instances to run the workflow processes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
            options: [
                { id: 0, text: "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability", correct: false },
                { id: 1, text: "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency", correct: false },
                { id: 2, text: "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed", correct: true },
                { id: 3, text: "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously", correct: true },
                { id: 4, text: "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
            options: [
                { id: 0, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: true },
                { id: 1, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 2, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 3, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
            options: [
                { id: 0, text: "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 1, text: "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables", correct: true },
                { id: 2, text: "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 3, text: "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
            options: [
                { id: 0, text: "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue", correct: false },
                { id: 1, text: "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service", correct: true },
                { id: 2, text: "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again", correct: false },
                { id: 3, text: "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy", correct: false },
                { id: 4, text: "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 3, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet", correct: false },
                { id: 1, text: "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block", correct: false },
                { id: 2, text: "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC", correct: true },
                { id: 3, text: "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
            options: [
                { id: 0, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 1, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 2, text: "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 3, text: "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse", correct: false },
                { id: 1, text: "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes", correct: false },
                { id: 2, text: "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks", correct: false },
                { id: 3, text: "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
            options: [
                { id: 0, text: "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services", correct: false },
                { id: 1, text: "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC", correct: true },
                { id: 2, text: "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services", correct: false },
                { id: 3, text: "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
            options: [
                { id: 0, text: "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior", correct: false },
                { id: 1, text: "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration", correct: false },
                { id: 2, text: "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data", correct: true },
                { id: 3, text: "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands", correct: true },
                { id: 4, text: "Use Amazon Aurora Global Database to replicate data across regions for compatibility", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
            options: [
                { id: 0, text: "Use VPC security groups to control the network traffic to and from your file system", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system", correct: false },
                { id: 2, text: "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system", correct: false },
                { id: 3, text: "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance", correct: false },
                { id: 4, text: "Use an IAM policy to control access for clients who can mount your file system with the required permissions", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 1, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
                { id: 2, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
            options: [
                { id: 0, text: "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively", correct: false },
                { id: 1, text: "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures", correct: false },
                { id: 2, text: "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements", correct: false },
                { id: 3, text: "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
            options: [
                { id: 0, text: "The junior scientist does not need to pay any transfer charges for the image upload", correct: true },
                { id: 1, text: "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload", correct: false },
                { id: 2, text: "The junior scientist only needs to pay S3TA transfer charges for the image upload", correct: false },
                { id: 3, text: "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
            options: [
                { id: 0, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 1, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "It is not possible to access cross-account resources", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
            options: [
                { id: 0, text: "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EFS mount points", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with access to Amazon S3 based storage", correct: false },
                { id: 3, text: "Use Instance Store based Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
            options: [
                { id: 0, text: "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies", correct: false },
                { id: 1, text: "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods", correct: true },
                { id: 3, text: "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
            options: [
                { id: 0, text: "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class", correct: false },
                { id: 1, text: "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class", correct: false },
                { id: 2, text: "Store the intermediary query results in Amazon S3 Standard storage class", correct: true },
                { id: 3, text: "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow", correct: false },
                { id: 2, text: "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards", correct: false },
                { id: 3, text: "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
            options: [
                { id: 0, text: "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%", correct: false },
                { id: 2, text: "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%", correct: true },
                { id: 3, text: "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point", correct: true },
                { id: 1, text: "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution", correct: false },
                { id: 2, text: "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers", correct: true },
                { id: 1, text: "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers", correct: false },
                { id: 2, text: "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia", correct: false },
                { id: 3, text: "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
            options: [
                { id: 0, text: "AWS Global Accelerator", correct: true },
                { id: 1, text: "AWS Elastic Load Balancing (ELB)", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon CloudFront", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance", correct: false },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB", correct: false },
                { id: 2, text: "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB", correct: true },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Microsoft SQL Server on AWS", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: false },
                { id: 3, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume", correct: false },
                { id: 2, text: "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required", correct: true },
                { id: 3, text: "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
            options: [
                { id: 0, text: "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud", correct: true },
                { id: 1, text: "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
            options: [
                { id: 0, text: "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B", correct: true },
                { id: 1, text: "1 Amazon EC2 instance and 2 AMIs exist in Region B", correct: false },
                { id: 2, text: "1 Amazon EC2 instance and 1 AMI exist in Region B", correct: false },
                { id: 3, text: "1 Amazon EC2 instance and 1 snapshot exist in Region B", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
            options: [
                { id: 0, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 1, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: true },
                { id: 2, text: "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 3, text: "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 1, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
            options: [
                { id: 0, text: "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 1, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: true },
                { id: 2, text: "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 3, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
            options: [
                { id: 0, text: "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets", correct: false },
                { id: 1, text: "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations", correct: true },
                { id: 2, text: "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files", correct: false },
                { id: 3, text: "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test2: [
        {
            id: 1,
            text: "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)", correct: false },
                { id: 2, text: "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
            options: [
                { id: 0, text: "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: true },
                { id: 1, text: "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 2, text: "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 3, text: "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
            options: [
                { id: 0, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type", correct: false },
                { id: 1, text: "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed", correct: true },
                { id: 2, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance", correct: false },
                { id: 3, text: "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 1, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 2, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
                { id: 3, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 4, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
            options: [
                { id: 0, text: "AWS Lambda", correct: false },
                { id: 1, text: "AWS Step Functions", correct: false },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon CloudWatch", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 2, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Remove full database access for all IAM users in the organization", correct: false },
                { id: 1, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Only root user should have full database access in the organization", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to analyze API calls", correct: true },
                { id: 1, text: "Implement an IAM policy to forbid users to change Amazon S3 bucket settings", correct: false },
                { id: 2, text: "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations", correct: false },
                { id: 3, text: "Use Amazon S3 access logs to analyze user access using Athena", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
            options: [
                { id: 0, text: "Amazon Redshift", correct: false },
                { id: 1, text: "Amazon RDS", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: true },
                { id: 3, text: "Amazon ElastiCache", correct: false },
                { id: 4, text: "AWS Lambda", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
                { id: 3, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
                { id: 1, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 2, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 3, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
            options: [
                { id: 0, text: "Move to Amazon Redshift", correct: false },
                { id: 1, text: "Switch application code to AWS Lambda for better performance", correct: false },
                { id: 2, text: "Setup Amazon ElastiCache in front of Amazon RDS", correct: true },
                { id: 3, text: "Setup Amazon RDS Read Replicas", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 1, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 2, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
            options: [
                { id: 0, text: "Amazon ElastiCache for Redis/Memcached", correct: true },
                { id: 1, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon DocumentDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 1, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 1, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 3, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 1, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 2, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 3, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 4, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
                { id: 5, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator", correct: false },
                { id: 1, text: "Use AWS Direct Connect", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon S3", correct: true },
                { id: 1, text: "Amazon Kinesis", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 4, text: "Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 2, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 3, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 4, text: "The route for the health check is misconfigured", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
            options: [
                { id: 0, text: "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated", correct: false },
                { id: 1, text: "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume", correct: true },
                { id: 2, text: "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume", correct: false },
                { id: 3, text: "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use Spot Instances", correct: false },
                { id: 1, text: "Use a Cluster placement group", correct: true },
                { id: 2, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
                { id: 1, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 2, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 3, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
            options: [
                { id: 0, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data", correct: false },
                { id: 1, text: "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object", correct: true },
                { id: 2, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data", correct: false },
                { id: 3, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
                { id: 3, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 1, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
            options: [
                { id: 0, text: "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once", correct: true },
                { id: 1, text: "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts", correct: false },
                { id: 2, text: "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs", correct: false },
                { id: 3, text: "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda", correct: false },
                { id: 1, text: "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: false },
                { id: 2, text: "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda", correct: false },
                { id: 3, text: "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon Neptune", correct: true },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon Aurora", correct: false },
                { id: 3, text: "Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
                { id: 1, text: "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 3, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
                { id: 1, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 2, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 3, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 4, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "AWS Glue", correct: false },
                { id: 1, text: "Amazon EMR", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: true },
                { id: 3, text: "Amazon FSx for Windows File Server", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 2, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 3, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
                { id: 4, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
            options: [
                { id: 0, text: "Use Identity and Access Management (IAM) policies", correct: false },
                { id: 1, text: "Use Amazon S3 Bucket Policies", correct: true },
                { id: 2, text: "Use Security Groups", correct: false },
                { id: 3, text: "Use Access Control Lists (ACLs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 1, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 2, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 4, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 5, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 1, text: "Use AWS Glue to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 2, text: "Use AWS EMR to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 2, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 3, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 2, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
                { id: 3, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
            options: [
                { id: 0, text: "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs", correct: false },
                { id: 1, text: "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: true },
                { id: 2, text: "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: false },
                { id: 3, text: "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
            options: [
                { id: 0, text: "Use AWS_IAM authorization", correct: false },
                { id: 1, text: "Use Amazon Cognito User Pools", correct: true },
                { id: 2, text: "Use Amazon Cognito Identity Pools", correct: false },
                { id: 3, text: "Use AWS Lambda authorizer for Amazon API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 1, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 1, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 2, text: "NAT instance supports port forwarding", correct: true },
                { id: 3, text: "NAT gateway supports port forwarding", correct: false },
                { id: 4, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 5, text: "Security Groups can be associated with a NAT gateway", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "It is not possible to access cross-account resources", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
            options: [
                { id: 0, text: "7", correct: false },
                { id: 1, text: "3", correct: true },
                { id: 2, text: "14", correct: false },
                { id: 3, text: "15", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 1, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
                { id: 3, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
            options: [
                { id: 0, text: "Distribute the static content through Amazon EFS", correct: false },
                { id: 1, text: "Distribute the dynamic content through Amazon EFS", correct: false },
                { id: 2, text: "Distribute the static content through Amazon S3", correct: true },
                { id: 3, text: "Distribute the dynamic content through Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Database Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Glue", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 1, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 2, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 3, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 4, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "Software VPN", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "VPC Endpoint", correct: false },
                { id: 3, text: "AWS VPN CloudHub", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
            options: [
                { id: 0, text: "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
                { id: 1, text: "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis", correct: false },
                { id: 2, text: "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift", correct: true },
                { id: 3, text: "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 1, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
                { id: 3, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test3: [
        {
            id: 1,
            text: "A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. A Solutions Architect has been instructed to create a cost-effective solution without affecting the performance of the application.Which dynamic scaling policy should be used to satisfy this requirement?",
            options: [
                { id: 0, text: "Use simple scaling.", correct: false },
                { id: 1, text: "Use scheduled scaling.", correct: false },
                { id: 2, text: "Use suspend and resume scaling.", correct: false },
                { id: 3, text: "Use target tracking scaling.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "An\nAuto Scaling group\ncontains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling.\nStep scaling policies and simple scaling policies are two of the dynamic scaling options available for you to use. Both require you to create CloudWatch alarms for the scaling policies. Both require you to specify the high and low thresholds for the alarms. Both require you to define whether to add or remove instances, and how many, or set the group to an exact size. The main difference between the policy types is the step adjustments that you get with step scaling policies. When step adjustments are applied, and they increase or decrease the current capacity of your Auto Scaling group, the adjustments vary based on the size of the alarm breach.\nThe primary issue with simple scaling is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. Cooldown periods help to prevent the initiation of additional scaling activities before the effects of previous activities are visible.\nWith a target tracking scaling policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over-provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.\nHence, the correct answer is:\nUse target tracking scaling.\nThe option that says:\nUse simple scaling\nis incorrect because you need to wait for the cooldown period to complete before initiating additional scaling activities. Target tracking or step scaling policies can trigger a scaling activity immediately without waiting for the cooldown period to expire.\nThe option that says:\nUse scheduled scaling\nis incorrect because this policy is mainly used for predictable traffic patterns. You need to use the target tracking scaling policy to optimize the cost of your infrastructure without affecting the performance.\nThe option that says:\nUse suspend and resume scaling\nis incorrect because this type is used to temporarily pause scaling activities triggered by your scaling policies and scheduled actions.\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\nCheck out this AWS Auto Scaling Cheat Sheet:\nhttps://tutorialsdojo.com/aws-auto-scaling/",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "A financial services company plans to migrate its trading application from on-premises Microsoft Windows Server to Amazon Web Services (AWS).The solution must ensure high availability across multiple Availability Zones and offer low-latency access to block storage.Which of the following solutions will fulfill these requirements?",
            options: [
                { id: 0, text: "Deploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for Windows File Server for shared storage.", correct: false },
                { id: 1, text: "Deploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon Elastic File System (Amazon EFS) to provide shared storage between the instances. Configure Amazon EFS with cross-region replication to sync data across Availability Zones.", correct: false },
                { id: 2, text: "Configure the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for NetApp ONTAP to create a Multi-AZ file system and access the data via iSCSI protocol.", correct: true },
                { id: 3, text: "Configure the trading application on Amazon EC2 Windows instances across two Availability Zones. Use Amazon Simple Storage Service (Amazon S3) for storage and configure cross-region replication to sync data between S3 buckets in each Availability Zone.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Amazon FSx for NetApp ONTAP\nis a fully managed AWS service that provides high-performance, scalable file storage based on NetApp's ONTAP file system. It offers versatile storage options, supporting both file (NFS, SMB) and block (iSCSI) protocols, making it compatible with Windows, Linux, and macOS environments.\nThe Amazon FSx for NetApp ONTAP features Multi-AZ file systems designed to ensure continuous availability across AWS Availability Zones, providing high availability for your Windows Server workloads. It offers consistent sub-millisecond file operation latencies with SSD storage, essential for block storage workloads in Windows environments. FSx for NetApp ONTAP fully supports block storage protocols like iSCSI, commonly used in Windows Server settings, and it works seamlessly with the SMB protocol, ensuring compatibility with Windows Server and related applications.\nMoreover, FSx for NetApp ONTAP simplifies migrating from on-premises NetApp systems to AWS for users currently utilizing NetApp storage. It can scale to accommodate petabyte-scale datasets, making it suitable for large Windows Server environments.\nHence, the correct answer is:\nConfigure the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for NetApp ONTAP to create a Multi-AZ file system and access the data via iSCSI protocol.\nThe option that says:\nDeploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for Windows File Server for shared storage\nis incorrect. Amazon FSx for Windows File Server only provides shared,\nlow-latency file storage\nfor Windows environments. It doesn't support low-latency access to shared\nblock storage.\nThe option that says:\nDeploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon Elastic File System (Amazon EFS) to provide shared storage between the instances. Configure Amazon EFS with cross-region replication to sync data across Availability Zones\nis incorrect. While this option provides high availability across AZs, Amazon EFS is not optimized for Windows workloads and doesn't offer low-latency block storage.\nThe option that says:\nConfigure the trading application on Amazon EC2 Windows instances across two Availability Zones. Use Amazon Simple Storage Service (Amazon S3) for storage and configure cross-region replication to sync data between S3 buckets in each Availability Zone\nis incorrect. While this option provides high availability across AZs, Amazon S3 is primarily an object storage, not block storage. It doesn't offer the low-latency access required for a trading application.\nReferences:\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/high-availability-AZ.html\nCheck out this Amazon FSx Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-fsx/",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 3,
            text: "A company is using AWS Fargate to run a batch job whenever an object is uploaded to an Amazon S3 bucket. The minimum ECS task count is initially set to 1 to save on costs and should only be increased based on new objects uploaded to the S3 bucket.Which is the most suitable option to implement with the LEAST amount of effort?",
            options: [
                { id: 0, text: "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to a Lambda function that will run theStartTaskAPI command.", correct: false },
                { id: 1, text: "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task.", correct: true },
                { id: 2, text: "Set up an alarm in Amazon CloudWatch to monitor S3 object-level operations that are recorded on CloudTrail. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers the ECS cluster when new CloudTrail events are detected.", correct: false },
                { id: 3, text: "Set up an alarm in CloudWatch to monitor S3 object-level operations recorded on CloudTrail. Set two alarm actions to update the ECS task count to scale-out/scale-in depending on the S3 event.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon EventBridge (Amazon CloudWatch Events)\nis a serverless event bus that makes it easy to connect applications together. It uses data from your own applications, integrated software as a service (SaaS) applications, and AWS services. This simplifies the process of building event-driven architectures by decoupling event producers from event consumers. This allows producers and consumers to be scaled, updated, and deployed independently. Loose coupling improves developer agility in addition to application resiliency.\nYou can use Amazon EventBridge (Amazon CloudWatch Events) to run Amazon ECS tasks when certain AWS events occur. You can set up an EventBridge rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using the Amazon S3 PUT operation.\nHence, the correct answer is:\nSet up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task.\nThe option that says:\nSet up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to a Lambda function that will run the\nStartTask\nAPI command\nis incorrect. Although this solution meets the requirement, creating your own Lambda function for this scenario is not really necessary. It is much simpler to control ECS tasks directly as targets for the CloudWatch Event rule. Take note that the scenario asks for a solution that is the easiest to implement.\nThe option that says:\nSet up an alarm in Amazon CloudWatch to monitor S3 object-level operations that are recorded on CloudTrail. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers the ECS cluster when new CloudTrail events are detected\nis incorrect because using CloudTrail and CloudWatch Alarm creates an unnecessary complexity to what you want to achieve. Amazon EventBridge (Amazon CloudWatch Events) can directly target an ECS task on the Targets section when you create a new rule.\nThe option that says:\nSet up an alarm in CloudWatch to monitor CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event\nis incorrect because you can’t directly set CloudWatch Alarms to update the ECS task count.\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-ECS.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html\nCheck out this Amazon CloudWatch Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-cloudwatch/",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 1,
            text: "A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances with different instance types and sizes. The application is extensively used during office hours from 9 in the morning to 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.Which of the following is the MOST operationally efficient solution to implement to ensure the application works properly at the beginning of the day?",
            options: [
                { id: 0, text: "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization.", correct: false },
                { id: 1, text: "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization.", correct: false },
                { id: 2, text: "Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day.", correct: true },
                { id: 3, text: "Configure a Predictive scaling policy for the Auto Scaling group to automatically adjust the number of Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Scaling based on a schedule allows you to scale your application in response to predictable load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling activities based on the predictable traffic patterns of your web application.\nTo configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size specified by the scaling action. You can create scheduled actions for scaling one time only or for scaling on a recurring schedule.\nHence,\nconfiguring a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day\nis the correct answer. You need to configure a Scheduled scaling policy. This will ensure that the instances are already scaled up and ready before the start of the day since this is when the application is used the most.\nThe following options are both incorrect. Although these are valid solutions, it is still better to configure a Scheduled scaling policy as you already know the exact peak hours of your application. By the time either the CPU or Memory hits a peak, the application already has performance issues, so you need to ensure the scaling is done beforehand using a Scheduled scaling policy:\n-Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization\n-Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization\nThe option that says:\nConfigure a Predictive scaling policy for the Auto Scaling group to automatically adjust the number of Amazon EC2 instances\nis incorrect. Although this type of scaling policy can be used in this scenario, it is not the most operationally efficient option. Take note that the scenario mentioned that the Auto Scaling group consists of Amazon EC2 instances with different instance types and sizes. Predictive scaling assumes that your Auto Scaling group is homogenous, which means that all EC2 instances are of equal capacity. The forecasted capacity can be inaccurate if you are using a variety of EC2 instance sizes and types on your Auto Scaling group.\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html#predictive-scaling-limitations\nCheck out this AWS Auto Scaling Cheat Sheet:\nhttps://tutorialsdojo.com/aws-auto-scaling/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A company has a highly available architecture consisting of an Elastic Load Balancer and multiple Amazon EC2 instances configured with Auto Scaling across three Availability Zones. The company needs to monitor EC2 instances based on a specific metric that is not readily available in Amazon CloudWatch.Which of the following is a custom metric in CloudWatch that requires manual setup?",
            options: [
                { id: 0, text: "Memory Utilizationof an EC2 instance", correct: true },
                { id: 1, text: "CPU Utilizationof an EC2 instance", correct: false },
                { id: 2, text: "Disk Read activityof an EC2 instance", correct: false },
                { id: 3, text: "Network packets outof an EC2 instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Amazon CloudWatch\nhas Amazon EC2 Metrics available for monitoring. CPU Utilization identifies the processing power required to run an application upon a selected instance.\nNetwork Utilization\nidentifies the volume of incoming and outgoing network traffic to a single instance. The\nDisk Read\nmetric is used to determine the volume of data the application reads from the hard disk of the instance. This can be used to determine the speed of the application. However, there are certain metrics that are not readily available in CloudWatch which can be collected by setting up a custom metric.\nYou need to prepare a custom metric using CloudWatch Monitoring Scripts which is written in Perl. You can also install CloudWatch Agent to collect more system-level metrics from Amazon EC2 instances. Here's the list of some of the custom metrics that you can set up:\n-\nMemory utilization\n-\nDisk swap utilization\n-\nDisk space utilization\n-\nPage file utilization\n-\nLog collection\nHence, the correct answer is:\nMemory Utilization\nof an EC2 instance.\nThe option that says:\nCPU Utilization\nof an EC2 instance\nis incorrect because this metric is typically available by default in CloudWatch and does not require a custom metric setup.\nThe option that says:\nDisk Read activity\nof an EC2 instance\nis incorrect because only certain storage-related metrics, such as\ndisk space utilization\n, require custom monitoring. Whereas\ndisk read activity\nis already included in CloudWatch.\nThe option that says:\nNetwork packets out\nof an EC2 instance\nis incorrect because this metric is typically included in CloudWatch's default network utilization monitoring, so there is no need to configure a custom metric.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html#using_put_script\nCheck out these Amazon EC2 and Amazon CloudWatch Cheat Sheets:\nhttps://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/\nhttps://tutorialsdojo.com/amazon-cloudwatch/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "A company is using Amazon S3 to store frequently accessed data. When an object is created or deleted, the S3 bucket will send an event notification to the Amazon SQS queue. A solutions architect needs to create a solution that will notify the development and operations team about the created or deleted objects.Which of the following would satisfy this requirement?",
            options: [
                { id: 0, text: "Set up another SQS queue for the other team. Grant S3 permission to send a notification to the second SQS queue.", correct: false },
                { id: 1, text: "Create a new Amazon SNS FIFO topic for the other team. Grant S3 permission to send the notification to the second SNS topic.", correct: false },
                { id: 2, text: "Set up an Amazon SNS topic and configure two SQS queues to poll the SNS topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.", correct: false },
                { id: 3, text: "Create an Amazon SNS topic and configure two SQS queues to subscribe to the topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The\nAmazon S3\nnotification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.\nAmazon S3 supports the following destinations where it can publish events:\n- Amazon Simple Notification Service (Amazon SNS) topic\n- Amazon Simple Queue Service (Amazon SQS) queue\n- AWS Lambda\nIn Amazon SNS, the\nfanout\nscenario is when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.\nFor example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, SQS queues that are subscribed to the SNS topic receive identical notifications for the new order. An Amazon Elastic Compute Cloud (Amazon EC2) server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another Amazon EC2 server instance to a data warehouse for analysis of all orders received.\nBased on the given scenario, the existing setup sends the event notification to an SQS queue. Since you need to send the notification to the development and operations team, you can use a combination of Amazon SNS and SQS. By using the message fanout pattern, you can create a topic and use two Amazon SQS queues to subscribe to the topic. If Amazon SNS receives an event notification, it will publish the message to both subscribers.\nTake note that Amazon S3 event notifications are designed to be delivered at least once and to one destination only. You cannot attach two or more SNS topics or SQS queues for S3 event notification. Therefore, you must send the event notification to Amazon SNS.\nHence, the correct answer is:\nCreate an Amazon SNS topic and configure two SQS queues to subscribe to the topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.\nThe option that says:\nSet up another SQS queue for the other team. Grant S3 permission to send a notification to the second SQS queue\nis incorrect because you can only add 1 SQS or SNS at a time for Amazon S3 events notification. If you need to send the events to multiple subscribers, you should implement a message fanout pattern with Amazon SNS and Amazon SQS.\nThe option that says:\nCreate a new Amazon SNS FIFO topic for the other team. Grant S3 permission to send the notification to the second SNS topic\nis incorrect. Just as mentioned in the previous option, you can only add 1 SQS or SNS at a time for Amazon S3 events notification. In addition, neither Amazon SNS FIFO topic nor Amazon SQS FIFO queue is warranted in this scenario. Both of them can be used together to provide strict message ordering and message deduplication. The FIFO capabilities of each of these services work together to act as a fully managed service to integrate distributed applications that require data consistency in near-real-time.\nThe option that says:\nSet up an Amazon SNS topic and configure two SQS queues to poll the SNS topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic\nis incorrect because you can't poll Amazon SNS. Instead of configuring queues to poll Amazon SNS, you should configure each Amazon SQS queue to subscribe to the SNS topic.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#notification-how-to-overview\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
            options: [
                { id: 0, text: "Use Amazon FSx For Lustre and the Provisioned IOPS SSD (io1) volumes of Amazon EBS for hot and cold storage respectively.", correct: false },
                { id: 1, text: "Use Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.", correct: true },
                { id: 2, text: "Use Amazon Elastic File System and Amazon S3 for hot and cold storage respectively.", correct: false },
                { id: 3, text: "Use Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Hot storage\nrefers to the storage that keeps frequently accessed data (hot data).\nWarm storage\nrefers to the storage that keeps less frequently accessed data (warm data).\nCold storage\nrefers to the storage that keeps rarely accessed data (cold data). In terms of pricing, the colder the data,\nthe cheaper it is to store, and the costlier it is to access when needed.\nAmazon FSx For Lustre\nis a high-performance file system for fast processing of workloads. Lustre is a popular open-source\nparallel file system\nwhich stores data across multiple network file servers to maximize performance and reduce bottlenecks.\nAmazon FSx for Windows File Server\nis a fully managed Microsoft Windows file system with full support for the SMB protocol, Windows NTFS, and Microsoft Active Directory (AD) Integration.\nAmazon Elastic File System\nis a fully-managed file storage service that makes it easy to set up and scale file storage in the Amazon Cloud.\nAmazon S3 is\nan object storage service that offers industry-leading scalability, data availability, security, and performance. S3 offers different storage tiers for different use cases (frequently accessed data, infrequently accessed data, and rarely accessed data).\nThe question has two requirements:\nHigh-performance, parallel hot storage to process the training datasets concurrently.\nCost-effective cold storage to keep the archived datasets that are accessed infrequently\nIn this case, we can use\nAmazon FSx For Lustre\nfor the first requirement, as it provides a high-performance, parallel file system for hot data. On the second requirement, we can use Amazon S3 for storing cold data. Amazon S3 supports a cold storage system via Amazon S3 Glacier / Glacier Deep Archive.\nHence, the correct answer is:\nUse Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.\nThe option that says:\nUse Amazon FSx For Lustre and Amazon EBS Provisioned IOPS SSD (io1) volumes for hot and cold storage respectively\nis incorrect because the Provisioned IOPS SSD (io1) volumes are primarily designed for storing hot data (data that are frequently accessed) used in I/O-intensive workloads. EBS has a storage option called \"Cold HDD,\" but due to its price, it is not ideal for data archiving. EBS Cold HDD is much more expensive than Amazon S3 Glacier / Glacier Deep Archive and is often utilized in applications where sequential cold data is read less frequently.\nThe option that says:\nUse Amazon Elastic File System and Amazon S3 for hot and cold storage respectively\nis incorrect. Although EFS supports concurrent access to data, it does not have the high-performance ability that is typically required for machine learning workloads.\nThe option that says:\nUse Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively\nis incorrect because Amazon FSx For Windows File Server does not have a parallel file system, unlike Lustre.\nReferences:\nhttps://aws.amazon.com/fsx/\nhttps://docs.aws.amazon.com/whitepapers/latest/cost-optimization-storage-optimization/aws-storage-services.html\nhttps://aws.amazon.com/blogs/startups/picking-the-right-data-store-for-your-workload/\nCheck out this Amazon FSx Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-fsx/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement?",
            options: [
                { id: 0, text: "Create an API using Amazon API Gateway and use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic in seconds.", correct: false },
                { id: 1, text: "Create an API using Amazon API Gateway and use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic in seconds.", correct: false },
                { id: 2, text: "Create an API using Amazon API Gateway and use AWS Lambda to handle the bursts of traffic in seconds.", correct: true },
                { id: 3, text: "Create an API using Amazon API Gateway and use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic in seconds.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "AWS Lambda\nlets you run code without provisioning or managing servers. You pay only for the compute time you consume. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.\nThe first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. As more events come in, Lambda routes them to available instances and creates new instances as needed. When the number of requests decreases, Lambda stops unused instances to free up the scaling capacity for other functions.\nYour functions'\nconcurrency\nis the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region.\nBased on the given scenario, you need to create a solution that will satisfy the two requirements. The first requirement is to create a solution that will allow the users to access the data using an API. To implement this solution, you can use Amazon API Gateway. The second requirement is to handle the burst of traffic within seconds. You should use AWS Lambda in this scenario because Lambda functions can absorb reasonable bursts of traffic for approximately 15-30 minutes.\nLambda can scale faster than the regular Auto Scaling feature of Amazon EC2, Amazon Elastic Beanstalk, or Amazon ECS. This is because AWS Lambda is more lightweight than other computing services. Under the hood, Lambda can run your code to thousands of available AWS-managed EC2 instances (that could already be running) within seconds to accommodate traffic. This is faster than the Auto Scaling process of launching new EC2 instances that could take a few minutes or so. An alternative is to overprovision your compute capacity but that will incur significant costs. The best option to implement given the requirements is a combination of AWS Lambda and Amazon API Gateway.\nHence, the correct answer is:\nCreate an API using Amazon API Gateway and use AWS Lambda to handle the bursts of traffic.\nThe option that says:\nCreate an API using Amazon API Gateway and use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic in seconds\nis incorrect. AWS Lambda is a better option than Amazon ECS since it can handle a sudden burst of traffic within seconds and not minutes.\nThe option that says:\nCreate an API using Amazon API Gateway and use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic in seconds\nis incorrect because just like the previous option, the use of Auto Scaling has a delay of a few minutes as it launches new EC2 instances that will be used by Amazon Elastic Beanstalk.\nThe option that says:\nCreate an API using Amazon API Gateway and use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic in seconds\nis incorrect because the processing time of Amazon EC2 Auto Scaling to provision new resources takes minutes. Take note that in the scenario, a burst of traffic within seconds is expected to happen.\nReferences:\nhttps://aws.amazon.com/blogs/startups/from-0-to-100-k-in-seconds-instant-scale-with-aws-lambda/\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\nCheck out this AWS Lambda Cheat Sheet:\nhttps://tutorialsdojo.com/aws-lambda/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using Amazon DynamoDB. The application requires improved database performance by distributing the workload evenly and utilizing the provisioned throughput efficiently.Which of the following should be implemented for the DynamoDB table?",
            options: [
                { id: 0, text: "Reduce the number of partition keys in the DynamoDB table.", correct: false },
                { id: 1, text: "Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.", correct: true },
                { id: 2, text: "Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.", correct: false },
                { id: 3, text: "Avoid using a composite primary key, which is composed of a partition key and a sort key.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This in turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create \"hot\" partitions that result in throttling and use your provisioned I/O capacity inefficiently.\nThe optimal usage of a table's provisioned throughput depends not only on the workload patterns of individual items, but also on the partition-key design. This doesn't mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases.\nHence, the correct answer is:\nUse partition keys with high-cardinality attributes, which have a large number of distinct values for each item\n.\nThe option that says:\nReducing the number of partition keys in the DynamoDB table\nis incorrect. Instead of doing this, you should actually add more to improve its performance to distribute the I/O requests evenly and not simply avoid \"hot\" partitions.\nThe option that says:\nUsing partition keys with low-cardinality attributes, which have a few number of distinct values for each item\nis incorrect because this is only the exact opposite of the correct answer. Remember that the more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, the less distinct partition key values, the less evenly spread it would be across the partitioned space, which effectively slows the performance.\nThe option that says:\nAvoid using a composite primary key, which is composed of a partition key and a sort key\nis incorrect because as mentioned, a composite primary key will provide more partition for the table and in turn, improves the performance. Hence, it should be used and not avoided.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\nCheck out this Amazon DynamoDB Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-dynamodb/\nAmazon DynamoDB Overview:",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand Amazon EC2 instances that use Amazon Aurora as its database. Currently, the system stores the file documents that users upload in one of the attached Amazon EBS volumes. The system's performance has been observed to be slow, and the manager has instructed the team to improve the architecture.In this scenario, which solution should be implemented to achieve a scalable, highly available, POSIX-compliant shared file system?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket and use this as the storage for the CMS", correct: false },
                { id: 1, text: "Use Amazon EFS to provide a shared file system for concurrent access to data", correct: true },
                { id: 2, text: "Upgrade your existing EBS volumes to Provisioned IOPS SSD volumes", correct: false },
                { id: 3, text: "Leverage Amazon ElastiCache to cache frequently accessed data and reduce latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon Elastic File System (Amazon EFS)\nprovides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. When mounted on Amazon EC2 instances, an Amazon EFS file system provides a standard file system interface and file system access semantics, allowing you to seamlessly integrate Amazon EFS with your existing applications and tools. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, allowing Amazon EFS to provide a common data source for workloads and applications running on more than one Amazon EC2 instance.\nThis particular scenario tests your understanding of EBS, EFS, and S3. In this scenario, there is a fleet of On-Demand EC2 instances that store file documents from the users to one of the attached EBS Volumes. The system performance is quite slow because the architecture doesn't provide the EC2 instances parallel shared access to the file documents.\nAlthough an EBS Volume can be attached to multiple EC2 instances, you can only do so on instances within an availability zone. What we need is highly available storage that can span multiple availability zones. Take note as well that the type of storage needed here is file storage, which means that S3 is not the best service to use because it is primarily used for object storage.\nHence, the correct answer is:\nUse Amazon EFS to provide a shared file system for concurrent access to data\n.\nThe option that says:\nCreate an Amazon S3 bucket and use this as the storage for the CMS\nis incorrect because Amazon S3 is just an object storage service, which does not provide the required file system interface and file locking needed by a CMS.\nThe option that says:\nUpgrade your existing EBS volumes to Provisioned IOPS SSD volumes\nis incorrect because simply upgrading EBS does not address the requirement for a shared, POSIX-compliant file system. EBS volumes are block storage devices attached to a single instance, making them unsuitable for shared storage.\nThe option that says:\nLeverage Amazon ElastiCache to cache frequently accessed data and reduce latency\nis incorrect because this is an in-memory data store that improves the performance of your applications, which is not what you need since it is not a file storage.\nReferences:\nhttps://aws.amazon.com/efs/\nhttps://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\nhttps://docs.aws.amazon.com/efs/latest/ug/mount-multiple-ec2-instances.html\nCheck out this Amazon EFS Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-efs/\nCheck out this Amazon S3 vs EBS vs EFS Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "An e-commerce company runs a highly scalable web application that depends on an Amazon Aurora database. As the number of users increases, the read replica faces difficulties keeping up with the increasing read traffic, causing performance bottlenecks during peak periods.Which of the following will resolve the issue with the most cost-effective solution?",
            options: [
                { id: 0, text: "Increase the size of the Aurora DB cluster.", correct: false },
                { id: 1, text: "Use automatic scaling for the Aurora read replica using Aurora Auto Scaling.", correct: true },
                { id: 2, text: "Implement read scaling with Aurora Global Database.", correct: false },
                { id: 3, text: "Set up a read replica that can operate across different regions.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon Aurora\nis a cloud-based relational database service that provides better performance and reliability for database workloads. It is highly available and scalable, making it a great choice for businesses of any size. One of the key features of Amazon Aurora is Aurora Auto Scaling, which automatically adjusts the capacity of your Aurora database cluster based on the workload. This means that you don't have to worry about manually adjusting the ability of your database cluster to handle changes in demand. With Aurora Auto Scaling, you can be sure that your database cluster will always have the appropriate capacity to handle your workload while minimizing costs.\nAurora Auto Scaling\nis particularly useful for businesses that have fluctuating workloads. It ensures that your database cluster scales up or down as needed without manual intervention. This feature saves time and resources, allowing businesses to focus on other aspects of their operations. Aurora Auto Scaling is also cost-effective, as it helps minimize unnecessary expenses associated with overprovisioning or underprovisioning database resources.\nIn this scenario, the company can benefit from using Aurora Auto Scaling. This solution allows the system to dynamically manage resources, effectively addressing the surge in read traffic during peak periods. This dynamic management of resources ensures that the company pays only for the extra resources when they are genuinely required.\nHence, the correct answer is:\nUse automatic scaling for the Aurora read replica using Aurora Auto Scaling.\nThe option that says:\nIncrease the size of the Aurora DB cluster\nis incorrect because it's not economical to upsize the cluster just to alleviate the bottleneck during peak periods. A static increase in the DB cluster size results in constant costs, regardless of whether your database's resources are being fully utilized during off-peak periods or not.\nThe option that says:\nImplement read scaling with Aurora Global Database\nis incorrect. Amazon Aurora Global Database is primarily designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS Regions. While this can provide global availability, it introduces additional complexity and can be more expensive due to infrastructure and data transfer costs.\nThe option that says:\nSet up a read replica that can operate across different regions\nis incorrect. Setting up a read replica that operates across different regions can provide read scalability and load-balancing benefits by typically distributing the read traffic across regions. However, it is not the most cost-effective solution in this scenario since it incurs additional costs associated with inter-region data replication. Moreover, the issue is not related to cross-region availability but rather the read replica's performance within the current region.\nReferences:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\nCheck out this Amazon Aurora Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-aurora/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "A popular social network is hosted in AWS and is using a Amazon DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email.Which of the following is the most suitable solution to implement to meet the requirement?",
            options: [
                { id: 0, text: "Using the Amazon Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using Amazon SNS.", correct: false },
                { id: 1, text: "Create an AWS Lambda function that uses DynamoDB Streams Amazon Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an Amazon SNS Topic that will notify the subscribers via email when there is an update made by a particular user.", correct: false },
                { id: 2, text: "Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and an AWS Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using Amazon SNS.", correct: false },
                { id: 3, text: "Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to Amazon SNS Topic that will notify the subscribers via email.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "A\nDynamoDB stream\nis an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.\nWhenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A\nstream record\ncontains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items.\nAmazon DynamoDB is integrated with AWS Lambda so that you can create\ntriggers\n—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.\nIf you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow.\nHence, the correct answer is:\nEnable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to Amazon SNS Topic that will notify the subscribers via email\n.\nThe option that says:\nUsing the Amazon Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using Amazon SNS\nis incorrect. Although this is a valid solution, it is missing a vital step which is to enable DynamoDB Streams. With the DynamoDB Streams Kinesis Adapter in place, you can begin developing applications via the KCL interface, with the API calls seamlessly directed at the DynamoDB Streams endpoint. Remember that the DynamoDB Stream feature is not enabled by default.\nThe option that says:\nCreate an AWS Lambda function that uses DynamoDB Streams Amazon Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an Amazon SNS Topic that will notify the subscribers via email when there is an update made by a particular user\nis incorrect because just like in the above, you have to manually enable DynamoDB Streams first before you can use its endpoint.\nThe option that says:\nSet up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and an AWS Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using Amazon SNS\nis incorrect because the DynamoDB Accelerator (DAX) feature is primarily used to significantly improve the in-memory read performance of your database, and not to capture the time-ordered sequence of item-level modifications. You should use DynamoDB Streams in this scenario instead.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html\nCheck out this Amazon DynamoDB Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-dynamodb/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A retail company receives raw.csvdata files into its Amazon S3 bucket from multiple sources on an hourly basis, with an average file size of 2 GB.An automated process must be implemented to convert these.csvfiles into the more efficient Apache Parquet format and store the converted files in another S3 bucket. Additionally, the conversion process must be automatically initiated each time a new file is uploaded into the S3 bucket.Which of the following options must be implemented to meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use an AWS Lambda function triggered by anS3 PUTevent to convert the.csvfiles to Parquet format. Use the AWS Transfer Family with SFTP service to move the output files to the target S3 bucket.", correct: false },
                { id: 1, text: "Utilize an AWS Glue extract, transform, and load (ETL) job to process and convert the.csvfiles to Apache Parquet format and then store the output files into the target S3 bucket. Set up an S3 Event Notification to track everyS3 PUTevent and invoke the ETL job in Glue through Amazon SQS.", correct: true },
                { id: 2, text: "Set up an Apache Spark job running in an Amazon EC2 instance and create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitorS3 PUTevents in the S3 bucket. Configure AWS Lambda to invoke the Spark job for every new.csvfile added via a Function URL.", correct: false },
                { id: 3, text: "Create an ETL (Extract, Transform, Load) job and a Data Catalog table in AWS Glue. Configure the Glue crawler to run on a schedule to check for new files in the S3 bucket every hour and convert them to Parquet format.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "AWS Glue\nis a powerful ETL service that easily moves data between different data stores. By using AWS Glue, you can easily create and manage ETL jobs to transfer data from various sources, such as Amazon S3, Amazon RDS, and Amazon Redshift. Additionally, AWS Glue enables you to transform your data as needed to fit your specific needs. One of the key advantages of AWS Glue is its automatic schema discovery and mapping, which allows you to easily map data from different sources with different schemas.\nWhen working with big data processing, it is often necessary to convert data from one format to another to optimize processing efficiency. Apache Parquet is a columnar storage format that is designed to provide higher efficiency and performance for big data processing. By storing and processing large amounts of data with high compression rates and faster query times, Parquet can offer significant benefits to the company. Fortunately, Parquet is compatible with many data processing frameworks such as Spark, Hive, and Hadoop, making it a versatile format for big data processing. By using AWS Glue and other AWS services, you can easily convert their .csv files to the more efficient Apache Parquet format and store the output files in an S3 bucket, making it easy to access and process large amounts of data.\nHence the correct answer is:\nUtilize an AWS Glue extract, transform, and load (ETL) job to process and convert the\n.csv\nfiles to Apache Parquet format and then store the output files into the target S3 bucket. Set up an S3 Event Notification to track every\nS3 PUT\nevent and invoke the ETL job in Glue through Amazon SQS.\nThe option that says:\nUse an AWS Lambda function triggered by an\nS3 PUT\nevent to convert the\n.csv\nfiles to Parquet format. Use the AWS Transfer Family with SFTP service to move the output files to the target S3 bucket\nis incorrect. The conversion of the CSV files to Parquet format by using a combination of a Lambda function and S3 event notification would work; however, this is not the most efficient solution when primarily handling large amounts of data. The Lambda function has a maximum execution time limit which means that converting large files may result in timeout issues. Using the AWS Transfer Family with SFTP service to move the output files to the target S3 bucket is unnecessary too. Moreover, reading the records has to be delivered via a data stream since a Lambda function has a memory limit. This entails additional effort compared with using AWS Glue.\nThe option that says:\nSet up an Apache Spark job running in an Amazon EC2 instance and create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor\nS3 PUT\nevents in the S3 bucket. Configure AWS Lambda to invoke the Spark job for every new\n.csv\nfile added via a Function URL\nis incorrect. Running Spark on EC2 instances requires manual provisioning, monitoring, and maintenance, leading to time and additional costs. Additionally, using Amazon EventBridge (Amazon CloudWatch Events) to trigger the Spark job through a Function URL adds complexity and potential points of failure. Thus, this option just introduces unnecessary complexity and operational overhead.\nThe option that says:\nCreate an ETL (Extract, Transform, Load) job and a Data Catalog table in AWS Glue. Configure the Glue crawler to run on a schedule to check for new files in the S3 bucket every hour and convert them to Parquet format\nis incorrect. Although it is right to create an ETL job using AWS Glue, simply triggering the job on a scheduled basis rather than being triggered automatically by a new file upload is not ideal. It is not as efficient as using an S3 event trigger to initiate the conversion process immediately upon file upload.\nReferences:\nhttps://aws.amazon.com/blogs/big-data/run-aws-glue-crawlers-using-amazon-s3-event-notifications/\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html\nhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html\nCheck out this AWS Glue Cheat Sheet:\nhttps://tutorialsdojo.com/aws-glue/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead.In this scenario, how can you protect the backend systems of the platform from traffic spikes?",
            options: [
                { id: 0, text: "Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.", correct: false },
                { id: 1, text: "Enable throttling limits and result caching in API Gateway.", correct: true },
                { id: 2, text: "Use CloudFront in front of the API Gateway to act as a cache.", correct: false },
                { id: 3, text: "Move the Lambda function in a VPC.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon API Gateway\nprovides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any request over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence,\nenabling throttling limits and result caching in API Gateway\nis the correct answer.\nYou can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.\nThe option that says:\nSwitch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling\nis incorrect since there is no need to transfer your applications to other services.\nUsing CloudFront in front of the API Gateway to act as a cache\nis incorrect because CloudFront only speeds up content delivery which provides a better latency experience for your users. It does not help much for the backend.\nMoving the Lambda function in a VPC\nis incorrect because this answer is irrelevant to what is being asked. A VPC is your own virtual private cloud where you can launch AWS services.\nReference\n:\nhttps://aws.amazon.com/api-gateway/faqs/\nCheck out this Amazon API Gateway Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-api-gateway/\nHere is an in-depth tutorial on Amazon API Gateway:",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A company is using a combination of API Gateway and AWS Lambda for the web services of an online web portal that is accessed by hundreds of thousands of clients each day. The company will be announcing a new revolutionary product, and it is expected that the web portal will receive a massive number of visitors from all around the globe.How can the back-end systems and applications be protected from traffic spikes?",
            options: [
                { id: 0, text: "Use throttling limits in API Gateway", correct: true },
                { id: 1, text: "API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything.", correct: false },
                { id: 2, text: "Manually upgrade the Amazon EC2 instances being used by API Gateway", correct: false },
                { id: 3, text: "Deploy Multi-AZ in API Gateway with Read Replica", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Amazon API Gateway\nprovides throttling at multiple levels including global and by a service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.\nAmazon API Gateway tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response.\nHence, the correct answer is:\nUse throttling limits in API Gateway.\nThe option that says:\nAPI Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything\nis incorrect. Although it can scale using AWS Edge locations, you still need to configure the throttling, typically to further manage the bursts of your APIs.\nThe option that says:\nManually upgrade the Amazon EC2 instances being used by API Gateway\nis incorrect because API Gateway is a fully managed service and hence, you do not have access to its underlying resources.\nThe option that says:\nDeploying Multi-AZ in API Gateway with Read Replica\nis incorrect because only RDS has Multi-AZ and Read Replica capabilities, and not API Gateway.\nReferences:\nhttps://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching\nhttps://docs.aws.amazon.com/apigateway/\nCheck out this Amazon API Gateway Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-api-gateway/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "An online learning company hosts its Microsoft .NET e-Learning application on a Windows Server in its on-premises data center. The application uses an Oracle Database Standard Edition as its backend database.The company wants a high-performing solution to migrate this workload to the AWS cloud to take advantage of the cloud’s high availability. The migration process should minimize development changes, and the environment should be easier to manage.Which of the following options should be implemented to meet the company requirements? (Select TWO.)",
            options: [
                { id: 0, text: "Perform a homogeneous migration by moving the Oracle database to Amazon RDS for Oracle in a Multi-AZ deployment using AWS Database Migration Service (AWS DMS).", correct: true },
                { id: 1, text: "Refactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.", correct: false },
                { id: 2, text: "Use AWS Application Migration Service (AWS MGN) to migrate the on-premises Oracle database server to a new Amazon EC2 instance.", correct: false },
                { id: 3, text: "Rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment which runs in multiple Availability Zones.", correct: true },
                { id: 4, text: "Provision and replatform the application to Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. Use the Windows Server Amazon Machine Image (AMI) and deploy the .NET application using to the ECS cluster via the ECS Anywhere service.", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "AWS Database Migration Service (AWS DMS)\nis a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud or between combinations of cloud and on-premises setups.\nWith AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync. If you want to migrate to a different database engine, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform. You then use AWS DMS to migrate the data.\nAWS Elastic Beanstalk\nreduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.\nAWS Elastic Beanstalk for .NET makes it easier to deploy, manage, and scale your ASP.NET web applications that use Amazon Web Services. Elastic Beanstalk for .NET is available to anyone who is developing or hosting a web application that uses IIS.\nHence, the correct answers are:\n- Perform a homogeneous migration by moving the Oracle database to Amazon RDS for Oracle in a Multi-AZ deployment using AWS Database Migration Service (AWS DMS).\n- Rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment which runs in multiple Availability Zones.\nThe option that says:\nRefactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate\nis incorrect. This will take significant changes to the application as you will refactor, or do a code change to, the codebase in order for it to become a serverless container application. Remember that the scenario explicitly mentioned that the migration process should minimize development changes. A better solution is to simply rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment, which doesn't require any code changes.\nThe option that says:\nUse AWS Application Migration Service (AWS MGN) to migrate the on-premises Oracle database server to a new Amazon EC2 instance\nis incorrect. Amazon RDS primarily supports standard Oracle databases so it would be better to use AWS DMS for the database migration, not AWS MGN.\nThe option that says:\nProvision and replatform the application to Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. Use the Windows Server Amazon Machine Image (AMI) and deploy the .NET application using to the ECS cluster via the ECS Anywhere service\nis incorrect. This may be possible, but it is not recommended for this scenario because you will have to manage the underlying EC2 instances of your Amazon ECS cluster that will run the application. It would be better just to use Elastic Beanstalk to take care of provisioning the resources for your .NET application. Keep in mind that doing a replatform-type migration like this one entails significant development changes, which is not suitable with the requirements given in the scenario.\nReferences:\nhttps://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\nCheck out these AWS DMS and AWS Elastic Beanstalk Cheat Sheets:\nhttps://tutorialsdojo.com/aws-database-migration-service/\nhttps://tutorialsdojo.com/aws-elastic-beanstalk/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "A popular social media website uses a Amazon CloudFront web distribution to serve static content to millions of users around the globe. Recently, the website has received a number of complaints about long login times. Additionally, there are instances where users encounter HTTP 504 errors. The manager has instructed the team to significantly reduce login time and further optimize the system.Which of the following options should be used together to set up a cost-effective solution that improves the application's performance? (Select TWO.)",
            options: [
                { id: 0, text: "Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your AWS Lambda functions to execute the authentication process in AWS locations closer to the users.", correct: true },
                { id: 1, text: "Establish multiple Amazon VPCs in different AWS regions and configure a transit VPC to interconnect all of your resources. To handle the requests faster, set up AWS Lambda functions in each region with the AWS Serverless Application Model (SAM) service.", correct: false },
                { id: 2, text: "Configure your origin to add aCache-Control max-agedirective to your objects, and specify the longest practical value formax-ageto increase the cache hit ratio of your CloudFront distribution.", correct: false },
                { id: 3, text: "Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.", correct: false },
                { id: 4, text: "Implement an origin failover by creating an origin group that includes two origins. Assign one as the primary origin and the other as secondary, which enables CloudFront to automatically switch to if the primary origin encounters specific HTTP status code failure responses.", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:\n- After CloudFront receives a request from a viewer (viewer request)\n- Before CloudFront forwards the request to the origin (origin request)\n- After CloudFront receives the response from the origin (origin response)\n- Before CloudFront forwards the response to the viewer (viewer response)\nIn the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing.\nTherefore, the correct answers are:\n-\nCustomize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your AWS Lambda functions to execute the authentication process in AWS locations closer to the users.\n-\nImplement an origin failover by creating an origin group that includes two origins. Assign one as the primary origin and the other as secondary, which enables CloudFront to automatically switch to if the primary origin encounters specific HTTP status code failure responses.\nThe option that says:\nEstablish multiple Amazon VPCs in different AWS regions and configure a transit VPC to interconnect all of your resources. To handle the requests faster, set up AWS Lambda functions in each region with the AWS Serverless Application Model (SAM) service\nis incorrect because of the same reason provided above. Although setting up multiple VPCs across various regions which are just connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead.\nThe option that says:\nConfigure your origin to add a\nCache-Control max-age\ndirective to your objects, and specify the longest practical value for\nmax-age\nto increase the cache hit ratio of your CloudFront distribution\nis incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can only improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the sluggish authentication process of your global users and not just the caching of the static objects.\nThe option that says:\nDeploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user\nis incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with\nminimal cost\n.\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\nCheck out these Amazon CloudFront and AWS Lambda Cheat Sheets:\nhttps://tutorialsdojo.com/amazon-cloudfront/\nhttps://tutorialsdojo.com/aws-lambda/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A healthcare organization wants to build a system that can predict drug prescription abuse. The organization will gather real-time data from multiple sources, which include Personally Identifiable Information (PII). It's crucial that this sensitive information is anonymized prior to landing in a NoSQL database for further processing.Which solution would meet the requirements?",
            options: [
                { id: 0, text: "Create a data lake in Amazon S3 and use it as the primary storage for patient health data. Use an S3 trigger to run an AWS Lambda function that performs anonymization. Send the anonymized data to Amazon DynamoDB.", correct: false },
                { id: 1, text: "Stream the data in an Amazon DynamoDB table. Enable DynamoDB Streams, and configure an AWS Lambda function withAmazonDynamoDBFullAccesspermissions to perform anonymization on newly written items.", correct: false },
                { id: 2, text: "Deploy an Amazon Data Firehose stream to capture and transform the streaming data. Deliver the anonymized data to Amazon Redshift for analysis.", correct: false },
                { id: 3, text: "Ingest real-time data using Amazon Kinesis Data Stream. Use an AWS Lambda function to anonymize the PII, then store it in Amazon DynamoDB.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Amazon Kinesis Data Streams (KDS)\nis a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources.\nKinesis Data Streams integrates seamlessly with AWS Lambda, which can be utilized to transform and anonymize Personally Identifiable Information (PII) in transit before it is stored in any system. This ensures that sensitive information is anonymized immediately, preventing unanonymized PII from being stored in any storage system, as required. The anonymized data is then stored in Amazon DynamoDB, a NoSQL database suitable for handling the processed data for further analysis, such as predicting drug prescription abuse.\nHence, the correct answer is:\nIngest real-time data using Amazon Kinesis Data Stream. Use an AWS Lambda function to anonymize the PII, then store it in Amazon DynamoDB.\nThe option that says:\nCreate a data lake in Amazon S3 and use it as the primary storage for patient health data. Use an S3 trigger to run an AWS Lambda function that performs anonymization. Send the anonymized data to Amazon DynamoDB\nis incorrect. This approach stores unanonymized PII in Amazon S3 before the Lambda function anonymizes it. This simply violates the requirement that PII be anonymized before landing in any storage system. Storing sensitive data in S3, even temporarily, only increases the risk of exposure and does not comply with the privacy requirements.\nThe option that says:\nStream the data in an Amazon DynamoDB table. Enable DynamoDB Streams, and configure an AWS Lambda function with\nAmazonDynamoDBFullAccess\npermissions to perform anonymization on newly written items\nis incorrect. DynamoDB Streams processes changes to already written data, meaning unanonymized PII would be stored in DynamoDB before anonymization, violating the requirement. Additionally, using\nAmazonDynamoDBFullAccess\nviolates the principle of least privilege, as it primarily grants more permissions than necessary.\nThe option that says:\nDeploy an Amazon Data Firehose stream to capture and transform the streaming data. Deliver the anonymized data to Amazon Redshift for analysis\nis incorrect. The requirement specifies that the anonymized data must be stored in a NoSQL database. Amazon Redshift is a relational data warehousing solution, not a NoSQL database, making this option unsuitable.\nReferences:\nhttps://aws.amazon.com/kinesis/data-streams/\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\nCheck out this Amazon Kinesis Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-kinesis/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company has a web application that uses Internet Information Services (IIS) for Windows Server. A file share is used to store the application data on the network-attached storage of the company’s on-premises data center. To achieve a highly available system, the company plans to migrate the application and file share to AWS.Which of the following can be used to fulfill this requirement?",
            options: [
                { id: 0, text: "Migrate the existing file share configuration to AWS Storage Gateway.", correct: false },
                { id: 1, text: "Migrate the existing file share configuration to Amazon FSx for Windows File Server.", correct: true },
                { id: 2, text: "Migrate the existing file share configuration to Amazon EFS.", correct: false },
                { id: 3, text: "Migrate the existing file share configuration to Amazon EBS.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon FSx for Windows File Server\nprovides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. Amazon FSx for Windows File Server has the features, performance, and compatibility to easily lift and shift enterprise applications to the AWS Cloud. It is accessible from Windows, Linux, and macOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently.\nIn this scenario, you need to migrate your existing file share configuration to the cloud. Among the options given, the best possible answer is Amazon FSx. A file share is a specific folder in your file system, including the folder's subfolders, which you make accessible to your compute instances via the SMB protocol. To migrate file share configurations from your on-premises file system, you must migrate your files first to Amazon FSx before migrating your file share configuration.\nHence, the correct answer is:\nMigrate the existing file share configuration to Amazon FSx for Windows File Server\n.\nThe option that says:\nMigrate the existing file share configuration to AWS Storage Gateway\nis incorrect because AWS Storage Gateway is primarily used to integrate your on-premises network to AWS but not for migrating your applications. Using a file share in Storage Gateway implies that you will still keep your on-premises systems, and not entirely migrate it.\nThe option that says:\nMigrate the existing file share configuration to Amazon EFS\nis incorrect because it is stated in the scenario that the company is using a file share that runs on a Windows server. Remember that Amazon EFS only supports Linux workloads.\nThe option that says:\nMigrate the existing file share configuration to Amazon EBS\nis incorrect because EBS is primarily used as block storage for EC2 instances and not as a shared file system. A file share is a specific folder in a file system that you can access using a server message block (SMB) protocol. Amazon EBS does not support SMB protocol.\nReferences:\nhttps://aws.amazon.com/fsx/windows/faqs/\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-file-share-config-to-fsx.html\nCheck out this Amazon FSx Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-fsx/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. The Solutions Architect must determine the fastest way to aggregate all the data.Which of the following options can satisfy the given requirement?",
            options: [
                { id: 0, text: "Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.", correct: true },
                { id: 1, text: "Upload the data to the closest Amazon S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket.", correct: false },
                { id: 2, text: "Use AWS Snowball Edge to transfer large amounts of data.", correct: false },
                { id: 3, text: "Set up a Site-to-Site VPN connection.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Amazon S3\nis object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers industry-leading durability, availability, performance, security, and virtually unlimited scalability at very low costs. Amazon S3 is also designed to be highly flexible. Store any type and amount of data that you want; read the same piece of data a million times or only for emergency disaster recovery; build a simple FTP application or a sophisticated web application.\nSince the weather forecasting application is located in N.Virginia, you need to transfer all the data in the same AWS Region. With Amazon S3 Transfer Acceleration, you can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Multipart upload allows you to upload a single object as a set of parts. After all the parts of your object are uploaded, Amazon S3 then presents the data as a single object. This approach is the fastest way to aggregate all the data.\nHence, the correct answer is:\nEnable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.\nThe option that says:\nUpload the data to the closest Amazon S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket\nis incorrect because replicating the objects to the destination bucket typically takes about 15 minutes. Take note that the requirement in the scenario is to aggregate the data in the fastest way.\nThe option that says:\nUse AWS Snowball Edge to transfer large amounts of data\nis incorrect because the end-to-end time to transfer up to 80 TB of data into AWS Snowball Edge is only approximately one week.\nThe option that says:\nSet up a Site-to-Site VPN connection\nis incorrect because setting up a VPN connection is not needed in this scenario. Site-to-Site VPN is just used for establishing secure connections between an on-premises network and Amazon VPC. Also, this approach is not the fastest way to transfer your data. You must use Amazon S3 Transfer Acceleration.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A company wishes to query data that resides in multiple AWS accounts from a central data lake. Each account has its own Amazon S3 bucket that stores data unique to its business function. Access to the data lake must be granted based on user roles.Which solution will minimize overhead and costs while meeting the required access patterns?",
            options: [
                { id: 0, text: "Use AWS Lake Formation to consolidate data from multiple accounts into a single account.", correct: true },
                { id: 1, text: "Use Amazon Data Firehose to consolidate data from multiple accounts into a single account.", correct: false },
                { id: 2, text: "Create a scheduled AWS Lambda function using Amazon EventBridge for transferring data from multiple accounts to the S3 buckets of the central account.", correct: false },
                { id: 3, text: "Use AWS Control Tower to centrally manage each account's S3 buckets.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "AWS Lake Formation\nis a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions.\nAmazon S3 forms the storage layer for Lake Formation. If you already use S3, you typically begin by registering existing S3 buckets that contain your data. Lake Formation creates new buckets for the data lake and imports data into them. AWS always stores this data in your account, and only you have direct access to it.\nAWS Lake Formation is integrated with AWS Glue which you can use to create a data catalog that describes available datasets and their appropriate business applications. Lake Formation lets you define policies and control data access with simple “grant and revoke permissions to data” sets at granular levels. You can assign permissions to IAM users, roles, groups, and Active Directory users using federation. You specify permissions on catalog objects (like tables and columns) rather than on buckets and objects.\nThus, the correct answer is:\nUse AWS Lake Formation to consolidate data from multiple accounts into a single account.\nThe option that says:\nUse Amazon Data Firehose to consolidate data from multiple accounts into a single account\nis incorrect because setting up a Data Firehose in each and every account to move data into a single location is just costly and impractical. A better approach is to set up cross-account sharing which is free with AWS Lake Formation.\nThe option that says:\nCreate a scheduled AWS Lambda function using Amazon EventBridge for transferring data from multiple accounts to the S3 buckets of the central account\nis incorrect. This could be done by utilizing the AWS SDK, but implementation would be difficult and quite challenging to manage. Remember that the scenario explicitly mentioned that the solution must minimize management overhead.\nThe option that says:\nUse AWS Control Tower to centrally manage each account's S3 buckets\nis incorrect because the AWS Central Tower service is primarily used to manage and govern multiple AWS accounts and not just S3 buckets. Using the AWS Lake Formation service is a more suitable choice.\nReferences:\nhttps://aws.amazon.com/blogs/big-data/building-securing-and-managing-data-lakes-with-aws-lake-formation/\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html\nCheck out this AWS Lake Formation Cheat Sheet:\nhttps://tutorialsdojo.com/aws-lake-formation/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 19,
            text: "A global IT company with offices around the world has multiple AWS accounts. To improve efficiency and drive costs down, the Chief Information Officer (CIO) wants to set up a solution that centrally manages their AWS resources. This will allow them to procure AWS resources centrally and share resources such as AWS Transit Gateways, AWS License Manager configurations, or Amazon Route 53 Resolver rules across their various accounts.As the Solutions Architect, which combination of options should you implement in this scenario? (Select TWO.)",
            options: [
                { id: 0, text: "Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.", correct: true },
                { id: 1, text: "Use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts.", correct: false },
                { id: 2, text: "Use AWS Control Tower to easily and securely share your resources with your AWS accounts.", correct: false },
                { id: 3, text: "Consolidate all of the company's accounts using AWS Organizations.", correct: true },
                { id: 4, text: "Consolidate all of the company's accounts using AWS ParallelCluster.", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM.\nMany organizations use multiple accounts to create administrative or billing isolation, and limit the impact of errors. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge.\nYou can procure AWS resources centrally, and use RAM to share resources such as subnets or License Manager configurations with other accounts. This eliminates the need to provision duplicate resources in every account in a multi-account environment, reducing the operational overhead of managing those resources in every account.\nAWS Organizations is an account management service that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage. With Organizations, you can create member accounts and invite existing accounts to join your organization. You can organize those accounts into groups and attach policy-based controls.\nHence, the correct combination of options in this scenario is:\n- Consolidate all of the company's accounts using AWS Organizations.\n- Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.\nThe option that says:\nUse the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts\nis incorrect. Although you can delegate access to resources that are in different AWS accounts using IAM, this process is extremely tedious and entails a lot of operational overhead since you have to manually set up cross-account access to each and every AWS account of the company. A better solution is to use AWS Resources Access Manager instead.\nThe option that says:\nUse AWS Control Tower to easily and securely share your resources with your AWS accounts\nis incorrect because AWS Control Tower simply offers the easiest way to set up and govern a new, secure, multi-account AWS environment. This is not the most suitable service to use to securely share your resources across AWS accounts or within your Organization. You have to use AWS Resources Access Manager (RAM) instead.\nThe option that says:\nConsolidate all of the company's accounts using AWS ParallelCluster\nis incorrect because AWS ParallelCluster is simply an AWS-supported open-source cluster management tool that makes it easy for you to deploy and manage High-Performance Computing (HPC) clusters on AWS. In this particular scenario, it is more appropriate to use AWS Organizations to consolidate all of your AWS accounts.\nReferences:\nhttps://aws.amazon.com/ram/\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A car dealership website hosted in Amazon EC2 stores car listings in an Amazon Aurora database managed by Amazon RDS. Once a vehicle has been sold, its data must be removed from the current listings and forwarded to a distributed processing system.Which of the following options can satisfy the given requirement?",
            options: [
                { id: 0, text: "Create an RDS event subscription and send the notifications to Amazon SQS. Configure the SQS queues to fan out the event notifications to multiple Amazon SNS topics. Process the data using AWS Lambda functions.", correct: false },
                { id: 1, text: "Create an RDS event subscription and send the notifications to AWS Lambda. Configure the Lambda function to fanout the event notifications to multiple Amazon SQS queues to update the target groups.", correct: false },
                { id: 2, text: "Create an RDS event subscription and send the notifications to Amazon SNS. Configure the SNS topic to fan out the event notifications to multiple Amazon SQS queues. Process the data using AWS Lambda functions.", correct: false },
                { id: 3, text: "Create a native function or a stored procedure that invokes an AWS Lambda function. Configure the Lambda function to send event notifications to an Amazon SQS queue for the processing system to consume.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "You can invoke an AWS Lambda function from an Amazon Aurora MySQL-Compatible Edition DB cluster with a native function or a stored procedure. This approach can be useful when you want to integrate your database running on Aurora MySQL with other AWS services. For example, you might want to capture data changes whenever a row in a table is modified in your database.\nIn the scenario, you can trigger a Lambda function whenever a listing is deleted from the database. You can then write the logic of the function to send the listing data to an SQS queue and have different processes consume it.\nHence, the correct answer is:\nCreate a native function or a stored procedure that invokes an AWS Lambda function. Configure the Lambda function to send event notifications to an Amazon SQS queue for the processing system to consume.\nThe option that says:\nCreate an RDS event subscription and send the notifications to Amazon SQS. Configure the SQS queues to fan out the event notifications to multiple Amazon SNS topics. Process the data using AWS Lambda functions\nis incorrect because RDS event subscriptions typically notify about operational changes rather than data modifications. This method does not capture database modifications like\nINSERT, DELETE,\nor\nUPDATE\n.\nThe option that says:\nCreate an RDS event subscription and send the notifications to AWS Lambda. Configure the Lambda function to fan out the event notifications to multiple Amazon SQS queues to update the processing system\nis incorrect because RDS event subscriptions primarily focus on operational-level changes rather than capturing direct data modifications.\nThe option that says:\nCreate an RDS event subscription and send the notifications to Amazon SNS. Configure the SNS topic to fan out the event notifications to multiple Amazon SQS queues. Process the data using AWS Lambda functions\nis incorrect because RDS event subscriptions only track infrastructure-related events and not actual database changes.\nReferences:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html\nhttps://aws.amazon.com/blogs/database/capturing-data-changes-in-amazon-aurora-using-aws-lambda/\nCheck out this Amazon Aurora Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-aurora/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
            options: [
                { id: 0, text: "Use a DynamoDB VPC endpoint and an S3 VPC endpoint to route all access to these services via private endpoints.", correct: true },
                { id: 1, text: "Use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints.", correct: false },
                { id: 2, text: "Enable DynamoDB Encryption at Rest with the default AWS-managed key and S3 Server-Side Encryption with the default AWS KMS key to route all traffic to DynamoDB and S3 via private endpoints.", correct: false },
                { id: 3, text: "Use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "A\nVPC endpoint\nallows you to privately connect your VPC to supported AWS and VPC endpoint services powered by AWS PrivateLink without needing an Internet gateway, NAT computer, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\nIn the scenario, you are asked to configure private endpoints to send data to Amazon DynamoDB and Amazon S3 without accessing the public Internet. Among the options given, VPC endpoint is the most suitable service that will allow you to use private IP addresses to access both DynamoDB and S3 without any exposure to the public internet.\nHence, the correct answer is:\nUse a DynamoDB VPC endpoint and an S3 VPC endpoint to route all access to these services via private endpoints.\nThe option that says:\nEnable DynamoDB Encryption at Rest with the default AWS-managed key and S3 Server-Side Encryption with the default AWS KMS key to route all traffic to DynamoDB and S3 via private endpoints\nis incorrect because encryption at rest does not affect the traffic routing. Encryption manages data security but does not control how traffic is routed between services.\nThe option that says:\nUse AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints\nis incorrect because AWS Direct Connect is primarily used to establish a dedicated network connection from your premises to AWS. The scenario didn't say that the company is using its on-premises server or has a hybrid cloud architecture.\nThe option that says:\nUse AWS VPN CloudHub to route all access in S3 and DynamoDB to a private endpoint\nis incorrect because AWS VPN CloudHub is typically used to provide secure communication between remote sites and not for creating a private endpoint to access Amazon S3 and DynamoDB within the Amazon network.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\nhttps://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html\nCheck out this Amazon VPC Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-vpc/",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 1,
            text: "There was an incident in a production environment where user data stored in an Amazon S3 bucket was accidentally deleted by a Junior DevOps Engineer. The issue was escalated to management, and after a few days, an instruction was given to improve the security and protection of AWS resources.What combination of the following options will protect the S3 objects in the bucket from both accidental deletion and overwriting? (Select TWO.)",
            options: [
                { id: 0, text: "Enable Versioning", correct: true },
                { id: 1, text: "Provide access to S3 data strictly through pre-signed URL only", correct: false },
                { id: 2, text: "Disallow S3 Delete using an IAM bucket policy", correct: false },
                { id: 3, text: "Enable S3 Intelligent-Tiering", correct: false },
                { id: 4, text: "Enable Multi-Factor Authentication Delete", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "By using Versioning and enabling MFA (Multi-Factor Authentication) Delete, you can secure and recover your S3 objects from accidental deletion or overwrite.\nVersioning is a means of keeping multiple variants of an object in the same bucket. Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.\nYou can also optionally add another layer of security by configuring a bucket to enable MFA (Multi-Factor Authentication) Delete, which requires additional authentication for either of the following operations:\n- Change the versioning state of your bucket\n- Permanently delete an object version\nMFA Delete requires two forms of authentication together:\n- Your security credentials\n- The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device.\nHence, the correct answers are:\n- Enable Versioning\n- Enable Multi-Factor Authentication Delete\nThe option that says:\nProviding access to S3 data strictly through pre-signed URL only\nis incorrect since a pre-signed URL gives access to the object identified in the URL. Pre-signed URLs are useful when customers perform an object upload to your S3 bucket, but does not help in preventing accidental deletes.\nThe option that says:\nDisallowing S3 Delete using an IAM bucket policy\nis incorrect since you still want users to be able to delete objects in the bucket, and you just want to prevent accidental deletions. Disallowing S3 Delete using an IAM bucket policy will restrict all delete operations to your bucket.\nThe option that says:\nEnabling S3 Intelligent-Tiering\nis incorrect since S3 intelligent tiering does not help in this situation.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "An application that records weather data every minute is deployed in a fleet of Amazon EC2 Spot instances and uses a MySQL RDS database instance. Currently, there is only one Amazon RDS instance running in one Availability Zone. The database needs to be improved to ensure high availability by enabling synchronous data replication to another RDS instance.Which of the following performs synchronous data replication in RDS?",
            options: [
                { id: 0, text: "RDS DB instance running as a Multi-AZ deployment", correct: true },
                { id: 1, text: "RDS Read Replica", correct: false },
                { id: 2, text: "Amazon DynamoDB Read Replica", correct: false },
                { id: 3, text: "Amazon CloudFront running as a Multi-AZ deployment", correct: false },
            ],
            correctAnswers: [0],
            explanation: "When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous\nstandby\nreplica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.\nTherefore, the correct answer is:\nRDS DB instance running as a Multi-AZ deployment\nRDS Read Replica\nis incorrect as a Read Replica primarily provides an asynchronous replication instead of synchronous.\nAmazon DynamoDB Read Replica\nis incorrect since it does not offer a Read Replica feature. It typically uses global tables to replicate data across multiple AWS Regions.\nAmazon CloudFront running as a Multi-AZ deployment\nis incorrect as it also does not have a Read Replica feature. It simply caches content at edge locations rather than replicating data in the database.\nReferences:\nhttps://aws.amazon.com/rds/details/multi-az/\nhttps://aws.amazon.com/rds/features/multi-az/\nCheck out this Amazon RDS Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "An online cryptocurrency exchange platform is hosted in AWS, utilizing an Amazon ECS Cluster and Amazon RDS in a Multi-AZ Deployments configuration. The application heavily uses the RDS instance to process complex read and write database operations. To maintain reliability, availability, and performance, it is necessary to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of CPU bandwidth and total memory consumed by each process.Which of the following is the most suitable solution to monitor the database properly?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to monitor the CPU Utilization of your database.", correct: false },
                { id: 1, text: "Create a script that collects and publishes custom metrics to Amazon CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.", correct: false },
                { id: 2, text: "Enable Enhanced Monitoring in RDS.", correct: true },
                { id: 3, text: "Check theCPU%andMEM%metrics which are readily available in the RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Amazon RDS offers a powerful feature known as\nEnhanced Monitoring\n, which provides detailed metrics in real-time about the operating system (OS) underlying your database instances. This feature allows users to monitor performance at a granular level through the AWS Management Console or by accessing the Enhanced Monitoring JSON output via CloudWatch Logs. By default, these metrics are retained in CloudWatch Logs for 30 days, but this retention period can be adjusted by modifying the retention settings for the\nRDSOSMetrics\nlog group in CloudWatch.\nEnhanced Monitoring differs from standard CloudWatch metrics in that it gathers data directly from an agent installed on the instance, rather than from the hypervisor, which is used by CloudWatch. This distinction can lead to slight variations between the two sets of metrics. For instance, CloudWatch provides CPU utilization metrics based on the hypervisor's view, while Enhanced Monitoring captures detailed insights from the instance itself, offering a more accurate representation of resource usage at the OS level.\nThis feature is particularly beneficial for users who need in-depth visibility into how individual processes or threads on a DB instance utilize CPU resources. The differences in metric data may become more pronounced when using smaller instance classes, as multiple virtual machines are often managed by the same hypervisor, affecting the accuracy of hypervisor-based metrics.\nHence, the correct answer is:\nEnable Enhanced Monitoring in RDS.\nThe option that says:\nUsing Amazon CloudWatch to monitor the CPU Utilization of your database\nis incorrect. Although you can use this to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch primarily gathers metrics about CPU utilization from the hypervisor for a DB instance while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.\nThe option that says:\nCreate a script that collects and publishes custom metrics to Amazon CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics\nis incorrect. Although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is just not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database processes. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance.\nThe option that says:\nCheck the\nCPU%\nand\nMEM%\nmetrics which are readily available in the RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance\nis incorrect because the CPU% and MEM% metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option.\nReferences:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch\nCheck out these Amazon CloudWatch and Amazon RDS Cheat Sheets:\nhttps://tutorialsdojo.com/amazon-cloudwatch/\nhttps://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 4,
            text: "A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in an on-premises data center and uses an Oracle database. Due to a recent cooling problem in its data center, the company urgently needs to migrate its infrastructure to AWS to improve the performance of its applications. As the Solutions Architect, the responsibility is to ensure that the database is properly migrated and remains available in case of database server failure in the future, following AWS Prescriptive Guidance for database migration and high availability.Which combination of actions would meet the requirement? (Select TWO.)",
            options: [
                { id: 0, text: "Launch an Oracle database instance in Amazon RDS with Recovery Manager (RMAN) enabled.", correct: false },
                { id: 1, text: "Convert the database schema using the AWS Schema Conversion Tool.", correct: false },
                { id: 2, text: "Create an Oracle database in Amazon RDS with Multi-AZ deployments.", correct: true },
                { id: 3, text: "Migrate the Oracle database to a non-cluster Amazon Aurora with a single instance.", correct: false },
                { id: 4, text: "Migrate the Oracle database to AWS using the AWS Database Migration Service", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.\nIn case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora) so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.\nIn this scenario, the best RDS configuration to use is an Oracle database in RDS with Multi-AZ deployments to ensure high availability even if the primary database instance goes down. You can use AWS DMS to move the on-premises database to AWS with minimal downtime and zero data loss. It supports over 20 engines, including Oracle to Aurora MySQL, MySQL to RDS for MySQL, SQL Server to Aurora PostgreSQL, MongoDB to DocumentDB, Oracle to Redshift, and S3.\nHence, the correct answers are:\n- Create an Oracle database in Amazon RDS with Multi-AZ deployments.\n-\nMigrate the Oracle database to AWS using the AWS Database Migration Service.\nThe option that says:\nLaunch an Oracle database instance in Amazon RDS with Recovery Manager (RMAN) enabled\nis incorrect because Oracle RMAN is not supported in RDS.\nThe option that says:\nConvert the database schema using the AWS Schema Conversion Tool\nis incorrect. AWS Schema Conversion Tool is typically used for heterogeneous migrations where you're moving from one type of database to another (e.g., Oracle to PostgreSQL). In the scenario, the migration is homogenous, meaning it's an Oracle-to-Oracle migration. As a result, there's no need to convert the schema since you're staying within the same database type.\nThe option that says:\nMigrate the Oracle database to a non-cluster Amazon Aurora with a single instance\nis incorrect. While a single-instance Aurora can be a feasible solution for non-critical applications or environments like development or testing, it is typically not suitable for applications that demand high availability.\nReferences:\nhttps://aws.amazon.com/rds/details/multi-az/\nhttps://aws.amazon.com/dms/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nCheck out this Amazon RDS Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store several types of files. Based on user trends, files that are older than 2 years must be stored in a different storage class. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement? (Select TWO.)",
            options: [
                { id: 0, text: "Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Glacier after 2 years.", correct: true },
                { id: 1, text: "Use Amazon EFS and create a lifecycle policy that will move the objects to EFS-IA after 2 years.", correct: false },
                { id: 2, text: "Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Standard-IA after 2 years.", correct: true },
                { id: 3, text: "Use Amazon EBS volumes to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.", correct: false },
                { id: 4, text: "Use a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "Amazon S3\nstores data as objects within buckets. An object is a file and any optional metadata that describes the file. To store a file in Amazon S3, you upload it to a bucket. When you upload a file as an object, you can set permissions on the object and any metadata. Buckets are containers for objects. You can have one or more buckets. You can control access for each bucket, deciding who can create, delete, and list objects in it. You can also choose the geographical region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects.\nTo move a file to a different storage class, you can use Amazon S3 or Amazon EFS. Both services have lifecycle configurations. Take note that Amazon EFS can only transition a file to the IA storage class after 90 days. Since you need to move the files that are older than 2 years to a more cost-effective and scalable solution, you should use the Amazon S3 lifecycle configuration. With S3 lifecycle rules, you can transition files to S3 Standard IA or S3 Glacier. Using S3 Glacier expedited retrieval, you can quickly access your files within 1-5 minutes.\nHence, the correct answers are:\n- Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Glacier after 2 years.\n- Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Standard-IA after 2 years.\nThe option that says:\nUse Amazon EFS and create a lifecycle policy that will move the objects to EFS-IA after 2 years\nis incorrect because the maximum days of EFS lifecycle policies is only up to 365 days. Therefore, it still does not meet the requirement of moving files older than 2 years or 730 days.\nThe option that says:\nUse Amazon EBS volumes to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years\nis incorrect because the Amazon EBS simply costs more and is not as scalable as Amazon S3. It has some limitations when accessed by multiple EC2 instances. There are also huge costs involved in using the multi-attach feature on a Provisioned IOPS EBS volume to allow multiple EC2 instances to access the volume.\nThe option that says:\nUse a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years\nis incorrect because RAID (Redundant Array of Independent Disks) is just a data storage virtualization technology that combines multiple storage devices to achieve higher performance or data durability. RAID 0 can stripe multiple volumes together for greater I/O performance than you can achieve with a single volume. On the other hand, RAID 1 can mirror two volumes together to achieve on-instance redundancy.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\nhttps://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html\nhttps://docs.aws.amazon.com/efs/latest/ug/API_LifecyclePolicy.html\nhttps://aws.amazon.com/s3/faqs/\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "An e-commerce company utilizes a regional Amazon API Gateway to host its public REST APIs. The API Gateway endpoint is accessed through a custom domain name set up with an Amazon Route 53 alias record. To support continuous improvement, the company intends to launch a new version of its APIs with enhanced features and performance optimizations.How can the company reduce customer disruption and ensure MINIMAL data loss during the update process in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Create a new API Gateway with the updated version of the APIs in OpenAPI JSON or YAML file format, but keep the same custom domain name for the new API Gateway.", correct: false },
                { id: 1, text: "Implement a canary release deployment strategy for the API Gateway. Deploy the latest version of the APIs to a canary stage and direct a portion of the user traffic to this stage. Verify the new APIs. Gradually increase the traffic percentage, monitor for any issues, and, if successful, promote the canary stage to production.", correct: true },
                { id: 2, text: "Modify the existing API Gateway with the updated version of the APIs, but keep the same custom domain name for the new API Gateway by using the import-to-update operation in either overwrite or merge mode.", correct: false },
                { id: 3, text: "Implement a blue-green deployment strategy for the API Gateway, deploying the latest version of the APIs to the green environment. Route some user traffic to it, validate the new APIs, and once thoroughly validated, promote the green environment to production.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon API Gateway\nis a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It is a front door for your APIs, enabling you to design and implement scalable, highly available, and secure APIs. With Amazon API Gateway, you can create RESTful APIs that any HTTP client, such as web browsers and mobile devices, can consume.\nImplementing a canary release deployment strategy for the API Gateway is a great way to ensure your APIs remain stable and reliable. This strategy involves releasing a new version of your API to a small subset of users, allowing you to test the latest version in a controlled environment.\nIf the new version performs well, you can gradually roll out the update to the rest of your users. This approach lets you catch any issues before they affect your entire user base, minimizing the impact on your customers. By using Amazon API Gateway, you can quickly implement a canary release deployment strategy, ensuring that your APIs are always up-to-date and performing at their best.\nHence, the correct answer is:\nImplement a canary release deployment strategy for the API Gateway. Deploy the latest version of the APIs to a canary stage and direct a portion of the user traffic to this stage. Verify the new APIs. Gradually increase the traffic percentage, monitor for any issues, and, if successful, promote the canary stage to production.\nThe option that says:\nCreate a new API Gateway with the updated version of the APIs in OpenAPI JSON or YAML file format, but keep the same custom domain name for the new API Gateway\nis incorrect. Upgrading to a new API Gateway using an updated version of the APIs in OpenAPI JSON or YAML file format while keeping the same custom domain name can typically result in downtime and confusion during the switch. This is because of DNS propagation delays, which can negatively affect users and even lead to data loss.\nThe option that says:\nModify the existing API Gateway with the updated version of the APIs, but keep the same custom domain name for the new API Gateway by using the import-to-update operation in either overwrite or merge mode\nis incorrect. Using the import-to-update operation in either overwrite or merge mode may not provide enough isolation and control testing for the new version of the APIs. If something goes wrong during the update process, it could just lead to data loss on the existing API Gateway, potentially affecting all customers simultaneously.\nThe option that says:\nImplement a blue-green deployment strategy for the API Gateway, deploying the latest version of the APIs to the green environment. Route some user traffic to it, validate the new APIs, and once thoroughly validated, promote the green environment to production\nis incorrect. In a blue-green deployment, the blue (existing) and green (updated) environments must be provisioned and maintained. This adds complexity and cost to the update process, which breaks the cost requirement that's explicitly mentioned in the scenario. Additionally, directing some user traffic to the green environment may only lead to issues for those users, especially if there are undiscovered bugs or performance problems in the updated APIs.\nReferences:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api-update.html\nCheck out this Amazon API Gateway Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-api-gateway/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "A company plans to migrate its on-premises workload to AWS. The current architecture is composed of a Microsoft SharePoint server that uses a Windows shared file storage. The Solutions Architect needs to use a cloud storage solution that is highly available and can be integrated with Active Directory for access control and authentication.Which of the following options can satisfy the given requirement?",
            options: [
                { id: 0, text: "Launch an Amazon EC2 Windows Server to mount a new Amazon S3 bucket as a file volume.", correct: false },
                { id: 1, text: "Create a file system using Amazon EFS and join it to an Active Directory domain.", correct: false },
                { id: 2, text: "Create a Network File System (NFS) file share using AWS Storage Gateway.", correct: false },
                { id: 3, text: "Create a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Amazon FSx for Windows File Server\nprovides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. Amazon FSx is accessible from Windows, Linux, and MacOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently.\nAmazon FSx works with Microsoft Active Directory to integrate with your existing Microsoft Windows environments. You have two options to provide user authentication and access control for your file system: AWS Managed Microsoft Active Directory and Self-managed Microsoft Active Directory.\nTake note that after you create an Active Directory configuration for a file system, you can't change that configuration. However, you can create a new file system from a backup and change the Active Directory integration configuration for that file system. These configurations allow the users in your domain to use their existing identity to access the Amazon FSx file system and to control access to individual files and folders.\nHence, the correct answer is:\nCreate a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS.\nThe option that says:\nCreate a file system using Amazon EFS and join it to an Active Directory domain\nis incorrect because Amazon EFS does not support Windows systems, only Linux OS. You should use Amazon FSx for Windows File Server instead to satisfy the requirement in the scenario.\nThe option that says:\nLaunch an Amazon EC2 Windows Server to mount a new Amazon S3 bucket as a file volume\nis incorrect because you can't integrate Amazon S3 with your existing Active Directory to provide authentication and access control.\nThe option that says:\nCreate a Network File System (NFS) file share using AWS Storage Gateway\nis incorrect because NFS file share is primarily used for Linux systems. Remember that the requirement in the scenario is to use a Windows shared file storage. Therefore, you must use an SMB file share instead, which supports Windows OS and Active Directory configuration. Alternatively, you can also use the Amazon FSx for Windows File Server file system.\nReferences:\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html\nhttps://aws.amazon.com/fsx/windows/faqs/\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/CreatingAnSMBFileShare.html\nCheck out this Amazon FSx Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-fsx/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A company has a cloud architecture composed of Linux and Windows Amazon EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, the Solutions Architect must create a solution that enables monitoring of memory and disk utilization metrics for all instances.Which of the following is the most suitable monitoring solution to implement?",
            options: [
                { id: 0, text: "Use the default Amazon CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all the EC2 instances.", correct: false },
                { id: 1, text: "Install the Amazon CloudWatch agent to all the EC2 instances that gather the memory and disk utilization data. View the custom metrics in the CloudWatch console.", correct: true },
                { id: 2, text: "Enable the Enhanced Monitoring option in EC2 and install Amazon CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.", correct: false },
                { id: 3, text: "Use Amazon Inspector and install the Inspector agent to all EC2 instances.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon CloudWatch\nhas available Amazon EC2 Metrics for you to use for monitoring CPU utilization, Network utilization, Disk performance, and Disk Reads/Writes. In case you need to monitor the below items, you need to prepare a custom metric using a Perl or other shell script, as there are no ready-to-use metrics for the following:\nMemory utilization\nDisk swap utilization\nDisk space utilization\nPage file utilization\nLog collection\nTake note that there is a multi-platform CloudWatch agent which can be installed on both Linux and Windows-based instances. You can use a single agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. This agent supports both Windows Server and Linux and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. It is recommended that you use the new agent instead of the older monitoring scripts to collect metrics and logs.\nHence, the correct answer is:\nInstall the Amazon CloudWatch agent to all the EC2 instances that gather the memory and disk utilization data. View the custom metrics in the CloudWatch console.\nThe option that says:\nUse the default Amazon CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all the EC2 instances\nis incorrect because, by default, CloudWatch does not automatically provide memory and disk utilization metrics of your instances. You have to simply set up custom CloudWatch metrics to monitor the memory, disk swap, disk space, and page file utilization of your instances.\nThe option that says:\nEnable the Enhanced Monitoring option in EC2 and install Amazon CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard\nis incorrect because Enhanced Monitoring is only a feature of Amazon RDS. By default, Enhanced Monitoring metrics are only stored for 30 days in the CloudWatch Logs.\nThe option that says:\nUse Amazon Inspector and install the Inspector agent to all EC2 instances\nis incorrect because Amazon Inspector is primarily an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. It does not provide a custom metric to track the memory and disk utilization of each and every EC2 instance in your VPC.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html#using_put_script\nCheck out this Amazon CloudWatch Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-cloudwatch/\nCloudWatch Agent vs  SSM Agent vs Custom Daemon Scripts:\nhttps://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/\nComparison of AWS Services Cheat Sheets:\nhttps://tutorialsdojo.com/comparison-of-aws-services/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 9,
            text: "An organization requires a persistent block storage volume to support its mission-critical workloads. The backup data will be stored in an object storage service and, after 30 days, transitioned to an archival storage service for long-term retention.What should be done to meet the above requirement?",
            options: [
                { id: 0, text: "Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.", correct: true },
                { id: 1, text: "Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA.", correct: false },
                { id: 2, text: "Attach an instance store volume in your existing Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.", correct: false },
                { id: 3, text: "Attach an instance store volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Amazon Elastic Block Store (EBS)\nis an easy-to-use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. A broad range of workloads, such as relational and non-relational databases, enterprise applications, containerized applications, big data analytics engines, file systems, and media workflows are widely deployed on Amazon EBS.\nAmazon Simple Storage Service (Amazon S3)\nis an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.\nIn an\nS3 Lifecycle configuration\n, you can define rules to transition objects from one storage class to another to save on storage costs. Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:\nIn this scenario, three services are required to implement this solution. The mission-critical workloads mean that you need to have a persistent block storage volume and the designed service for this is Amazon EBS volumes. The second workload needs to have an object storage service, such as Amazon S3, to store your backup data. Amazon S3 enables you to configure the lifecycle policy from S3 Standard to different storage classes. For the last one, it needs archive storage such as Amazon S3 Glacier Flexible Retrieval.\nHence, the correct answer is:\nAttach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.\nThe option that says:\nAttach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA\nis incorrect because this lifecycle policy will transition your objects into an infrequently accessed storage class and not a storage class for data archiving.\nThe option that says:\nAttach an instance store volume in your existing Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval\nis incorrect because an Instance Store volume is simply a temporary block-level storage for EC2 instances. Also, you can't attach instance store volumes to an instance after you've launched it. You can specify the instance store volumes for your instance only when you launch it.\nThe option that says:\nAttach an instance store volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA\nis incorrect. Just like the previous option, the use of instance store volume is not suitable for mission-critical workloads because the data can be lost if the underlying disk drive fails, the instance stops, or if the instance is terminated. In addition, Amazon S3 Glacier Flexible Retrieval is a more suitable option for data archival instead of Amazon S3 One Zone-IA.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\nhttps://aws.amazon.com/s3/storage-classes/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/\nTutorials Dojo's AWS Storage Services Cheat Sheets:\nhttps://tutorialsdojo.com/aws-cheat-sheets-storage-services/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 10,
            text: "A suite of web applications is hosted in an Auto Scaling group of Amazon EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application.Which EC2 instance will be the first one to be terminated by the Auto Scaling group?",
            options: [
                { id: 0, text: "The EC2 instance which has the least number of user sessions", correct: false },
                { id: 1, text: "The EC2 instance which has been running for the longest time", correct: false },
                { id: 2, text: "The EC2 instance launched from the oldest launch template.", correct: true },
                { id: 3, text: "The instance will be randomly selected by the Auto Scaling group", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:\n1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch template.\n2. Determine which unprotected instances in the selected Availability Zone use the oldest launch template. If there is one such instance, terminate it.\n3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.\n4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.\nThe following flow diagram illustrates how the default termination policy works:\nHence, the correct answer is:\nThe EC2 instance launched from the oldest launch template.\nThe option that says:\nThe EC2 instance which has the least number of user sessions\nis incorrect because the number of user sessions is not typically a factor considered by Amazon EC2 Auto Scaling groups when deciding which instances to terminate during a scale-in event.\nThe option that says:\nThe EC2 instance which has been running for the longest time\nis incorrect because the duration for which an EC2 instance has been running is not primarily a factor considered by Amazon EC2 Auto Scaling groups when deciding which instances to terminate during a scale-in event.\nThe option that says:\nThe instance will be randomly selected by the Auto Scaling group\nis incorrect because Amazon EC2 Auto Scaling groups do not randomly select instances for termination during a scale-in event.\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\nCheck out this AWS Auto Scaling Cheat Sheet:\nhttps://tutorialsdojo.com/aws-auto-scaling/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A logistics company plans to automate its order management application. The company wants to use SFTP file transfer for uploading business-critical documents. Since the files are confidential, encryption at rest is required, and high availability must be ensured. Additionally, each file must be automatically deleted one month after creation.Which of the following options should be implemented to meet the company’s requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Create an Amazon S3 bucket with encryption enabled. Configure AWS Transfer for SFTP to securely upload files to the S3 bucket. Configure the retention policy on the SFTP server to delete files after a month.", correct: false },
                { id: 1, text: "Create an Amazon Elastic File System (Amazon EFS) and enable encryption. Configure AWS Transfer for SFTP to securely upload files to the EFS file system. Apply an EFS lifecycle policy to delete files after 30 days.", correct: false },
                { id: 2, text: "Provision an Amazon EC2 instance and install the SFTP service. Mount an encrypted Amazon EFS file system on the EC2 instance to store the uploaded files. Add a cron job to delete the files older than a month.", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with encryption enabled. Launch an AWS Transfer for SFTP endpoint to securely upload files to the S3 bucket. Configure an S3 lifecycle rule to delete files after a month.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "AWS Transfer for SFTP\nenables you to easily move your file transfer workloads that use the Secure Shell File Transfer Protocol (SFTP) to AWS without needing to modify your applications or manage any SFTP servers.\nTo get started with AWS Transfer for SFTP (AWS SFTP) you create an SFTP server and map your domain to the server endpoint, select authentication for your SFTP clients using service-managed identities, or integrate your own identity provider, and select your Amazon S3 buckets to store the transferred data. Your existing users can continue to operate with their existing SFTP clients or applications. Data uploaded or downloaded using SFTP is available in your Amazon S3 bucket, and can be used for archiving or processing in AWS.\nAn\nAmazon S3 Lifecycle\nconfiguration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:\nTransition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them.\nExpiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf.\nTherefore, the correct answer is:\nCreate an Amazon S3 bucket with encryption enabled. Launch an AWS Transfer for SFTP endpoint to securely upload files to the S3 bucket. Configure an S3 lifecycle rule to delete files after a month.\nYou can use S3 as the storage service for your AWS Transfer SFTP-enabled server.\nThe option that says:\nCreate an Amazon S3 bucket with encryption enabled. Configure AWS Transfer for SFTP to securely upload files to the S3 bucket. Configure the retention policy on the SFTP server to delete files after a month\nis incorrect. The 30-day retention policy must be primarily configured on the Amazon S3 bucket. There is no retention policy option on AWS Transfer for SFTP.\nThe option that says:\nCreate an Amazon Elastic File System (Amazon EFS) and enable encryption. Configure AWS Transfer for SFTP to securely upload files to the EFS file system. Apply an EFS lifecycle policy to delete files after 30 days\nis incorrect. This may be possible, however, the EFS lifecycle management doesn't delete objects. It can only transition files in and out of the \"Infrequent Access\" tier.\nThe option that says:\nProvision an Amazon EC2 instance and install the SFTP service. Mount an encrypted Amazon EFS file system on the EC2 instance to store the uploaded files. Add a cron job to delete the files older than a month\nis incorrect. This option is possible, however, it entails greater operational overhead since you need to manage the EC2 instance and SFTP service.\nReferences:\nhttps://aws.amazon.com/aws-transfer-family/\nhttps://docs.aws.amazon.com/transfer/latest/userguide/create-server-sftp.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\nCheck out these AWS Transfer Family and Amazon S3 Cheat Sheets:\nhttps://tutorialsdojo.com/amazon-s3/\nhttps://tutorialsdojo.com/aws-transfer-family/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "An online shopping platform is hosted on an Auto Scaling group of Amazon EC2 Spot instances and utilizes Amazon Aurora PostgreSQL as its database. It is required to optimize database workloads in the cluster by directing the production traffic to high-capacity instances and routing the reporting queries from the internal staff to the low-capacity instances.Which is the most suitable configuration for the application as well as the Aurora database cluster to achieve this requirement?",
            options: [
                { id: 0, text: "Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.", correct: false },
                { id: 1, text: "In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries.", correct: false },
                { id: 2, text: "Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.", correct: true },
                { id: 3, text: "Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Amazon Aurora\ntypically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an\nendpoint\n. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available.\nFor certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.\nUsing endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.\nThe custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances.\nHence, the correct answer is:\nCreate a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.\nThe option that says:\nConfiguring your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas\nis incorrect. Although it is true that a reader endpoint enables your Aurora database to automatically perform load-balancing among all the Aurora Replicas, it is quite limited to doing read operations only. You still need to use a custom endpoint to load-balance the database connections based on the specified criteria.\nThe option that says:\nIn your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries\nis incorrect because a cluster endpoint (also known as a writer endpoint) for an Aurora DB cluster simply connects to the current primary DB instance for that DB cluster. This endpoint can perform write operations in the database such as DDL statements, which is perfect for handling production traffic but not suitable for handling queries for reporting since there will be no write database operations that will be sent. Moreover, the endpoint does not point to lower-capacity or high-capacity instances as per the requirement. A better solution for this is to use a custom endpoint.\nThe option that says:\nDo nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances\nis incorrect because Aurora does not do this by default. You have to create custom endpoints in order to accomplish this requirement.\nReferences:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Endpoints.Custom.html\nCheck out this Amazon Aurora Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-aurora/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 13,
            text: "A company is experiencing repeated outages in the Availability Zone where its Amazon RDS database instance is deployed, resulting in a complete loss of access to the database during each incident.Which solution should be implemented to prevent losing database access if this occurs again?",
            options: [
                { id: 0, text: "Make a snapshot of the database", correct: false },
                { id: 1, text: "Enable Multi-AZ failover", correct: true },
                { id: 2, text: "Increase the database instance size", correct: false },
                { id: 3, text: "Create a read replica", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon RDS Multi-AZ deployments\nare designed to enhance the availability and durability of database instances, making them well-suited for production workloads. When you enable\nMulti-AZ failover\n, Amazon RDS automatically creates a standby replica in a different Availability Zone (AZ) and synchronously replicates data from the primary instance. This ensures high data consistency and protection against infrastructure-related disruptions. Each AZ operates on physically distinct infrastructure, which adds fault isolation and resilience.\nIn the event of planned maintenance or an unexpected failure such as an Availability Zone outage or hardware issue, Amazon RDS automatically performs a failover to the standby instance. This process is fully managed by Amazon RDS and does not require manual intervention, helping to minimize downtime and maintain business continuity. For Amazon Aurora, the failover involves promoting a replica to become the new writer instance.\nHence, the correct answer is:\nEnable Multi-AZ failover.\nThe option that says:\nMake a snapshot of the database\nis incorrect because snapshots are typically used for backup and disaster recovery, not for maintaining high availability. A snapshot allows you to restore a database to a specific point in time, but it does not prevent downtime or provide continuous access during an Availability Zone outage.\nThe option that says:\nIncrease the database instance size\nis incorrect because this action just improves the instance's compute and memory capacity, which may help performance but does not address Availability Zone-level failures. The database would still be a single point of failure within the same zone, leaving it vulnerable to the same type of outage described in the question.\nThe option that says:\nCreate a read replica\nis incorrect because read replicas are primarily intended to handle read-heavy workloads and do not automatically take over in the event of a failure. Promoting a read replica to a standalone instance requires manual intervention, which simply does not meet the goal of preventing loss of access during unplanned outages.\nReferences:\nhttps://aws.amazon.com/rds/features/multi-az/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nCheck out this Amazon RDS Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/\nTutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:\nhttps://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c03/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 14,
            text: "A company has recently migrated its microservices-based application to Amazon Elastic Kubernetes Service (Amazon EKS). As part of the migration, the company must ensure that all sensitive configuration data and credentials, such as database passwords and API keys, are stored securely and encrypted within the Amazon EKS cluster's etcd key-value store.What is the most suitable solution to meet the company's requirements?",
            options: [
                { id: 0, text: "Enable secret encryption with a new AWS KMS key on an existing Amazon EKS cluster to encrypt sensitive data stored in the EKS cluster's etcd key-value store.", correct: true },
                { id: 1, text: "Use AWS Secrets Manager with a new AWS KMS key to securely manage and store sensitive data within the EKS cluster's etcd key-value store.", correct: false },
                { id: 2, text: "Enable default Amazon EBS volume encryption for the account with a new AWS KMS key to ensure encryption of sensitive data within the Amazon EKS cluster.", correct: false },
                { id: 3, text: "Use Amazon EKS default options and the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on to securely store sensitive data within the Amazon EKS cluster.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Enabling secret encryption with a new AWS Key Management Service (KMS) key in an existing Amazon Elastic Kubernetes Service (EKS) cluster is critical to securing sensitive data stored in the cluster's etcd key-value store. Amazon EKS is a service for running and managing containerized applications, storing configuration data and secrets, etc., and is a distributed data store. By default, these secrets are not encrypted, posing potential security risks. Integrating AWS KMS with Amazon EKS allows for the encryption of these secrets, leveraging AWS KMS's capabilities to manage cryptographic keys and control their use across AWS services and applications.\nThe process involves creating an AWS KMS key specifically for the EKS cluster and configuring the cluster to encrypt secrets before they are saved in etcd. This setup ensures that all sensitive information within the etcd database is encrypted at rest, enhancing data security. By adopting this approach, organizations can significantly improve their security posture, ensuring that sensitive data and credentials are protected according to industry standards and compliance requirements, thus maintaining data confidentiality and integrity within their Kubernetes environments.\nHence, the correct answer is:\nEnable secret encryption with a new AWS KMS key on an existing Amazon EKS cluster to encrypt sensitive data stored in the EKS cluster's etcd key-value store.\nThe option that says:\nUse AWS Secrets Manager with a new AWS KMS key to securely manage and store sensitive data within the EKS cluster's etcd key-value store\nis incorrect. AWS Secrets Manager is a powerful tool for managing secrets but it doesn't directly address encrypting data within the etcd key-value store of an EKS cluster. Secrets Manager is more about managing and retrieving secrets rather than encrypting data within etcd.\nThe option that says:\nEnable default Amazon EBS volume encryption for the account with a new AWS KMS key to ensure encryption of sensitive data within the Amazon EKS cluster\nis incorrect. Enabling default Amazon EBS volume encryption is a way to ensure that data at rest in EBS volumes is encrypted. However, the EBS volumes are primarily used for persistent storage of the worker nodes. They are not directly related to the storage of sensitive configuration data and credentials within the EKS cluster's etcd key-value store.\nThe option that says:\nUse Amazon EKS default options and the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on to securely store sensitive data within the Amazon EKS cluster\nis incorrect. Amazon EBS CSI driver enables Amazon Elastic Block Store (EBS) volumes as persistent storage for Kubernetes applications running on the Amazon EKS. While this can provide secure persistent storage for your microservices, it does not address the specific requirement of securely storing sensitive data within the EKS cluster's etcd key-value store.\nReferences:\nhttps://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\nhttps://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/\nhttps://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html\nCheck out this Amazon Elastic Kubernetes Service Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-elastic-kubernetes-service-eks/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A telecommunications company is planning to grant AWS Console access to its developers. Company policy requires the use of identity federation and role-based access control. Currently, the roles are already assigned via groups in the corporate Active Directory.In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)",
            options: [
                { id: 0, text: "AWS Directory Service AD Connector", correct: true },
                { id: 1, text: "AWS Directory Service Simple AD", correct: false },
                { id: 2, text: "IAM Groups", correct: false },
                { id: 3, text: "IAM Roles", correct: true },
                { id: 4, text: "AWS Lambda", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "AWS Directory Service\nprovides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)–aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access.\nConsidering that the company is using a corporate Active Directory, it is best to use AWS Directory Service AD Connector for easier integration. In addition, since the roles are already assigned using groups in the corporate Active Directory, it would be better to also use IAM Roles. Take note that you can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector.\nHence, the correct answers are:\n-\nAWS Directory Service AD Connector.\n-\nIAM Roles.\nAWS Directory Service Simple AD\nis incorrect because this only provides a subset of the features offered by AWS Managed Microsoft AD, including the ability to manage user accounts and group memberships, create and apply group policies, securely connect to Amazon EC2 instances, and provide Kerberos-based single sign-on (SSO). In this scenario, the more suitable component to use is the AD Connector since it is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory.\nIAM Groups\nis incorrect because this is just a collection of\nIAM\nusers.\nGroups\nlet you specify permissions for multiple users, which can make it easier to manage the permissions for those users. In this scenario, the more suitable one to use is IAM Roles in order for permissions to create AWS Directory Service resources.\nAWS Lambda\nis incorrect because this is primarily used for serverless computing.\nReferences:\nhttps://aws.amazon.com/blogs/security/how-to-connect-your-on-premises-active-directory-to-aws-using-ad-connector/\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_getting_started.html\nCheck out these AWS IAM and Directory Service Cheat Sheets:\nhttps://tutorialsdojo.com/aws-identity-and-access-management-iam/\nhttps://tutorialsdojo.com/aws-directory-service/\nHere is a video tutorial on AWS Directory Service:\nhttps://youtu.be/4XeqotTYBtY",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "An application consists of multiple Amazon EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective?",
            options: [
                { id: 0, text: "Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone", correct: true },
                { id: 1, text: "Create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.", correct: false },
                { id: 2, text: "Create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.", correct: false },
                { id: 3, text: "Create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "A\nNAT Gateway\nis a highly available, managed Network Address Translation (NAT) service for your resources in a private subnet to access the Internet\n. NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone.\nYou must create a NAT gateway on a public subnet to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.\nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose Internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.\nHence, the correct answer is:\nCreate a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone\n.\nThe option that says:\nCreate a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone\nis incorrect because you should configure the route table in the private subnet and not the public subnet to associate the right instances in the private subnet.\nThe options that say:\nCreate two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone\nis incorrect because you are primarily required to set up the NAT Gateway in the private subnet to allow outbound internet access for private instances.\nThe options that say:\nCreate three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone\nis incorrect because the only necessity here is ensuring outbound traffic for private instances, and adding multiple NAT Gateways does not align with cost optimization unless explicitly required for high availability.\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\nCheck out this Amazon VPC Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-vpc/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A Solutions Architect is designing a highly available relational database solution to mitigate the risk of a multi-region failure. The database must meet a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute. The architect needs a disaster recovery plan that enables automatic cross-region replication with minimal data loss and rapid recovery in the event of a failure.Which AWS feature best fulfills this requirement?",
            options: [
                { id: 0, text: "Amazon DynamoDB Global table", correct: false },
                { id: 1, text: "Amazon RDS for PostgreSQL with cross-region read replicas", correct: false },
                { id: 2, text: "Amazon Timestream for Analytics", correct: false },
                { id: 3, text: "Amazon Aurora Global Database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Amazon Aurora Global Database\nis designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.\nAurora Global Database supports storage-based replication that has a latency of less than 1 second. If there is an unplanned outage, one of the secondary regions you assigned can be promoted to read and write capabilities in less than 1 minute. This feature is called Cross-Region Disaster Recovery. An RPO of 1 second and an RTO of less than 1 minute provide you a strong foundation for a global business continuity plan.\nHence, the correct answer is:\nAmazon Aurora Global Database\n.\nThe option that says:\nAmazon DynamoDB Global table\nis incorrect because while this supports multi-region, fully replicated tables with low latency, it's more suitable for NoSQL workloads, not relational databases.\nThe option that says:\nAmazon RDS for PostgreSQL with cross-region read replicas\nis incorrect. While this option can help with disaster recovery, it simply doesn't meet the specified RPO and RTO requirements in the scenario. Replication lag in cross-region read replicas can take several minutes to complete, which could prevent the company from meeting the RPO of 1 second.\nThe option that says:\nAmazon Timestream for Analytics\nis incorrect because it is primarily a serverless time series database service that is commonly used for IoT and operational applications. The most suitable solution for this scenario is to use the Amazon Aurora Global Database since it can provide the required RPO and RTO.\nReferences:\nhttps://aws.amazon.com/rds/aurora/global-database/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\nCheck out this Amazon Aurora Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-aurora/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A company needs to deploy at least two Amazon EC2 instances to support the normal workloads of its application and automatically scale up to six EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.As a Solutions Architect, what should be done to meet this requirement?",
            options: [
                { id: 0, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A.", correct: false },
                { id: 1, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.", correct: true },
                { id: 2, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ.", correct: false },
                { id: 3, text: "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Amazon EC2 Auto Scaling\nhelps ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can also specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size.\nTo achieve highly available and fault-tolerant architecture for your applications, you must deploy all your instances in different Availability Zones. This will help you isolate your resources if an outage occurs. Take note that to achieve fault tolerance, you need to have redundant resources in place to avoid any system degradation in the event of a server fault or an Availability Zone outage. Having a fault-tolerant architecture entails an extra cost in running additional resources than what is usually needed. This is to ensure that the mission-critical workloads are processed.\nSince the scenario requires at least 2 instances to handle regular traffic, you should have 2 instances running all the time even if an AZ outage occurred. You can use an Auto Scaling Group to automatically scale your compute resources across two or more Availability Zones. You have to specify the minimum capacity to 4 instances and the maximum capacity to 6 instances. If each AZ has 2 instances running, even if an AZ fails, your system will still run a minimum of 2 instances.\nHence, the correct answer is:\nCreate an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.\nThe option that says:\nCreate an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A\nis incorrect because the instances are only deployed in a single Availability Zone. It cannot protect your applications and data from datacenter or AZ failures.\nThe option that says:\nCreate an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ\nis incorrect. It is required to have 2 instances running all the time. If an AZ outage happened, ASG will launch a new instance on the unaffected AZ. This provisioning does not happen instantly, which means that for a certain period of time, there will only be 1 running instance left.\nThe option that says:\nCreate an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B\nis incorrect. Although this fulfills the requirement of at least 2 EC2 instances and high availability, the maximum capacity setting is wrong. It should be set to 6 to properly handle the peak load. If an AZ outage occurs and the system is at its peak load, the number of running instances in this setup will only be 4 instead of 6 and this will affect the performance of your application.\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/regions-and-azs.html\nCheck out this AWS Auto Scaling Cheat Sheet:\nhttps://tutorialsdojo.com/aws-auto-scaling/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A company has a hybrid cloud architecture that connects its on-premises data center and cloud infrastructure in AWS. It requires a durable storage backup for its corporate documents stored on-premises and a local cache that provides low-latency access to recently accessed data to reduce data egress charges. The documents must be stored on and retrieved from AWS via the Server Message Block (SMB) protocol. These files must be immediately accessible within minutes for six months and archived for another decade to meet data compliance.Which of the following is the best and most cost-effective approach to implement in this scenario?",
            options: [
                { id: 0, text: "Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.", correct: true },
                { id: 1, text: "Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.", correct: false },
                { id: 2, text: "Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an Amazon S3 bucket, and then later to Glacier for archival.", correct: false },
                { id: 3, text: "Use AWS DataSync to transfer all files from the on-premises network directly to an Amazon S3 bucket, and set up a lifecycle policy to move the data into Glacier for archival.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.\nThe gateway provides access to objects in S3 as files or file share mount points. With a file gateway, you can do the following:\n- You can store and retrieve files directly using the NFS version 3 or 4.1 protocol.\n- You can store and retrieve files directly using the SMB file system version, 2 and 3 protocol.\n- You can access your data directly in Amazon S3 from any AWS Cloud application or service.\n- You can manage your Amazon S3 data using lifecycle policies, cross-region replication, and versioning. You can think of a file gateway as a file system mount on S3.\nAWS Storage Gateway supports the Amazon S3 Standard, Amazon S3 Standard-Infrequent Access, Amazon S3 One Zone-Infrequent Access and Amazon Glacier storage classes. When you create or update a file share, you have the option to select a storage class for your objects. You can either choose the Amazon S3 Standard or any of the infrequent access storage classes such as S3 Standard IA or S3 One Zone IA. Objects stored in any of these storage classes can be transitioned to Amazon Glacier using a Lifecycle Policy.\nAlthough you can write objects directly from a file share to the S3-Standard-IA or S3-One Zone-IA storage class, it is recommended that you use a Lifecycle Policy to transition your objects rather than write directly from the file share, especially if you're expecting to update or delete the object within 30 days of archiving it.\nTherefore, the correct answer is:\nLaunch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.\nThe option that says:\nLaunch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival\nis incorrect because although tape gateways provide cost-effective and durable archive backup data in Amazon Glacier, it does not meet the criteria of being retrievable immediately within minutes. It also doesn't maintain a local cache that provides low latency access to the recently accessed data and reduce data egress charges. Thus, it is still better to set up a file gateway instead.\nThe option that says:\nEstablish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an Amazon S3 bucket, and then later to Glacier for archival\nis incorrect because EBS Volumes are not only less durable compared with S3 and it would be more cost-efficient if you directly store the documents to an S3 bucket. An alternative solution is to use AWS Direct Connect with AWS Storage Gateway to create a connection for high-throughput workload needs, providing a dedicated network connection between your on-premises file gateway and AWS. But this solution is using EBS, hence, this option is still wrong.\nThe option that says:\nUse AWS DataSync to transfer all files from the on-premises network directly to an Amazon S3 bucket, and set up a lifecycle policy to move the data into Glacier for archival\nis incorrect because DataSync is primarily a data migration service that facilitates bulk transfers, but it does not offer local caching for low-latency access to recently accessed files. Without caching, the company may incur higher data egress charges, making this option less cost-effective.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/\nTutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:\nhttps://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/",
            domain: "Design Resilient Architectures",
        },
        {
            id: 1,
            text: "A financial application consists of an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer, and a MySQL RDS instance set up in a Multi-AZ Deployment configuration. To protect customers' confidential data, it must be ensured that the Amazon RDS database is only accessible using an authentication token specific to the profile credentials of EC2 instances.Which of the following actions should be taken to meet this requirement?",
            options: [
                { id: 0, text: "Enable the IAM DB Authentication.", correct: true },
                { id: 1, text: "Configure SSL in your application to encrypt the database connection to RDS.", correct: false },
                { id: 2, text: "Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance.", correct: false },
                { id: 3, text: "Use a combination of IAM and STS to enforce restricted access to your RDS instance using a temporary authentication token.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.\nAn\nauthentication token\nis a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.\nIAM database authentication provides the following benefits:\nNetwork traffic to and from the database is encrypted using Secure Sockets Layer (SSL).\nYou can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.\nFor applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security\nHence, the correct answer is:\nEnable the IAM DB Authentication\n.\nThe option that says:\nConfiguring SSL in your application to encrypt the database connection to RDS\nis incorrect because an SSL connection is not just using an authentication token from IAM. Although configuring SSL to your application can improve the security of your data in flight, it is still not a suitable option to use in this scenario.\nThe option that says:\nCreating an IAM Role and assigning it to your EC2 instances which will grant exclusive access to your RDS instance\nis incorrect because although you can create and assign an IAM Role to your EC2 instances, you still need to configure your RDS to use IAM DB Authentication.\nThe option that says:\nUse a combination of IAM and STS to enforce restricted access to your RDS instance using a temporary authentication token\nis incorrect because you have to use IAM DB Authentication for this scenario, and not simply a combination of an IAM and STS. Although STS is used to send temporary tokens for authentication, this is not a compatible use case for RDS.\nReferences:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\nhttps://aws.amazon.com/rds/\nCheck out this Amazon RDS cheat sheet:\nhttps://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company.Which S3 encryption technique should the Architect use?",
            options: [
                { id: 0, text: "Use S3 client-side encryption with an AWS KMS key.", correct: false },
                { id: 1, text: "Use S3 client-side encryption with a client-side master key.", correct: true },
                { id: 2, text: "Use S3 server-side encryption with an AWS KMS key.", correct: false },
                { id: 3, text: "Use S3 server-side encryption with customer provided key.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Client-side encryption\nis the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:\n- Use an AWS KMS key.\n- Use a client-side master key.\nWhen using an AWS KMS key to enable client-side data encryption, you provide an AWS KMS key identifier (KeyId) to AWS. On the other hand, when you use client-side master key for client-side data encryption,\nyour client-side master keys and your unencrypted data are never sent to AWS\n. It's important that you safely manage your encryption keys because if you lose them, you can't decrypt your data.\nThis is how client-side encryption using a client-side master key works:\nWhen uploading an object\n- You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. The process works like this:\n1. The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally. It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object.\n2. The client encrypts the data encryption key using the master key that you provide. The client uploads the encrypted data key and its material description as part of the object metadata. The client uses the material description to determine which client-side master key to use for decryption.\n3. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (\nx-amz-meta-x-amz-key\n) in Amazon S3.\nWhen downloading an object -\nThe client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.\nHence, the correct answer is:\nUse S3 client-side encryption with a client-side master key\n.\nThe option that says:\nUse S3 client-side encryption with an AWS KMS key\nis incorrect because, in client-side encryption with a KMS key, you provide an AWS KMS key identifier (KeyId) to AWS. The scenario clearly indicates that both the master keys and the unencrypted data should never be sent to AWS.\nThe option that says:\nUse S3 server-side encryption with an AWS KMS key\nis incorrect because the scenario mentioned that the unencrypted data should never be sent to AWS, which means that you have to use client-side encryption in order to encrypt the data first before sending to AWS. In this way, you can only ensure that there is no unencrypted data being uploaded to AWS. In addition, the master key used by Server-Side Encryption with AWS KMS Key (SSE-KMS) is uploaded and managed by AWS, which directly violates the requirement of not uploading the master key.\nThe option that says:\nUse S3 server-side encryption with customer provided key\nis incorrect because, just as mentioned above, you have to use client-side encryption in this scenario instead of server-side encryption. For the S3 server-side encryption with a customer-provided key (SSE-C), you actually provide the encryption key as part of your request to upload the object to S3. Using this key, Amazon S3 manages both the encryption (as it writes to disks) and decryption (when you access your objects).\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A Solutions Architect is hosting a website in an Amazon S3 bucket namedtutorialsdojo. The users load the website using the following URL:http://tutorialsdojo.s3-website-us-east-1.amazonaws.com. A new requirement has been introduced to add JavaScript on the webpages to make authenticated HTTPGETrequests against the same bucket using the S3 API endpoint (tutorialsdojo.s3.amazonaws.com). However, upon testing, the web browser blocks JavaScript from allowing those requests.Which of the following options is the MOST suitable solution to implement for this scenario?",
            options: [
                { id: 0, text: "Enable cross-account access.", correct: false },
                { id: 1, text: "Enable Cross-Zone Load Balancing.", correct: false },
                { id: 2, text: "Enable Cross-origin resource sharing (CORS) configuration in the bucket.", correct: true },
                { id: 3, text: "Enable Cross-Region Replication (CRR).", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.\nSuppose that you are hosting a website in an Amazon S3 bucket named\nyour-website\nand your users load the website endpoint\nhttp://your-website.s3-website-us-east-1.amazonaws.com\n. Now you want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket,\nyour-website.s3.amazonaws.com\n. A browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to explicitly enable cross-origin requests from\nyour-website.s3-website-us-east-1.amazonaws.com\n.\nHence, the correct answer is:\nEnable Cross-origin resource sharing (CORS) configuration in the bucket.\nThe option that says:\nEnable cross-account access\nis incorrect because cross-account access is just a feature in IAM and not in Amazon S3.\nThe option that says:\nEnable Cross-Zone Load Balancing\nis incorrect because Cross-Zone Load Balancing is only used in ELB and not in S3.\nThe option that says:\nEnable Cross-Region Replication (CRR)\nis incorrect because CRR is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.\nReferences:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ManageCorsUsing.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. To secure session data and ensure that Cloud Engineers must authenticate before executing Redis commands, specificallyMULTI EXECcommands, the system should enforce strong authentication by requiring users to enter a password. Additionally, access should be managed with long-lived credentials while supporting robust security practices.Which of the following actions should be taken to meet the above requirement?",
            options: [
                { id: 0, text: "Generate an IAM authentication token using AWS credentials and provide this token as a password.", correct: false },
                { id: 1, text: "Set up a Redis replication group and enable theAtRestEncryptionEnabledparameter.", correct: false },
                { id: 2, text: "Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the--transit-encryption-enabledand--auth-tokenparameters enabled.", correct: true },
                { id: 3, text: "Enable the in-transit encryption for Redis replication groups.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "Using\nRedis\nAUTH\ncommand can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server.\nHence, the correct answer is:\nAuthenticate the users using Redis AUTH by creating a new Redis Cluster with both the\n--transit-encryption-enabled\nand\n--auth-token\nparameters enabled.\nTo require that users enter a password on a password-protected Redis server, include the parameter\n--auth-token\nwith the correct password when you create your replication group or cluster and on all subsequent commands to the replication group or cluster.\nThe option that says:\nGenerate an IAM authentication token using AWS credentials and provide this token as a password\nis incorrect. IAM authentication is simply not supported for executing Redis commands like\nMULTI EXEC\n, and IAM tokens expire every 12 hours, which does not align with the need for long-lived credentials.\nThe option that says:\nSet up a Redis replication group and enable the\nAtRestEncryptionEnabled\nparameter\nis incorrect because the Redis At-Rest Encryption feature only secures the data inside the in-memory data store. You have to use Redis AUTH option instead.\nThe option that says:\nEnable the in-transit encryption for Redis replication groups\nis incorrect. Although in-transit encryption is part of the solution, it is missing the most important thing which is the Redis AUTH option.\nReferences:\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html\nCheck out this Amazon Elasticache Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-elasticache/\nRedis (cluster mode enabled vs disabled) vs Memcached:\nhttps://tutorialsdojo.com/redis-cluster-mode-enabled-vs-disabled-vs-memcached/",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "A company is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and handle frequent schema changes. The application should not have any downtime or performance issues whenever there is a schema change in the database. It should also provide a low latency response to high-traffic queries.Which is the most suitable database solution to use to achieve this requirement?",
            options: [
                { id: 0, text: "An Amazon RDS instance in Multi-AZ Deployments configuration", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "An Amazon Aurora database with Read Replicas", correct: false },
                { id: 3, text: "Redshift", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Before we proceed in answering this question, we must first be clear with the actual definition of a \"\nschema\n\". Basically, the english definition of a schema is:\na representation of a plan or theory in the form of an outline or model\n.\nJust think of a schema as the \"structure\" or a \"model\" of your data in your database. Since the scenario requires that the schema, or the structure of your data, changes frequently, then you have to pick a database which provides a non-rigid and flexible way of adding or removing new types of data. This is a classic example of choosing between a relational database and non-relational (NoSQL) database.\nA relational database is known for having a rigid schema, with a lot of constraints and limits as to which (and what type of ) data can be inserted or not. It is primarily used for scenarios where you have to support complex queries which fetch data across a number of tables. It is best for scenarios where you have complex table relationships but for use cases where you need to have a flexible schema, this is not a suitable database to use.\nFor NoSQL, it is not as rigid as a relational database because you can easily add or remove rows or elements in your table/collection entry. It also has a more flexible schema because it can store complex hierarchical data within a single item which, unlike a relational database, does not entail changing multiple related tables. Hence, the best answer to be used here is a NoSQL database, like DynamoDB. When your business requires a low-latency response to high-traffic queries, taking advantage of a NoSQL system generally makes technical and economic sense.\nAmazon DynamoDB helps solve the problems that limit the relational system scalability by avoiding them. In DynamoDB, you design your schema specifically to make the most common and important queries as fast and as inexpensive as possible. Your data structures are tailored to the specific requirements of your business use cases.\nRemember that a relational database system\ndoes not scale\nwell for the following reasons:\n- It normalizes data and stores it on multiple tables that require multiple queries to write to disk.\n- It generally incurs the performance costs of an ACID-compliant transaction system.\n- It uses expensive joins to reassemble required views of query results.\nFor\nDynamoDB\n, it scales well due to these reasons:\n- Its\nschema flexibility\nlets DynamoDB store complex hierarchical data within a single item. DynamoDB is not a totally\nschemaless\ndatabase since the very definition of a schema is just the model or structure of your data.\n- Composite key design lets it store related items close together on the same table.\nAn Amazon RDS instance in Multi-AZ Deployments configuration\nand\nan Amazon Aurora database with Read Replicas\nare incorrect because both of them are a type of relational database.\nRedshift\nis incorrect because it is primarily used for OLAP systems.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-relational-modeling.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html\nAlso check the\nAWS Certified Solutions Architect Official Study Guide: Associate Exam\n1st Edition and turn to page 161 which talks about NoSQL Databases.\nCheck out this Amazon DynamoDB Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-dynamodb\nTutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:\nhttps://tutorialsdojo.com/aws-certified-solutions-architect-associate/",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A software development company is using serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. The company has a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform, and also uses a third-party API to fetch certain data for its application. One of the developers was instructed to create the environment variables for the MongoDB database hostname, username, and password, as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT, and PROD environments.Considering that the Lambda function is storing sensitive database and API credentials, how can this information be secured to prevent other developers on the team, or anyone, from seeing these credentials in plain text? Select the best option that provides maximum security.",
            options: [
                { id: 0, text: "There is no need to do anything because, by default, Lambda already encrypts the environment variables using the AWS Key Management Service.", correct: false },
                { id: 1, text: "Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information.", correct: false },
                { id: 2, text: "Lambda does not provide encryption for the environment variables. Deploy your code to an Amazon EC2 instance instead.", correct: false },
                { id: 3, text: "Create a new AWS KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "When you create or update Lambda functions that use environment variables, AWS Lambda encrypts them using the AWS Key Management Service. When your Lambda function is invoked, those values are decrypted and made available to the Lambda code.\nThe first time you create or update Lambda functions that use environment variables in a region, a default service key is created for you automatically within AWS KMS. This key is used to encrypt environment variables. However, if you wish to use encryption helpers and use KMS to encrypt environment variables after your Lambda function is created, you must create your own AWS KMS key and choose it instead of the default key. The default key will give errors when chosen. Creating your own key gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data.\nHence, the correct answer is:\nCreate a new AWS KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.\nThe option that says:\nThere is no need to do anything because, by default, Lambda already encrypts the environment variables using the AWS Key Management Service\nis incorrect. Although Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables.\nThe option that says:\nEnable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information\nis also incorrect since enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Typically, AWS KMS is the recommended choice for encrypting sensitive data at rest.\nThe option that says:\nLambda does not provide encryption for the environment variables. Deploy your code to an Amazon EC2 instance instead\nis incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.\nReferences:\nhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html#env_encrypt\nhttps://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_console.html\nCheck out this AWS Lambda Cheat Sheet:\nhttps://tutorialsdojo.com/aws-lambda/",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A payment processing company plans to migrate its on-premises application to an Amazon EC2 instance. An IPv6 CIDR block is attached to the company’s Amazon VPC. Strict security policy mandates that the production VPC must only allow outbound communication over IPv6 between the instance and the internet but should prevent the internet from initiating an inbound IPv6 connection. The new architecture should also allow traffic flow inspection and traffic filtering.What should a solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Launch the EC2 instance to a public subnet and attach an Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use Traffic Mirroring to set up the required rules for traffic inspection and traffic filtering.", correct: false },
                { id: 1, text: "Launch the EC2 instance to a private subnet and attach AWS PrivateLink interface endpoint to the VPC to control outbound IPv6 communication to the internet. Use Amazon GuardDuty to set up the required rules for traffic inspection and traffic filtering.", correct: false },
                { id: 2, text: "Launch the EC2 instance to a private subnet and attach a NAT Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Firewall Manager to set up the required rules for traffic inspection and traffic filtering.", correct: false },
                { id: 3, text: "Launch the EC2 instance to a private subnet and attach an Egress-Only Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "An\negress-only internet gateway\nis a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet and prevents it from initiating an IPv6 connection with your instances.\nIPv6 addresses are globally unique and are therefore public by default. If you want your instance to be able to access the internet, but you want to prevent resources on the internet from initiating communication with your instance, you can use an egress-only internet gateway.\nA\nsubnet\nis a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a\npublic subnet\nfor resources that must be connected to the internet and a\nprivate subnet\nfor resources that won't be connected to the internet.\nAWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs). The service can be set up with just a few clicks and scales automatically with your network traffic, so you don't have to worry about deploying and managing any infrastructure. AWS Network Firewall includes features that provide protection from common network threats.\nAWS Network Firewall’s stateful firewall can incorporate context from traffic flows, like tracking connections and protocol identification, to enforce policies such as preventing your VPCs from accessing domains using an unauthorized protocol. AWS Network Firewall’s intrusion prevention system (IPS) provides active traffic flow inspection so you can identify and block vulnerability exploits using signature-based detection. AWS Network Firewall also offers web filtering that can stop traffic to known bad URLs and monitor fully qualified domain names.\nIn this scenario, you can use an egress-only internet gateway to allow outbound IPv6 communication to the internet and then use the AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.\nHence, the correct answer is:\nLaunch the EC2 instance to a private subnet and attach an Egress-Only Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.\nThe option that says:\nLaunch the EC2 instance to a private subnet and attach AWS PrivateLink interface endpoint to the VPC to control outbound IPv6 communication to the internet. Use Amazon GuardDuty to set up the required rules for traffic inspection and traffic filtering\nis incorrect because the AWS PrivateLink (which is also known as VPC Endpoint) is just a highly available, scalable technology that enables you to privately connect your VPC to the AWS services as if they were in your VPC. This service is not capable of controlling outbound IPv6 communication to the Internet. Furthermore, the Amazon GuardDuty service doesn't have the features to do traffic inspection or filtering.\nThe option that says:\nLaunch the EC2 instance to a public subnet and attach an Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use Traffic Mirroring to set up the required rules for traffic inspection and traffic filtering\nis incorrect because an Internet Gateway does not limit or control any outgoing IPv6 connection. Take note that the requirement is to prevent the Internet from initiating an inbound IPv6 connection to your instance. This solution allows all kinds of traffic to initiate a connection to your EC2 instance hence, this option is wrong. In addition, the use of Traffic Mirroring is not appropriate as well. This is just an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type interface, not to filter or inspect the incoming/outgoing traffic.\nThe option that says:\nLaunch the EC2 instance to a private subnet and attach a NAT Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Firewall Manager to set up the required rules for traffic inspection and traffic filtering\nis incorrect. While NAT Gateway has a NAT64 feature that translates an IPv6 address to IPv4, it will not prevent inbound IPv6 traffic from reaching the EC2 instance. You have to use the egress-only Internet Gateway instead. Moreover, the AWS Firewall Manager is neither capable of doing traffic inspection nor traffic filtering.\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\nCheck out this Amazon VPC Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-vpc/",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A pharmaceutical company has resources hosted on both its on-premises network and in the AWS cloud. The company requires all Software Architects to access resources in both environments using on-premises credentials, which are stored in Active Directory.In this scenario, which of the following can be used to fulfill this requirement?",
            options: [
                { id: 0, text: "Set up SAML 2.0-Based Federation by using a Web Identity Federation.", correct: false },
                { id: 1, text: "Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service.", correct: true },
                { id: 2, text: "Use IAM users", correct: false },
                { id: 3, text: "Use Amazon VPC", correct: false },
            ],
            correctAnswers: [1],
            explanation: "Since the company is using Microsoft Active Directory which implements Security Assertion Markup Language (SAML), you can set up a SAML-Based Federation for API Access to your AWS cloud. In this way, you can easily connect to AWS using the login credentials of your on-premises network.\nAWS supports identity federation with SAML 2.0, an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS APIs without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code.\nBefore you can use SAML 2.0-based federation as described in the preceding scenario and diagram, you must configure your organization's IdP and your AWS account to trust each other. The general process for configuring this trust is described in the following steps. Inside your organization, you must have an IdP that supports SAML 2.0, like Microsoft Active Directory Federation Service (AD FS, part of Windows Server), Shibboleth, or another compatible SAML 2.0 provider.\nHence, the correct answer is:\nSet up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service.\nThe option that says:\nSetting up SAML 2.0-Based Federation by using a Web Identity Federation\nis incorrect because this is primarily used to let users sign in via a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google. It does not utilize Active Directory.\nThe option that says:\nUsing IAM users\nis incorrect because the situation requires you to use the existing credentials stored in their Active Directory, and not user accounts that will be generated by IAM.\nThe option that says:\nUsing Amazon VPC\nis incorrect because this only lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. This has nothing to do with user authentication or Active Directory.\nReferences:\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html\nCheck out this AWS IAM Cheat Sheet:\nhttps://tutorialsdojo.com/aws-identity-and-access-management-iam/",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A Solutions Architect needs to make sure that the On-Demand Amazon EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection.Which configuration below will satisfy this requirement?",
            options: [
                { id: 0, text: "Security Group Inbound Rule: Protocol – TCP, Port Range – 22, Source 110.238.98.71/32", correct: true },
                { id: 1, text: "Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32", correct: false },
                { id: 2, text: "Security Group Outbound Rule: Protocol – TCP, Port Range – 22, Destination 110.238.98.71/32", correct: false },
                { id: 3, text: "Security Group Outbound Rule: Protocol – UDP, Port Range – 22, Destination 0.0.0.0/0", correct: false },
            ],
            correctAnswers: [0],
            explanation: "A\nsecurity group\nacts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups.\nThe requirement is to only allow the individual IP of the client and not the entire network. The /32 CIDR notation denotes a single IP address. Take note that the SSH protocol uses TCP, not UDP, and runs on port 22 (default). In the scenario, we can create a security group with an inbound rule allowing incoming traffic from the specified IP address on port 22.\nSecurity groups are stateful, meaning they automatically allow return traffic associated with the client who initiated the connection to the instance. Therefore, any return traffic from the specified IP address on port 22 will be allowed to pass through the security group, regardless of whether or not there is an explicit outbound rule allowing it.\nHence, the correct answer is:\nSecurity Group Inbound Rule: Protocol – TCP, Port Range – 22, Source 110.238.98.71/32\nThe option that says:\nSecurity Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32\nis incorrect because it typically uses UDP instead of TCP. SSH runs over the TCP protocol, so specifying UDP would not allow the desired access.\nThe option that says:\nSecurity Group Outbound Rule: Protocol – TCP, Port Range – 22, Destination 110.238.98.71/32\nis incorrect because it's an outbound rule, not an inbound rule. Outbound rules control traffic leaving the instance. In the scenario, we need to limit inbound traffic coming from a specific address.\nThe option that says:\nSecurity Group Outbound Rule: Protocol – UDP, Port Range – 22, Destination 0.0.0.0/0\nis incorrect because it is an outbound rule rather than an inbound rule. Moreover, SSH connections only require TCP.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#security-group-rules\nhttps://docs.aws.amazon.com/vpc/\nTutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:\nhttps://tutorialsdojo.com/aws-certified-solutions-architect-associate/\nCheck out this Amazon EC2 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for the storage of their personal documents.Which of the following will you need to consider so you can set up a solution that incorporates a single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.)",
            options: [
                { id: 0, text: "Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.", correct: false },
                { id: 1, text: "Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.", correct: true },
                { id: 2, text: "Map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents.", correct: false },
                { id: 3, text: "Configure an IAM role and an IAM Policy to access the bucket.", correct: true },
                { id: 4, text: "Set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket.", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The question refers to one of the common scenarios for temporary credentials in AWS. Temporary credentials are useful in scenarios that involve identity federation, delegation, cross-account access, and IAM roles. In this example, it is called\nenterprise identity federation,\nconsidering that you also need to set up a single sign-on (SSO) capability.\nThe correct answers are:\n- Setup a Federation proxy or an Identity provider, and\nuse AWS Security Token Service to generate temporary tokens\n- Configure an IAM role and an IAM Policy to access the bucket.\nIn an enterprise identity federation, you can authenticate users in your organization's network, and then provide those users access to AWS without creating new AWS identities for them and requiring them to sign in with a separate user name and password. This is known as the\nsingle sign-on\n(SSO) approach to temporary access. AWS STS supports open standards like Security Assertion Markup Language (SAML) 2.0, with which you can use Microsoft AD FS to leverage your Microsoft Active Directory. You can also use SAML 2.0 to manage your own solution for federating user identities.\nUsing 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others\nis incorrect since you don't have to use 3rd party solutions to provide the access. AWS already provides the necessary tools that you can use in this situation.\nMapping each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents\nis incorrect as there is no direct way of integrating Amazon S3 with Amazon WorkDocs for this particular scenario. Amazon WorkDocs is simply a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content. And because it’s stored centrally on AWS, you can access it from anywhere on any device.\nSetting up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket\nis incorrect since creating that many IAM users would be unnecessary. Also, you want the account to integrate with your AD or LDAP directory, hence, IAM Users does not fit these criteria.\nReferences:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-s3-user-specific-folder/\nCheck out this AWS IAM Cheat Sheet:\nhttps://tutorialsdojo.com/aws-identity-and-access-management-iam/",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A business has recently migrated its applications to AWS. The audit team must be able to assess whether the services the company is using meet common security and regulatory standards. A solutions architect needs to provide the team with a report of all compliance-related documents for their account.Which action should a solutions architect consider?",
            options: [
                { id: 0, text: "Run an Amazon Inspector assessment job to download all of the AWS compliance-related information.", correct: false },
                { id: 1, text: "Use AWS Artifact to view the security reports as well as other AWS compliance-related information.", correct: true },
                { id: 2, text: "Run an Amazon Macie job to view the Service Organization Control (SOC), Payment Card Industry (PCI), and other compliance reports from AWS Certificate Manager (ACM).", correct: false },
                { id: 3, text: "View all of the AWS security compliance reports from AWS Security Hub.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "AWS Artifact\nis your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements. Reports available in AWS Artifact include our Service Organization Control (SOC) reports, Payment Card Industry (PCI) reports, and certifications from accreditation bodies across geographies and compliance verticals that validate the implementation and operating effectiveness of AWS security controls. Agreements available in AWS Artifact include the Business Associate Addendum (BAA) and the Nondisclosure Agreement (NDA).\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their accounts by agreeing to the associated terms and conditions. You will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact while restricting access to other services and resources within your AWS Account.\nHence, the correct answer in this scenario is:\nUse AWS Artifact to view the security reports as well as other AWS compliance-related information.\nThe option that says:\nRun an Amazon Inspector assessment job to download all of the AWS compliance-related information\nis incorrect. Amazon Inspector is simply a security tool for detecting vulnerabilities in AWS workloads. For this scenario, it is better to use the readily-available security reports in AWS Artifact instead.\nThe option that says:\nRun an Amazon Macie job to view the Service Organization Control (SOC), Payment Card Industry (PCI), and other compliance reports from AWS Certificate Manager (ACM)\nis incorrect because ACM is just a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. This service does not store certifications or compliance-related documents.\nThe option that says:\nView all of the AWS security compliance reports from AWS Security Hub\nis incorrect because AWS Security Hub only provides you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts.\nReferences:\nhttps://aws.amazon.com/artifact/getting-started/\nhttps://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html\nCheck out this AWS Artifact Cheat Sheet:\nhttps://tutorialsdojo.com/aws-artifact/",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A company has a web application that uses Amazon CloudFront to distribute its images, videos, and other static content stored in its Amazon S3 bucket to users around the world. The company has recently introduced a new member-only access feature for some of its high-quality media files. There is a requirement to provide access to multiple private media files only to paying subscribers without having to change the current URLs.Which of the following is the most suitable solution to implement to satisfy this requirement?",
            options: [
                { id: 0, text: "Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.", correct: false },
                { id: 1, text: "Create a Signed URL with a custom policy which only allows the members to see the private files.", correct: false },
                { id: 2, text: "Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.", correct: false },
                { id: 3, text: "Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the requiredSet-Cookieheaders to the viewer which will unlock the content only to them.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee. To securely serve this private content by using CloudFront, you can do the following:\n- Require that your users access your private content by using special CloudFront signed URLs or signed cookies.\n- Require that your users access your content by using CloudFront URLs, not URLs that access content directly on the origin server (for example, Amazon S3 or a private HTTP server). Requiring CloudFront URLs isn't necessary, but we recommend it to prevent users from bypassing the restrictions that you specify in signed URLs or signed cookies.\nCloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content.\nIf you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following:\nUse\nsigned URLs\nfor the following cases:\n- You want to use an RTMP distribution. Signed cookies aren't supported for RTMP distributions.\n- You want to restrict access to individual files, for example, an installation download for your application.\n- Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.\nUse\nsigned cookies\nfor the following cases:\n- You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of a website.\n- You don't want to change your current URLs.\nHence, the correct answer is:\nUse Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required\nSet-Cookie\nheaders to the viewer which will unlock the content only to them.\nThe option that says:\nConfigure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member\nis incorrect because a Match Viewer is an Origin Protocol Policy that configures CloudFront to communicate with your origin using HTTP or HTTPS, depending on the protocol of the viewer request. CloudFront caches the object only once even if viewers make requests using both HTTP and HTTPS protocols.\nThe option that says:\nCreate a Signed URL with a custom policy which only allows the members to see the private files\nis incorrect because Signed URLs are primarily used for providing access to individual files, as shown in the above explanation. In addition, the scenario explicitly says that they don't want to change their current URLs which is why implementing Signed Cookies is more suitable than Signed URLs.\nThe option that says:\nConfigure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members\nis incorrect because Field-Level Encryption only allows you to securely upload user-submitted sensitive information to your web servers. It does not provide access to download multiple private files.\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\nCheck out this Amazon CloudFront Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-cloudfront/",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A Solutions Architect identified a series of DDoS attacks while monitoring the Amazon VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
            options: [
                { id: 0, text: "Use AWS Shield Advanced to detect and mitigate DDoS attacks.", correct: true },
                { id: 1, text: "Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks, and other DDoS attacks.", correct: false },
                { id: 2, text: "Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.", correct: false },
                { id: 3, text: "A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.\nAWS Shield Advanced\nalso gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 charges.\nHence, the correct answer is:\nUse AWS Shield Advanced to detect and mitigate DDoS attacks.\nThe option that says:\nUsing the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks and other DDoS attacks\nis incorrect because AWS Firewall Manager is mainly used to simplify your AWS WAF administration and maintenance tasks across multiple accounts and resources. It does not protect your VPC against DDoS attacks.\nThe option that says:\nSet up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic\nis incorrect. Even though AWS WAF can help you block common attack patterns to your VPC such as SQL injection or cross-site scripting, this is still not enough to withstand DDoS attacks. It is just better to use AWS Shield in this scenario.\nThe option that says:\nA combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC\nis incorrect. Although using a combination of Security Groups and NACLs are valid to provide security to your VPC, this is not enough to mitigate a DDoS attack. You should use AWS Shield for better security protection.\nReferences:\nhttps://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\nhttps://aws.amazon.com/shield/\nCheck out this AWS Shield Cheat Sheet:\nhttps://tutorialsdojo.com/aws-shield/",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A travel photo-sharing website is using Amazon S3 to serve high-quality photos to visitors. After a few days, it was discovered that other travel websites are linking to and using these photos. This has resulted in financial losses for the business.What is the MOST effective method to mitigate this issue?",
            options: [
                { id: 0, text: "Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.", correct: true },
                { id: 1, text: "Use Amazon CloudFront distributions for your photos.", correct: false },
                { id: 2, text: "Block the IP addresses of the offending websites using NACL.", correct: false },
                { id: 3, text: "Store and privately serve the high-quality photos on Amazon WorkDocs instead.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "In Amazon S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.\nWhen you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and the expiration date and time. The pre-signed URLs are valid only for the specified duration.\nAnyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.\nHence, the correct answer is:\nConfigure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.\nThe option that says:\nUsing Amazon CloudFront distributions for your photos\nis incorrect. CloudFront is primarily a content delivery network service that speeds up the delivery of content to your customers.\nThe option that says:\nBlocking the IP addresses of the offending websites using NACL\nis also incorrect. Blocking IP addresses using NACLs is not a very efficient method because a quick change in IP address would easily bypass this configuration.\nThe option that says:\nStoring and privately serving the high-quality photos on Amazon WorkDocs instead\nis incorrect as WorkDocs is simply a fully managed, secure content creation, storage, and collaboration service. It is not a suitable service for storing static content. Amazon WorkDocs is more often used to easily create, edit, and share documents for collaboration and not for serving object data like Amazon S3.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectOperations.html\nCheck out this Amazon CloudFront Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-cloudfront/\nComparison of AWS Services Cheat Sheets:\nhttps://tutorialsdojo.com/comparison-of-aws-services/",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A company uses an Application Load Balancer (ALB) for its public-facing multi-tier web applications. The security team has recently reported that there has been a surge of SQL injection attacks lately, which causes critical data discrepancy issues. The same issue is also encountered by its other web applications in other AWS accounts that are behind an ALB. An immediate solution is required to prevent the remote injection of unauthorized SQL queries and protect their applications hosted across multiple accounts.As a Solutions Architect, what solution would you recommend?",
            options: [
                { id: 0, text: "Use AWS Network Firewall to filter web vulnerabilities and brute force attacks using stateful rule groups across all Application Load Balancers on all AWS accounts. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment.", correct: false },
                { id: 1, text: "Use AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.", correct: true },
                { id: 2, text: "Use Amazon Macie to scan for vulnerabilities and unintended network exposure. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment. Utilize the AWS Audit Manager to reuse the security assessment across all AWS accounts.", correct: false },
                { id: 3, text: "Use Amazon GuardDuty and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer and utilize the AWS Security Hub service to reuse the managed rules across all the AWS accounts", correct: false },
            ],
            correctAnswers: [1],
            explanation: "AWS WAF\nis a web application firewall that lets you monitor the HTTP(S) requests that are forwarded to an Amazon CloudFront distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API.\n-Web ACLs\n– You use a web access control list (ACL) to protect a set of AWS resources. You create a web ACL and define its protection strategy by adding rules. Rules define criteria for inspecting web requests and specify how to handle requests that match the criteria. You set a default action for the web ACL that indicates whether to block or allow through those requests that pass the rules inspections.\n-Rules\n– Each rule contains a statement that defines the inspection criteria and an action to take if a web request meets the criteria. When a web request meets the criteria, that's a match. You can configure rules to block matching requests, allow them through, count them, or run CAPTCHA controls against them.\n-Rules groups\n– You can use rules individually or in reusable rule groups. AWS Managed Rules and AWS Marketplace sellers provide managed rule groups for your use. You can also define your own rule groups.\nAWSManagedRulesSQLiRuleSet\n- The SQL database rule group contains rules to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. This can help prevent remote injection of unauthorized queries. Evaluate this rule group for use if your application interfaces with an SQL database.\nAWS WAF is easy to deploy and protect applications deployed on either Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts all your origin servers, Amazon API Gateway for your REST APIs, or AWS AppSync for your GraphQL APIs. There is no additional software to deploy, DNS configuration, SSL/TLS certificate to manage, or need for a reverse proxy setup.\nWith AWS Firewall Manager integration, you can centrally define and manage your rules and reuse them across all the web applications that you need to protect.\nTherefore, the correct answer is:\nUse AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.\nThe option that says:\nUse Amazon GuardDuty and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer and utilize the AWS Security Hub service to reuse the managed rules across all the AWS accounts\nis incorrect because Amazon GuardDuty is only a threat detection service and cannot directly be integrated with the Application Load Balancer.\nThe options that says:\nUse AWS Network Firewall to filter web vulnerabilities and brute force attacks using stateful rule groups across all Application Load Balancers on all AWS accounts. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment\nis incorrect because AWS Network Firewall is a managed service that is primarily used to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs) and not particularly to your Application Load Balancers. Take note that the AWS Network Firewall is account-specific by default and needs to be integrated with the AWS Firewall Manager to easily share the firewall across your other AWS accounts. In addition, refactoring the web application will require an immense amount of time.\nThe options that says:\nUse Amazon Macie to scan for vulnerabilities and unintended network exposure. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment. Utilize the AWS Audit Manager to reuse the security assessment across all AWS accounts\nis incorrect because Amazon Macie is only used for data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data. Just like before, refactoring the web application will require an immense amount of time. The use of the AWS Audit Manager is not relevant as well. The AWS Audit Manager simply helps you continually audit your AWS usage to simplify how you manage risk and compliance with regulations and industry standards.\nReferences:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/how-aws-waf-works.html\nhttps://docs.aws.amazon.com/waf/latest/developerguide/fms-chapter.html\nhttps://docs.aws.amazon.com/waf/latest/developerguide/aws-managed-rule-groups-use-case.html#aws-managed-rule-groups-use-case-sql-db\nCheck out this AWS Web Application Firewall Cheat Sheet:\nhttps://tutorialsdojo.com/aws-waf",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A company has 3 DevOps engineers that are handling its software development and infrastructure management processes. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service.What can the DevOps engineers do to prevent this from happening again?",
            options: [
                { id: 0, text: "Use S3 Infrequently Accessed storage to store the data.", correct: false },
                { id: 1, text: "Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.", correct: true },
                { id: 2, text: "Set up a signed URL for all users.", correct: false },
                { id: 3, text: "Create an IAM bucket policy that disables delete operation.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "To enhance data protection and meet best practices for securing stored objects, AWS recommends implementing safeguards against accidental deletions. To avoid accidental deletion in an Amazon S3 bucket, you can:\n- Enable Versioning\n- Enable MFA (Multi-Factor Authentication) Delete\nVersioning\nis a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.\nIf the\nMFA (Multi-Factor Authentication) Delete\nis enabled, it requires additional authentication for either of the following operations:\n- Change the versioning state of your bucket\n- Permanently delete an object version\nHence, the correct answer is:\nEnable S3 Versioning and Multi-Factor Authentication Delete on the bucket.\nUsing S3 Infrequently Accessed storage to store the data\nis incorrect. Switching your storage class to S3 Infrequent Access won't help mitigate accidental deletions.\nSetting up a signed URL for all users\nis incorrect. Signed URLs give you more control over access to your content, so this feature primarily deals more with accessing rather than deletion.\nCreating an IAM bucket policy that disables delete operation\nis incorrect. If you create a bucket policy preventing deletion, other users won't be able to delete objects that should be deleted. You only want to prevent accidental deletion, not disable the action itself.\nReferences:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "A company requires all the data stored in the cloud to be encrypted at rest. To easily integrate this with other AWS services, they must have full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS. The solution should also be able to audit the key usage independently of AWS CloudTrail.Which of the following options will meet this requirement?",
            options: [
                { id: 0, text: "Use AWS Key Management Service to create AWS owned Keys and store the non-extractable key material in AWS CloudHSM.", correct: false },
                { id: 1, text: "Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in Amazon S3.", correct: false },
                { id: 2, text: "Use AWS Key Management Service to create AWS managed keys and store the non-extractable key material in AWS CloudHSM.", correct: false },
                { id: 3, text: "Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in AWS CloudHSM.", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The\nAWS Key Management Service (KMS)\ncustom key store feature combines the controls provided by\nAWS CloudHSM\nwith the integration and ease of use of AWS KMS. You can configure your own CloudHSM cluster and authorize AWS KMS to use it as a dedicated key store for your keys rather than the default AWS KMS key store. When you create keys in AWS KMS you can choose to generate the key material in your CloudHSM cluster. KMS Keys that are generated in your custom key store never leave the HSMs in the CloudHSM cluster in plaintext and all AWS KMS operations that use those KMS keys are only performed on your HSMs.\nAWS KMS can help you integrate with other AWS services to encrypt the data that you store in these services and control access to the keys that decrypt it. To immediately remove the key material from AWS KMS, you can use a custom key store. Take note that each custom key store is associated with an AWS CloudHSM cluster in your AWS account. Therefore, when you create an AWS KMS Key in a custom key store, AWS KMS generates and stores the non-extractable key material for the KMS key in an AWS CloudHSM cluster that you own and manage. This is also suitable if you want to be able to audit the usage of all your keys independently of AWS KMS or AWS CloudTrail.\nSince you control your AWS CloudHSM cluster, you have the option to manage the lifecycle of your KMS keys independently of AWS KMS. Here are the criteria why you might find a custom key store useful:\nYou have encryption keys that must be safeguarded within a dedicated hardware security module (HSM) under your direct control, adhering to strict single-tenancy requirements.\nYou require the capability to promptly and independently revoke and remove key material from AWS KMS, exercising complete control over the key lifecycle.\nYour compliance obligations mandate independent auditing and monitoring of all key usage activities, beyond the logging provided by AWS KMS and AWS CloudTrail.\nHence, the correct answer in this scenario is:\nUse AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in AWS CloudHSM\n.\nThe option that says:\nUse AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in Amazon S3\nis incorrect. Amazon S3 is primarily for general storage purposes and does not provide the required level of security and control needed for cryptographic key management. You have to use AWS CloudHSM instead.\nThe options that says:\nUse AWS Key Management Service to create AWS owned Keys and store the non-extractable key material in AWS CloudHSM\nand\nUse AWS Key Management Service to create AWS managed Keys and store the non-extractable key material in AWS CloudHSM\nare both incorrect because the scenario primarily requires you to have full control over the encryption of the created key. AWS owned Keys, and AWS managed Keys are managed by AWS. Moreover, these options do not allow you to audit the key usage independently of AWS CloudTrail.\nReferences:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html\nhttps://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html\nhttps://aws.amazon.com/blogs/security/are-kms-custom-key-stores-right-for-you/\nCheck out this AWS KMS Cheat Sheet:\nhttps://tutorialsdojo.com/aws-key-management-service-aws-kms/",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A company hosted an e-commerce website on an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer. The Solutions Architect noticed that the website is receiving a high number of illegitimate external requests from multiple systems with frequently changing IP addresses. To address the performance issues, the Solutions Architect must implement a solution that would block these requests while having minimal impact on legitimate traffic.Which of the following options fulfills this requirement?",
            options: [
                { id: 0, text: "Create a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer.", correct: false },
                { id: 1, text: "Create a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests.", correct: false },
                { id: 2, text: "Create a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.", correct: true },
                { id: 3, text: "Create a custom rule in the security group of the Application Load Balancer to block the offending requests.", correct: false },
            ],
            correctAnswers: [2],
            explanation: "AWS WAF\nis tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync – services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on regional services, such as Application Load Balancer, Amazon API Gateway, and AWS AppSync, your rules run in the region and can be used to protect Internet-facing resources as well as internal resources.\nA rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests.\nBased on the given scenario, the requirement is to limit the number of requests from the illegitimate requests without affecting the genuine requests. To accomplish this requirement, you can use AWS WAF web ACL. There are two types of rules in creating your own web ACL rule: regular and rate-based rules. You need to select the latter to add a rate limit to your web ACL. After creating the web ACL, you can associate it with ALB. When the rule action triggers, AWS WAF applies the action to additional requests from the IP address until the request rate falls below the limit.\nHence, the correct answer is:\nCreate a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.\nThe option that says:\nCreate a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer\nis incorrect because a regular rule typically matches the statement defined in the rule. If you need to add a rate limit to your rule, you should create a rate-based rule.\nThe option that says:\nCreate a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests\nis incorrect. Although NACLs can help you block incoming traffic, this option wouldn't be able to limit the number of requests from a single IP address that is dynamically changing.\nThe option that says:\nCreate a custom rule in the security group of the Application Load Balancer to block the offending requests\nis incorrect because the security group can only allow incoming traffic. Remember that you can't deny traffic using security groups. In addition, it is not capable of limiting the rate of traffic to your application unlike AWS WAF.\nReferences:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\nhttps://aws.amazon.com/waf/faqs/\nCheck out this AWS WAF Cheat Sheet:\nhttps://tutorialsdojo.com/aws-waf/",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A government agency plans to store confidential tax documents on AWS. Due to the sensitive information in the files, the Solutions Architect must restrict the data access requests made to the storage solution to a specific Amazon VPC only. The solution should also prevent the files from being deleted or overwritten to meet the regulatory requirement of having a write-once-read-many (WORM) storage model.Which combination of the following options should the Architect implement? (Select TWO.)",
            options: [
                { id: 0, text: "Set up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific VPC.", correct: false },
                { id: 1, text: "Configure an Amazon S3 Access Point for the S3 bucket to restrict data access to a particular VPC only.", correct: true },
                { id: 2, text: "Create a new Amazon S3 bucket with the S3 Object Lock feature enabled. Store the documents in the bucket and set the Legal Hold option for object retention.", correct: true },
                { id: 3, text: "Store the tax documents in the Amazon S3 Glacier Instant Retrieval storage class. Use thePutBucketPolicyAPI to apply a bucket policy that restricts access requests to a specific VPC.", correct: false },
                { id: 4, text: "Enable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement.", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "Amazon S3 access points simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as\nGetObject\nand\nPutObject\n.\nEach access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. Each access point enforces a customized access point policy that works in conjunction with the bucket policy that is attached to the underlying bucket. You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.\nYou can also use Amazon S3 Multi-Region Access Points to provide a global endpoint that applications can use to fulfill requests from S3 buckets located in multiple AWS Regions. You can use Multi-Region Access Points to build multi-Region applications with the same simple architecture used in a single Region, and then run those applications anywhere in the world. Instead of sending requests over the congested public internet, Multi-Region Access Points provide built-in network resilience with acceleration of internet-based requests to Amazon S3. Application requests made to a Multi-Region Access Point global endpoint use AWS Global Accelerator to automatically route over the AWS global network to the S3 bucket with the lowest network latency.\nWith S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require WORM storage, or to simply add another layer of protection against object changes and deletion.\nBefore locking any objects, it is essential to enable S3 Object Lock on a bucket. Previously, Object Lock could only be enabled at the time of bucket creation, but now, Amazon S3 allows you to enable S3 Object Lock for existing buckets with just a few clicks. Once S3 Object Lock is enabled on a bucket, it allows you to lock objects within that bucket to prevent them from being deleted or overwritten for a fixed amount of time or indefinitely. While Object Lock can now be enabled on existing buckets, it is important to note that once enabled, Object Lock itself cannot be disabled. However, you can still manage and configure object lock settings, including retention periods and legal holds, but the core feature of Object Lock remains active and irreversible. Also, versioning, which is required for Object Lock, cannot be suspended or disabled once Object Lock is enabled on the bucket.\nHence, the correct answers are:\n- Configure an Amazon S3 Access Point for the S3 bucket to restrict data access to a particular VPC only.\n- Create a new Amazon S3 bucket with the S3 Object Lock feature enabled. Store the documents in the bucket and set the Legal Hold option for object retention.\nThe option that says:\nSet up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific VPC\nis incorrect because you cannot directly use an AWS Network Firewall to restrict S3 bucket data access requests to a specific Amazon VPC only. You have to use an Amazon S3 Access Point instead for this particular use case. An AWS Network Firewall is commonly integrated to your Amazon VPC and not to an S3 bucket.\nThe option that says:\nStore the tax documents in the Amazon S3 Glacier Instant Retrieval storage class. Use the\nPutBucketPolicy\nAPI to apply a bucket policy that restricts access requests to a specific VPC\nis incorrect because Amazon S3 Glacier Instant Retrieval is just an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. Additionally, using a bucket policy to restrict access from a VPC is less efficient compared to using an S3 Access Point.\nThe option that says:\nEnable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement\nis incorrect. Although the Object Lock feature does provide write-once-read-many (WORM) storage, the Object Versioning feature must also be enabled in order for this to work. In fact, you cannot manually disable the Object Versioning feature if you have already selected the Object Lock option.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A medical records company is planning to store sensitive clinical trial data in an Amazon S3 repository with the object-level versioning feature enabled. The Solutions Architect is tasked with ensuring that no object can be overwritten or deleted by any user in a period of one year only. To meet the strict compliance requirements, the root user of the company’s AWS account must also be restricted from making any changes to an object in the S3 bucket.Which of the following is the most secure way of storing the data in S3?",
            options: [
                { id: 0, text: "Enable S3 Object Lock in governance mode with a retention period of one year.", correct: false },
                { id: 1, text: "Enable S3 Object Lock in compliance mode with a retention period of one year.", correct: true },
                { id: 2, text: "Enable S3 Object Lock in governance mode with a legal hold of one year.", correct: false },
                { id: 3, text: "Enable S3 Object Lock in compliance mode with a legal hold of one year.", correct: false },
            ],
            correctAnswers: [1],
            explanation: "With S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require WORM storage or to simply add another layer of protection against object changes and deletion.\nBefore you lock any objects, you have to enable a bucket to use S3 Object Lock. You enable Object Lock when you create a bucket. After you enable Object Lock on a bucket, you can lock objects in that bucket. When you create a bucket with Object Lock enabled, you can't disable Object Lock or suspend versioning for that bucket.\nS3 Object Lock provides two retention modes:\n-Governance mode\n-Compliance mode\nThese retention modes apply different levels of protection to your objects. You can apply either retention mode to any object version that is protected by Object Lock.\nIn governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary. You can also use governance mode to test retention-period settings before creating a compliance-mode retention period.\nIn compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.\nTo override or remove governance-mode retention settings, a user must have the\ns3:BypassGovernanceRetention\npermission and must explicitly include\nx-amz-bypass-governance-retention:true\nas a request header with any request that requires overriding governance mode.\nLegal Hold vs. Retention Period\nWith Object Lock, you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the\ns3:PutObjectLegalHold\npermission.\nLegal holds are independent from retention periods. As long as the bucket that contains the object has Object Lock enabled, you can place and remove legal holds regardless of whether the specified object version has a retention period set. Placing a legal hold on an object version doesn't affect the retention mode or retention period for that object version.\nFor example, suppose that you place a legal hold on an object version while the object version is also protected by a retention period. If the retention period expires, the object doesn't lose its WORM protection. Rather, the legal hold continues to protect the object until an authorized user explicitly removes it. Similarly, if you remove a legal hold while an object version has a retention period in effect, the object version remains protected until the retention period expires.\nHence, the correct answer is:\nEnable S3 Object Lock in compliance mode with a retention period of one year.\nThe option that says:\nEnable S3 Object Lock in governance mode with a retention period of one year\nis incorrect because in the governance mode, users typically can't overwrite or delete an object version or alter its lock settings unless they have special permissions or if a user has access to the root AWS user account. A better option to choose here is to use the compliance mode.\nThe option that says:\nEnable S3 Object Lock in governance mode with a legal hold of one year\nis incorrect. You cannot set a time period for a legal hold. You can only do this using the \"retention period\" option. Take note that a legal hold will still restrict users from changing the S3 objects even after the one-year retention period has elapsed. In addition, a governance mode will allow the root user to modify your S3 objects and override any existing settings.\nThe option that says:\nEnable S3 Object Lock in compliance mode with a legal hold of one year\nis incorrect. Although the choice of using the compliance mode is right, you still cannot set a one-year time period for the legal hold option. Keep in mind that the legal hold is independent of the retention period.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "A government entity is conducting a population and housing census in the city. Each household information uploaded on their online portal is stored in encrypted files in Amazon S3. The government assigned its Solutions Architect to set compliance policies that verify data containing personally identifiable information (PII) in a manner that meets their compliance standards. They should also be alerted if there are potential policy violations with the privacy of their S3 buckets.Which of the following should the Architect implement to satisfy this requirement?",
            options: [
                { id: 0, text: "Set up and configure Amazon Macie to monitor their Amazon S3 data.", correct: true },
                { id: 1, text: "Set up and configure Amazon Kendra to monitor malicious activity on their Amazon S3 data", correct: false },
                { id: 2, text: "Set up and configure Amazon Polly to scan for usage patterns on Amazon S3 data", correct: false },
                { id: 3, text: "Set up and configure Amazon Fraud Detector to send out alert notifications whenever a security violation is detected on their Amazon S3 data.", correct: false },
            ],
            correctAnswers: [0],
            explanation: "Amazon Macie\nis an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.\nAmazon Macie generates two categories of findings: policy findings and sensitive data findings. A policy finding is a detailed report of a potential policy violation or issue with the security or privacy of an Amazon S3 bucket. Macie generates these findings as part of its ongoing monitoring activities for your Amazon S3 data. A sensitive data finding is a detailed report of sensitive data in an S3 object. Macie generates these findings when it discovers sensitive data in S3 objects that you configure a sensitive data discovery job to analyze.\nHence, the correct answer is:\nSet up and configure Amazon Macie to monitor their Amazon S3 data.\nThe option that says:\nSet up and configure Amazon Polly to scan for usage patterns on Amazon S3 data\nis incorrect because Amazon Polly is simply a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. Polly can't be used to scane usage patterns on your S3 data.\nThe option that says:\nSet up and configure Amazon Kendra to monitor malicious activity on their Amazon S3 data\nis incorrect Amazon Kendra is just an enterprise search service that allows developers to add search capabilities to their applications. This enables their end users to discover information stored within the vast amount of content spread across their company, but not monitor malcious activity on their S3 buckets.\nThe option that says:\nSet up and configure Amazon Fraud Detector to send out alert notifications whenever a security violation is detected on their Amazon S3 data\nis incorrect because the Amazon Fraud Detector is only a fully managed service for identifying potentially fraudulent activities and for catching more online fraud faster. It does not check any S3 data containing personally identifiable information (PII), unlike Amazon Macie.\nReferences:\nhttps://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html\nhttps://aws.amazon.com/macie/faq/\nhttps://docs.aws.amazon.com/macie/index.html\nCheck out this Amazon Macie Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-macie/",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that are used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"s3:Get*\", \"s3:List*\" ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::boracay/*\"\n    }\n  ]\n}\n\nWhat does the above IAM policy allow? (Select THREE.)",
            options: [
                { id: 0, text: "An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.", correct: true },
                { id: 1, text: "An IAM user with this IAM policy is allowed to write objects into theboracayS3 bucket.", correct: true },
                { id: 2, text: "An IAM user with this IAM policy is allowed to change access rights for theboracayS3 bucket.", correct: false },
                { id: 3, text: "An IAM user with this IAM policy is allowed to read objects in theboracayS3 bucket but not allowed to list the objects in the bucket.", correct: false },
                { id: 4, text: "An IAM user with this IAM policy is allowed to read objects from theboracayS3 bucket.", correct: true },
                { id: 5, text: "An IAM user with this IAM policy is allowed to read and delete objects from theboracayS3 bucket.", correct: false },
            ],
            correctAnswers: [0, 1, 4],
            explanation: "You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies.\nIAM policies define permissions for action regardless of the method that you use to perform the operation. For example, if a policy allows the\nGetUser\naction, then a user with that policy can get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM user, you can choose to allow console or programmatic access. If console access is allowed, the IAM user can sign in to the console using a user name and password. Or if programmatic access is allowed, the user can use access keys to work with the CLI or API.\nBased on the provided IAM policy, the user is only allowed to get, write, and list all of the objects for the\nboracay\ns3 bucket. The\ns3:PutObject\nbasically means that you can submit a PUT object request to the S3 bucket to store data.\nHence, the correct answers are:\n- An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.\n- An IAM user with this IAM policy is allowed to write objects into the\nboracay\nS3 bucket.\n- An IAM user with this IAM policy is allowed to read objects from the\nboracay\nS3 bucket.\nThe option that says:\nAn IAM user with this IAM policy is allowed to change access rights for the\nboracay\nS3 bucket\nis incorrect because the template does not have any statements which allow the user to change access rights in the bucket.\nThe option that says:\nAn IAM user with this IAM policy is allowed to read objects in the\nboracay\nS3 bucket but not allowed to list the objects in the bucket\nis incorrect because it can clearly be seen in the template that there is a\ns3:List*\nwhich permits the user to list objects.\nThe option that says:\nAn IAM user with this IAM policy is allowed to read and delete objects from the\nboracay\nS3 bucket\nis incorrect. Although you can read objects from the bucket, you cannot delete any objects.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectOps.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\nCheck out this Amazon S3 Cheat Sheet:\nhttps://tutorialsdojo.com/amazon-s3/",
            domain: "Design Secure Architectures",
        },
    ],
};

// Function to get all questions for a test
function getTestQuestions(testNumber) {
    const testKey = `test${testNumber}`;
    return examQuestions[testKey] || [];
}
