// AWS SAA-C03 Exam Questions
// Auto-generated from PDF files in questions directory

const examQuestions = {
    test1: [
        {
            id: 1,
            text: "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)", correct: false },
                { id: 2, text: "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
            options: [
                { id: 0, text: "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: true },
                { id: 1, text: "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 2, text: "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 3, text: "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
            options: [
                { id: 0, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type", correct: false },
                { id: 1, text: "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed", correct: true },
                { id: 2, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance", correct: false },
                { id: 3, text: "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 1, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 2, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
                { id: 3, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 4, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
            options: [
                { id: 0, text: "AWS Lambda", correct: false },
                { id: 1, text: "AWS Step Functions", correct: false },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon CloudWatch", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 2, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Remove full database access for all IAM users in the organization", correct: false },
                { id: 1, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Only root user should have full database access in the organization", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to analyze API calls", correct: true },
                { id: 1, text: "Implement an IAM policy to forbid users to change Amazon S3 bucket settings", correct: false },
                { id: 2, text: "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations", correct: false },
                { id: 3, text: "Use Amazon S3 access logs to analyze user access using Athena", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
            options: [
                { id: 0, text: "Amazon Redshift", correct: false },
                { id: 1, text: "Amazon RDS", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: true },
                { id: 3, text: "Amazon ElastiCache", correct: false },
                { id: 4, text: "AWS Lambda", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
                { id: 3, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
                { id: 1, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 2, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 3, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
            options: [
                { id: 0, text: "Move to Amazon Redshift", correct: false },
                { id: 1, text: "Switch application code to AWS Lambda for better performance", correct: false },
                { id: 2, text: "Setup Amazon ElastiCache in front of Amazon RDS", correct: true },
                { id: 3, text: "Setup Amazon RDS Read Replicas", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 1, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 2, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
            options: [
                { id: 0, text: "Amazon ElastiCache for Redis/Memcached", correct: true },
                { id: 1, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon DocumentDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 1, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 1, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 3, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 1, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 2, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 3, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 4, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
                { id: 5, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator", correct: false },
                { id: 1, text: "Use AWS Direct Connect", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon S3", correct: true },
                { id: 1, text: "Amazon Kinesis", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 4, text: "Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 2, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 3, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 4, text: "The route for the health check is misconfigured", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
            options: [
                { id: 0, text: "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated", correct: false },
                { id: 1, text: "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume", correct: true },
                { id: 2, text: "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume", correct: false },
                { id: 3, text: "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use Spot Instances", correct: false },
                { id: 1, text: "Use a Cluster placement group", correct: true },
                { id: 2, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
                { id: 1, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 2, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 3, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
            options: [
                { id: 0, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data", correct: false },
                { id: 1, text: "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object", correct: true },
                { id: 2, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data", correct: false },
                { id: 3, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
                { id: 3, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 1, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
            options: [
                { id: 0, text: "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once", correct: true },
                { id: 1, text: "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts", correct: false },
                { id: 2, text: "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs", correct: false },
                { id: 3, text: "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda", correct: false },
                { id: 1, text: "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: false },
                { id: 2, text: "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda", correct: false },
                { id: 3, text: "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon Neptune", correct: true },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon Aurora", correct: false },
                { id: 3, text: "Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
                { id: 1, text: "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 3, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
                { id: 1, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 2, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 3, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 4, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "AWS Glue", correct: false },
                { id: 1, text: "Amazon EMR", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: true },
                { id: 3, text: "Amazon FSx for Windows File Server", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 2, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 3, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
                { id: 4, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
            options: [
                { id: 0, text: "Use Identity and Access Management (IAM) policies", correct: false },
                { id: 1, text: "Use Amazon S3 Bucket Policies", correct: true },
                { id: 2, text: "Use Security Groups", correct: false },
                { id: 3, text: "Use Access Control Lists (ACLs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 1, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 2, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 4, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 5, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 1, text: "Use AWS Glue to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 2, text: "Use AWS EMR to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 2, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 3, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 2, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
                { id: 3, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
            options: [
                { id: 0, text: "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs", correct: false },
                { id: 1, text: "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: true },
                { id: 2, text: "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: false },
                { id: 3, text: "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
            options: [
                { id: 0, text: "Use AWS_IAM authorization", correct: false },
                { id: 1, text: "Use Amazon Cognito User Pools", correct: true },
                { id: 2, text: "Use Amazon Cognito Identity Pools", correct: false },
                { id: 3, text: "Use AWS Lambda authorizer for Amazon API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 1, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 1, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 2, text: "NAT instance supports port forwarding", correct: true },
                { id: 3, text: "NAT gateway supports port forwarding", correct: false },
                { id: 4, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 5, text: "Security Groups can be associated with a NAT gateway", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "It is not possible to access cross-account resources", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
            options: [
                { id: 0, text: "7", correct: false },
                { id: 1, text: "3", correct: true },
                { id: 2, text: "14", correct: false },
                { id: 3, text: "15", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 1, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
                { id: 3, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
            options: [
                { id: 0, text: "Distribute the static content through Amazon EFS", correct: false },
                { id: 1, text: "Distribute the dynamic content through Amazon EFS", correct: false },
                { id: 2, text: "Distribute the static content through Amazon S3", correct: true },
                { id: 3, text: "Distribute the dynamic content through Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Database Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Glue", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 1, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 2, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 3, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 4, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "Software VPN", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "VPC Endpoint", correct: false },
                { id: 3, text: "AWS VPN CloudHub", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
            options: [
                { id: 0, text: "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
                { id: 1, text: "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis", correct: false },
                { id: 2, text: "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift", correct: true },
                { id: 3, text: "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 1, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
                { id: 3, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test2: [
        {
            id: 1,
            text: "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 2, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: true },
                { id: 3, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3", correct: false },
                { id: 1, text: "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3", correct: true },
                { id: 2, text: "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
            options: [
                { id: 0, text: "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 1, text: "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 2, text: "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 3, text: "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 4, text: "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 1, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
            options: [
                { id: 0, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS", correct: true },
                { id: 1, text: "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS", correct: false },
                { id: 2, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS", correct: false },
                { id: 3, text: "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
            options: [
                { id: 0, text: "Different versions of a single object can have different retention modes and periods", correct: true },
                { id: 1, text: "The bucket default settings will override any explicit retention mode or period you request on an object version", correct: false },
                { id: 2, text: "You cannot place a retention period on an object version through a bucket default setting", correct: false },
                { id: 3, text: "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", correct: true },
                { id: 4, text: "When you use bucket default settings, you specify a Retain Until Date for the object version", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 1, text: "AWS Glue", correct: false },
                { id: 2, text: "Amazon EMR", correct: false },
                { id: 3, text: "Amazon FSx for Lustre", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
            options: [
                { id: 0, text: "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering => Amazon S3 Standard", correct: true },
                { id: 2, text: "Amazon S3 Standard-IA => Amazon S3 One Zone-IA", correct: false },
                { id: 3, text: "Amazon S3 One Zone-IA => Amazon S3 Standard-IA", correct: true },
                { id: 4, text: "Amazon S3 Standard => Amazon S3 Intelligent-Tiering", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
            options: [
                { id: 0, text: "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources", correct: false },
                { id: 1, text: "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI", correct: false },
                { id: 2, text: "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions", correct: false },
                { id: 3, text: "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
            options: [
                { id: 0, text: "Configure the security group for the Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)", correct: false },
                { id: 2, text: "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 3, text: "Configure the security group on the Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway with AWS Lambda", correct: false },
                { id: 1, text: "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Leverage Amazon QuickSight with Amazon Redshift", correct: false },
                { id: 3, text: "Leverage Amazon Athena with Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor", correct: false },
                { id: 1, text: "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets", correct: true },
                { id: 3, text: "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
            options: [
                { id: 0, text: "Encrypt the access keys and save them on Amazon S3", correct: false },
                { id: 1, text: "Create a strong password for the AWS account root user", correct: true },
                { id: 2, text: "Enable Multi Factor Authentication (MFA) for the AWS account root user account", correct: true },
                { id: 3, text: "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future", correct: false },
                { id: 4, text: "Create AWS account root user access keys and share those keys only with the business owner", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
            options: [
                { id: 0, text: "Grant maximum privileges to avoid assigning privileges again", correct: false },
                { id: 1, text: "Use user credentials to provide access specific permissions for Amazon EC2 instances", correct: false },
                { id: 2, text: "Create a minimum number of accounts and share these account credentials among employees", correct: false },
                { id: 3, text: "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users", correct: true },
                { id: 4, text: "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 1, text: "Only root user should have full database access in the organization", correct: false },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Remove full database access for all IAM users in the organization", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance", correct: true },
                { id: 1, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched", correct: false },
                { id: 2, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application", correct: true },
                { id: 3, text: "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously", correct: false },
                { id: 4, text: "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time", correct: false },
                { id: 1, text: "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly", correct: false },
                { id: 2, text: "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores", correct: true },
                { id: 3, text: "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
            options: [
                { id: 0, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate", correct: true },
                { id: 1, text: "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages", correct: false },
                { id: 2, text: "Use Amazon SQS standard queue to process the messages", correct: false },
                { id: 3, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
            options: [
                { id: 0, text: "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs", correct: true },
                { id: 1, text: "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches", correct: false },
                { id: 2, text: "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share", correct: false },
                { id: 3, text: "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
            options: [
                { id: 0, text: "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket", correct: false },
                { id: 1, text: "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)", correct: true },
                { id: 2, text: "Upload the compressed file using multipart upload", correct: false },
                { id: 3, text: "Upload the compressed file in a single operation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
            options: [
                { id: 0, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance", correct: false },
                { id: 1, text: "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance", correct: true },
                { id: 2, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance", correct: false },
                { id: 3, text: "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
            options: [
                { id: 0, text: "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution", correct: true },
                { id: 1, text: "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: true },
                { id: 2, text: "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 3, text: "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 4, text: "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection", correct: true },
                { id: 1, text: "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region", correct: false },
                { id: 2, text: "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region", correct: false },
                { id: 3, text: "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes", correct: true },
                { id: 3, text: "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
            options: [
                { id: 0, text: "The engineering team needs to provision more servers running the Amazon SNS service", correct: false },
                { id: 1, text: "The engineering team needs to provision more servers running the AWS Lambda service", correct: false },
                { id: 2, text: "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit", correct: true },
                { id: 3, text: "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
            options: [
                { id: 0, text: "General Purpose Solid State Drive (gp2)", correct: false },
                { id: 1, text: "Throughput Optimized Hard disk drive (st1)", correct: true },
                { id: 2, text: "Instance Store", correct: false },
                { id: 3, text: "Cold Hard disk drive (sc1)", correct: true },
                { id: 4, text: "Provisioned IOPS Solid state drive (io1)", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
            options: [
                { id: 0, text: "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 1, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 2, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: true },
                { id: 3, text: "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
            options: [
                { id: 0, text: "Host-based Routing", correct: false },
                { id: 1, text: "Path-based Routing", correct: true },
                { id: 2, text: "HTTP header-based routing", correct: false },
                { id: 3, text: "Query string parameter-based routing", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time", correct: false },
                { id: 1, text: "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: true },
                { id: 2, text: "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling", correct: false },
                { id: 3, text: "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
            options: [
                { id: 0, text: "Use AWS Lambda function to run the workflow processes", correct: false },
                { id: 1, text: "Use Amazon EC2 on-demand instances to run the workflow processes", correct: false },
                { id: 2, text: "Use Amazon EC2 reserved instances to run the workflow processes", correct: false },
                { id: 3, text: "Use Amazon EC2 spot instances to run the workflow processes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
            options: [
                { id: 0, text: "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability", correct: false },
                { id: 1, text: "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency", correct: false },
                { id: 2, text: "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed", correct: true },
                { id: 3, text: "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously", correct: true },
                { id: 4, text: "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
            options: [
                { id: 0, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: true },
                { id: 1, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 2, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 3, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
            options: [
                { id: 0, text: "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 1, text: "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables", correct: true },
                { id: 2, text: "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 3, text: "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
            options: [
                { id: 0, text: "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue", correct: false },
                { id: 1, text: "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service", correct: true },
                { id: 2, text: "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again", correct: false },
                { id: 3, text: "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy", correct: false },
                { id: 4, text: "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 3, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet", correct: false },
                { id: 1, text: "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block", correct: false },
                { id: 2, text: "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC", correct: true },
                { id: 3, text: "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
            options: [
                { id: 0, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 1, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 2, text: "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 3, text: "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse", correct: false },
                { id: 1, text: "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes", correct: false },
                { id: 2, text: "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks", correct: false },
                { id: 3, text: "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
            options: [
                { id: 0, text: "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services", correct: false },
                { id: 1, text: "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC", correct: true },
                { id: 2, text: "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services", correct: false },
                { id: 3, text: "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
            options: [
                { id: 0, text: "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior", correct: false },
                { id: 1, text: "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration", correct: false },
                { id: 2, text: "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data", correct: true },
                { id: 3, text: "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands", correct: true },
                { id: 4, text: "Use Amazon Aurora Global Database to replicate data across regions for compatibility", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
            options: [
                { id: 0, text: "Use VPC security groups to control the network traffic to and from your file system", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system", correct: false },
                { id: 2, text: "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system", correct: false },
                { id: 3, text: "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance", correct: false },
                { id: 4, text: "Use an IAM policy to control access for clients who can mount your file system with the required permissions", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 1, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
                { id: 2, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
            options: [
                { id: 0, text: "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively", correct: false },
                { id: 1, text: "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures", correct: false },
                { id: 2, text: "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements", correct: false },
                { id: 3, text: "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
            options: [
                { id: 0, text: "The junior scientist does not need to pay any transfer charges for the image upload", correct: true },
                { id: 1, text: "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload", correct: false },
                { id: 2, text: "The junior scientist only needs to pay S3TA transfer charges for the image upload", correct: false },
                { id: 3, text: "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
            options: [
                { id: 0, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 1, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "It is not possible to access cross-account resources", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
            options: [
                { id: 0, text: "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EFS mount points", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with access to Amazon S3 based storage", correct: false },
                { id: 3, text: "Use Instance Store based Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
            options: [
                { id: 0, text: "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies", correct: false },
                { id: 1, text: "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods", correct: true },
                { id: 3, text: "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
            options: [
                { id: 0, text: "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class", correct: false },
                { id: 1, text: "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class", correct: false },
                { id: 2, text: "Store the intermediary query results in Amazon S3 Standard storage class", correct: true },
                { id: 3, text: "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow", correct: false },
                { id: 2, text: "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards", correct: false },
                { id: 3, text: "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
            options: [
                { id: 0, text: "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%", correct: false },
                { id: 2, text: "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%", correct: true },
                { id: 3, text: "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point", correct: true },
                { id: 1, text: "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution", correct: false },
                { id: 2, text: "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers", correct: true },
                { id: 1, text: "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers", correct: false },
                { id: 2, text: "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia", correct: false },
                { id: 3, text: "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
            options: [
                { id: 0, text: "AWS Global Accelerator", correct: true },
                { id: 1, text: "AWS Elastic Load Balancing (ELB)", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon CloudFront", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance", correct: false },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB", correct: false },
                { id: 2, text: "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB", correct: true },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Microsoft SQL Server on AWS", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: false },
                { id: 3, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume", correct: false },
                { id: 2, text: "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required", correct: true },
                { id: 3, text: "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
            options: [
                { id: 0, text: "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud", correct: true },
                { id: 1, text: "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
            options: [
                { id: 0, text: "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B", correct: true },
                { id: 1, text: "1 Amazon EC2 instance and 2 AMIs exist in Region B", correct: false },
                { id: 2, text: "1 Amazon EC2 instance and 1 AMI exist in Region B", correct: false },
                { id: 3, text: "1 Amazon EC2 instance and 1 snapshot exist in Region B", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
            options: [
                { id: 0, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 1, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: true },
                { id: 2, text: "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 3, text: "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 1, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are the options marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
            options: [
                { id: 0, text: "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 1, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: true },
                { id: 2, text: "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 3, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
            options: [
                { id: 0, text: "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets", correct: false },
                { id: 1, text: "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations", correct: true },
                { id: 2, text: "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files", correct: false },
                { id: 3, text: "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is the option marked as correct. This solution best addresses the requirements described in the scenario.",
            domain: "Design Secure Architectures",
        },
    ]
};

// Function to get all questions for a test
function getTestQuestions(testNumber) {
    const testKey = `test${testNumber}`;
    return examQuestions[testKey] || [];
}
