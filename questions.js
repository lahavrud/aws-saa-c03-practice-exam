// AWS SAA-C03 Exam Questions
// Auto-generated from JSON files in questions directory

const examQuestions = {
    test1: [
        {
            id: 1,
            text: "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)", correct: false },
                { id: 2, text: "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nAmazon Kinesis Data Streams is specifically designed for real-time streaming data ingestion and processing, making it ideal for IoT applications that need to process high-volume data streams in real-time. Kinesis can handle millions of records per second and provides built-in capabilities for real-time analytics through Kinesis Data Analytics or integration with Lambda functions. Once the analytics processing is complete, Amazon SNS is the perfect service for delivering push notifications to mobile applications. SNS supports native mobile push notification protocols including APNS (Apple Push Notification Service) for iOS devices and FCM (Firebase Cloud Messaging) for Android devices. Unlike pull-based services, SNS uses a push model where AWS directly delivers messages to mobile apps, eliminating the need for apps to continuously poll for updates. This push-based approach is more efficient, reduces latency, and conserves mobile device battery life. The combination of Kinesis for real-time streaming analytics and SNS for mobile push notifications creates a complete, scalable, and efficient solution that meets all the requirements: real-time data processing, analytics, and mobile notification delivery.\n\n**Why option 0 is incorrect:**\nWhile Amazon SQS can work with SNS, SQS is fundamentally a message queuing service designed for decoupling applications and handling discrete messages, not continuous data streams. SQS uses a pull-based model where consumers must poll the queue to retrieve messages, which introduces latency and doesn't provide the real-time streaming capabilities needed for IoT data processing. For the real-time analytics requirement, Kinesis Data Streams is purpose-built with features like data retention, multiple consumers reading the same stream, and real-time processing capabilities that SQS lacks. Using SQS in this scenario would add unnecessary complexity and latency to the data processing pipeline, making it less suitable than Kinesis for real-time streaming analytics.\n\n**Why option 1 is incorrect:**\nAmazon SES (Simple Email Service) is specifically designed for sending email notifications, not push notifications to mobile applications. The requirement explicitly asks for notifications to be sent to mobile apps, which requires a service that supports mobile push notification protocols like APNS or FCM. SES only sends emails through SMTP and cannot deliver push notifications directly to mobile devices. Even though Kinesis is correctly chosen for the streaming analytics part, pairing it with SES fails to meet the mobile notification requirement. SNS is the appropriate service for mobile push notifications, as it supports the native push protocols that mobile applications use.\n\n**Why option 3 is incorrect:**\nAmazon SQS is a pull-based message queuing service where consumers must actively poll the queue to retrieve messages. For mobile applications, this means the app would need to continuously check SQS for new messages, which is highly inefficient. This polling approach increases latency (messages aren't delivered immediately), wastes network bandwidth, and significantly drains mobile device battery life. Additionally, SQS doesn't provide the real-time streaming data processing capabilities that Kinesis offers for the analytics requirement. SNS, on the other hand, uses a true push model where AWS delivers messages directly to mobile apps without requiring polling, making it the correct choice for mobile notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nS3 One Zone-IA stores data in a single Availability Zone, providing 99.5% availability at approximately 20% lower cost than S3 Standard-IA. Since the assets are re-creatable (can be regenerated if lost), the reduced durability of One Zone-IA is acceptable. The 30-day transition period ensures assets remain in Standard storage during the first week of high access, then transition to cheaper storage after access frequency drops. This balances cost optimization with the requirement for immediate accessibility.\n\n**Why option 0 is incorrect:**\nTransitioning to S3 One Zone-IA after 7 days is too early. The scenario states assets are accessed frequently for the first few days, and transitioning too early could impact performance or increase costs if assets are still being accessed frequently. The 30-day period ensures the high-access period has passed before transitioning to cheaper storage.\n\n**Why option 1 is incorrect:**\nS3 Standard-IA stores data across multiple Availability Zones providing 99.999999999% durability (11 9's), which is overkill for re-creatable assets. It's also more expensive than One Zone-IA. Additionally, the 7-day transition is too early, potentially impacting performance during high-access periods when assets are still being frequently accessed.\n\n**Why option 3 is incorrect:**\nWhile the 30-day timing is correct, S3 Standard-IA is more expensive than One Zone-IA. For re-creatable assets that don't require maximum durability, One Zone-IA provides the best cost optimization while maintaining immediate accessibility. Standard-IA's multi-AZ durability is unnecessary for data that can be regenerated.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
            options: [
                { id: 0, text: "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: true },
                { id: 1, text: "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 2, text: "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 3, text: "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nLaunch templates are the modern, recommended way to configure Auto Scaling groups. They support mixed instance types and mixed purchasing options (On-Demand and Spot Instances) through the Mixed Instances Policy feature. This allows you to specify multiple instance types and purchasing strategies, enabling cost optimization while maintaining performance. Launch templates also support versioning and can be updated without recreating the Auto Scaling group.\n\n**Why option 1 is incorrect:**\nThis is incorrect because launch templates DO support mixed instance types with Spot Instances through Mixed Instances Policy. This feature was specifically designed for this use case and allows you to provision capacity across multiple instance types using both On-Demand and Spot Instances.\n\n**Why option 2 is incorrect:**\nLaunch configurations are legacy and do NOT support mixed instance types or Spot Instances. They only support a single instance type and On-Demand instances. This option is incorrect because it suggests launch configurations can do something they cannot. Only launch templates support the Mixed Instances Policy feature.\n\n**Why option 3 is incorrect:**\nLaunch configurations cannot support mixed instance types or Spot Instances. They are limited to a single instance type and On-Demand purchasing only. This is a fundamental limitation of launch configurations. Only launch templates support mixed instance types and purchasing options.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
            options: [
                { id: 0, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type", correct: false },
                { id: 1, text: "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed", correct: true },
                { id: 2, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance", correct: false },
                { id: 3, text: "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nLaunch configurations are immutable - they cannot be modified once created. To fix an incorrect instance type, you must create a new launch configuration with the correct instance type, then update the Auto Scaling group to use the new launch configuration. The old launch configuration can be deleted once the Auto Scaling group is updated. New instances launched by the Auto Scaling group will use the correct instance type from the new launch configuration. Existing instances will continue running with the old instance type until they are terminated and replaced.\n\n**Why option 0 is incorrect:**\nAuto Scaling groups don't have a direct instance type setting - they get this from the launch configuration. You cannot change the instance type without changing the launch configuration. The Auto Scaling group references a launch configuration, which defines the instance type. Simply modifying the Auto Scaling group won't change the instance type.\n\n**Why option 2 is incorrect:**\nAdding more instances of the wrong type doesn't solve the performance problem. The issue is that the instance type itself is incorrect and not optimized for the application workflow, not the quantity. More instances of the wrong type won't improve performance - you need the correct instance type that matches the application's requirements.\n\n**Why option 3 is incorrect:**\nLaunch configurations are immutable and cannot be modified after creation. This is a fundamental characteristic of launch configurations. You must create a new launch configuration with the correct instance type rather than trying to modify an existing one.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 1, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 2, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
                { id: 3, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 4, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nAWS CLI aws s3 sync command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets (like 1 petabyte) efficiently with parallel transfers and retry logic. The sync command automatically handles differences between buckets and can resume interrupted transfers. It's a straightforward solution for one-time copies and can be run from any machine with AWS CLI access.\n\n**Why option 2 is correct:**\nS3 batch replication can copy existing objects (not just new ones) when configured with batch replication. After setting up replication and allowing it to copy existing objects, you can delete the replication configuration once the one-time copy is complete. This provides a managed, console-based solution that handles the transfer automatically without requiring command-line tools or manual intervention.\n\n**Why option 1 is incorrect:**\nS3 Transfer Acceleration optimizes transfers from clients (like on-premises) to S3 using CloudFront edge locations. It does NOT help with bucket-to-bucket transfers within AWS. For bucket-to-bucket transfers, you should use sync or replication. Transfer Acceleration is designed for client-to-S3 transfers, not S3-to-S3 transfers.\n\n**Why option 3 is incorrect:**\nSnowball is for transferring data from on-premises to AWS, not for S3 bucket-to-bucket transfers within AWS. The data is already in S3, so Snowball is not applicable. Snowball is used when you need to physically ship data from on-premises locations, not for cloud-to-cloud transfers between S3 buckets.\n\n**Why option 4 is incorrect:**\nWhile you can copy individual objects in the S3 console, copying 1 petabyte through the console UI is not practical. The console is designed for small-scale operations, not bulk transfers. This would require thousands of manual operations and is not feasible for such a large dataset.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
            options: [
                { id: 0, text: "AWS Lambda", correct: false },
                { id: 1, text: "AWS Step Functions", correct: false },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon CloudWatch", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "**Why option 2 is correct:**\nSNS can receive messages from CloudWatch alarms and send email notifications to subscribers. This requires minimal setup - just create an SNS topic, subscribe email addresses, and configure the CloudWatch alarm to publish to the topic. No Lambda functions or custom code needed.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- Amazon CloudWatch: CloudWatch automatically monitors EC2 instance metrics including CPU utilization. You can create CloudWatch alarms that trigger when CPU utilization breaches a threshold. CloudWatch alarms can directly publish to SNS topics, requiring no custom code. This is a native AWS service integration.\n- Amazon SNS: SNS can receive messages from CloudWatch alarms and send email notifications to subscribers. This requires minimal setup - just create an SNS topic, subscribe email addresses, and configure the CloudWatch alarm to publish to the topic. No Lambda functions or custom code needed.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nWhile Lambda can be used to process CloudWatch events and send emails, it requires writing custom code, which increases development effort. CloudWatch alarms with SNS provide a no-code solution. Lambda adds unnecessary complexity for simple monitoring and email notifications.\n\n**Why option 1 is incorrect:**\nStep Functions orchestrate multiple AWS services but adds unnecessary complexity for simple monitoring and email notifications. CloudWatch + SNS is simpler and requires less development effort. Step Functions are for complex workflows, not simple alerting.\n\n**Why option 3 is incorrect:**\nSQS is a message queue service and doesn't directly send email notifications. While it could be part of a more complex solution (SQS -> Lambda -> SES), it's not needed here and adds unnecessary complexity. SQS requires a consumer to process messages.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 2, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nAuto Scaling groups manage EC2 instance capacity but don't provide low-latency routing. They don't optimize network paths or provide global distribution. Auto Scaling is about instance management, not network optimization.\n\n**Why option 1 is incorrect:**\nELB distributes traffic across instances within a region but doesn't optimize for global latency. It doesn't use AWS's global network infrastructure or provide static IP addresses. ELB is regional, not global.\n\n**Why option 2 is incorrect:**\nCloudFront is a Content Delivery Network (CDN) optimized for HTTP/HTTPS traffic and static content caching. It's not designed for UDP protocol traffic or real-time data distribution. CloudFront caches content at edge locations, which isn't suitable for live, real-time sports results.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nSNS is a pub/sub messaging service for notifications and fan-out scenarios. It doesn't support custom data processing or real-time analytics. SNS delivers messages to subscribers but doesn't provide the streaming data processing capabilities needed here.\n\n**Why option 2 is incorrect:**\n- Use Amazon Kinesis Data Firehose: Kinesis Data Firehose is designed for loading streaming data into destinations like S3, Redshift, or Elasticsearch. It doesn't support custom processing applications - it's a fully managed service that automatically delivers data to destinations. For custom processing and analysis, you need Kinesis Data Streams.\n- Use Amazon Simple Queue Service (Amazon SQS): SQS is a message queuing service for decoupling applications, but it's not designed for real-time streaming data processing. It's pull-based (consumers poll for messages) and doesn't provide the real-time processing capabilities or data retention features of Kinesis. SQS is better for discrete messages, not continuous streams.\n- Use Amazon Simple Notification Service (Amazon SNS): SNS is a pub/sub messaging service for notifications and fan-out scenarios. It doesn't support custom data processing or real-time analytics. SNS delivers messages to subscribers but doesn't provide the streaming data processing capabilities needed here.\n\n**Why option 3 is incorrect:**\n- Use Amazon Kinesis Data Firehose: Kinesis Data Firehose is designed for loading streaming data into destinations like S3, Redshift, or Elasticsearch. It doesn't support custom processing applications - it's a fully managed service that automatically delivers data to destinations. For custom processing and analysis, you need Kinesis Data Streams.\n- Use Amazon Simple Queue Service (Amazon SQS): SQS is a message queuing service for decoupling applications, but it's not designed for real-time streaming data processing. It's pull-based (consumers poll for messages) and doesn't provide the real-time processing capabilities or data retention features of Kinesis. SQS is better for discrete messages, not continuous streams.\n- Use Amazon Simple Notification Service (Amazon SNS): SNS is a pub/sub messaging service for notifications and fan-out scenarios. It doesn't support custom data processing or real-time analytics. SNS delivers messages to subscribers but doesn't provide the streaming data processing capabilities needed here.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Remove full database access for all IAM users in the organization", correct: false },
                { id: 1, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Only root user should have full database access in the organization", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nThis is impractical and would severely limit productivity. Developers need database access to work. The root user should never be used for day-to-day operations. This would create operational bottlenecks.\n\n**Why option 2 is incorrect:**\nWhile this adds oversight, it's not scalable and relies on manual processes that can be error-prone. It doesn't provide a technical safeguard against accidental deletions. This is a process solution, not an architectural one.\n\n**Why option 3 is incorrect:**\n- Remove full database access for all IAM users in the organization: This is too restrictive and would prevent legitimate work. Developers need database access to build features. The solution should prevent accidental deletions, not remove all access. This would break normal operations.\n- The CTO should review the permissions for each new developer's IAM user: While this adds oversight, it's not scalable and relies on manual processes that can be error-prone. It doesn't provide a technical safeguard against accidental deletions. This is a process solution, not an architectural one.\n- Only root user should have full database access in the organization: This is impractical and would severely limit productivity. Developers need database access to work. The root user should never be used for day-to-day operations. This would create operational bottlenecks.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to analyze API calls", correct: true },
                { id: 1, text: "Implement an IAM policy to forbid users to change Amazon S3 bucket settings", correct: false },
                { id: 2, text: "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations", correct: false },
                { id: 3, text: "Use Amazon S3 access logs to analyze user access using Athena", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nThis adds security but doesn't help identify who is making changes or why. It also doesn't address the investigation requirement. MFA prevents unauthorized access but doesn't provide audit trails for authorized users.\n\n**Why option 2 is incorrect:**\n- Implement an IAM policy to forbid users to change Amazon S3 bucket settings: This restricts user rights, which the question explicitly asks to avoid. The requirement is to figure out what's happening without restricting rights. This would prevent investigation by blocking the activity entirely.\n- Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations: This adds security but doesn't help identify who is making changes or why. It also doesn't address the investigation requirement. MFA prevents unauthorized access but doesn't provide audit trails for authorized users.\n- Use Amazon S3 access logs to analyze user access using Athena: S3 access logs (server access logs) track object-level access (GET, PUT, DELETE operations on objects), but they don't track bucket-level configuration changes like bucket policies, versioning settings, or lifecycle policies. CloudTrail is needed for API-level auditing of bucket configuration changes.\n\n**Why option 3 is incorrect:**\nS3 access logs (server access logs) track object-level access (GET, PUT, DELETE operations on objects), but they don't track bucket-level configuration changes like bucket policies, versioning settings, or lifecycle policies. CloudTrail is needed for API-level auditing of bucket configuration changes.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
            options: [
                { id: 0, text: "Amazon Redshift", correct: false },
                { id: 1, text: "Amazon RDS", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: true },
                { id: 3, text: "Amazon ElastiCache", correct: false },
                { id: 4, text: "AWS Lambda", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "**Why option 2 is correct:**\nDynamoDB is a NoSQL database that stores data as key-value pairs, which matches the data format described (key-value pairs). It provides high availability with automatic multi-AZ replication and can handle high-throughput writes (one-minute frequency is easily manageable). DynamoDB is serverless, scales automatically, and provides 99.999% availability SLA. It's ideal for time-series data like weather metrics.\n\n**Why option 4 is correct:**\nLambda can process the incoming weather data, transform it if needed, and write it to DynamoDB. It can be triggered by various sources (API Gateway, Kinesis, SQS, etc.) and provides serverless, event-driven processing. Lambda automatically scales to handle the incoming data frequency and integrates seamlessly with DynamoDB.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nElastiCache is an in-memory caching service (Redis or Memcached) designed for temporary data storage, not persistent storage. Data in ElastiCache can be lost if the cache is cleared or nodes fail. The requirement is for reliable storage with high availability, which requires persistent storage like DynamoDB.\n\n**Why option 1 is incorrect:**\n- Amazon Redshift: Redshift is a data warehouse designed for analytical queries on large datasets, not for high-frequency writes of key-value pairs. It's optimized for complex SQL queries, not simple key-value storage. Redshift would be overkill and not optimized for this use case.\n- Amazon RDS: RDS is a relational database service that requires schema definition and is optimized for relational data, not key-value pairs. While it could work, DynamoDB is better suited for key-value data and provides better scalability and availability for this use case. RDS requires more management overhead.\n- Amazon ElastiCache: ElastiCache is an in-memory caching service (Redis or Memcached) designed for temporary data storage, not persistent storage. Data in ElastiCache can be lost if the cache is cleared or nodes fail. The requirement is for reliable storage with high availability, which requires persistent storage like DynamoDB.\n\n**Why option 3 is incorrect:**\n- Amazon Redshift: Redshift is a data warehouse designed for analytical queries on large datasets, not for high-frequency writes of key-value pairs. It's optimized for complex SQL queries, not simple key-value storage. Redshift would be overkill and not optimized for this use case.\n- Amazon RDS: RDS is a relational database service that requires schema definition and is optimized for relational data, not key-value pairs. While it could work, DynamoDB is better suited for key-value data and provides better scalability and availability for this use case. RDS requires more management overhead.\n- Amazon ElastiCache: ElastiCache is an in-memory caching service (Redis or Memcached) designed for temporary data storage, not persistent storage. Data in ElastiCache can be lost if the cache is cleared or nodes fail. The requirement is for reliable storage with high availability, which requires persistent storage like DynamoDB.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
                { id: 3, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nRoute 53 uses DNS-based routing, which is subject to DNS caching on mobile devices. Mobile phones cache DNS records, so users might not see the new deployment even after DNS changes propagate. This doesn't solve the DNS caching problem mentioned in the scenario.\n\n**Why option 1 is incorrect:**\nCodeDeploy manages application deployments but doesn't control traffic routing or distribution. It deploys code to instances but doesn't help with routing traffic between blue and green environments for testing. CodeDeploy is about deployment, not traffic management.\n\n**Why option 3 is incorrect:**\nELB distributes traffic within a region but doesn't provide traffic dials or weighted routing between different deployments. ELB also doesn't solve the DNS caching issue. ELB is regional and doesn't provide the global traffic management needed.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
                { id: 1, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 2, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "**Why option 3 is correct:**\nMFA delete requires multi-factor authentication before objects can be permanently deleted. This adds an extra layer of protection against accidental deletions. Even if someone has delete permissions, they need MFA to permanently delete versioned objects, providing compliance-grade protection.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- Enable versioning: S3 versioning keeps multiple versions of objects, so if an object is deleted, you can restore a previous version. This provides protection against accidental deletion by allowing recovery of deleted objects. Versioning is a fundamental S3 feature for data protection and compliance.\n- Enable MFA delete: MFA delete requires multi-factor authentication before objects can be permanently deleted. This adds an extra layer of protection against accidental deletions. Even if someone has delete permissions, they need MFA to permanently delete versioned objects, providing compliance-grade protection.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThere is no such configuration option in S3. S3 doesn't have a built-in \"confirmation dialog\" setting. This is not a real S3 feature.\n\n**Why option 1 is incorrect:**\nWhile this provides notification of deletions, it doesn't prevent accidental deletion or allow recovery. It's reactive, not protective. The object would still be deleted before the notification is sent.\n\n**Why option 2 is incorrect:**\nThis is a process solution, not a technical safeguard. It relies on human processes that can be bypassed or forgotten. Technical controls like versioning and MFA delete are more reliable and enforceable.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 3, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nIAM roles are the recommended and secure way to grant permissions to EC2 instances. Roles provide temporary credentials that are automatically rotated, eliminating the need to store long-lived access keys. An instance profile is a container for an IAM role that allows EC2 instances to assume the role. This is the AWS best practice for EC2 access to AWS services. Credentials are automatically provided to the instance via the instance metadata service.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nThis is a security anti-pattern. Hardcoding credentials in code is insecure because credentials can be exposed in code repositories, logs, or if the instance is compromised. Credentials don't rotate automatically and must be manually updated. This violates AWS security best practices.\n\n**Why option 2 is incorrect:**\nWhile encryption adds some security, this still requires storing credentials on the instance, which is not recommended. Credentials must still be decrypted and stored in memory, and the encryption key must be managed. IAM roles eliminate the need to store credentials entirely.\n\n**Why option 3 is incorrect:**\nSimilar to the previous options, this requires storing IAM user credentials on the instance, which is insecure. IAM user credentials are long-lived and don't rotate automatically. Using IAM roles is the recommended approach.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon RDS Custom is specifically designed for applications that require customization of the database environment and underlying operating system. RDS Custom for Oracle allows DBAs to access and customize the database environment, install custom software, and configure the OS while still providing managed database services. Multi-AZ configuration provides high availability with automatic failover. This is the only RDS option that allows both customization and high availability.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nStandard RDS for Oracle doesn't allow customization of the database environment or underlying OS. Read replicas provide read scaling but don't allow the level of customization required. Standard RDS is a managed service with limited OS and database customization.\n\n**Why option 2 is incorrect:**\nWhile this allows full customization, it requires managing the database yourself, including backups, patching, and high availability setup. This increases operational overhead compared to RDS Custom, which provides managed services with customization capabilities.\n\n**Why option 3 is incorrect:**\nStandard RDS for Oracle doesn't allow customization of the database environment or underlying operating system. The requirement specifically needs customization capabilities that standard RDS doesn't provide.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
            options: [
                { id: 0, text: "Move to Amazon Redshift", correct: false },
                { id: 1, text: "Switch application code to AWS Lambda for better performance", correct: false },
                { id: 2, text: "Setup Amazon ElastiCache in front of Amazon RDS", correct: true },
                { id: 3, text: "Setup Amazon RDS Read Replicas", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nRedshift is a data warehouse designed for analytical queries on large datasets, not for transactional read/write operations. It's not suitable for real-time game data retrieval. Redshift has higher latency and is optimized for complex analytical queries, not simple lookups.\n\n**Why option 1 is incorrect:**\nLambda is a compute service and doesn't address the database read performance issue. The problem is database load, not compute performance. Lambda would still need to query the same RDS database, so it doesn't solve the problem.\n\n**Why option 3 is incorrect:**\nWhile read replicas can help distribute read traffic, they don't reduce costs - you're adding more database instances, which increases costs. Read replicas also don't reduce latency as much as caching does. ElastiCache provides better performance improvement and cost reduction.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nMacie is a data security service that uses machine learning to automatically discover, classify, and protect sensitive data in S3. It can identify sensitive data like PII, credit card numbers, and other regulated information. Macie provides visibility into data access patterns and helps ensure compliance.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nThis reverses the roles of the services. GuardDuty monitors threats, Macie identifies sensitive data. The correct pairing is GuardDuty for threats and Macie for sensitive data.\n\n**Why option 2 is incorrect:**\n- Use Amazon GuardDuty to monitor malicious activity AND identify sensitive data: GuardDuty is designed for threat detection, not data classification. It doesn't have the capability to identify sensitive data patterns. Macie is specifically designed for sensitive data discovery.\n- Use Amazon Macie to monitor malicious activity AND identify sensitive data: Macie is designed for data discovery and classification, not threat detection. It doesn't monitor for malicious activity like unauthorized access attempts or data exfiltration. GuardDuty is needed for threat monitoring.\n- Use Amazon Macie to monitor malicious activity. Use Amazon GuardDuty to identify sensitive data: This reverses the roles of the services. GuardDuty monitors threats, Macie identifies sensitive data. The correct pairing is GuardDuty for threats and Macie for sensitive data.\n\n**Why option 3 is incorrect:**\n- Use Amazon GuardDuty to monitor malicious activity AND identify sensitive data: GuardDuty is designed for threat detection, not data classification. It doesn't have the capability to identify sensitive data patterns. Macie is specifically designed for sensitive data discovery.\n- Use Amazon Macie to monitor malicious activity AND identify sensitive data: Macie is designed for data discovery and classification, not threat detection. It doesn't monitor for malicious activity like unauthorized access attempts or data exfiltration. GuardDuty is needed for threat monitoring.\n- Use Amazon Macie to monitor malicious activity. Use Amazon GuardDuty to identify sensitive data: This reverses the roles of the services. GuardDuty monitors threats, Macie identifies sensitive data. The correct pairing is GuardDuty for threats and Macie for sensitive data.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 1, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 2, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nVPC peering connects VPCs but doesn't \"share\" them. Each VPC remains separate, and resources in one VPC cannot be launched in the other VPC's subnets. VPC peering is for network connectivity, not resource sharing.\n\n**Why option 2 is incorrect:**\n- Use VPC peering to share one or more subnets: VPC peering connects entire VPCs, not individual subnets. You cannot share subnets using VPC peering - it creates a network connection between two VPCs. VPC peering doesn't allow multiple accounts to launch resources in the same subnet.\n- Use VPC peering to share a VPC: VPC peering connects VPCs but doesn't \"share\" them. Each VPC remains separate, and resources in one VPC cannot be launched in the other VPC's subnets. VPC peering is for network connectivity, not resource sharing.\n- Use VPC sharing to share a VPC: VPC sharing works at the subnet level, not the VPC level. You share specific subnets with other accounts, not entire VPCs. This allows for more granular control and better resource isolation.\n\n**Why option 3 is incorrect:**\nVPC sharing works at the subnet level, not the VPC level. You share specific subnets with other accounts, not entire VPCs. This allows for more granular control and better resource isolation.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
            options: [
                { id: 0, text: "Amazon ElastiCache for Redis/Memcached", correct: true },
                { id: 1, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon DocumentDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nDocumentDB is a MongoDB-compatible document database, not a caching solution. It doesn't cache SQL query results and is designed for document storage, not query result caching.\n\n**Why option 2 is incorrect:**\n- Amazon DynamoDB Accelerator (DAX): DAX is specifically designed as a caching layer for DynamoDB, not for relational databases or SQL queries. It's optimized for DynamoDB's NoSQL data model and cannot cache SQL query results from other databases.\n- Amazon DynamoDB: DynamoDB is a NoSQL database, not a caching solution. While it's fast, it doesn't cache SQL query results. The requirement specifically asks for caching SQL query results, which requires a caching layer, not a different database.\n- Amazon DocumentDB: DocumentDB is a MongoDB-compatible document database, not a caching solution. It doesn't cache SQL query results and is designed for document storage, not query result caching.\n\n**Why option 3 is incorrect:**\n- Amazon DynamoDB Accelerator (DAX): DAX is specifically designed as a caching layer for DynamoDB, not for relational databases or SQL queries. It's optimized for DynamoDB's NoSQL data model and cannot cache SQL query results from other databases.\n- Amazon DynamoDB: DynamoDB is a NoSQL database, not a caching solution. While it's fast, it doesn't cache SQL query results. The requirement specifically asks for caching SQL query results, which requires a caching layer, not a different database.\n- Amazon DocumentDB: DocumentDB is a MongoDB-compatible document database, not a caching solution. It doesn't cache SQL query results and is designed for document storage, not query result caching.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DMS can read data from S3 (using S3 as a source endpoint) and stream it to Kinesis Data Streams (using Kinesis as a target endpoint). DMS supports full load of existing data and ongoing change data capture (CDC) for new files. This provides a managed, efficient way to stream both existing and new S3 files to Kinesis without requiring custom code or Lambda functions. DMS handles the complexity of reading from S3 and writing to Kinesis.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nSimilar to S3 event notifications, EventBridge can trigger on S3 events but only for new objects. It doesn't handle existing files, and you would still need Lambda to read the file content and write to Kinesis, adding complexity.\n\n**Why option 2 is incorrect:**\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event: While this works for new files, it doesn't handle existing files in S3. S3 event notifications only trigger for new object creation events, not for existing objects. You would need a separate process to handle existing files.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon SNS: S3 cannot directly write to SNS. S3 can send event notifications to SNS, but these are just notifications about object creation, not the actual data. SNS also cannot directly send data to Kinesis Data Streams - you would need Lambda or another service.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3: Similar to S3 event notifications, EventBridge can trigger on S3 events but only for new objects. It doesn't handle existing files, and you would still need Lambda to read the file content and write to Kinesis, adding complexity.\n\n**Why option 3 is incorrect:**\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event: While this works for new files, it doesn't handle existing files in S3. S3 event notifications only trigger for new object creation events, not for existing objects. You would need a separate process to handle existing files.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon SNS: S3 cannot directly write to SNS. S3 can send event notifications to SNS, but these are just notifications about object creation, not the actual data. SNS also cannot directly send data to Kinesis Data Streams - you would need Lambda or another service.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3: Similar to S3 event notifications, EventBridge can trigger on S3 events but only for new objects. It doesn't handle existing files, and you would still need Lambda to read the file content and write to Kinesis, adding complexity.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 1, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nFor cross-account access, you need permissions on both sides: the Lambda function's execution role needs permissions to access S3, AND the S3 bucket policy must grant access to the Lambda function's role. The bucket policy in Account B must explicitly allow the role ARN from Account A. This is the standard pattern for cross-account resource access - both the resource policy (bucket policy) and the identity policy (role policy) must allow the access.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nMaking the bucket public is a security risk and violates the principle of least privilege. Public buckets expose data to anyone on the internet. Cross-account access should use IAM roles and bucket policies, not public access.\n\n**Why option 2 is incorrect:**\nThis is incomplete. Just creating a role with permissions isn't enough - the bucket policy in Account B must also grant access to that role. Without the bucket policy, the Lambda function will be denied access.\n\n**Why option 3 is incorrect:**\nThis is incorrect. Lambda can absolutely access resources across accounts using IAM roles and resource policies. Cross-account access is a standard AWS pattern.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 1, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 3, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nFor 5 petabytes of data, Snowball Edge Storage Optimized devices are the most cost-effective solution. Each device can hold up to 80TB. Snowball devices are shipped to your location, you copy data to them, then AWS ships them back and imports the data into S3. After import, you can use lifecycle policies to automatically transition data to Glacier for long-term archival storage. This approach avoids expensive network transfer costs and is much faster than transferring 5PB over the internet.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nVPN connections have bandwidth limitations and would take an extremely long time to transfer 5PB. VPN is also not cost-effective for such large transfers. Like Direct Connect, you cannot directly write to Glacier.\n\n**Why option 1 is incorrect:**\nDirect Connect has high setup costs and monthly fees. For a one-time migration of 5PB, the cost would be prohibitive. Direct Connect is designed for ongoing connectivity, not one-time bulk transfers. Also, you cannot directly write to Glacier - data must go to S3 first, then transition to Glacier.\n\n**Why option 3 is incorrect:**\nSnowball devices import data into S3, not directly into Glacier. You must first import to S3, then use lifecycle policies or other methods to transition to Glacier. Glacier doesn't support direct imports from Snowball.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 1, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 2, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 3, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 4, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
                { id: 5, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "**Why option 1 is correct:**\nBy default, Lambda functions run in AWS-managed VPCs with internet access. When you attach a Lambda to your VPC, it loses default internet access and needs a NAT Gateway (or NAT Instance) in a public subnet to access the internet or AWS APIs. This is a critical consideration for VPC-enabled Lambdas.\n\n**Why option 3 is correct:**\n- Lambda VPC behavior: By default, Lambda functions run in AWS-managed VPCs with internet access. When you attach a Lambda to your VPC, it loses default internet access and needs a NAT Gateway (or NAT Instance) in a public subnet to access the internet or AWS APIs. This is a critical consideration for VPC-enabled Lambdas.\n- Lambda Layers: Lambda Layers allow you to package libraries, custom runtimes, or other function dependencies separately. This promotes code reuse, reduces deployment package size, and can speed up deployments. Layers are shared across functions, making them ideal for common dependencies.\n- CloudWatch Alarms for scaling: Lambda can scale to thousands of concurrent executions very quickly. Monitoring ConcurrentExecutions and Invocations helps prevent runaway costs and ensures you're aware of scaling events. CloudWatch alarms provide proactive monitoring.\n**Why other options are incorrect:**\n\n**Why option 5 is correct:**\nLambda can scale to thousands of concurrent executions very quickly. Monitoring ConcurrentExecutions and Invocations helps prevent runaway costs and ensures you're aware of scaling events. CloudWatch alarms provide proactive monitoring.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis is incorrect. Lambda DOES support container images (up to 10GB). You can package Lambda functions as container images and deploy them. Container images are supported for Lambda.\n\n**Why option 2 is incorrect:**\nThis is incorrect. AWS recommends RIGHT-SIZING timeout settings, not over-provisioning. Over-provisioning wastes money. You should set timeouts based on actual function execution time.\n\n**Why option 4 is incorrect:**\nWhile larger packages do increase cold start time, AWS recommends using Lambda Layers for dependencies, not separate packages. Layers are the recommended approach for managing dependencies.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator", correct: false },
                { id: 1, text: "Use AWS Direct Connect", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "**Why option 3 is correct:**\nAurora read replicas can handle read traffic, offloading the primary database during spikes. This improves read performance and provides additional capacity. Read replicas can be in the same or different regions, providing geographic distribution and better resilience.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- Amazon CloudFront: CloudFront is a CDN that caches content at edge locations worldwide, reducing latency and offloading traffic from the origin (Application Load Balancer). During traffic spikes, CloudFront serves cached content from edge locations, reducing load on the backend infrastructure. It also provides DDoS protection and can handle sudden traffic increases.\n- Amazon Aurora Replica: Aurora read replicas can handle read traffic, offloading the primary database during spikes. This improves read performance and provides additional capacity. Read replicas can be in the same or different regions, providing geographic distribution and better resilience.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nGlobal Accelerator routes traffic to optimal endpoints but doesn't cache content or reduce database load. It's designed for improving connection performance, not handling traffic spikes or database load. It doesn't address the database performance issue.\n\n**Why option 1 is incorrect:**\nDirect Connect provides dedicated network connectivity but doesn't help with traffic spikes or database performance. It's for network connectivity, not application resilience or performance optimization.\n\n**Why option 2 is incorrect:**\nShield provides DDoS protection but doesn't improve application performance or handle legitimate traffic spikes. It protects against attacks but doesn't optimize for high traffic volumes.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon S3", correct: true },
                { id: 1, text: "Amazon Kinesis", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 4, text: "Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nGateway endpoints are VPC endpoints that use route tables to route traffic to AWS services. Only S3 and DynamoDB support gateway endpoints. Gateway endpoints are free and don't require NAT Gateway or Internet Gateway for access. They're added as routes in your VPC route tables and automatically route traffic to the service without going over the internet.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\nGateway endpoints are VPC endpoints that use route tables to route traffic to AWS services. Only S3 and DynamoDB support gateway endpoints. Gateway endpoints are free and don't require NAT Gateway or Internet Gateway for access. They're added as routes in your VPC route tables and automatically route traffic to the service without going over the internet.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nSNS uses interface endpoints (PrivateLink), not gateway endpoints. Only S3 and DynamoDB support the simpler gateway endpoint model.\n\n**Why option 2 is incorrect:**\n- Amazon Kinesis: Kinesis uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints are ENIs in your VPC that provide private connectivity. They're different from gateway endpoints.\n- Amazon Simple Queue Service (Amazon SQS): SQS uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints require DNS resolution and are more complex than gateway endpoints.\n- Amazon Simple Notification Service (Amazon SNS): SNS uses interface endpoints (PrivateLink), not gateway endpoints. Only S3 and DynamoDB support the simpler gateway endpoint model.\n\n**Why option 3 is incorrect:**\n- Amazon Kinesis: Kinesis uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints are ENIs in your VPC that provide private connectivity. They're different from gateway endpoints.\n- Amazon Simple Queue Service (Amazon SQS): SQS uses interface endpoints (PrivateLink), not gateway endpoints. Interface endpoints require DNS resolution and are more complex than gateway endpoints.\n- Amazon Simple Notification Service (Amazon SNS): SNS uses interface endpoints (PrivateLink), not gateway endpoints. Only S3 and DynamoDB support the simpler gateway endpoint model.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAurora read replicas are separate database instances that replicate data from the primary. They have their own endpoint that applications can use for read queries. By directing read traffic to the read replica endpoint, you offload read operations from the primary database, reducing I/O contention and allowing writes to perform better. The application must be modified to use the read replica endpoint for read queries while continuing to use the primary endpoint for writes.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nAurora doesn't have a \"read-through caching\" feature. You can use ElastiCache in front of Aurora for caching, but that's a separate service, not an Aurora feature.\n\n**Why option 2 is incorrect:**\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This wording is confusing, but the key issue is that you need to modify the application to use the read replica endpoint. Simply creating a read replica isn't enough - the application must be configured to route reads to it.\n- Activate read-through caching on the Amazon Aurora database: Aurora doesn't have a \"read-through caching\" feature. You can use ElastiCache in front of Aurora for caching, but that's a separate service, not an Aurora feature.\n- Configure the application to read from the Multi-AZ standby instance: The Multi-AZ standby instance is for high availability and automatic failover, not for read scaling. It's not accessible for read queries - it's only used during failover. Read replicas are separate instances designed for read scaling.\n\n**Why option 3 is incorrect:**\nThe Multi-AZ standby instance is for high availability and automatic failover, not for read scaling. It's not accessible for read queries - it's only used during failover. Read replicas are separate instances designed for read scaling.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 2, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 3, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 4, text: "The route for the health check is misconfigured", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "**Why option 0 is correct:**\nThe EC2 instance's security group must allow inbound traffic from the ALB's security group (or the ALB's IP addresses). If the security group only allows traffic from specific IPs or doesn't allow ALB traffic, health checks will fail even though the website works when accessed directly via IP.\n\n**Why option 4 is correct:**\nThe ALB health check is configured with a specific path (like /health or /). If the application doesn't respond correctly to that path, or if the path is incorrect, health checks will fail. The website might work at / but fail at /health, causing the instances to be marked unhealthy.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nEBS volume mounting issues would prevent the application from running at all. If the website works when accessed directly, the volumes are mounted correctly. This wouldn't cause selective health check failures.\n\n**Why option 2 is incorrect:**\nEIPs are not required for ALB health checks. ALB routes traffic to instances using their private IP addresses. EIPs are for public internet access, not for ALB connectivity.\n\n**Why option 3 is incorrect:**\nALB works with any HTTP/HTTPS application regardless of runtime. It operates at Layer 7 (HTTP) and doesn't care about the application runtime. This is not a valid reason for health check failures.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
            options: [
                { id: 0, text: "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated", correct: false },
                { id: 1, text: "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume", correct: true },
                { id: 2, text: "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume", correct: false },
                { id: 3, text: "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nWhen you launch an EC2 instance, the root EBS volume has a \"Delete on Termination\" attribute that defaults to true. This means when you terminate the instance, the root volume is automatically deleted. Additional EBS volumes attached to the instance have \"Delete on Termination\" set to false by default, so they persist. The team likely stored data on the root volume instead of a separate data volume, causing the data loss.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis is incorrect. Only the root volume is terminated by default. Additional volumes persist unless explicitly configured to be deleted.\n\n**Why option 2 is incorrect:**\nEFS is a file system service, not a backup destination. The issue is volume termination settings, not backup location.\n\n**Why option 3 is incorrect:**\n- On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated: This is incorrect. Only the root volume is terminated by default. Additional volumes persist unless explicitly configured to be deleted.\n- The Amazon EBS volumes were not backed up on Amazon S3 storage: Backing up to S3 is a good practice but not required for volumes to persist. The issue is the \"Delete on Termination\" setting, not lack of backups. Even without S3 backups, volumes can persist if configured correctly.\n- The Amazon EBS volumes were not backed up on Amazon EFS file system storage: EFS is a file system service, not a backup destination. The issue is volume termination settings, not backup location.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use Spot Instances", correct: false },
                { id: 1, text: "Use a Cluster placement group", correct: true },
                { id: 2, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nSpot Instances are about cost optimization, not network performance. They don't improve network performance between instances. Network performance depends on instance type and placement group, not purchasing option.\n\n**Why option 2 is incorrect:**\nKernel optimization might provide minor improvements but won't significantly impact network performance between instances. Placement groups are the primary way to optimize inter-instance network performance.\n\n**Why option 3 is incorrect:**\nSpread placement groups place instances on distinct hardware to minimize correlated failures. They actually REDUCE network performance compared to Cluster placement groups because instances are spread across different hardware. For high network performance, Cluster placement groups are required.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
                { id: 1, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 2, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 3, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nRedshift is a data warehouse, not suitable for transactional applications. It's designed for analytical workloads, not operational databases. It also doesn't support the same MySQL compatibility.\n\n**Why option 2 is incorrect:**\n- Use Amazon DynamoDB Global Tables: DynamoDB is a NoSQL database and would require migrating from the relational MySQL schema. The requirement explicitly states \"without moving away from the underlying relational database schema.\" DynamoDB uses a different data model.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases: This requires managing databases yourself, including backups, patching, replication, and high availability. It's not cost-effective and increases operational overhead. Aurora Global Database provides managed global replication.\n- Spin up a Amazon Redshift cluster in each AWS region: Redshift is a data warehouse, not suitable for transactional applications. It's designed for analytical workloads, not operational databases. It also doesn't support the same MySQL compatibility.\n\n**Why option 3 is incorrect:**\n- Use Amazon DynamoDB Global Tables: DynamoDB is a NoSQL database and would require migrating from the relational MySQL schema. The requirement explicitly states \"without moving away from the underlying relational database schema.\" DynamoDB uses a different data model.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases: This requires managing databases yourself, including backups, patching, replication, and high availability. It's not cost-effective and increases operational overhead. Aurora Global Database provides managed global replication.\n- Spin up a Amazon Redshift cluster in each AWS region: Redshift is a data warehouse, not suitable for transactional applications. It's designed for analytical workloads, not operational databases. It also doesn't support the same MySQL compatibility.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
            options: [
                { id: 0, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data", correct: false },
                { id: 1, text: "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object", correct: true },
                { id: 2, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data", correct: false },
                { id: 3, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 provides strong read-after-write consistency for PUT operations. When you overwrite an object and immediately read it, S3 always returns the new version. There's no eventual consistency delay for overwrite PUTs. This is critical for high-frequency trading systems where data consistency is essential. S3 guarantees that after a successful PUT, subsequent GET requests will return the new data.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\n- Until the change is fully propagated, Amazon S3 might return the previous data: This describes eventual consistency, which S3 does NOT have for overwrite PUTs. S3 has strong consistency for overwrite PUTs and DELETE operations. The \"propagation delay\" concept doesn't apply to overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 might return the new data: This is backwards - if there were a propagation delay, you'd get old data, not new data. But more importantly, there's no propagation delay for overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 does not return any data: S3 doesn't block reads during updates. It always returns data - either the old or new version. For overwrite PUTs, it always returns the new version immediately.\n\n**Why option 2 is incorrect:**\n- Until the change is fully propagated, Amazon S3 might return the previous data: This describes eventual consistency, which S3 does NOT have for overwrite PUTs. S3 has strong consistency for overwrite PUTs and DELETE operations. The \"propagation delay\" concept doesn't apply to overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 might return the new data: This is backwards - if there were a propagation delay, you'd get old data, not new data. But more importantly, there's no propagation delay for overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 does not return any data: S3 doesn't block reads during updates. It always returns data - either the old or new version. For overwrite PUTs, it always returns the new version immediately.\n\n**Why option 3 is incorrect:**\n- Until the change is fully propagated, Amazon S3 might return the previous data: This describes eventual consistency, which S3 does NOT have for overwrite PUTs. S3 has strong consistency for overwrite PUTs and DELETE operations. The \"propagation delay\" concept doesn't apply to overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 might return the new data: This is backwards - if there were a propagation delay, you'd get old data, not new data. But more importantly, there's no propagation delay for overwrite PUTs.\n- Until the change is fully propagated, Amazon S3 does not return any data: S3 doesn't block reads during updates. It always returns data - either the old or new version. For overwrite PUTs, it always returns the new version immediately.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAWS DataSync is designed for efficient data transfer between on-premises and AWS. It can transfer from NFS file systems to EFS. Using Direct Connect with a private VIF provides dedicated network connectivity. PrivateLink interface VPC endpoints allow private connectivity to EFS from on-premises via Direct Connect without traversing the public internet. DataSync can be scheduled to run automatically, making it operationally efficient.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\n- Transfer to an AWS VPC peering endpoint for Amazon EFS: VPC peering connects VPCs, but EFS doesn't use \"VPC peering endpoints.\" EFS uses mount targets within VPCs. You need PrivateLink interface endpoints for EFS access from on-premises via Direct Connect.\n- Transfer to an Amazon S3 bucket... then Lambda to copy to EFS: This adds unnecessary complexity and an extra step. DataSync can transfer directly from NFS to EFS, eliminating the need for S3 as an intermediate storage and Lambda processing. Direct transfer is more efficient.\n- Transfer to an Amazon S3 bucket by using public VIF: Public VIFs are for internet-routable traffic, not private connectivity. For secure, private transfers, you should use private VIFs. Also, transferring to S3 first adds unnecessary steps when DataSync can go directly to EFS.\n\n**Why option 2 is incorrect:**\n- Transfer to an AWS VPC peering endpoint for Amazon EFS: VPC peering connects VPCs, but EFS doesn't use \"VPC peering endpoints.\" EFS uses mount targets within VPCs. You need PrivateLink interface endpoints for EFS access from on-premises via Direct Connect.\n- Transfer to an Amazon S3 bucket... then Lambda to copy to EFS: This adds unnecessary complexity and an extra step. DataSync can transfer directly from NFS to EFS, eliminating the need for S3 as an intermediate storage and Lambda processing. Direct transfer is more efficient.\n- Transfer to an Amazon S3 bucket by using public VIF: Public VIFs are for internet-routable traffic, not private connectivity. For secure, private transfers, you should use private VIFs. Also, transferring to S3 first adds unnecessary steps when DataSync can go directly to EFS.\n\n**Why option 3 is incorrect:**\n- Transfer to an AWS VPC peering endpoint for Amazon EFS: VPC peering connects VPCs, but EFS doesn't use \"VPC peering endpoints.\" EFS uses mount targets within VPCs. You need PrivateLink interface endpoints for EFS access from on-premises via Direct Connect.\n- Transfer to an Amazon S3 bucket... then Lambda to copy to EFS: This adds unnecessary complexity and an extra step. DataSync can transfer directly from NFS to EFS, eliminating the need for S3 as an intermediate storage and Lambda processing. Direct transfer is more efficient.\n- Transfer to an Amazon S3 bucket by using public VIF: Public VIFs are for internet-routable traffic, not private connectivity. For secure, private transfers, you should use private VIFs. Also, transferring to S3 first adds unnecessary steps when DataSync can go directly to EFS.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
                { id: 3, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nECS with EC2 launch type runs containers on EC2 instances you manage. You pay for the EC2 instances (compute) and EBS volumes (storage) you provision, regardless of how much the containers actually use. ECS with Fargate is serverless - you only pay for the vCPU and memory resources your containers request and consume. Fargate abstracts away the underlying infrastructure, so you don't pay for EC2 instances or EBS volumes directly.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\n- Both are charged based on EC2 instances and EBS volumes: This only applies to EC2 launch type. Fargate doesn't use EC2 instances you manage, so this pricing model doesn't apply.\n- Both are charged based on vCPU and memory resources: This only applies to Fargate. EC2 launch type charges for entire EC2 instances, not just the resources containers use.\n- Both are just charged based on Elastic Container Service used per hour: There's no separate \"ECS service\" charge. ECS itself is free - you pay for the underlying compute resources (EC2 instances for EC2 launch type, or vCPU/memory for Fargate).\n\n**Why option 1 is incorrect:**\n- Both are charged based on EC2 instances and EBS volumes: This only applies to EC2 launch type. Fargate doesn't use EC2 instances you manage, so this pricing model doesn't apply.\n- Both are charged based on vCPU and memory resources: This only applies to Fargate. EC2 launch type charges for entire EC2 instances, not just the resources containers use.\n- Both are just charged based on Elastic Container Service used per hour: There's no separate \"ECS service\" charge. ECS itself is free - you pay for the underlying compute resources (EC2 instances for EC2 launch type, or vCPU/memory for Fargate).\n\n**Why option 2 is incorrect:**\n- Both are charged based on EC2 instances and EBS volumes: This only applies to EC2 launch type. Fargate doesn't use EC2 instances you manage, so this pricing model doesn't apply.\n- Both are charged based on vCPU and memory resources: This only applies to Fargate. EC2 launch type charges for entire EC2 instances, not just the resources containers use.\n- Both are just charged based on Elastic Container Service used per hour: There's no separate \"ECS service\" charge. ECS itself is free - you pay for the underlying compute resources (EC2 instances for EC2 launch type, or vCPU/memory for Fargate).",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nSSL/TLS encryption secures data while it's being transmitted between the EC2 instances and the RDS database. This provides end-to-end security for data in transit. RDS supports SSL connections, and you can require SSL for all connections. This is the standard way to secure database connections and meets the requirement for \"end-to-end security for data-in-transit.\"\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nSimilar to security groups, NACLs control network traffic but don't encrypt it. Blocking SSH doesn't address data-in-transit encryption for database connections.\n\n**Why option 1 is incorrect:**\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: Security groups control network access but don't encrypt data in transit. Blocking SSH doesn't encrypt database connections. SSH is for server access, not database connections.\n- Create a new network ACL that blocks SSH from the entire Amazon EC2 subnet: Similar to security groups, NACLs control network traffic but don't encrypt it. Blocking SSH doesn't address data-in-transit encryption for database connections.\n- Use IAM authentication to access the database instead of the database user's access credentials: IAM authentication provides authentication (who you are) but doesn't provide encryption for data in transit. You still need SSL/TLS to encrypt the connection. IAM authentication and SSL encryption are complementary, not alternatives.\n\n**Why option 2 is incorrect:**\nIAM authentication provides authentication (who you are) but doesn't provide encryption for data in transit. You still need SSL/TLS to encrypt the connection. IAM authentication and SSL encryption are complementary, not alternatives.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 1, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "**Why option 0 is correct:**\n- S3 Transfer Acceleration: Uses CloudFront's globally distributed edge locations to optimize the path from clients to S3. Data is routed through the nearest edge location, then over AWS's optimized network backbone to S3. This significantly improves upload speeds from distant locations (Europe and Asia) to US-based S3 buckets.\n- Multipart uploads: Break large files into smaller parts that are uploaded in parallel. This improves throughput, allows resuming failed uploads, and is more efficient for large files. Multipart uploads are especially beneficial for large video files.\n**Why other options are incorrect:**\n\n**Why option 1 is correct:**\nBreak large files into smaller parts that are uploaded in parallel. This improves throughput, allows resuming failed uploads, and is more efficient for large files. Multipart uploads are especially beneficial for large video files.\n**Why other options are incorrect:**\n\n**Why option 2 is incorrect:**\nVPN connections have bandwidth limitations and don't optimize the network path like Transfer Acceleration does. They also require setup at each location and don't provide the same performance improvements.\n\n**Why option 3 is incorrect:**\nGlobal Accelerator optimizes traffic routing to applications, not to S3. It's designed for application endpoints, not object storage. S3 Transfer Acceleration is specifically designed for S3 uploads.\n\n**Why option 4 is incorrect:**\n- Create multiple AWS Direct Connect connections: Direct Connect requires physical installation at each location and has high setup costs. For multiple locations (Europe and Asia), this would be extremely expensive and time-consuming. Transfer Acceleration and multipart uploads are software solutions that work immediately.\n- Use AWS Global Accelerator for faster file uploads: Global Accelerator optimizes traffic routing to applications, not to S3. It's designed for application endpoints, not object storage. S3 Transfer Acceleration is specifically designed for S3 uploads.\n- Create multiple AWS Site-to-Site VPN connections: VPN connections have bandwidth limitations and don't optimize the network path like Transfer Acceleration does. They also require setup at each location and don't provide the same performance improvements.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
            options: [
                { id: 0, text: "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once", correct: true },
                { id: 1, text: "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts", correct: false },
                { id: 2, text: "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs", correct: false },
                { id: 3, text: "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nAWS Shield Advanced has a monthly subscription fee. When you have multiple AWS accounts, each account is charged separately unless consolidated billing is enabled through AWS Organizations. With consolidated billing, the monthly Shield Advanced fee is charged once for the organization, not per account. This is a significant cost savings when you have multiple accounts.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nShield Advanced doesn't support Savings Plans. Savings Plans are for compute services (EC2, Lambda, Fargate), not security services like Shield Advanced.\n\n**Why option 2 is incorrect:**\nShield Advanced only protects AWS resources (CloudFront, ELB, Route 53, EC2, etc.). It cannot protect on-premises or non-AWS infrastructure. This wouldn't cause unexpected costs.\n\n**Why option 3 is incorrect:**\nShield Advanced includes Shield Standard features, but this doesn't cause increased costs. Shield Standard is free, and Shield Advanced replaces it, not adds to it.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda", correct: false },
                { id: 1, text: "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: false },
                { id: 2, text: "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda", correct: false },
                { id: 3, text: "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nCan buffer and throttle streaming data. Kinesis Data Streams can handle high-throughput data ingestion and throttle consumers, preventing downstream systems from being overwhelmed.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nGateway endpoints are VPC endpoints for S3 and DynamoDB - they don't provide throttling. They're for private connectivity, not traffic management.\n\n**Why option 1 is incorrect:**\nSNS doesn't provide throttling or buffering - it's a pub/sub messaging service. Lambda can be throttled but doesn't throttle incoming requests. This combination doesn't provide the throttling capabilities needed.\n\n**Why option 2 is incorrect:**\nELB distributes traffic but doesn't throttle it. ELB can handle high traffic but doesn't prevent spikes from reaching backend services. API Gateway provides better throttling capabilities.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon Neptune", correct: true },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon Aurora", correct: false },
                { id: 3, text: "Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\n**\n\n**Why option 1 is incorrect:**\nAurora is a relational database optimized for SQL queries. While it can handle complex queries with JOINs, graph databases like Neptune are specifically designed for relationship-heavy queries and perform much better for social network-style queries.\n\n**Why option 2 is incorrect:**\n- Amazon OpenSearch Service: OpenSearch (formerly Elasticsearch) is a search and analytics engine, not a graph database. It's good for full-text search and log analytics but not optimized for relationship traversal queries like \"friends of friends.\"\n- Amazon Aurora: Aurora is a relational database optimized for SQL queries. While it can handle complex queries with JOINs, graph databases like Neptune are specifically designed for relationship-heavy queries and perform much better for social network-style queries.\n- Amazon Redshift: Redshift is a data warehouse for analytical queries on large datasets. It's not designed for transactional queries or relationship traversal. It's optimized for aggregations and analytical workloads, not graph queries.\n\n**Why option 3 is incorrect:**\nRedshift is a data warehouse for analytical queries on large datasets. It's not designed for transactional queries or relationship traversal. It's optimized for aggregations and analytical workloads, not graph queries.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
                { id: 1, text: "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 3, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n- AWS Cost Explorer Resource Optimization: Analyzes EC2 usage patterns and identifies idle or underutilized instances that can be terminated or rightsized. It provides actionable recommendations for cost savings.\n- AWS Compute Optimizer: Analyzes historical utilization metrics and recommends optimal instance types. It can suggest moving to smaller instance types or different instance families that better match workload requirements, reducing costs.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nTrusted Advisor doesn't automatically renew Reserved Instances. Also, the startup likely doesn't have Reserved Instances yet. Trusted Advisor provides recommendations but doesn't focus on idle instances or instance type optimization.\n\n**Why option 2 is incorrect:**\nCompute Optimizer recommends instance types, not purchasing options (On-Demand vs Reserved vs Spot). For purchasing options, you'd use Cost Explorer or Reserved Instance recommendations.\n\n**Why option 3 is incorrect:**\nThis is for S3 cost optimization, not EC2 or RDS. The startup's infrastructure includes EC2 and RDS, so S3 optimization doesn't address their main cost concerns.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
                { id: 1, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 2, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 3, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 4, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "**Why option 0 is correct:**\nUser data scripts run as root by default, allowing them to perform system-level operations like installing packages, modifying system files, and configuring services. This is necessary for many bootstrap operations.\n\n**Why option 1 is correct:**\n- Root privileges: User data scripts run as root by default, allowing them to perform system-level operations like installing packages, modifying system files, and configuring services. This is necessary for many bootstrap operations.\n- Runs only during first boot: User data executes once when the instance first launches, not on every restart. This is by design - user data is for initial setup, not ongoing maintenance. If you need scripts to run on every boot, you must configure that explicitly (e.g., using systemd or cron).\n**Why other options are incorrect:**\n\n**Why option 2 is incorrect:**\nUser data cannot be modified after instance launch. It's set at launch time and cannot be changed. You can view it, but not update it. To change user data, you must launch a new instance.\n\n**Why option 3 is incorrect:**\nThis is incorrect. User data scripts run as root by default, which is why they can perform system-level operations.\n\n**Why option 4 is incorrect:**\n- When an instance is running, you can update user data by using root user credentials: User data cannot be modified after instance launch. It's set at launch time and cannot be changed. You can view it, but not update it. To change user data, you must launch a new instance.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: User data runs only on first launch, not on restarts. Restarting an instance doesn't re-execute user data. This is a common misconception.\n- By default, scripts entered as user data do not have root user privileges: This is incorrect. User data scripts run as root by default, which is why they can perform system-level operations.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "AWS Glue", correct: false },
                { id: 1, text: "Amazon EMR", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: true },
                { id: 3, text: "Amazon FSx for Windows File Server", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nGlue is an ETL (Extract, Transform, Load) service for data preparation and transformation. It's not a file system and doesn't provide the high-performance file access needed for EDA applications. Glue is for data processing pipelines, not file storage.\n\n**Why option 1 is incorrect:**\nEMR is a managed Hadoop/Spark cluster service for big data processing. While it can handle large datasets, it's not optimized for the high-performance file access patterns of EDA applications. EMR is for distributed data processing, not file system performance.\n\n**Why option 3 is incorrect:**\nFSx for Windows is designed for Windows-based file shares and Active Directory integration. It's not optimized for high-performance compute workloads like EDA. Lustre is specifically designed for HPC and compute-intensive applications.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 2, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 3, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
                { id: 4, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\nA Golden AMI is a pre-configured AMI with common components already installed. By including static installation components (like operating system, common libraries, frameworks) in the AMI, you eliminate the need to install them on every instance launch, dramatically reducing launch time.\n\n**Why option 2 is correct:**\n- Golden AMI: A Golden AMI is a pre-configured AMI with common components already installed. By including static installation components (like operating system, common libraries, frameworks) in the AMI, you eliminate the need to install them on every instance launch, dramatically reducing launch time.\n- User data for dynamic parts: User data scripts can handle dynamic, instance-specific configuration that needs to happen at boot time. This allows customization while leveraging the pre-configured AMI for static components. The combination reduces launch time from 45 minutes to under 2 minutes.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nWhile S3 provides fast retrieval, you still need to download and install the files, which takes time. This doesn't solve the 45-minute installation problem. Pre-installing in an AMI is much faster.\n\n**Why option 3 is incorrect:**\nInstalling everything via user data still takes 45 minutes. The solution is to pre-install static components in an AMI, not install everything at boot time.\n\n**Why option 4 is incorrect:**\nElastic Beanstalk doesn't have a \"deployment caching\" feature that would significantly reduce installation time. The solution requires AMI optimization and user data, not Beanstalk features.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
            options: [
                { id: 0, text: "Use Identity and Access Management (IAM) policies", correct: false },
                { id: 1, text: "Use Amazon S3 Bucket Policies", correct: true },
                { id: 2, text: "Use Security Groups", correct: false },
                { id: 3, text: "Use Access Control Lists (ACLs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nS3 Bucket Policies are resource-based policies attached to S3 buckets that can grant permissions to IAM users, roles, and even other AWS accounts. Bucket policies can control both user-level access (within the same account) and account-level access (cross-account). They're the most flexible and optimized way to control S3 access, supporting complex permission scenarios including cross-account access, which is mentioned in the requirement.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nIAM policies are identity-based and attached to users, groups, or roles. While they can grant S3 access, they don't provide account-level control for cross-account scenarios. Bucket policies are needed for cross-account access control.\n\n**Why option 2 is incorrect:**\nSecurity Groups are for EC2 instances and other VPC resources, not for S3 access control. S3 is accessed via API calls, not network-level security. Security Groups don't apply to S3.\n\n**Why option 3 is incorrect:**\nACLs are legacy and less flexible than bucket policies. They don't support complex permission scenarios or cross-account access as effectively as bucket policies. Bucket policies are the recommended approach.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 1, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nInternet Gateways provide public internet access for resources in public subnets. Using an Internet Gateway would route SQS traffic over the public internet, which is what the team wants to avoid. It also requires public subnets and public IPs.\n\n**Why option 1 is incorrect:**\nVPN connections are for connecting on-premises networks to VPCs, not for accessing AWS services from within a VPC. VPN doesn't provide private connectivity to AWS services - traffic would still go over the internet.\n\n**Why option 3 is incorrect:**\nNAT instances allow private subnet resources to access the internet, but traffic still goes over the public internet. VPC endpoints provide true private connectivity without internet traversal.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nData transfer within the same AWS region is free for data transfer between AWS services. By deploying the visualization tool in the same region as the data warehouse, the 60MB query responses don't incur data transfer costs. Users accessing the visualization tool over Direct Connect only transfer the 600KB web pages, not the 60MB query results. Direct Connect has lower egress costs than internet transfer, and since query results stay within AWS (same region), there's no egress charge for them.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nWhile query results stay in-region (free), accessing the tool over the internet has higher costs than Direct Connect for the web page transfers.\n\n**Why option 2 is incorrect:**\nThis would transfer 60MB query results over Direct Connect for each query, which incurs data transfer costs. The visualization tool in AWS with same-region queries avoids this cost.\n\n**Why option 3 is incorrect:**\nInternet egress from AWS has higher costs than Direct Connect. Transferring 60MB query results over the internet would be more expensive.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 2, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 4, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 5, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "**Why option 1 is correct:**\n- Throughput limit of 3,000 messages per second: FIFO queues have a throughput limit of 3,000 messages per second (or 300 messages per second without batching). With batching, you can achieve up to 3,000 messages per second. This is a hard limit that must be considered during migration.\n- Queue name must end with .fifo: FIFO queues require the .fifo suffix in their name. This is a mandatory naming convention that distinguishes FIFO queues from standard queues.\n- Delete and recreate: Standard queues cannot be converted to FIFO queues. You must delete the standard queue and create a new FIFO queue. This is because FIFO and standard queues have fundamentally different architectures and behaviors.\n**Why other options are incorrect:**\n\n**Why option 2 is correct:**\n- Throughput limit of 3,000 messages per second: FIFO queues have a throughput limit of 3,000 messages per second (or 300 messages per second without batching). With batching, you can achieve up to 3,000 messages per second. This is a hard limit that must be considered during migration.\n- Queue name must end with .fifo: FIFO queues require the .fifo suffix in their name. This is a mandatory naming convention that distinguishes FIFO queues from standard queues.\n- Delete and recreate: Standard queues cannot be converted to FIFO queues. You must delete the standard queue and create a new FIFO queue. This is because FIFO and standard queues have fundamentally different architectures and behaviors.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\nStandard queues cannot be converted to FIFO queues. You must delete the standard queue and create a new FIFO queue. This is because FIFO and standard queues have fundamentally different architectures and behaviors.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nFIFO queues must have different names (with .fifo suffix). You cannot reuse the same name. Also, you need to delete the standard queue first.\n\n**Why option 3 is incorrect:**\nThis is the limit WITHOUT batching. With batching (which the question mentions), the limit is 3,000 messages per second.\n\n**Why option 5 is incorrect:**\nStandard queues cannot be converted to FIFO queues. They must be deleted and recreated. This is a fundamental limitation.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 1, text: "Use AWS Glue to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 2, text: "Use AWS EMR to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\nAWS DMS is a managed service designed specifically for database migration and replication. It can continuously replicate data from multiple source databases (Oracle, PostgreSQL) to Redshift with minimal configuration. DMS handles schema conversion, data transformation, and ongoing replication automatically. It requires no infrastructure management and minimal development effort - you configure source and target endpoints and DMS handles the rest.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nKinesis is for real-time streaming data, not database replication. It would require custom code to read from databases and write to Redshift. Kinesis doesn't understand database schemas or handle replication automatically.\n\n**Why option 1 is incorrect:**\nGlue is an ETL service for data transformation and preparation, not continuous replication. It's designed for batch ETL jobs, not ongoing database replication. Glue would require more development and management effort.\n\n**Why option 2 is incorrect:**\nEMR is for big data processing with Hadoop/Spark, not database replication. It would require significant development effort to build replication logic. EMR is overkill for database replication and requires infrastructure management.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 2, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 3, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nFor AWS Managed IPSec VPN connections, the Virtual Private Gateway (VGW) is an AWS-managed VPN endpoint that's attached to your VPC. The Customer Gateway is a resource in AWS that represents your on-premises VPN device. The VGW goes on the AWS side, and the Customer Gateway represents the on-premises side. This is the standard AWS VPN architecture.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nVGWs are AWS-managed and only exist on the AWS side. On-premises uses your own VPN equipment, represented by a Customer Gateway resource in AWS.\n\n**Why option 2 is incorrect:**\n- Create a VGW on the on-premises side and a Customer Gateway on the AWS side: This reverses the components. VGW is AWS-managed and goes on the AWS side. Customer Gateway represents on-premises equipment.\n- Create a Customer Gateway on both sides: Customer Gateways represent on-premises equipment. You only need one Customer Gateway (in AWS) to represent your on-premises device. The AWS side uses a VGW.\n- Create a VGW on both sides: VGWs are AWS-managed and only exist on the AWS side. On-premises uses your own VPN equipment, represented by a Customer Gateway resource in AWS.\n\n**Why option 3 is incorrect:**\n- Create a VGW on the on-premises side and a Customer Gateway on the AWS side: This reverses the components. VGW is AWS-managed and goes on the AWS side. Customer Gateway represents on-premises equipment.\n- Create a Customer Gateway on both sides: Customer Gateways represent on-premises equipment. You only need one Customer Gateway (in AWS) to represent your on-premises device. The AWS side uses a VGW.\n- Create a VGW on both sides: VGWs are AWS-managed and only exist on the AWS side. On-premises uses your own VPN equipment, represented by a Customer Gateway resource in AWS.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 2, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
                { id: 3, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nCloudWatch doesn't provide a standard metric for certificate expiration. AWS Config is the service designed for compliance and configuration monitoring, including certificate expiration.\n\n**Why option 2 is incorrect:**\nACM-created certificates are automatically managed by AWS and don't need expiration monitoring. The requirement is for imported third-party certificates.\n\n**Why option 3 is incorrect:**\n- Monitor CloudWatch metric for certificates created via ACM: ACM-created certificates are automatically renewed by AWS, so monitoring expiration isn't necessary. Also, CloudWatch doesn't have a standard metric for certificate expiration. The question specifies \"third-party\" certificates imported into ACM.\n- Leverage AWS Config managed rule for certificates created via ACM: ACM-created certificates are automatically managed by AWS and don't need expiration monitoring. The requirement is for imported third-party certificates.\n- Monitor CloudWatch metric for certificates imported into ACM: CloudWatch doesn't provide a standard metric for certificate expiration. AWS Config is the service designed for compliance and configuration monitoring, including certificate expiration.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
            options: [
                { id: 0, text: "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs", correct: false },
                { id: 1, text: "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: true },
                { id: 2, text: "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: false },
                { id: 3, text: "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nAmazon GuardDuty analyzes these specific data sources to detect threats:\n- VPC Flow Logs: Network traffic information showing source, destination, ports, and protocols. GuardDuty analyzes this for suspicious network activity.\n- DNS logs: DNS query logs from Route 53 Resolver. GuardDuty analyzes DNS queries for malicious domains, data exfiltration attempts, and other threats.\n- CloudTrail events: API calls and management events. GuardDuty analyzes API calls for unauthorized access, privilege escalation, and other security threats.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nGuardDuty doesn't analyze API Gateway logs or S3 access logs. It uses CloudTrail (which includes API calls) and DNS logs, not application-level logs.\n\n**Why option 2 is incorrect:**\n- VPC Flow Logs, API Gateway logs, S3 access logs: GuardDuty doesn't analyze API Gateway logs or S3 access logs. It uses CloudTrail (which includes API calls) and DNS logs, not application-level logs.\n- ELB logs, DNS logs, CloudTrail events: GuardDuty doesn't analyze ELB access logs. It uses VPC Flow Logs for network traffic analysis, not ELB logs.\n- CloudFront logs, API Gateway logs, CloudTrail events: GuardDuty doesn't analyze CloudFront or API Gateway logs. It focuses on VPC Flow Logs, DNS logs, and CloudTrail events.\n\n**Why option 3 is incorrect:**\nGuardDuty doesn't analyze CloudFront or API Gateway logs. It focuses on VPC Flow Logs, DNS logs, and CloudTrail events.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
            options: [
                { id: 0, text: "Use AWS_IAM authorization", correct: false },
                { id: 1, text: "Use Amazon Cognito User Pools", correct: true },
                { id: 2, text: "Use Amazon Cognito Identity Pools", correct: false },
                { id: 3, text: "Use AWS Lambda authorizer for Amazon API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\n**\n\n**Why option 0 is incorrect:**\nIAM authorization uses AWS IAM credentials, which are for AWS services and applications, not end users. IAM doesn't provide user management features like registration, password reset, or user profiles. It's for service-to-service authentication.\n\n**Why option 2 is incorrect:**\nIdentity Pools provide temporary AWS credentials for users, but they don't provide user management. Identity Pools work with User Pools or other identity providers. They're for granting AWS resource access, not user management.\n\n**Why option 3 is incorrect:**\nLambda authorizers allow custom authorization logic, but they don't provide user management. You'd need to build user registration, authentication, and management features yourself, which contradicts the \"built-in user management\" requirement.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 1, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "**Why option 0 is correct:**\nDedicated Hosts are physical servers dedicated to your use. They allow you to use your existing server-bound software licenses (like Windows Server, SQL Server, etc.) because you have visibility and control over the underlying physical server. Dedicated Hosts are the most cost-effective way to use existing licenses on AWS while maintaining compliance with license terms that require physical server dedication.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nReserved instances are a purchasing option, not an instance type. They don't address the license requirement for physical server visibility. Reserved instances can be On-Demand, Dedicated Instances, or Dedicated Hosts.\n\n**Why option 2 is incorrect:**\n- Use Amazon EC2 dedicated instances: Dedicated instances run on dedicated hardware but you don't have visibility into the physical server. Many license agreements require visibility into the physical server, which Dedicated Hosts provide but Dedicated Instances don't.\n- Use Amazon EC2 on-demand instances: On-demand instances don't provide the physical server visibility needed for server-bound licenses. They're shared or dedicated at the instance level, not the host level.\n- Use Amazon EC2 reserved instances: Reserved instances are a purchasing option, not an instance type. They don't address the license requirement for physical server visibility. Reserved instances can be On-Demand, Dedicated Instances, or Dedicated Hosts.\n\n**Why option 3 is incorrect:**\n- Use Amazon EC2 dedicated instances: Dedicated instances run on dedicated hardware but you don't have visibility into the physical server. Many license agreements require visibility into the physical server, which Dedicated Hosts provide but Dedicated Instances don't.\n- Use Amazon EC2 on-demand instances: On-demand instances don't provide the physical server visibility needed for server-bound licenses. They're shared or dedicated at the instance level, not the host level.\n- Use Amazon EC2 reserved instances: Reserved instances are a purchasing option, not an instance type. They don't address the license requirement for physical server visibility. Reserved instances can be On-Demand, Dedicated Instances, or Dedicated Hosts.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 1, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 2, text: "NAT instance supports port forwarding", correct: true },
                { id: 3, text: "NAT gateway supports port forwarding", correct: false },
                { id: 4, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 5, text: "Security Groups can be associated with a NAT gateway", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "**Why option 0 is correct:**\nNAT instances are EC2 instances, so they support security groups for fine-grained network access control.\n**Why other options are incorrect:**\n\n**Why option 2 is correct:**\n- NAT instance as bastion: NAT instances are EC2 instances, so they can be used as bastion servers for SSH access to private subnet instances. You can SSH into the NAT instance, then SSH from there to private instances.\n- NAT instance supports port forwarding: NAT instances run software (like iptables) that can be configured for port forwarding. This allows you to forward specific ports to instances in private subnets.\n- Security Groups on NAT instance: NAT instances are EC2 instances, so they support security groups for fine-grained network access control.\n**Why other options are incorrect:**\n\n**Why option 4 is correct:**\n- NAT instance as bastion: NAT instances are EC2 instances, so they can be used as bastion servers for SSH access to private subnet instances. You can SSH into the NAT instance, then SSH from there to private instances.\n- NAT instance supports port forwarding: NAT instances run software (like iptables) that can be configured for port forwarding. This allows you to forward specific ports to instances in private subnets.\n- Security Groups on NAT instance: NAT instances are EC2 instances, so they support security groups for fine-grained network access control.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nNAT gateways don't support port forwarding. They only provide basic NAT functionality (source/destination NAT). Port forwarding requires instance-level configuration.\n\n**Why option 3 is incorrect:**\n- NAT gateway can be used as a bastion server: NAT gateways are managed AWS services, not EC2 instances. You cannot SSH into them or use them as bastion servers. They're only for NAT functionality.\n- NAT gateway supports port forwarding: NAT gateways don't support port forwarding. They only provide basic NAT functionality (source/destination NAT). Port forwarding requires instance-level configuration.\n- Security Groups can be associated with a NAT gateway: NAT gateways are managed services and don't support security groups. They use NACLs for network-level control, not security groups.\n\n**Why option 5 is incorrect:**\nNAT gateways are managed services and don't support security groups. They use NACLs for network-level control, not security groups.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "It is not possible to access cross-account resources", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nIAM roles are the recommended way to provide cross-account access. Users from the development account can assume a role in the production account that has the necessary permissions. The production account's role trust policy allows the development account's users/roles to assume it. This provides secure, temporary access without sharing credentials.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nIAM users are not recommended for cross-account access. Roles provide temporary credentials and better security. Users have long-lived credentials that are harder to manage across accounts.\n\n**Why option 2 is incorrect:**\nCross-account access is absolutely possible using IAM roles. This is a standard AWS pattern for multi-account architectures.\n\n**Why option 3 is incorrect:**\nSharing credentials violates security best practices. Credentials can be compromised, are hard to rotate, and don't provide audit trails. Roles are the secure way to provide cross-account access.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
            options: [
                { id: 0, text: "7", correct: false },
                { id: 1, text: "3", correct: true },
                { id: 2, text: "14", correct: false },
                { id: 3, text: "15", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nSpread placement groups place instances on distinct underlying hardware to minimize correlated failures. Each Availability Zone can have a maximum of 7 running instances per spread placement group. To deploy 15 instances, you need to distribute them across multiple Availability Zones. With 7 instances per AZ maximum, you need at least 3 Availability Zones (7 + 7 + 1 = 15 instances minimum across 3 AZs).\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis doesn't account for the 7-instance-per-AZ limit. You'd need 3 AZs minimum.\n\n**Why option 2 is incorrect:**\n- 7: You can only have 7 instances per AZ in a spread placement group. With 15 instances, you need more than one AZ.\n- 14: This doesn't account for the 7-instance-per-AZ limit. You'd need 3 AZs minimum.\n- 15: You cannot put 15 instances in a single AZ with a spread placement group due to the 7-instance limit.\n\n**Why option 3 is incorrect:**\nYou cannot put 15 instances in a single AZ with a spread placement group due to the 7-instance limit.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 1, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nDAX is an in-memory caching layer specifically designed for DynamoDB. It provides microsecond latency for read operations and can cache commonly accessed data. Since 90% of reads are for commonly accessed data, DAX will dramatically improve DynamoDB read performance.\n\n**Why option 0 is incorrect:**\nDAX is better than ElastiCache for DynamoDB, and CloudFront is better than ElastiCache for S3 static content. This combination doesn't use the optimal services.\n\n**Why option 2 is incorrect:**\n- ElastiCache Redis for DynamoDB: ElastiCache is a general-purpose cache, but DAX is specifically optimized for DynamoDB with better integration and performance. DAX understands DynamoDB's data model and provides better caching for DynamoDB workloads.\n- DAX for DynamoDB and ElastiCache Memcached for S3: S3 doesn't need ElastiCache - CloudFront is the appropriate caching/CDN solution for S3 static content. ElastiCache is for application-level caching, not object storage.\n- ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3: DAX is better than ElastiCache for DynamoDB, and CloudFront is better than ElastiCache for S3 static content. This combination doesn't use the optimal services.\n\n**Why option 3 is incorrect:**\n- ElastiCache Redis for DynamoDB: ElastiCache is a general-purpose cache, but DAX is specifically optimized for DynamoDB with better integration and performance. DAX understands DynamoDB's data model and provides better caching for DynamoDB workloads.\n- DAX for DynamoDB and ElastiCache Memcached for S3: S3 doesn't need ElastiCache - CloudFront is the appropriate caching/CDN solution for S3 static content. ElastiCache is for application-level caching, not object storage.\n- ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3: DAX is better than ElastiCache for DynamoDB, and CloudFront is better than ElastiCache for S3 static content. This combination doesn't use the optimal services.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
                { id: 3, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nVPC peering requires a peering connection between each pair of VPCs. For 5 VPCs, this requires 10 peering connections (A-B, A-C, A-D, A-E, B-C, B-D, B-E, C-D, C-E, D-E). This is complex to manage and doesn't scale well.\n\n**Why option 1 is incorrect:**\nInternet Gateways provide internet access, not VPC-to-VPC connectivity. Routing VPC traffic through the internet is insecure and inefficient. VPCs should communicate privately.\n\n**Why option 2 is incorrect:**\nVPC endpoints are for accessing AWS services (like S3, DynamoDB) from your VPC, not for connecting VPCs together. Endpoints don't provide VPC-to-VPC connectivity.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
            options: [
                { id: 0, text: "Distribute the static content through Amazon EFS", correct: false },
                { id: 1, text: "Distribute the dynamic content through Amazon EFS", correct: false },
                { id: 2, text: "Distribute the static content through Amazon S3", correct: true },
                { id: 3, text: "Distribute the dynamic content through Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\nStatic content (images, CSS, JavaScript) should be served from S3 (ideally with CloudFront) rather than from ECS containers. This offloads 90% of network traffic from the ECS cluster, reducing costs and improving performance. S3 is designed for static content delivery and is much more cost-effective than serving static files from compute resources. This is a standard best practice for containerized applications.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nDynamic content is generated by the application and changes per request. S3 is for static, unchanging content. Dynamic content should remain in the application layer.\n\n**Why option 1 is incorrect:**\n- Distribute the static content through Amazon EFS: EFS is a network file system designed for shared storage, not static content delivery. It's more expensive than S3 for static content and doesn't provide the same performance or cost benefits. EFS is for dynamic, shared file storage.\n- Distribute the dynamic content through Amazon EFS: Dynamic content needs to be generated by the application, so it should stay in ECS. The problem is static content, not dynamic content.\n- Distribute the dynamic content through Amazon S3: Dynamic content is generated by the application and changes per request. S3 is for static, unchanging content. Dynamic content should remain in the application layer.\n\n**Why option 3 is incorrect:**\n- Distribute the static content through Amazon EFS: EFS is a network file system designed for shared storage, not static content delivery. It's more expensive than S3 for static content and doesn't provide the same performance or cost benefits. EFS is for dynamic, shared file storage.\n- Distribute the dynamic content through Amazon EFS: Dynamic content needs to be generated by the application, so it should stay in ECS. The problem is static content, not dynamic content.\n- Distribute the dynamic content through Amazon S3: Dynamic content is generated by the application and changes per request. S3 is for static, unchanging content. Dynamic content should remain in the application layer.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Database Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Glue", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "**Why option 0 is correct:**\n- AWS SCT: Converts database schemas from one database engine to another. It handles complex database objects like secondary indexes, foreign keys, stored procedures, and triggers. SCT analyzes the source database and generates conversion scripts for the target database.\n- AWS DMS: Handles the actual data migration and ongoing replication. It migrates data from the source database to the target while keeping them in sync. DMS works with SCT to provide a complete migration solution.\n**Why other options are incorrect:**\n\n**Why option 2 is correct:**\n- AWS SCT: Converts database schemas from one database engine to another. It handles complex database objects like secondary indexes, foreign keys, stored procedures, and triggers. SCT analyzes the source database and generates conversion scripts for the target database.\n- AWS DMS: Handles the actual data migration and ongoing replication. It migrates data from the source database to the target while keeping them in sync. DMS works with SCT to provide a complete migration solution.\n**Why other options are incorrect:**\n\n**Why option 1 is incorrect:**\nThis is not an AWS service. Schema conversion requires understanding different database syntaxes and features, which SCT handles automatically.\n\n**Why option 3 is incorrect:**\nSnowball is for physical data transfer, not database migration. It's for moving large datasets from on-premises to S3, not for migrating databases with complex schemas.\n\n**Why option 4 is incorrect:**\nGlue is an ETL service for data transformation, not database schema conversion. It doesn't handle stored procedures, foreign keys, or other complex database objects that SCT specializes in.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 1, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nNetwork Load Balancers operate at Layer 4 (TCP/UDP) and route traffic based on IP addresses and ports. They route to instances using the instance's private IP address from the primary network interface. NLB doesn't use instance IDs, public IPs, or Elastic IPs for routing - it uses the private IP address that's assigned to the instance's primary network interface.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nNLB routes to private IP addresses, not public IPs. Public IPs are for internet-facing access, but NLB-to-instance communication uses private IPs.\n\n**Why option 2 is incorrect:**\n- Traffic is routed using the primary elastic IP address: NLB doesn't route based on Elastic IP addresses. Elastic IPs are for public internet access, but NLB routing uses private IP addresses.\n- Traffic is routed using the instance ID: Instance IDs are identifiers, not network addresses. NLB routes based on IP addresses, not instance IDs.\n- Traffic is routed using the primary public IP address: NLB routes to private IP addresses, not public IPs. Public IPs are for internet-facing access, but NLB-to-instance communication uses private IPs.\n\n**Why option 3 is incorrect:**\n- Traffic is routed using the primary elastic IP address: NLB doesn't route based on Elastic IP addresses. Elastic IPs are for public internet access, but NLB routing uses private IP addresses.\n- Traffic is routed using the instance ID: Instance IDs are identifiers, not network addresses. NLB routes based on IP addresses, not instance IDs.\n- Traffic is routed using the primary public IP address: NLB routes to private IP addresses, not public IPs. Public IPs are for internet-facing access, but NLB-to-instance communication uses private IPs.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 2, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 3, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 4, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "**Why option 1 is correct:**\nAny snapshot taken from an encrypted volume is automatically encrypted. You cannot create an unencrypted snapshot from an encrypted volume.\n\n**Why option 3 is correct:**\nData moving between the encrypted volume and the EC2 instance is encrypted. This provides end-to-end encryption.\n\n**Why option 5 is correct:**\nWhen an EBS volume is encrypted:\n- Snapshots are encrypted: Any snapshot taken from an encrypted volume is automatically encrypted. You cannot create an unencrypted snapshot from an encrypted volume.\n- Data in transit is encrypted: Data moving between the encrypted volume and the EC2 instance is encrypted. This provides end-to-end encryption.\n- Data at rest is encrypted: All data stored on the encrypted volume is encrypted at rest using AWS KMS. This meets HIPAA compliance requirements for data protection.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nThis is incorrect. Encrypted EBS volumes provide encryption for data in transit between the volume and instance.\n\n**Why option 2 is incorrect:**\nThis is incorrect. Snapshots from encrypted volumes are always encrypted.\n\n**Why option 4 is incorrect:**\nThis contradicts the definition of an encrypted volume. Encrypted volumes encrypt all data at rest.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "Software VPN", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "VPC Endpoint", correct: false },
                { id: 3, text: "AWS VPN CloudHub", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nThis is a generic term, not an AWS service. Software VPNs don't provide the managed hub-and-spoke connectivity that CloudHub offers.\n\n**Why option 1 is incorrect:**\nVPC peering connects VPCs, but the branch offices are connecting via VPN to a single VPC, not to separate VPCs. CloudHub is specifically designed for this VPN hub scenario.\n\n**Why option 2 is incorrect:**\nVPC endpoints are for accessing AWS services (like S3, DynamoDB) from your VPC, not for interconnecting remote locations. Endpoints don't provide connectivity between branch offices.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
            options: [
                { id: 0, text: "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
                { id: 1, text: "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis", correct: false },
                { id: 2, text: "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift", correct: true },
                { id: 3, text: "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
            ],
            correctAnswers: [2],
            explanation: "**Why option 2 is correct:**\n**\n\n**Why option 0 is incorrect:**\nThis requires exporting data and doing manual analysis, which is time-consuming and doesn't provide seamless cross-referencing. Spectrum allows direct SQL joins.\n\n**Why option 1 is incorrect:**\n- Use Redshift COPY command to load S3 data into Redshift, then remove it: This requires loading data into Redshift (incurring storage costs) and then removing it, which is inefficient. You'd need to repeat this process for each query. Spectrum queries S3 directly without loading.\n- Setup access via Amazon Athena... export to flat files for cross-reference: This requires exporting data and doing manual analysis, which is time-consuming and doesn't provide seamless cross-referencing. Spectrum allows direct SQL joins.\n- Use AWS Glue ETL job to load S3 data into Redshift, then remove it: Similar to the COPY approach, this loads data into Redshift temporarily, which is inefficient and costly. Spectrum queries S3 directly without ETL.\n\n**Why option 3 is incorrect:**\nSimilar to the COPY approach, this loads data into Redshift temporarily, which is inefficient and costly. Spectrum queries S3 directly without ETL.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 1, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
                { id: 3, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
            ],
            correctAnswers: [3],
            explanation: "**Why option 3 is correct:**\n**\n\n**Why option 0 is incorrect:**\nSQS doesn't provide the same real-time streaming capabilities as Kinesis. SQS is pull-based and doesn't support multiple consumers reading the same data stream efficiently. Kinesis is designed for streaming analytics.\n\n**Why option 1 is incorrect:**\nSimilar to standard queues, FIFO queues don't provide streaming data capabilities. They're for message queuing, not real-time data streams. FIFO queues also have lower throughput limits.\n\n**Why option 2 is incorrect:**\n- Swap out Kinesis Data Streams with Amazon SQS Standard queues: SQS doesn't provide the same real-time streaming capabilities as Kinesis. SQS is pull-based and doesn't support multiple consumers reading the same data stream efficiently. Kinesis is designed for streaming analytics.\n- Swap out Kinesis Data Streams with Amazon SQS FIFO queues: Similar to standard queues, FIFO queues don't provide streaming data capabilities. They're for message queuing, not real-time data streams. FIFO queues also have lower throughput limits.\n- Swap out Kinesis Data Streams with Amazon Kinesis Data Firehose: Firehose is for loading streaming data into destinations (S3, Redshift, etc.), not for multiple consumer applications. It doesn't support the multiple consumer pattern that Kinesis Data Streams provides.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "**Why option 1 is correct:**\nCloudWatch can monitor EC2 instance status checks (system status and instance status). When a status check fails, CloudWatch alarms can trigger EC2 reboot actions directly without requiring Lambda functions. This is the most cost-effective and resource-efficient solution - CloudWatch alarms are inexpensive, and EC2 reboot actions don't require Lambda execution (no Lambda costs). It's also the simplest solution with minimal components.\n**Why other options are incorrect:**\n\n**Why option 0 is incorrect:**\nRebooting every 5 minutes regardless of status is wasteful and doesn't solve the problem. You need to detect failures first, then reboot. Also, Lambda execution incurs costs.\n\n**Why option 2 is incorrect:**\nThis adds unnecessary complexity (SNS + Lambda) when CloudWatch alarms can directly trigger EC2 reboot actions. Lambda execution costs money, while direct EC2 actions don't.\n\n**Why option 3 is incorrect:**\n- Use EventBridge events to trigger Lambda to check instance status every 5 minutes: This requires Lambda execution every 5 minutes (costs money) and custom code. CloudWatch alarms with EC2 actions are simpler and more cost-effective. Also, checking every 5 minutes might miss failures.\n- Setup CloudWatch alarm... publish to SNS... trigger Lambda to reboot: This adds unnecessary complexity (SNS + Lambda) when CloudWatch alarms can directly trigger EC2 reboot actions. Lambda execution costs money, while direct EC2 actions don't.\n- Use EventBridge events to trigger Lambda to reboot every 5 minutes: Rebooting every 5 minutes regardless of status is wasteful and doesn't solve the problem. You need to detect failures first, then reboot. Also, Lambda execution incurs costs.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test2: [
        {
            id: 1,
            text: "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 2, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: true },
                { id: 3, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3", correct: false },
                { id: 1, text: "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3", correct: true },
                { id: 2, text: "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
            options: [
                { id: 0, text: "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 1, text: "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 2, text: "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 3, text: "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 4, text: "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements, Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 1, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
            options: [
                { id: 0, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS", correct: true },
                { id: 1, text: "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS", correct: false },
                { id: 2, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS", correct: false },
                { id: 3, text: "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
            options: [
                { id: 0, text: "Different versions of a single object can have different retention modes and periods", correct: true },
                { id: 1, text: "The bucket default settings will override any explicit retention mode or period you request on an object version", correct: false },
                { id: 2, text: "You cannot place a retention period on an object version through a bucket default setting", correct: false },
                { id: 3, text: "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", correct: true },
                { id: 4, text: "When you use bucket default settings, you specify a Retain Until Date for the object version", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Different versions of a single object can have different retention modes and periods, When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The bucket default settings will override any explicit retention mode or period you request on an object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot place a retention period on an object version through a bucket default setting: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When you use bucket default settings, you specify a Retain Until Date for the object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 1, text: "AWS Glue", correct: false },
                { id: 2, text: "Amazon EMR", correct: false },
                { id: 3, text: "Amazon FSx for Lustre", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
            options: [
                { id: 0, text: "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering => Amazon S3 Standard", correct: true },
                { id: 2, text: "Amazon S3 Standard-IA => Amazon S3 One Zone-IA", correct: false },
                { id: 3, text: "Amazon S3 One Zone-IA => Amazon S3 Standard-IA", correct: true },
                { id: 4, text: "Amazon S3 Standard => Amazon S3 Intelligent-Tiering", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Amazon S3 Intelligent-Tiering => Amazon S3 Standard, Amazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA => Amazon S3 One Zone-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
            options: [
                { id: 0, text: "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources", correct: false },
                { id: 1, text: "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI", correct: false },
                { id: 2, text: "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions", correct: false },
                { id: 3, text: "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
            options: [
                { id: 0, text: "Configure the security group for the Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)", correct: false },
                { id: 2, text: "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 3, text: "Configure the security group on the Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure the security group for the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the security group on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway with AWS Lambda", correct: false },
                { id: 1, text: "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Leverage Amazon QuickSight with Amazon Redshift", correct: false },
                { id: 3, text: "Leverage Amazon Athena with Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway with AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon QuickSight with Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Athena with Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor", correct: false },
                { id: 1, text: "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets", correct: true },
                { id: 3, text: "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
            options: [
                { id: 0, text: "Encrypt the access keys and save them on Amazon S3", correct: false },
                { id: 1, text: "Create a strong password for the AWS account root user", correct: true },
                { id: 2, text: "Enable Multi Factor Authentication (MFA) for the AWS account root user account", correct: true },
                { id: 3, text: "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future", correct: false },
                { id: 4, text: "Create AWS account root user access keys and share those keys only with the business owner", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Create a strong password for the AWS account root user, Enable Multi Factor Authentication (MFA) for the AWS account root user account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the access keys and save them on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create AWS account root user access keys and share those keys only with the business owner: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
            options: [
                { id: 0, text: "Grant maximum privileges to avoid assigning privileges again", correct: false },
                { id: 1, text: "Use user credentials to provide access specific permissions for Amazon EC2 instances", correct: false },
                { id: 2, text: "Create a minimum number of accounts and share these account credentials among employees", correct: false },
                { id: 3, text: "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users", correct: true },
                { id: 4, text: "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users, Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Grant maximum privileges to avoid assigning privileges again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use user credentials to provide access specific permissions for Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a minimum number of accounts and share these account credentials among employees: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 1, text: "Only root user should have full database access in the organization", correct: false },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Remove full database access for all IAM users in the organization", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance", correct: true },
                { id: 1, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched", correct: false },
                { id: 2, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application", correct: true },
                { id: 3, text: "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously", correct: false },
                { id: 4, text: "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance, As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time", correct: false },
                { id: 1, text: "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly", correct: false },
                { id: 2, text: "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores", correct: true },
                { id: 3, text: "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
            options: [
                { id: 0, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate", correct: true },
                { id: 1, text: "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages", correct: false },
                { id: 2, text: "Use Amazon SQS standard queue to process the messages", correct: false },
                { id: 3, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS standard queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
            options: [
                { id: 0, text: "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs", correct: true },
                { id: 1, text: "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches", correct: false },
                { id: 2, text: "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share", correct: false },
                { id: 3, text: "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
            options: [
                { id: 0, text: "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket", correct: false },
                { id: 1, text: "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)", correct: true },
                { id: 2, text: "Upload the compressed file using multipart upload", correct: false },
                { id: 3, text: "Upload the compressed file in a single operation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file using multipart upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file in a single operation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
            options: [
                { id: 0, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance", correct: false },
                { id: 1, text: "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance", correct: true },
                { id: 2, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance", correct: false },
                { id: 3, text: "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
            options: [
                { id: 0, text: "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution", correct: true },
                { id: 1, text: "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: true },
                { id: 2, text: "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 3, text: "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 4, text: "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution, Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection", correct: true },
                { id: 1, text: "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region", correct: false },
                { id: 2, text: "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region", correct: false },
                { id: 3, text: "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes", correct: true },
                { id: 3, text: "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
            options: [
                { id: 0, text: "The engineering team needs to provision more servers running the Amazon SNS service", correct: false },
                { id: 1, text: "The engineering team needs to provision more servers running the AWS Lambda service", correct: false },
                { id: 2, text: "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit", correct: true },
                { id: 3, text: "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The engineering team needs to provision more servers running the Amazon SNS service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The engineering team needs to provision more servers running the AWS Lambda service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
            options: [
                { id: 0, text: "General Purpose Solid State Drive (gp2)", correct: false },
                { id: 1, text: "Throughput Optimized Hard disk drive (st1)", correct: true },
                { id: 2, text: "Instance Store", correct: false },
                { id: 3, text: "Cold Hard disk drive (sc1)", correct: true },
                { id: 4, text: "Provisioned IOPS Solid state drive (io1)", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Throughput Optimized Hard disk drive (st1), Cold Hard disk drive (sc1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose Solid State Drive (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance Store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned IOPS Solid state drive (io1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
            options: [
                { id: 0, text: "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 1, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 2, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: true },
                { id: 3, text: "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
            options: [
                { id: 0, text: "Host-based Routing", correct: false },
                { id: 1, text: "Path-based Routing", correct: true },
                { id: 2, text: "HTTP header-based routing", correct: false },
                { id: 3, text: "Query string parameter-based routing", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Path-based Routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host-based Routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- HTTP header-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Query string parameter-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time", correct: false },
                { id: 1, text: "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: true },
                { id: 2, text: "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling", correct: false },
                { id: 3, text: "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
            options: [
                { id: 0, text: "Use AWS Lambda function to run the workflow processes", correct: false },
                { id: 1, text: "Use Amazon EC2 on-demand instances to run the workflow processes", correct: false },
                { id: 2, text: "Use Amazon EC2 reserved instances to run the workflow processes", correct: false },
                { id: 3, text: "Use Amazon EC2 spot instances to run the workflow processes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 spot instances to run the workflow processes\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Lambda function to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
            options: [
                { id: 0, text: "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability", correct: false },
                { id: 1, text: "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency", correct: false },
                { id: 2, text: "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed", correct: true },
                { id: 3, text: "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously", correct: true },
                { id: 4, text: "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed, Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
            options: [
                { id: 0, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: true },
                { id: 1, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 2, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 3, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
            options: [
                { id: 0, text: "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 1, text: "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables", correct: true },
                { id: 2, text: "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 3, text: "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
            options: [
                { id: 0, text: "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue", correct: false },
                { id: 1, text: "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service", correct: true },
                { id: 2, text: "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again", correct: false },
                { id: 3, text: "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy", correct: false },
                { id: 4, text: "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service, Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 3, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet", correct: false },
                { id: 1, text: "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block", correct: false },
                { id: 2, text: "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC", correct: true },
                { id: 3, text: "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
            options: [
                { id: 0, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 1, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 2, text: "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 3, text: "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse", correct: false },
                { id: 1, text: "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes", correct: false },
                { id: 2, text: "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks", correct: false },
                { id: 3, text: "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
            options: [
                { id: 0, text: "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services", correct: false },
                { id: 1, text: "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC", correct: true },
                { id: 2, text: "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services", correct: false },
                { id: 3, text: "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
            options: [
                { id: 0, text: "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior", correct: false },
                { id: 1, text: "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration", correct: false },
                { id: 2, text: "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data", correct: true },
                { id: 3, text: "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands", correct: true },
                { id: 4, text: "Use Amazon Aurora Global Database to replicate data across regions for compatibility", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data, Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Aurora Global Database to replicate data across regions for compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
            options: [
                { id: 0, text: "Use VPC security groups to control the network traffic to and from your file system", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system", correct: false },
                { id: 2, text: "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system", correct: false },
                { id: 3, text: "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance", correct: false },
                { id: 4, text: "Use an IAM policy to control access for clients who can mount your file system with the required permissions", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use VPC security groups to control the network traffic to and from your file system, Use an IAM policy to control access for clients who can mount your file system with the required permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 1, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
                { id: 2, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket, Enable versioning on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
            options: [
                { id: 0, text: "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively", correct: false },
                { id: 1, text: "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures", correct: false },
                { id: 2, text: "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements", correct: false },
                { id: 3, text: "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
            options: [
                { id: 0, text: "The junior scientist does not need to pay any transfer charges for the image upload", correct: true },
                { id: 1, text: "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload", correct: false },
                { id: 2, text: "The junior scientist only needs to pay S3TA transfer charges for the image upload", correct: false },
                { id: 3, text: "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The junior scientist does not need to pay any transfer charges for the image upload\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay Amazon S3 transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
            options: [
                { id: 0, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 1, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon S3 Glacier Deep Archive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "It is not possible to access cross-account resources", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
            options: [
                { id: 0, text: "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EFS mount points", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with access to Amazon S3 based storage", correct: false },
                { id: 3, text: "Use Instance Store based Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Instance Store based Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EFS mount points: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with access to Amazon S3 based storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
            options: [
                { id: 0, text: "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies", correct: false },
                { id: 1, text: "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods", correct: true },
                { id: 3, text: "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
            options: [
                { id: 0, text: "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class", correct: false },
                { id: 1, text: "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class", correct: false },
                { id: 2, text: "Store the intermediary query results in Amazon S3 Standard storage class", correct: true },
                { id: 3, text: "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Store the intermediary query results in Amazon S3 Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow", correct: false },
                { id: 2, text: "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards", correct: false },
                { id: 3, text: "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
            options: [
                { id: 0, text: "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%", correct: false },
                { id: 2, text: "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%", correct: true },
                { id: 3, text: "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point", correct: true },
                { id: 1, text: "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution", correct: false },
                { id: 2, text: "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers", correct: true },
                { id: 1, text: "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers", correct: false },
                { id: 2, text: "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia", correct: false },
                { id: 3, text: "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
            options: [
                { id: 0, text: "AWS Global Accelerator", correct: true },
                { id: 1, text: "AWS Elastic Load Balancing (ELB)", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon CloudFront", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Global Accelerator\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- AWS Elastic Load Balancing (ELB): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance", correct: false },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB", correct: false },
                { id: 2, text: "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB", correct: true },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Microsoft SQL Server on AWS", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: false },
                { id: 3, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Microsoft SQL Server on AWS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume", correct: false },
                { id: 2, text: "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required", correct: true },
                { id: 3, text: "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks: SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.\n- Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
            options: [
                { id: 0, text: "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud", correct: true },
                { id: 1, text: "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
            options: [
                { id: 0, text: "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B", correct: true },
                { id: 1, text: "1 Amazon EC2 instance and 2 AMIs exist in Region B", correct: false },
                { id: 2, text: "1 Amazon EC2 instance and 1 AMI exist in Region B", correct: false },
                { id: 3, text: "1 Amazon EC2 instance and 1 snapshot exist in Region B", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- 1 Amazon EC2 instance and 2 AMIs exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 AMI exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 snapshot exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
            options: [
                { id: 0, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 1, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: true },
                { id: 2, text: "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 3, text: "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 1, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
            options: [
                { id: 0, text: "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 1, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: true },
                { id: 2, text: "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 3, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
            options: [
                { id: 0, text: "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets", correct: false },
                { id: 1, text: "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations", correct: true },
                { id: 2, text: "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files", correct: false },
                { id: 3, text: "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test3: [
        {
            id: 1,
            text: "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database. Which of the following would you recommend to securely share the database with the auditor?",
            options: [
                { id: 0, text: "Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access", correct: false },
                { id: 1, text: "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket", correct: false },
                { id: 2, text: "Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket", correct: false },
                { id: 3, text: "Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored. What do you recommend?",
            options: [
                { id: 0, text: "Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is", correct: false },
                { id: 1, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID", correct: true },
                { id: 2, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is", correct: false },
                { id: 3, text: "Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "\"An enterprise organization is expanding its cloud footprint and needs to centralize its security event data from various AWS accounts and services. The goal is to evaluate security posture across all environments and improve threat detection and response — without requiring significant custom code or manual integration. Which solution will fulfill these needs with the least development effort?",
            options: [
                { id: 0, text: "Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake", correct: true },
                { id: 1, text: "Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard", correct: false },
                { id: 2, text: "Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis", correct: false },
                { id: 3, text: "Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A manufacturing analytics company has a large collection of automated scripts that perform data cleanup, validation, and system integration tasks. These scripts are currently run by a local Linux cron scheduler and have an execution time of up to 30 minutes. The company wants to migrate these scripts to AWS without significant changes, and would prefer a containerized, serverless architecture that automatically scales and can respond to event-based triggers in the future. The solution must minimize infrastructure management. Which solution will best meet these requirements with minimal refactoring and operational overhead?",
            options: [
                { id: 0, text: "Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions", correct: false },
                { id: 1, text: "Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS", correct: true },
                { id: 2, text: "Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate", correct: false },
                { id: 3, text: "Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
            options: [
                { id: 0, text: "A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive", correct: false },
                { id: 1, text: "The health check grace period for the instance has not expired", correct: true },
                { id: 2, text: "The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)", correct: false },
                { id: 3, text: "The instance has failed the Elastic Load Balancing (ELB) health check status", correct: true },
                { id: 4, text: "A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks", correct: false },
                { id: 5, text: "The instance maybe in Impaired status", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: The health check grace period for the instance has not expired, The instance has failed the Elastic Load Balancing (ELB) health check status, The instance maybe in Impaired status\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located inus-east-1region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time. As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)",
            options: [
                { id: 0, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53", correct: false },
                { id: 1, text: "Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region", correct: false },
                { id: 2, text: "Create Amazon Aurora read replicas in the eu-west-1 region", correct: true },
                { id: 3, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53", correct: false },
                { id: 4, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Create Amazon Aurora read replicas in the eu-west-1 region, Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "Which of the following IAM policies provides read-only access to the Amazon S3 bucketmybucketand its content?",
            options: [
                { id: 0, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }", correct: true },
                { id: 1, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }", correct: false },
                { id: 2, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }", correct: false },
                { id: 3, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 weighted routing to spread traffic across different deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CodeDeploy deployment options to choose the right deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to distribute traffic across deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures. Which is the MOST cost-optimal solution for this workload?",
            options: [
                { id: 0, text: "Run the workload on Reserved Instances (RI)", correct: false },
                { id: 1, text: "Run the workload on a Spot Fleet", correct: true },
                { id: 2, text: "Run the workload on Spot Instances", correct: false },
                { id: 3, text: "Run the workload on Dedicated Hosts", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Run the workload on a Spot Fleet\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run the workload on Reserved Instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the workload on Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the workload on Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos. Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)",
            options: [
                { id: 0, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos", correct: true },
                { id: 1, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos", correct: false },
                { id: 2, text: "Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos", correct: true },
                { id: 3, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos", correct: false },
                { id: 4, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos, Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use Amazon EC2 Meta-Data", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that", correct: false },
                { id: 2, text: "Use Amazon EC2 User-Data", correct: false },
                { id: 3, text: "Use Amazon EC2 Instance Hibernate", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 Instance Hibernate\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Meta-Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 User-Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "An enterprise is building a secure business intelligence API using Amazon API Gateway to serve internal users with confidential analytics data. The API must be accessible only from a set of trusted IP addresses that are part of the organization's internal network ranges. No external IP traffic should be able to invoke the API. A solutions architect must design this access control mechanism with the least operational complexity. What should the architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list", correct: true },
                { id: 1, text: "Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs", correct: false },
                { id: 2, text: "Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses", correct: false },
                { id: 3, text: "Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms: Which is the MOST operationally efficient solution?",
            options: [
                { id: 0, text: "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation", correct: true },
                { id: 1, text: "Server-side encryption (SSE-S3) with automatic key rotation", correct: false },
                { id: 2, text: "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation", correct: false },
                { id: 3, text: "Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption (SSE-S3) with automatic key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available. As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
            options: [
                { id: 0, text: "Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)", correct: false },
                { id: 1, text: "Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)", correct: false },
                { id: 2, text: "Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)", correct: true },
                { id: 3, text: "Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ)us-east-1aas it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ)us-east-1alike so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour. Which of the following instances would be terminated per the default termination policy?",
            options: [
                { id: 0, text: "Instance C", correct: false },
                { id: 1, text: "Instance A", correct: false },
                { id: 2, text: "Instance B", correct: true },
                { id: 3, text: "Instance D", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instance B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Instance C: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance A: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance D: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform. Which of the following solutions would have the LEAST amount of downtime?",
            options: [
                { id: 0, text: "Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3", correct: false },
                { id: 1, text: "Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3", correct: true },
                { id: 2, text: "Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer", correct: false },
                { id: 3, text: "Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?",
            options: [
                { id: 0, text: "Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager", correct: true },
                { id: 1, text: "Create a VPC peering connection between all virtual private cloud (VPCs)", correct: false },
                { id: 2, text: "Create a Private Link between all the Amazon EC2 instances", correct: false },
                { id: 3, text: "Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a VPC peering connection between all virtual private cloud (VPCs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between all the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner. What could be the reason for this denial of permission for the bucket owner?",
            options: [
                { id: 0, text: "By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster", correct: true },
                { id: 1, text: "When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures", correct: false },
                { id: 2, text: "The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress", correct: false },
                { id: 3, text: "When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS. How should you configure the security groups? (Select three)",
            options: [
                { id: 0, text: "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443", correct: true },
                { id: 1, text: "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80", correct: false },
                { id: 2, text: "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80", correct: false },
                { id: 3, text: "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80", correct: true },
                { id: 4, text: "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432", correct: true },
                { id: 5, text: "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432", correct: false },
            ],
            correctAnswers: [0, 3, 4],
            explanation: "The correct answers are: The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443, The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80, The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved. As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
            options: [
                { id: 0, text: "Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations", correct: false },
                { id: 1, text: "Remove any overlapping namespaces for the private and public hosted zones", correct: false },
                { id: 2, text: "Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken", correct: false },
                { id: 3, text: "Enable DNS hostnames and DNS resolution for private hosted zones", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable DNS hostnames and DNS resolution for private hosted zones\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove any overlapping namespaces for the private and public hosted zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An enterprise uses a centralized Amazon S3 bucket to store logs and reports generated by multiple analytics services. Each service writes to and reads from a dedicated prefix (folder path) in the bucket. The company wants to enforce fine-grained access control so that each service can access only its own prefix, without being able to see or modify other services' data. The solution must support scalable and maintainable permissions management with minimal operational overhead. Which approach will best meet these requirements?",
            options: [
                { id: 0, text: "Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services", correct: false },
                { id: 1, text: "Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket", correct: true },
                { id: 2, text: "Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags", correct: false },
                { id: 3, text: "Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
            options: [
                { id: 0, text: "Enable encryption on the Amazon RDS database using the AWS Console", correct: false },
                { id: 1, text: "Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database", correct: false },
                { id: 2, text: "Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ", correct: false },
                { id: 3, text: "Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable encryption on the Amazon RDS database using the AWS Console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Purchase 70 on-demand instances and 30 reserved instances", correct: false },
                { id: 1, text: "Purchase 70 reserved instances (RIs) and 30 spot instances", correct: true },
                { id: 2, text: "Purchase 70 reserved instances and 30 on-demand instances", correct: false },
                { id: 3, text: "Purchase 70 on-demand instances and 30 spot instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Purchase 70 reserved instances (RIs) and 30 spot instances\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Purchase 70 on-demand instances and 30 reserved instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 70 reserved instances and 30 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 70 on-demand instances and 30 spot instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience. The company is looking at alternate database options and migrating database engines if required. What would you suggest?",
            options: [
                { id: 0, text: "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora", correct: true },
                { id: 1, text: "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump", correct: false },
                { id: 2, text: "Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database", correct: false },
                { id: 3, text: "Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A media production studio is building a content rendering and editing platform on AWS. The editing workstations and rendering tools require access to shared files over the SMB (Server Message Block) protocol. The studio wants a managed storage solution that is simple to set up, integrates easily with SMB clients, and minimizes ongoing operational tasks. Which solution will best meet the requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled", correct: false },
                { id: 1, text: "Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients", correct: false },
                { id: 2, text: "Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs", correct: false },
                { id: 3, text: "Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live. Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
            options: [
                { id: 0, text: "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration", correct: false },
                { id: 1, text: "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration", correct: false },
                { id: 2, text: "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration", correct: true },
                { id: 3, text: "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
                { id: 3, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage. As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
            options: [
                { id: 0, text: "Enable storage auto-scaling for Amazon RDS MySQL", correct: true },
                { id: 1, text: "Create read replica for Amazon RDS MySQL", correct: false },
                { id: 2, text: "Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required", correct: false },
                { id: 3, text: "Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable storage auto-scaling for Amazon RDS MySQL\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create read replica for Amazon RDS MySQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic. As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
            options: [
                { id: 0, text: "Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer", correct: false },
                { id: 1, text: "Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution", correct: false },
                { id: 2, text: "Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 1, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
                { id: 2, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 3, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A multinational logistics company is migrating its core systems to AWS. As part of this migration, the company has built an Amazon S3–based data lake to ingest and analyze supply chain data from external carriers and vendors. While some vendors have adopted the company’s modern REST-based APIs for S3 uploads, others operate legacy systems that rely exclusively on SFTP for file transfers. These vendors are unable or unwilling to modify their workflows to support S3 APIs. The company wants to provide these vendors with an SFTP-compatible solution that allows direct uploads to Amazon S3, and must use fully managed AWS services to avoid managing any infrastructure. It must also support identity federation so that internal teams can map vendor access securely to specific S3 buckets or prefixes. Which combination of options will provide a scalable and low-maintenance solution for this use case? (Select two)",
            options: [
                { id: 0, text: "Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access", correct: true },
                { id: 1, text: "Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions", correct: true },
                { id: 2, text: "Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes", correct: false },
                { id: 3, text: "Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint", correct: false },
                { id: 4, text: "Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access, Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas. Which of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
            options: [
                { id: 0, text: "There are data transfer charges for replicating data across AWS Regions", correct: true },
                { id: 1, text: "There are data transfer charges for replicating data within the same Availability Zone (AZ)", correct: false },
                { id: 2, text: "There are no data transfer charges for replicating data across AWS Regions", correct: false },
                { id: 3, text: "There are data transfer charges for replicating data within the same AWS Region", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: There are data transfer charges for replicating data across AWS Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- There are data transfer charges for replicating data within the same Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- There are no data transfer charges for replicating data across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- There are data transfer charges for replicating data within the same AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves theAdministratorAccessmanaged policy. How should you proceed?",
            options: [
                { id: 0, text: "For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves", correct: true },
                { id: 1, text: "Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy", correct: false },
                { id: 2, text: "Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves", correct: false },
                { id: 3, text: "Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
                { id: 1, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 2, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 3, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 4, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: By default, user data runs only during the boot cycle when you first launch an instance, By default, scripts entered as user data are executed with root user privileges\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- By default, scripts entered as user data do not have root user privileges for executing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When an instance is running, you can update user data by using root user credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A wildlife research organization uses IoT-based motion sensors attached to thousands of migrating animals to monitor their movement across regions. Every few minutes, a sensor checks for significant movement and sends updated location data to a backend application running on Amazon EC2 instances spread across multiple Availability Zones in a single AWS Region. Recently, an unexpected surge in motion data overwhelmed the application, leading to lost location records with no mechanism to replay missed data. A solutions architect must redesign the ingestion mechanism to prevent future data loss and to minimize operational overhead. What should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages", correct: false },
                { id: 1, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages", correct: true },
                { id: 2, text: "Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically", correct: false },
                { id: 3, text: "Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 36,
            text: "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key. Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
            options: [
                { id: 0, text: "Server-Side Encryption with Customer-Provided Keys (SSE-C)", correct: true },
                { id: 1, text: "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3", correct: false },
                { id: 2, text: "Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)", correct: false },
                { id: 3, text: "Server-Side Encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Server-Side Encryption with Customer-Provided Keys (SSE-C)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-Side Encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has historically operated only in theus-east-1region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into theus-west-1AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions. Which of the following represents the best solution to address these requirements?",
            options: [
                { id: 0, text: "Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region", correct: false },
                { id: 1, text: "Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets", correct: false },
                { id: 2, text: "Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region", correct: false },
                { id: 3, text: "Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "A media publishing company is migrating its legacy content management application to AWS. Currently, the application and its MySQL database run on a single on-premises virtual machine, which creates a single point of failure and limits scalability. As traffic has increased due to growing reader engagement and video uploads, the company needs to redesign the solution to ensure automatic scaling, high availability, and separation of application and database layers. The company wants to continue using a MySQL-compatible engine and needs a cost-effective, managed solution that minimizes operational overhead. Which AWS architecture will best fulfill these requirements?",
            options: [
                { id: 0, text: "Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database", correct: false },
                { id: 1, text: "Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries", correct: false },
                { id: 2, text: "Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage", correct: false },
                { id: 3, text: "Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 39,
            text: "Consider the following policy associated with an IAM group containing several users: Which of the following options is correct?",
            options: [
                { id: 0, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200", correct: false },
                { id: 1, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200", correct: true },
                { id: 2, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200", correct: false },
                { id: 3, text: "Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution. Which of the following represents the MOST cost-optimal and high-performance solution?",
            options: [
                { id: 0, text: "Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution", correct: true },
                { id: 1, text: "Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin", correct: false },
                { id: 2, text: "Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin", correct: false },
                { id: 3, text: "Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A video conferencing platform serves users worldwide through a globally distributed deployment of Amazon EC2 instances behind Network Load Balancers (NLBs) in several AWS Regions. The platform's architecture currently allows clients to connect to any Region via public endpoints, depending on how DNS resolves. However, users in regions far from the load balancers frequently experience high latency and slow connection times, especially during session initiation. The company wants to optimize the experience for global users by reducing end-to-end latency and load time while keeping the existing NLBs and EC2-based application infrastructure in place. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution", correct: false },
                { id: 1, text: "Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances", correct: false },
                { id: 2, text: "Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy", correct: false },
                { id: 3, text: "Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?",
            options: [
                { id: 0, text: "General Purpose", correct: false },
                { id: 1, text: "Bursting Throughput", correct: false },
                { id: 2, text: "Provisioned Throughput", correct: false },
                { id: 3, text: "Max I/O", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Max I/O\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Bursting Throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned Throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 44,
            text: "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
            options: [
                { id: 0, text: "Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)", correct: true },
                { id: 1, text: "Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand", correct: false },
                { id: 2, text: "Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand", correct: false },
                { id: 3, text: "Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "What does this IAM policy do?",
            options: [
                { id: 0, text: "It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region", correct: false },
                { id: 1, text: "It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region", correct: false },
                { id: 2, text: "It allows running Amazon EC2 instances anywhere but in the eu-west-1 region", correct: false },
                { id: 3, text: "It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows running Amazon EC2 instances anywhere but in the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A financial institution is transitioning its critical back-office systems to AWS. These systems currently rely on Microsoft SQL Server databases hosted on on-premises infrastructure. The data is highly sensitive and subject to regulatory compliance. The organization wants to enhance security and minimize database management tasks as part of the migration. Which solution will best meet these goals with the least operational burden?",
            options: [
                { id: 0, text: "Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data", correct: false },
                { id: 1, text: "Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key", correct: true },
                { id: 2, text: "Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption", correct: false },
                { id: 3, text: "Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture. As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)",
            options: [
                { id: 0, text: "For security group B: Add an inbound rule that allows traffic only from security group A on port 443", correct: false },
                { id: 1, text: "For security group B: Add an inbound rule that allows traffic only from all sources on port 1433", correct: false },
                { id: 2, text: "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443", correct: false },
                { id: 3, text: "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433", correct: true },
                { id: 4, text: "For security group B: Add an inbound rule that allows traffic only from security group A on port 1433", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433, For security group B: Add an inbound rule that allows traffic only from security group A on port 1433\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- For security group B: Add an inbound rule that allows traffic only from security group A on port 443: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- For security group B: Add an inbound rule that allows traffic only from all sources on port 1433: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation. Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
            options: [
                { id: 0, text: "Build a shared services Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 1, text: "Use VPCs connected with AWS Direct Connect", correct: false },
                { id: 2, text: "Use Fully meshed VPC Peering connection", correct: false },
                { id: 3, text: "Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Build a shared services Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPCs connected with AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Fully meshed VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 2, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 3, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this IAM policy do?",
            options: [
                { id: 0, text: "It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block", correct: false },
                { id: 1, text: "It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block", correct: true },
                { id: 2, text: "It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block", correct: false },
                { id: 3, text: "It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A financial services company runs a Kubernetes-based microservices application in its on-premises data center. The application uses the Advanced Message Queuing Protocol (AMQP) to interact with a message queue. The company is experiencing rapid growth and its on-prem infrastructure cannot scale fast enough. The company wants to migrate the application to AWS with minimal code changes and reduce infrastructure management overhead. The messaging component must continue using AMQP, and the solution should offer high scalability and low operational effort. Which combination of options will together meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes", correct: true },
                { id: 1, text: "Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic", correct: false },
                { id: 3, text: "Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery", correct: false },
                { id: 4, text: "Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes, Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances. As a solutions architect, which of the following actions would you recommend to stop the attacks?",
            options: [
                { id: 0, text: "Create a deny rule for the malicious IP in the Security Groups associated with each of the instances", correct: false },
                { id: 1, text: "Create an IP match condition in the AWS WAF to block the malicious IP address", correct: true },
                { id: 2, text: "Create a ticket with AWS support to take action against the malicious IP", correct: false },
                { id: 3, text: "Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an IP match condition in the AWS WAF to block the malicious IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a deny rule for the malicious IP in the Security Groups associated with each of the instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a ticket with AWS support to take action against the malicious IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A SaaS company is modernizing one of its legacy web applications by migrating it to AWS. The company aims to improve the availability of the application during both normal and peak traffic periods. Additionally, the company wants to implement protection against common web exploits and malicious traffic. The architecture must be scalable and integrate AWS WAF to secure incoming traffic. Which solution will best meet these requirements with high availability and minimal configuration complexity?",
            options: [
                { id: 0, text: "Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection", correct: false },
                { id: 1, text: "Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering", correct: false },
                { id: 2, text: "Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic", correct: false },
                { id: 3, text: "Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
            options: [
                { id: 0, text: "Create an AWS Snowball job and target a Amazon S3 Glacier Vault", correct: false },
                { id: 1, text: "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day", correct: true },
                { id: 2, text: "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day", correct: false },
                { id: 3, text: "Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Snowball job and target a Amazon S3 Glacier Vault: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 1, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute. Which of the following options would you recommend?",
            options: [
                { id: 0, text: "Set up an Amazon Aurora provisioned Database cluster", correct: false },
                { id: 1, text: "Set up an Amazon Aurora Global Database cluster", correct: true },
                { id: 2, text: "Set up an Amazon Aurora multi-master Database cluster", correct: false },
                { id: 3, text: "Set up an Amazon Aurora serverless Database cluster", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon Aurora Global Database cluster\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon Aurora provisioned Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Aurora multi-master Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Aurora serverless Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a read replica and modify the application to use the appropriate endpoint\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Activate read-through caching on the Amazon Aurora database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the application to read from the Multi-AZ standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed. Which of the following options represents the best solution for the given requirements?",
            options: [
                { id: 0, text: "Amazon Elastic File System (EFS) Standard–IA storage class", correct: true },
                { id: 1, text: "Amazon Elastic Block Store (EBS)", correct: false },
                { id: 2, text: "Amazon Elastic File System (EFS) Standard storage class", correct: false },
                { id: 3, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Elastic File System (EFS) Standard–IA storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic Block Store (EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic File System (EFS) Standard storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global media agency is developing a cultural analysis project to explore how major sports stories have evolved over the last five years. The team has collected thousands of archived news bulletins and magazine spreads stored in PDF format. These documents are rich in unstructured text and come from various sources with differing layouts and font styles. The agency wants to better understand how public tone and narrative have shifted over time. The team has chosen to use Amazon Textract for its ability to accurately extract printed and scanned text from complex PDF layouts. They need a solution that can then analyze the emotional tone and subject matter of the extracted text with the least possible operational burden, using fully managed AWS services where possible. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.", correct: false },
                { id: 1, text: "Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.", correct: false },
                { id: 2, text: "Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.", correct: true },
                { id: 3, text: "Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which of the following actions meets the given requirements?",
            options: [
                { id: 0, text: "Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user", correct: false },
                { id: 1, text: "Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account", correct: false },
                { id: 2, text: "Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled", correct: false },
                { id: 3, text: "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A mobile app allows users to submit photos, which are stored in an Amazon S3 bucket. Currently, a batch of Amazon EC2 Spot Instances is launched nightly to process all the day’s uploads. Each photo requires approximately 3 minutes and 512 MB of memory to process. To improve responsiveness and minimize costs, the company wants to shift to near real-time image processing that begins as soon as an image is uploaded. Which solution will provide the MOST cost-effective and scalable architecture to meet these new requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt", correct: false },
                { id: 1, text: "Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously", correct: true },
                { id: 2, text: "Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image", correct: false },
                { id: 3, text: "Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)",
            options: [
                { id: 0, text: "Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation", correct: true },
                { id: 1, text: "Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format", correct: false },
                { id: 2, text: "Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format", correct: true },
                { id: 3, text: "Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation", correct: false },
                { id: 4, text: "Create an AWS Lambda function based job to delete the raw zone data after 1 day", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation, Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Lambda function based job to delete the raw zone data after 1 day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines. Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
            options: [
                { id: 0, text: "Spot Instances", correct: false },
                { id: 1, text: "On-Demand Instances", correct: false },
                { id: 2, text: "Dedicated Hosts", correct: false },
                { id: 3, text: "Dedicated Instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend. Which of the following is the correct outcome during the maintenance window?",
            options: [
                { id: 0, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade", correct: false },
                { id: 1, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete", correct: true },
                { id: 2, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade", correct: false },
                { id: 3, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud. What do you recommend as a replacement for the DFSR?",
            options: [
                { id: 0, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 3, text: "Amazon FSx for Lustre", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test4: [
        {
            id: 1,
            text: "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
            options: [
                { id: 0, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold", correct: false },
                { id: 1, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold", correct: false },
                { id: 2, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold", correct: false },
                { id: 3, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
                { id: 1, text: "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: true },
                { id: 2, text: "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: false },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 2, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 4, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
                { id: 5, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue, Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
            options: [
                { id: 0, text: "Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint", correct: false },
                { id: 1, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: false },
                { id: 2, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: true },
                { id: 3, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: true },
                { id: 4, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint, Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 1, text: "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 2, text: "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 3, text: "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte. As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
            options: [
                { id: 0, text: "Use Amazon EFS with Bursting Throughput mode", correct: false },
                { id: 1, text: "Use Amazon EFS with Provisioned Throughput mode", correct: true },
                { id: 2, text: "Use Amazon DynamoDB table that is accessible by all ECS cluster instances", correct: false },
                { id: 3, text: "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EFS with Provisioned Throughput mode\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EFS with Bursting Throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB table that is accessible by all ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon EBS volume mounted to the Amazon ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead. What solution would best meet these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume", correct: true },
                { id: 1, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
                { id: 2, text: "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume", correct: false },
                { id: 3, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
            options: [
                { id: 0, text: "Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads", correct: true },
                { id: 1, text: "Use Amazon ElastiCache to improve the performance of compute-intensive workloads", correct: true },
                { id: 2, text: "Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads", correct: false },
                { id: 3, text: "Use Amazon ElastiCache to run highly complex JOIN queries", correct: false },
                { id: 4, text: "Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads, Use Amazon ElastiCache to improve the performance of compute-intensive workloads\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to run highly complex JOIN queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
            options: [
                { id: 0, text: "You can use an IP address as the custom source for the inbound rule", correct: false },
                { id: 1, text: "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule", correct: false },
                { id: 2, text: "You can use an Internet Gateway ID as the custom source for the inbound rule", correct: true },
                { id: 3, text: "You can use a security group as the custom source for the inbound rule", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: You can use an Internet Gateway ID as the custom source for the inbound rule\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You can use an IP address as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a security group as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A national logistics company has a dedicated AWS Direct Connect connection from its corporate data center to AWS. Within its AWS account, the company operates 25 Amazon VPCs in the same Region, each supporting different regional distribution services. The VPCs were configured with non-overlapping CIDR blocks and currently use private VIFs for Direct Connect access to on-premises resources. As the architecture scales, the company wants to enable communication across all VPCs and the on-premises environment. The solution must scale efficiently, support full-mesh connectivity, and reduce the complexity of maintaining separate private VIFs for each VPC. Which combination of solutions will best fulfill these requirements with the least amount of operational overhead? (Select two)",
            options: [
                { id: 0, text: "Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs", correct: false },
                { id: 1, text: "Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services", correct: false },
                { id: 2, text: "Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway", correct: true },
                { id: 3, text: "Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing", correct: true },
                { id: 4, text: "Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway, Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic", correct: false },
                { id: 1, text: "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic", correct: false },
                { id: 2, text: "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint", correct: true },
                { id: 3, text: "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A regional transportation authority operates a high-traffic public information portal hosted on AWS. The backend consists of Amazon EC2 instances behind an Application Load Balancer (ALB). In recent weeks, the operations team has observed intermittent slowdowns and performance issues. After investigation, the team suspect the application is being targeted by distributed denial-of-service (DDoS) attacks coming from a wide range of IP addresses. The team needs a solution that provides DDoS mitigation, detailed logs for audit purposes, and requires minimal changes to the existing architecture. Which solution best addresses these needs?",
            options: [
                { id: 0, text: "Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly", correct: false },
                { id: 1, text: "Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events", correct: true },
                { id: 2, text: "Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service", correct: false },
                { id: 1, text: "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users", correct: false },
                { id: 2, text: "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face", correct: true },
                { id: 3, text: "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A healthcare startup is modernizing its monolithic Python-based analytics application by transitioning to a microservices architecture on AWS. As a pilot, the team wants to refactor one module into a standalone microservice that can handle hundreds of requests per second. They are seeking an AWS-native solution that supports Python, scales automatically with traffic, and requires minimal infrastructure management and operational overhead to build, test, and deploy the service efficiently. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup", correct: false },
                { id: 1, text: "Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads", correct: true },
                { id: 2, text: "Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments", correct: false },
                { id: 3, text: "Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A company has its application servers in the public subnet that connect to the database instances in the private subnet. For regular maintenance, the database instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
            options: [
                { id: 0, text: "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC", correct: false },
                { id: 1, text: "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables", correct: false },
                { id: 2, text: "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC", correct: false },
                { id: 3, text: "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Egress-only internet gateway for the resources in the private subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
            options: [
                { id: 0, text: "General Purpose SSD (gp2)", correct: false },
                { id: 1, text: "Provisioned IOPS SSD (io1)", correct: true },
                { id: 2, text: "Throughput Optimized HDD (st1)", correct: false },
                { id: 3, text: "Cold HDD (sc1)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD (io1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose SSD (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Throughput Optimized HDD (st1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD (sc1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer", correct: false },
                { id: 1, text: "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions", correct: false },
                { id: 2, text: "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints", correct: true },
                { id: 3, text: "Configure Elastic IPs for each of the Application Load Balancers in each Region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Elastic IPs for each of the Application Load Balancers in each Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account. Which of the following solutions should a solutions architect configure for the necessary permissions?",
            options: [
                { id: 0, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly", correct: false },
                { id: 1, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly", correct: false },
                { id: 2, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role", correct: false },
                { id: 3, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance\n\nIAM roles provide temporary credentials and are the recommended way to grant permissions to AWS services and applications. They're more secure than access keys as credentials are automatically rotated and don't need to be stored.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
            options: [
                { id: 0, text: "Create a CNAME record", correct: true },
                { id: 1, text: "Create an A record", correct: false },
                { id: 2, text: "Create a PTR record", correct: false },
                { id: 3, text: "Create an Alias Record", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a CNAME record\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an A record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a PTR record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Alias Record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets", correct: false },
                { id: 1, text: "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets", correct: true },
                { id: 2, text: "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture", correct: false },
                { id: 3, text: "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 22,
            text: "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
            options: [
                { id: 0, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: false },
                { id: 1, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each", correct: false },
                { id: 2, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each", correct: false },
                { id: 3, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A financial services firm runs a containerized risk analytics tool in its on-premises data center using Docker. The tool depends on persistent data storage for maintaining customer simulation results and operates on a single host machine where the volume is locally mounted. The infrastructure team is looking to replatform the tool to AWS using a fully managed service because they want to avoid managing EC2 instances, volumes, or underlying servers. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually", correct: false },
                { id: 1, text: "Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically", correct: false },
                { id: 2, text: "Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage", correct: false },
                { id: 3, text: "Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 2, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 3, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
            options: [
                { id: 0, text: "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 1, text: "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 2, text: "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services", correct: true },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A financial services company is modernizing its analytics platform on AWS. Their legacy data processing scripts, built for both Windows and Linux environments, require shared access to a file system that supports Windows ACLs and SMB protocol for compatibility with existing Windows workloads. At the same time, their Linux-based applications need to read from and write to the same shared storage to maintain cross-platform consistency. The solutions architect needs to design a storage solution that allows both Windows and Linux EC2 instances to access the shared file system simultaneously, while preserving Windows-specific features like NTFS permissions and Active Directory (AD) integration. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms", correct: false },
                { id: 1, text: "Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances", correct: false },
                { id: 2, text: "Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads", correct: false },
                { id: 3, text: "Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
            options: [
                { id: 0, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume", correct: true },
                { id: 1, text: "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected", correct: false },
                { id: 2, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 1, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 2, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
            options: [
                { id: 0, text: "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets", correct: false },
                { id: 1, text: "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data", correct: false },
                { id: 2, text: "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively", correct: true },
                { id: 3, text: "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in theus-west-2region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of theus-west-2region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
            options: [
                { id: 0, text: "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts", correct: false },
                { id: 1, text: "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 2, text: "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 3, text: "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
            options: [
                { id: 0, text: "Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions", correct: false },
                { id: 1, text: "Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions", correct: true },
                { id: 2, text: "Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy", correct: false },
                { id: 3, text: "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use a simple scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 1, text: "Use a target tracking scaling policy based on a custom Amazon SQS queue metric", correct: true },
                { id: 2, text: "Use a step scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 3, text: "Use a scheduled scaling policy based on a custom Amazon SQS queue metric", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a target tracking scaling policy based on a custom Amazon SQS queue metric\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a simple scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a step scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a scheduled scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 1, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
                { id: 2, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
            options: [
                { id: 0, text: "Use SQS message timer to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 1, text: "Use SQS long polling to retrieve messages from your Amazon SQS queues", correct: true },
                { id: 2, text: "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 3, text: "Use SQS short polling to retrieve messages from your Amazon SQS queues", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use SQS long polling to retrieve messages from your Amazon SQS queues\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use SQS message timer to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS visibility timeout to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS short polling to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
            options: [
                { id: 0, text: "Service control policy (SCP) affects service-linked roles", correct: false },
                { id: 1, text: "Service control policy (SCP) does not affect service-linked role", correct: true },
                { id: 2, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action", correct: false },
                { id: 3, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action", correct: true },
                { id: 4, text: "Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts", correct: true },
                { id: 5, text: "Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts", correct: false },
            ],
            correctAnswers: [1, 3, 4],
            explanation: "The correct answers are: Service control policy (SCP) does not affect service-linked role, If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action, Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Service control policy (SCP) affects service-linked roles: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
            options: [
                { id: 0, text: "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: false },
                { id: 1, text: "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ", correct: false },
                { id: 2, text: "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: true },
                { id: 3, text: "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
            options: [
                { id: 0, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot", correct: false },
                { id: 1, text: "You cannot share an Amazon Machine Image (AMI) with another AWS account", correct: false },
                { id: 2, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot", correct: true },
                { id: 3, text: "You can share an Amazon Machine Image (AMI) with another AWS account", correct: true },
                { id: 4, text: "You cannot copy an Amazon Machine Image (AMI) across AWS Regions", correct: false },
                { id: 5, text: "You can copy an Amazon Machine Image (AMI) across AWS Regions", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot, You can share an Amazon Machine Image (AMI) with another AWS account, You can copy an Amazon Machine Image (AMI) across AWS Regions\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot share an Amazon Machine Image (AMI) with another AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot copy an Amazon Machine Image (AMI) across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
            options: [
                { id: 0, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy", correct: false },
                { id: 1, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy", correct: true },
                { id: 2, text: "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy", correct: false },
                { id: 3, text: "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 1, text: "Security Groups can be associated with a NAT gateway", correct: false },
                { id: 2, text: "NAT gateway supports port forwarding", correct: false },
                { id: 3, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 4, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 5, text: "NAT instance supports port forwarding", correct: true },
            ],
            correctAnswers: [0, 4, 5],
            explanation: "The correct answers are: Security Groups can be associated with a NAT instance, NAT instance can be used as a bastion server, NAT instance supports port forwarding\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 1, text: "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones", correct: true },
                { id: 2, text: "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 3, text: "Add Amazon EventBridge to decouple the complex architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon EventBridge to decouple the complex architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Glue", correct: false },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Database Migration Service (AWS DMS)", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
            options: [
                { id: 0, text: "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 1, text: "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 2, text: "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 3, text: "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 1, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs", correct: false },
                { id: 2, text: "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data. How will you implement this requirement without adding the overhead of splitting the data into logical groups?",
            options: [
                { id: 0, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: true },
                { id: 1, text: "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data", correct: false },
                { id: 2, text: "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: false },
                { id: 3, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data", correct: false },
                { id: 1, text: "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data", correct: true },
                { id: 2, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data", correct: false },
                { id: 3, text: "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
            options: [
                { id: 0, text: "Amazon Cloud Directory", correct: false },
                { id: 1, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 2, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 3, text: "Active Directory Connector", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Cloud Directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 3, text: "File Gateway Configuration of AWS Storage Gateway", correct: true },
                { id: 4, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon FSx for Windows File Server, File Gateway Configuration of AWS Storage Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic Block Store (Amazon EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A startup is building a serverless microservices architecture where client applications (web and mobile) authenticate users via a third-party OIDC-compliant identity provider. The backend APIs must validate JSON Web Tokens (JWTs) issued by this provider, enforce scope-based access control, and be cost-effective with minimal latency. The development team wants to use a fully managed service that supports JWT validation natively, without writing custom authentication logic. Which solution should the team implement to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider", correct: true },
                { id: 1, text: "Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer", correct: false },
                { id: 2, text: "Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens", correct: false },
                { id: 3, text: "Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects", correct: true },
                { id: 2, text: "Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy", correct: false },
                { id: 4, text: "Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects, Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A media streaming startup is building a set of backend APIs that will be consumed by external mobile applications. To prevent API abuse, protect downstream resources, and ensure fair usage across clients, the architecture must enforce rate limiting and throttling on a per-client basis. The team also wants to define usage quotas and apply different limits to different API consumers. Which solution should the team implement to enforce rate limiting and usage quotas at the API layer?",
            options: [
                { id: 0, text: "Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances", correct: false },
                { id: 1, text: "Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits", correct: false },
                { id: 2, text: "Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client", correct: true },
                { id: 3, text: "Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling", correct: true },
                { id: 1, text: "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas", correct: false },
                { id: 2, text: "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling", correct: false },
                { id: 3, text: "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A geospatial analytics firm recently completed a large-scale seafloor imaging project and used AWS Snowball Edge Storage Optimized devices to collect and transfer over 150 TB of raw sonar data. The firm uses a high-performance computing (HPC) cluster on AWS to process and analyze massive datasets to identify natural resource formations. The processing workloads require sub-millisecond latency, high-throughput, fast and parallel file access to the imported dataset once the Snowball Edge devices are returned to AWS and the data is ingested. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel", correct: false },
                { id: 1, text: "Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data", correct: true },
                { id: 2, text: "Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access", correct: false },
                { id: 3, text: "Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
            options: [
                { id: 0, text: "Use Amazon FSx for Windows File Server as a shared storage solution", correct: true },
                { id: 1, text: "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies", correct: false },
                { id: 2, text: "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution", correct: false },
                { id: 3, text: "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon FSx for Windows File Server as a shared storage solution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic File System (Amazon EFS) as a shared storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway of AWS Storage Gateway to create a hybrid storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 1, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
            options: [
                { id: 0, text: "If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery", correct: false },
                { id: 1, text: "Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance", correct: false },
                { id: 2, text: "During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained", correct: false },
                { id: 3, text: "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery", correct: true },
                { id: 4, text: "A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If your instance has a public IPv4 address, it retains the public IPv4 address after recovery, A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A software company manages a fleet of Amazon EC2 instances that support internal analytics applications. These instances use an IAM role with custom policies to connect to Amazon RDS and AWS Secrets Manager for secure access to credentials and database endpoints. The IT operations team wants to implement a centralized patch management solution that simplifies compliance and security tasks. Their goal is to automate OS patching across EC2 instances without disrupting the running applications. Which approach will allow the company to meet these goals with the least administrative overhead?",
            options: [
                { id: 0, text: "Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts", correct: false },
                { id: 1, text: "Enable Default Host Management Configuration in AWS Systems Manager Quick Setup", correct: true },
                { id: 2, text: "Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features", correct: false },
                { id: 3, text: "Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Default Host Management Configuration in AWS Systems Manager Quick Setup\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
            options: [
                { id: 0, text: "If a spot request is persistent, then it is opened again after your Spot Instance is interrupted", correct: true },
                { id: 1, text: "Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: true },
                { id: 2, text: "When you cancel an active spot request, it terminates the associated instance as well", correct: false },
                { id: 3, text: "Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: false },
                { id: 4, text: "If a spot request is persistent, then it is opened again after you stop the Spot Instance", correct: false },
                { id: 5, text: "When you cancel an active spot request, it does not terminate the associated instance", correct: true },
            ],
            correctAnswers: [0, 1, 5],
            explanation: "The correct answers are: If a spot request is persistent, then it is opened again after your Spot Instance is interrupted, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, When you cancel an active spot request, it does not terminate the associated instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- When you cancel an active spot request, it terminates the associated instance as well: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a spot request is persistent, then it is opened again after you stop the Spot Instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
            options: [
                { id: 0, text: "Internet Gateway (I1)", correct: true },
                { id: 1, text: "Route Table (R1)", correct: false },
                { id: 2, text: "Subnet (S1)", correct: false },
                { id: 3, text: "Network Address Translation (NAT) instance (N1)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Internet Gateway (I1)\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Route Table (R1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subnet (S1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Network Address Translation (NAT) instance (N1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 3, text: "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
            options: [
                { id: 0, text: "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job", correct: true },
                { id: 1, text: "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script", correct: false },
                { id: 2, text: "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
                { id: 3, text: "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions", correct: false },
                { id: 1, text: "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions", correct: true },
                { id: 2, text: "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions", correct: false },
                { id: 3, text: "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
            options: [
                { id: 0, text: "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data", correct: false },
                { id: 1, text: "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written", correct: false },
                { id: 2, text: "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written", correct: false },
                { id: 3, text: "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
            options: [
                { id: 0, text: "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ", correct: false },
                { id: 1, text: "Create a read replica and connect the report generation tool/application to it", correct: true },
                { id: 2, text: "Migrate from General Purpose SSD to magnetic storage to enhance IOPS", correct: false },
                { id: 3, text: "Increase the size of Amazon RDS instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a read replica and connect the report generation tool/application to it\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate from General Purpose SSD to magnetic storage to enhance IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the size of Amazon RDS instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test5: [
        {
            id: 1,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 2, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 3, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
                { id: 4, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
            options: [
                { id: 0, text: "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions", correct: true },
                { id: 2, text: "Use Amazon EC2 user data to speed up the installation process", correct: false },
                { id: 3, text: "Store the installation files in Amazon S3 for quicker retrieval", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to speed up the installation process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the installation files in Amazon S3 for quicker retrieval: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
            options: [
                { id: 0, text: "Set up Amazon DynamoDB table with a global secondary index", correct: false },
                { id: 1, text: "Set up Amazon DynamoDB table in the on-demand capacity mode", correct: true },
                { id: 2, text: "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled", correct: false },
                { id: 3, text: "Set up Amazon DynamoDB global table in the provisioned capacity mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up Amazon DynamoDB table in the on-demand capacity mode\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon DynamoDB table with a global secondary index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB global table in the provisioned capacity mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
            options: [
                { id: 0, text: "EXAMPLE.COM", correct: false },
                { id: 1, text: "example.test.com", correct: false },
                { id: 2, text: "test.example.com", correct: true },
                { id: 3, text: "example.com", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: test.example.com\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- EXAMPLE.COM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.test.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 1, text: "Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
                { id: 3, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
            options: [
                { id: 0, text: "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels", correct: true },
                { id: 1, text: "Use AWS Global Accelerator for the VPN connection to maximize the throughput", correct: false },
                { id: 2, text: "Use Transfer Acceleration for the VPN connection to maximize the throughput", correct: false },
                { id: 3, text: "Create a virtual private gateway with equal cost multipath routing and multiple channels", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transfer Acceleration for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway with equal cost multipath routing and multiple channels: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Dedicated hosts for the minimum capacity", correct: false },
                { id: 1, text: "Set the minimum capacity to 3", correct: false },
                { id: 2, text: "Use Reserved Instances (RIs) for the minimum capacity", correct: true },
                { id: 3, text: "Set the minimum capacity to 2", correct: true },
                { id: 4, text: "Set the minimum capacity to 1", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Reserved Instances (RIs) for the minimum capacity, Set the minimum capacity to 2\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Dedicated hosts for the minimum capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 1: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
            options: [
                { id: 0, text: "Use a wildcard Secure Sockets Layer certificate (SSL certificate)", correct: false },
                { id: 1, text: "Use Secure Sockets Layer certificate (SSL certificate) with SNI", correct: true },
                { id: 2, text: "Change the Elastic Load Balancing (ELB) SSL Security Policy", correct: false },
                { id: 3, text: "Use an HTTP to HTTPS redirect", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Secure Sockets Layer certificate (SSL certificate) with SNI\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a wildcard Secure Sockets Layer certificate (SSL certificate): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the Elastic Load Balancing (ELB) SSL Security Policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an HTTP to HTTPS redirect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Spot Instances", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon EC2 Reserved Instances (RIs)", correct: false },
                { id: 4, text: "Amazon EC2 On-Demand Instances", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Amazon EC2 Spot Instances, Amazon Simple Queue Service (Amazon SQS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Reserved Instances (RIs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
            options: [
                { id: 0, text: "The Time To Live (TTL) is still in effect", correct: true },
                { id: 1, text: "The health checks are failing", correct: false },
                { id: 2, text: "The Alias Record is misconfigured", correct: false },
                { id: 3, text: "The CNAME Record is misconfigured", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The Time To Live (TTL) is still in effect\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The health checks are failing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Alias Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CNAME Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size10.0.1.0/24and the Auto Scaling group is deployed in a subnet of size10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
            options: [
                { id: 0, text: "Add a rule to authorize the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "Add a rule to authorize the CIDR 10.0.1.0/24", correct: false },
                { id: 2, text: "Add a rule to authorize the security group of the Auto Scaling group", correct: false },
                { id: 3, text: "Add a rule to authorize the CIDR 10.0.4.0/22", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Add a rule to authorize the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Add a rule to authorize the CIDR 10.0.1.0/24: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the security group of the Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the CIDR 10.0.4.0/22: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A fintech startup hosts its real-time transaction metadata in Amazon DynamoDB tables. During a recent system maintenance event, a junior engineer accidentally deleted a production table, resulting in major service downtime and irreversible data loss. Leadership has mandated an immediate solution that prevents future data loss from human error, while requiring minimal ongoing maintenance or manual effort from the engineering team. Which approach best addresses these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Enable deletion protection on DynamoDB tables", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3", correct: false },
                { id: 2, text: "Enable point-in-time recovery (PITR) on each DynamoDB table", correct: false },
                { id: 3, text: "Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable deletion protection on DynamoDB tables\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable point-in-time recovery (PITR) on each DynamoDB table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare provider is experiencing rapid data growth in its on-premises servers due to increased patient imaging and record retention requirements. The organization wants to extend its storage capacity to AWS in a way that preserves quick access to critical records, including from its local file systems. The company must optimize bandwidth usage during migration and avoid any retrieval fees or delays when accessing the data in the cloud. The provider wants a hybrid cloud solution that requires minimal application reconfiguration, allows frequent local access to key datasets, and ensures that cloud storage costs remain predictable without paying extra for data retrieval. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates", correct: false },
                { id: 1, text: "Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement", correct: false },
                { id: 2, text: "Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3", correct: true },
                { id: 3, text: "Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A genomics research firm is processing sporadic bursts of data-intensive workloads using Amazon EC2 instances. The shared storage must support unpredictable spikes in file operations, but the average daily throughput demand remains relatively low. The team has selected Amazon Elastic File System (EFS) for its scalability and wants to ensure optimal cost and performance during bursts without provisioning throughput manually. Which approach should the team take to best meet these requirements?",
            options: [
                { id: 0, text: "Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads", correct: false },
                { id: 1, text: "Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class", correct: true },
                { id: 2, text: "Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode", correct: false },
                { id: 3, text: "Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use a Cluster placement group", correct: true },
                { id: 1, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 2, text: "Use Spot Instances", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
            options: [
                { id: 0, text: "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment", correct: true },
                { id: 1, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration", correct: false },
                { id: 2, text: "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration", correct: false },
                { id: 3, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
            options: [
                { id: 0, text: "Use a Network Address Translation gateway (NAT gateway)", correct: false },
                { id: 1, text: "Use a VPC peering connection", correct: true },
                { id: 2, text: "Use an AWS Direct Connect connection", correct: false },
                { id: 3, text: "Use an Internet Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a VPC peering connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use a Network Address Translation gateway (NAT gateway): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Direct Connect connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Internet Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 18,
            text: "A transportation logistics company runs a shipment tracking application on Amazon EC2 instances with an Amazon Aurora MySQL database cluster. The application is experiencing rapid growth due to increased demand from mobile app users querying package delivery statuses. Although the compute layer (EC2) has remained stable, the Aurora DB cluster is under growing read pressure, especially from frequent repeated queries about package locations and delivery history. The company added an Aurora read replica, which temporarily alleviated the load, but read traffic continues to spike as user queries grow. The company wants to reduce the repeated reads pressure on the DB cluster. Which solution will best meet these requirements in a cost-effective manner?",
            options: [
                { id: 0, text: "Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas", correct: false },
                { id: 1, text: "Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database", correct: true },
                { id: 2, text: "Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions", correct: false },
                { id: 3, text: "Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect – Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
            options: [
                { id: 0, text: "Use field level encryption in Amazon CloudFront to protect sensitive data for specific content", correct: true },
                { id: 1, text: "Amazon CloudFront can route to multiple origins based on the price class", correct: false },
                { id: 2, text: "Use geo restriction to configure Amazon CloudFront for high-availability and failover", correct: false },
                { id: 3, text: "Amazon CloudFront can route to multiple origins based on the content type", correct: true },
                { id: 4, text: "Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content", correct: false },
                { id: 5, text: "Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover", correct: true },
            ],
            correctAnswers: [0, 3, 5],
            explanation: "The correct answers are: Use field level encryption in Amazon CloudFront to protect sensitive data for specific content, Amazon CloudFront can route to multiple origins based on the content type, Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon CloudFront can route to multiple origins based on the price class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use geo restriction to configure Amazon CloudFront for high-availability and failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
            options: [
                { id: 0, text: "Amazon ElastiCache", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 3, text: "Amazon Neptune", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Neptune: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
            options: [
                { id: 0, text: "Pilot Light", correct: false },
                { id: 1, text: "Warm Standby", correct: true },
                { id: 2, text: "Multi Site", correct: false },
                { id: 3, text: "Backup and Restore", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Warm Standby\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Pilot Light: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi Site: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Backup and Restore: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
            options: [
                { id: 0, text: "Access control list (ACL)", correct: false },
                { id: 1, text: "Trust policy", correct: true },
                { id: 2, text: "Permissions boundary", correct: false },
                { id: 3, text: "AWS Organizations Service Control Policies (SCP)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Trust policy\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Access control list (ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Permissions boundary: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Organizations Service Control Policies (SCP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon Kinesis", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon S3", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Amazon DynamoDB, Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
                { id: 1, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 2, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 3, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 4, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 5, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are: Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold, By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
            options: [
                { id: 0, text: "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years", correct: true },
                { id: 1, text: "Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes", correct: false },
                { id: 2, text: "Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs", correct: false },
                { id: 3, text: "Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
            options: [
                { id: 0, text: "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases", correct: false },
                { id: 1, text: "Use AWS CloudFormation to manage Amazon RDS databases", correct: true },
                { id: 2, text: "Attach an IAM policy to interns preventing them from creating an Amazon RDS database", correct: false },
                { id: 3, text: "Store your recommendations in a custom AWS Trusted Advisor rule", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation to manage Amazon RDS databases\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy to interns preventing them from creating an Amazon RDS database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store your recommendations in a custom AWS Trusted Advisor rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator", correct: true },
                { id: 1, text: "Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer", correct: false },
                { id: 3, text: "Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs):AZ-AandAZ-B. Cross-zone load balancing is disabled.AZ-Ahas four targets andAZ-Bhas six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
            options: [
                { id: 0, text: "Each of the four targets in AZ-A receives 12.5% of the traffic", correct: true },
                { id: 1, text: "Each of the six targets in AZ-B receives 10% of the traffic", correct: false },
                { id: 2, text: "Each of the four targets in AZ-A receives 10% of the traffic", correct: false },
                { id: 3, text: "Each of the four targets in AZ-A receives 8% of the traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Each of the four targets in AZ-A receives 12.5% of the traffic\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Each of the six targets in AZ-B receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 8% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 30,
            text: "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
            options: [
                { id: 0, text: "Use Amazon RDS Read Replicas", correct: true },
                { id: 1, text: "Use Amazon DynamoDB", correct: false },
                { id: 2, text: "Use Amazon ElastiCache", correct: false },
                { id: 3, text: "Use Amazon RDS Multi-AZ feature", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon RDS Read Replicas\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Multi-AZ feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A global pharmaceutical company operates a hybrid cloud network. Its primary AWS workloads run in the us-west-2 Region, connected to its on-premises data center via an AWS Direct Connect connection. After acquiring a biotech firm headquartered in Europe, the company must integrate the biotech’s workloads, which are hosted in several VPCs in the eu-central-1 Region and connected to the biotech's on-premises facility through a separate Direct Connect link. All CIDR blocks are non-overlapping, and the business requires full connectivity between both data centers and all VPCs across the two Regions. The company also wants a scalable solution that minimizes manual network configuration and long-term operational overhead. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication", correct: false },
                { id: 1, text: "Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions", correct: true },
                { id: 2, text: "Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic", correct: false },
                { id: 3, text: "Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon OpenSearch Service", correct: false },
                { id: 1, text: "Amazon Redshift", correct: false },
                { id: 2, text: "Amazon Neptune", correct: true },
                { id: 3, text: "Amazon Aurora", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
            options: [
                { id: 0, text: "Change the Amazon EC2 instance type to something much smaller", correct: false },
                { id: 1, text: "Keep the Amazon EBS volume to io1 and reduce the IOPS", correct: false },
                { id: 2, text: "Convert the Amazon EC2 instance EBS volume to gp2", correct: true },
                { id: 3, text: "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Convert the Amazon EC2 instance EBS volume to gp2\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Change the Amazon EC2 instance type to something much smaller: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Keep the Amazon EBS volume to io1 and reduce the IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs).AZ-Ahas 3 Amazon EC2 instances andAZ-Bhas 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
            options: [
                { id: 0, text: "A random instance in the AZ-A will be terminated", correct: false },
                { id: 1, text: "A random instance will be terminated in AZ-B", correct: false },
                { id: 2, text: "An instance in the AZ-A will be created", correct: false },
                { id: 3, text: "The instance with the oldest launch template or launch configuration will be terminated in AZ-B", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The instance with the oldest launch template or launch configuration will be terminated in AZ-B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- A random instance in the AZ-A will be terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A random instance will be terminated in AZ-B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance in the AZ-A will be created: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
            options: [
                { id: 0, text: "Use a Classic Load Balancer with an Auto Scaling Group", correct: false },
                { id: 1, text: "Use an Application Load Balancer with an Auto Scaling Group", correct: false },
                { id: 2, text: "Use a Network Load Balancer with an Auto Scaling Group", correct: true },
                { id: 3, text: "Use an Auto Scaling Group with Dynamic Elastic IPs attachment", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use a Network Load Balancer with an Auto Scaling Group\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a Classic Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling Group with Dynamic Elastic IPs attachment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
            options: [
                { id: 0, text: "Use a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 1, text: "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 2, text: "Use permissions boundary to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 3, text: "Use a bucket policy to grant permission to users in its account as well as to users in another account", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a bucket policy to grant permission to users in its account as well as to users in another account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use permissions boundary to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "You have an Amazon S3 bucket that contains files in two different folders -s3://my-bucket/imagesands3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
            options: [
                { id: 0, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days", correct: false },
                { id: 1, text: "Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days", correct: false },
                { id: 2, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days", correct: false },
                { id: 3, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days", correct: true },
                { id: 4, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days, Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
            options: [
                { id: 0, text: "Deploy AWS Lambda in a VPC", correct: false },
                { id: 1, text: "Attach an AWS Identity and Access Management (IAM) role to AWS Lambda", correct: true },
                { id: 2, text: "Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL", correct: true },
                { id: 3, text: "Restrict the Amazon RDS database security group to the AWS Lambda's security group", correct: false },
                { id: 4, text: "Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Attach an AWS Identity and Access Management (IAM) role to AWS Lambda, Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Lambda in a VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Restrict the Amazon RDS database security group to the AWS Lambda's security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A ride-hailing startup has launched a mobile app that matches passengers with nearby drivers based on real-time GPS coordinates. The application backend uses an Amazon RDS for PostgreSQL instance with read replicas to store the latitude and longitude of drivers and passengers. As the service scales, the backend experiences performance bottlenecks during peak hours, especially when thousands of updates and reads occur per second to keep location data current. The company expects its user base to double in the next few months and needs a high-performance, scalable solution that can handle frequent write and read operations with minimal latency. What do you recommend?",
            options: [
                { id: 0, text: "Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration", correct: false },
                { id: 1, text: "Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards", correct: false },
                { id: 2, text: "Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy", correct: true },
                { id: 3, text: "Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 1, text: "The route for the health check is misconfigured", correct: true },
                { id: 2, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 3, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 4, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: The route for the health check is misconfigured, The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A digital media platform is preparing to launch a new interactive content service that is expected to receive sudden spikes in user engagement, especially during live events and media releases. The backend uses an Amazon Aurora PostgreSQL Serverless v2 cluster to handle dynamic workloads. The architecture must be capable of scaling both compute and storage performance to maintain low latency and avoid bottlenecks under load. The engineering team is evaluating storage configuration options and wants a solution that will scale automatically with traffic, optimize I/O performance, and remain cost-effective without manual provisioning or tuning. Which configuration will best meet these requirements?",
            options: [
                { id: 0, text: "Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage", correct: false },
                { id: 1, text: "Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes", correct: false },
                { id: 2, text: "Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks", correct: false },
                { id: 3, text: "Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 2, text: "Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon MQ", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
            options: [
                { id: 0, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time", correct: false },
                { id: 1, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3", correct: true },
                { id: 2, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3", correct: false },
                { id: 3, text: "Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
            options: [
                { id: 0, text: "Latency-based routing", correct: false },
                { id: 1, text: "Geolocation routing", correct: false },
                { id: 2, text: "Geoproximity routing", correct: true },
                { id: 3, text: "Weighted routing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Geoproximity routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Latency-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Geolocation routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Weighted routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A financial services company is implementing two separate data retention policies to comply with regulatory standards: Policy A: Critical transaction records must be immediately available for audit and must not be deleted or overwritten for 7 years. Policy B: Archived compliance data must be stored in a low-cost, long-term storage solution and locked from deletion or modification for at least 10 years. As a solutions architect, which combination of AWS features should you recommend to enforce these policies effectively?",
            options: [
                { id: 0, text: "Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B", correct: false },
                { id: 1, text: "Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B", correct: true },
                { id: 2, text: "Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention", correct: false },
                { id: 3, text: "Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
            options: [
                { id: 0, text: "Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables", correct: false },
                { id: 1, text: "Contact AWS support to map your VPC with subnet", correct: false },
                { id: 2, text: "Check if the security groups allow ping from the source", correct: true },
                { id: 3, text: "Disable Source / Destination check on the Amazon EC2 instance", correct: false },
                { id: 4, text: "Check if the route table is configured with internet gateway", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Check if the security groups allow ping from the source, Check if the route table is configured with internet gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Contact AWS support to map your VPC with subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable Source / Destination check on the Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
            options: [
                { id: 0, text: "Enable Amazon API Gateway Caching", correct: true },
                { id: 1, text: "Switch to using an Application Load Balancer", correct: false },
                { id: 2, text: "Add Amazon Aurora Read Replicas", correct: false },
                { id: 3, text: "Enable AWS Lambda In Memory Caching", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon API Gateway Caching\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Switch to using an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Aurora Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Lambda In Memory Caching: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy?",
            options: [
                { id: 0, text: "It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket", correct: false },
                { id: 1, text: "It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket", correct: true },
                { id: 2, text: "It ensures Amazon EC2 instances that have inherited a security group can access the bucket", correct: false },
                { id: 3, text: "It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures Amazon EC2 instances that have inherited a security group can access the bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Egress Only Internet Gateway as a backup connection", correct: false },
                { id: 1, text: "Use AWS Site-to-Site VPN as a backup connection", correct: true },
                { id: 2, text: "Use AWS Direct Connect connection as a primary connection", correct: true },
                { id: 3, text: "Use AWS Site-to-Site VPN as a primary connection", correct: false },
                { id: 4, text: "Use AWS Direct Connect connection as a backup connection", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use AWS Site-to-Site VPN as a backup connection, Use AWS Direct Connect connection as a primary connection\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Egress Only Internet Gateway as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Site-to-Site VPN as a primary connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect connection as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this AWS CloudFormation snippet do? (Select three)",
            options: [
                { id: 0, text: "It lets traffic flow from one IP on port 22", correct: true },
                { id: 1, text: "It configures a security group's outbound rules", correct: false },
                { id: 2, text: "It configures a security group's inbound rules", correct: true },
                { id: 3, text: "It configures the inbound rules of a network access control list (network ACL)", correct: false },
                { id: 4, text: "It only allows the IP 0.0.0.0 to reach HTTP", correct: false },
                { id: 5, text: "It allows any IP to pass through on the HTTP port", correct: true },
                { id: 6, text: "It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1", correct: false },
            ],
            correctAnswers: [0, 2, 5],
            explanation: "The correct answers are: It lets traffic flow from one IP on port 22, It configures a security group's inbound rules, It allows any IP to pass through on the HTTP port\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- It configures a security group's outbound rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It configures the inbound rules of a network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It only allows the IP 0.0.0.0 to reach HTTP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "An e-commerce analytics company is preparing to archive several years of transaction records and customer analytics reports in Amazon S3 for long-term storage. To meet compliance requirements, the archived data must be encrypted at rest. Additionally, the solution must be cost-effective and ensure that key rotation occurs automatically every 12 months to comply with the company’s internal data governance policy. Which solution will meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket", correct: true },
                { id: 1, text: "Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow", correct: false },
                { id: 2, text: "Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings", correct: false },
                { id: 3, text: "Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 1, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 2, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 3, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
                { id: 4, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
            options: [
                { id: 0, text: "Select an Elastic Inference accelerator while launching Amazon EC2 instances", correct: false },
                { id: 1, text: "Select the appropriate capacity reservation while launching Amazon EC2 instances", correct: false },
                { id: 2, text: "Select dedicated instance tenancy while launching Amazon EC2 instances", correct: false },
                { id: 3, text: "Select a cluster placement group while launching Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Select a cluster placement group while launching Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Select an Elastic Inference accelerator while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select the appropriate capacity reservation while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select dedicated instance tenancy while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
            options: [
                { id: 0, text: "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure", correct: true },
                { id: 1, text: "Schedule manual backups using Redis append-only file (AOF)", correct: false },
                { id: 2, text: "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure", correct: false },
                { id: 3, text: "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Schedule manual backups using Redis append-only file (AOF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Schedule daily automatic backups at a time when you expect low resource utilization for your cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 1, text: "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks", correct: true },
                { id: 2, text: "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 3, text: "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 56,
            text: "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
            options: [
                { id: 0, text: "Use AWS Snowball", correct: false },
                { id: 1, text: "Use multi-part upload feature of Amazon S3", correct: true },
                { id: 2, text: "Use AWS Direct Connect to provide extra bandwidth", correct: false },
                { id: 3, text: "Use Amazon S3 Versioning", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use multi-part upload feature of Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Snowball: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to provide extra bandwidth: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Versioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 57,
            text: "A media company operates a video rendering pipeline on Amazon EKS, where containerized jobs are scheduled using Kubernetes deployments. The application experiences bursty traffic patterns, particularly during peak streaming hours. The platform uses the Kubernetes Horizontal Pod Autoscaler (HPA) to scale pods based on CPU utilization. A solutions architect observes that the total number of EC2 worker nodes remains constant during traffic spikes, even when all nodes are at maximum resource utilization. The company needs a solution that enables automatic scaling of the underlying compute infrastructure when pod demand exceeds cluster capacity. Which solution should the architect implement to resolve this issue with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure", correct: false },
                { id: 1, text: "Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands", correct: true },
                { id: 2, text: "Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group", correct: false },
                { id: 3, text: "Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two)",
            options: [
                { id: 0, text: "Use Amazon EMR for serverless orchestration of the containerized services", correct: false },
                { id: 1, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 3, text: "Use Amazon SageMaker for serverless orchestration of the containerized services", correct: false },
                { id: 4, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services, Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EMR for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
            options: [
                { id: 0, text: "Use an Amazon CloudFront distribution in front of your website", correct: false },
                { id: 1, text: "Use an Auto Scaling Group", correct: true },
                { id: 2, text: "Use an Amazon Route 53 Multi Value record", correct: false },
                { id: 3, text: "Deploy the website on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon CloudFront distribution in front of your website: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Route 53 Multi Value record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the website on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A streaming media company operates a high-traffic content delivery platform on AWS. The application backend is deployed on Amazon EC2 instances within an Auto Scaling group across multiple Availability Zones in a VPC. The team has observed that workloads follow predictable usage patterns, such as higher viewership on weekends and in the evenings, along with occasional real-time spikes due to viral content. To reduce cost and improve responsiveness, the team wants an automated scaling approach that can forecast future demand using historical usage patterns, scale in advance based on those predictions, and react quickly to unplanned usage surges in real time. Which scaling strategy should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments", correct: false },
                { id: 1, text: "Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes", correct: true },
                { id: 2, text: "Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours", correct: false },
                { id: 3, text: "Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture", correct: false },
                { id: 1, text: "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture", correct: false },
                { id: 3, text: "Use Amazon EventBridge to decouple the system architecture", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EventBridge to decouple the system architecture\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) for effective decoupling of system architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A research organization is running a high-performance computing (HPC) workload using Amazon EC2 instances that are distributed across multiple Availability Zones (AZs) within a single AWS Region. The workload requires access to a shared file system with the lowest possible latency for frequent reads and writes.The team decides to use Amazon Elastic File System (Amazon EFS) for its scalability and simplicity. To ensure optimal performance and reduce network latency, the solution architect must design the architecture so that each EC2 instance can access the file system with the least possible delay. Which of the following is the most appropriate solution to meet these requirements?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones", correct: false },
                { id: 1, text: "Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target", correct: false },
                { id: 2, text: "Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances", correct: false },
                { id: 3, text: "Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A global insurance company is modernizing its infrastructure by migrating multiple line-of-business applications from its on-premises data centers to AWS. These applications will be deployed across several AWS accounts, all governed under a centralized AWS Organizations structure. The company manages all user identities, groups, and access policies within its on-premises Microsoft Active Directory and wants to continue doing so. The goal is to enable seamless single sign-in across all AWS accounts without duplicating user identity stores or manually provisioning accounts. Which solution best meets these requirements in the most operationally efficient manner?",
            options: [
                { id: 0, text: "Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts", correct: false },
                { id: 1, text: "Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider", correct: false },
                { id: 2, text: "Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts", correct: true },
                { id: 3, text: "Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Streams", correct: false },
                { id: 1, text: "Use Amazon DynamoDB DAX", correct: true },
                { id: 2, text: "Use Amazon DynamoDB Global Tables", correct: false },
                { id: 3, text: "Use Amazon ElastiCache", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon DynamoDB DAX\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test6: [
        {
            id: 1,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
            options: [
                { id: 0, text: "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch", correct: true },
                { id: 1, text: "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3", correct: false },
                { id: 2, text: "Disable the Termination from the Auto Scaling Group any time a user reports an issue", correct: false },
                { id: 3, text: "Make a snapshot of the Amazon EC2 instance just before it gets terminated", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable the Termination from the Auto Scaling Group any time a user reports an issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make a snapshot of the Amazon EC2 instance just before it gets terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too", correct: false },
                { id: 1, text: "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data", correct: false },
                { id: 2, text: "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures", correct: true },
                { id: 3, text: "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
            options: [
                { id: 0, text: "The application will be down until the primary database has recovered itself", correct: false },
                { id: 1, text: "The URL to access the database will change to the standby database", correct: false },
                { id: 2, text: "An email will be sent to the System Administrator asking for manual intervention", correct: false },
                { id: 3, text: "The CNAME record will be updated to point to the standby database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The CNAME record will be updated to point to the standby database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- The application will be down until the primary database has recovered itself: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The URL to access the database will change to the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An email will be sent to the System Administrator asking for manual intervention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A healthcare startup runs a lightweight reporting application on a single Amazon EC2 On-Demand instance. The application is designed to be stateless, fault-tolerant, and optimized for fast rendering of analytics dashboards. During major health events or news cycles, the team observes latency issues and occasional 5xx errors due to traffic spikes. To meet growing demand without over-provisioning resources during off-peak hours, the company wants to implement a cost-effective, scalable solution that ensures consistent performance even under unpredictable load. Which approach best meets the requirements while minimizing costs?",
            options: [
                { id: 0, text: "Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%", correct: false },
                { id: 1, text: "Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly", correct: false },
                { id: 2, text: "Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load", correct: false },
                { id: 3, text: "Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two)",
            options: [
                { id: 0, text: "The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic", correct: true },
                { id: 1, text: "The subnet has been configured to be public and has no access to the internet", correct: false },
                { id: 2, text: "The instance's subnet is not associated with any route table", correct: false },
                { id: 3, text: "The route table in the instance’s subnet should have a route to an Internet Gateway", correct: true },
                { id: 4, text: "The instance's subnet is associated with multiple route tables with conflicting configurations", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic, The route table in the instance’s subnet should have a route to an Internet Gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The subnet has been configured to be public and has no access to the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is not associated with any route table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is associated with multiple route tables with conflicting configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A digital content production company has transitioned all of its media assets to Amazon S3 in an effort to reduce storage costs. However, the rendering engine used in production continues to run in an on-premises data center and requires frequent and low-latency access to large media files. The company wants to implement a storage solution that maintains application performance while keeping costs low. Which approach should the company choose to meet these requirements in the most cost-effective way?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket", correct: false },
                { id: 1, text: "Set up an Amazon S3 File Gateway to provide storage for the on-premises application", correct: true },
                { id: 2, text: "Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel", correct: false },
                { id: 3, text: "Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 File Gateway to provide storage for the on-premises application\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
            options: [
                { id: 0, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 1, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation", correct: true },
                { id: 2, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A digital media startup allows users to submit images through its web portal. These images are uploaded directly into an Amazon S3 bucket. On average, around 200 images are uploaded daily. The company wants to automatically generate a smaller preview version (thumbnail) of each new image and store the resulting thumbnails in a separate Amazon S3 bucket. The team prefers a design that is low-cost, requires minimal infrastructure management, and automatically reacts to new uploads. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail", correct: false },
                { id: 1, text: "Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket", correct: true },
                { id: 2, text: "Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket", correct: false },
                { id: 3, text: "Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A digital publishing platform stores large volumes of media assets (such as images and documents) in an Amazon S3 bucket. These assets are accessed frequently during business hours by internal editors and content delivery tools. The company has strict encryption policies and currently uses AWS KMS to handle server-side encryption. The cloud operations team notices that AWS KMS request costs are increasing significantly due to the high frequency of object uploads and accesses. The team is now looking for a way to maintain the same encryption method but reduce the cost of KMS usage, especially for frequent access patterns. Which solution meets the company's encryption and cost optimization goals?",
            options: [
                { id: 0, text: "Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object", correct: true },
                { id: 1, text: "Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control", correct: false },
                { id: 2, text: "Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges", correct: false },
                { id: 3, text: "Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
            options: [
                { id: 0, text: "The database user credentials (username and password) configured for the application are incorrect", correct: false },
                { id: 1, text: "The database user credentials (username and password) configured for the application do not have the required privilege for the given database", correct: false },
                { id: 2, text: "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers", correct: true },
                { id: 3, text: "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The database user credentials (username and password) configured for the application are incorrect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The database user credentials (username and password) configured for the application do not have the required privilege for the given database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A biomedical research firm operates a file exchange system for external research partners to upload and download experimental data. Currently, the system runs on two Amazon EC2 Linux instances, each configured with Elastic IP addresses to allow access from trusted IPs. File transfers use the SFTP protocol, and Linux user accounts are manually provisioned to enforce file-level access control. Data is stored on a shared file system mounted to both EC2 instances. The firm wants to modernize the solution to a fully managed, serverless model with high IOPS, fine-grained user permission control, and strict IP-based access restrictions. They also want to reduce operational overhead without sacrificing performance or security. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings", correct: false },
                { id: 1, text: "Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses", correct: false },
                { id: 2, text: "Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF", correct: false },
                { id: 3, text: "Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "An organization operates a legacy reporting tool hosted on an Amazon EC2 instance located within a public subnet of a VPC. This tool aggregates scanned PDF reports from field devices and temporarily stores them on an attached Amazon EBS volume. At the end of each day, the tool transfers the accumulated files to an Amazon S3 bucket for archival. A solutions architect identifies that the files are being uploaded over the internet using S3's public endpoint. To improve security and avoid exposing data traffic to the public internet, the architect needs to reconfigure the setup so that uploads to Amazon S3 occur privately without using the public S3 endpoint. Which solution will fulfill these requirements?",
            options: [
                { id: 0, text: "Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data", correct: false },
                { id: 1, text: "Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint", correct: true },
                { id: 2, text: "Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately", correct: false },
                { id: 3, text: "Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A fintech company currently operates a real-time search and analytics platform on-premises. This platform ingests streaming data from multiple data-producing systems and provides immediate search capabilities and interactive visualizations for end users. As part of its cloud migration strategy, the company wants to rearchitect the solution using AWS-native services. Which of the following represents the most efficient solution?",
            options: [
                { id: 0, text: "Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations", correct: false },
                { id: 1, text: "Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights", correct: false },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data", correct: false },
                { id: 3, text: "Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A global e-commerce platform currently operates its order processing system in a single on-premises data center located in Europe. As the company grows its customer base across Asia and North America, it plans to deploy the application across multiple AWS Regions to improve availability and reduce latency. The company requires that updates to the central order database be completed in under one second with global consistency. The application layer will be deployed separately in each Region, but the order management data must remain centrally managed and globally synchronized. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region", correct: false },
                { id: 1, text: "Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.", correct: false },
                { id: 2, text: "Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions", correct: false },
                { id: 3, text: "Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose", correct: false },
                { id: 1, text: "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 2, text: "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 3, text: "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
            options: [
                { id: 0, text: "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed", correct: false },
                { id: 1, text: "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page", correct: false },
                { id: 2, text: "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: true },
                { id: 3, text: "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A retail enterprise is expanding its hybrid IT infrastructure and plans to securely connect its on-premises corporate network to its AWS environment. The company wants to ensure that all data exchanged between on-premises systems and AWS is encrypted at both the network and session layers. Additionally, the solution must incorporate granular security controls that restrict unnecessary or unauthorized access between the cloud and on-premises environments. A solutions architect must recommend a scalable and secure approach that supports these goals. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems", correct: true },
                { id: 1, text: "Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed", correct: false },
                { id: 2, text: "Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies", correct: false },
                { id: 3, text: "Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
            options: [
                { id: 0, text: "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type", correct: true },
                { id: 1, text: "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type", correct: false },
                { id: 2, text: "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type", correct: false },
                { id: 3, text: "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
            options: [
                { id: 0, text: "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service", correct: false },
                { id: 1, text: "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage", correct: false },
                { id: 2, text: "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises", correct: false },
                { id: 3, text: "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure Amazon EFS to provide a fast, cost-effective and sharable storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
            options: [
                { id: 0, text: "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option", correct: false },
                { id: 3, text: "Use Amazon EC2 instances with Instance Store as the storage option", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 instances with Instance Store as the storage option\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 3, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A logistics company runs a two-step job handling process on AWS. The first step quickly receives job submissions from clients, while the second step requires longer processing time to complete each job. Currently, both steps run on separate Amazon EC2 Auto Scaling groups. However, during high-demand hours, the job processing stage falls behind, and there is concern that jobs may be lost due to instance termination during scaling events. A solutions architect needs to design a more scalable and reliable architecture that preserves job data and accommodates fluctuating demand in both stages. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue", correct: true },
                { id: 1, text: "Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue", correct: false },
                { id: 2, text: "Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior", correct: false },
                { id: 3, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
                { id: 1, text: "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: true },
                { id: 2, text: "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: false },
                { id: 3, text: "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A global enterprise is modernizing its hybrid IT infrastructure to improve both availability and network performance. The company operates a TCP-based application hosted on Amazon EC2 instances that are deployed across multiple AWS Regions, while a secondary UDP-based component of the application is hosted in its on-premises data centers. These application components must be accessed by customers around the world with minimal latency and consistent uptime. Which combination of options should a solutions architect implement for the given use case? (Select two)",
            options: [
                { id: 0, text: "Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers", correct: true },
                { id: 1, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups", correct: false },
                { id: 2, text: "Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover", correct: false },
                { id: 3, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups", correct: true },
                { id: 4, text: "Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers, Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A SaaS analytics company is deploying a microservices-based application on Amazon ECS using the Fargate launch type. The application requires access to a shared, POSIX-compliant file system that is available across multiple Availability Zones for redundancy and availability. To meet compliance requirements, the system must support regional backups and cross-Region data recovery with a recovery point objective (RPO) of no more than 8 hours. A backup strategy will be implemented using AWS Backup to automate replication across Regions. As the lead cloud architect, you are evaluating file storage solutions that align with these requirements. Which option best meets the application’s availability, durability, and RPO objectives?",
            options: [
                { id: 0, text: "Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically", correct: false },
                { id: 1, text: "Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata", correct: false },
                { id: 2, text: "Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region", correct: false },
                { id: 3, text: "Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
                { id: 1, text: "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets", correct: false },
                { id: 2, text: "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection", correct: false },
                { id: 3, text: "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances", correct: false },
                { id: 4, text: "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files, Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A financial auditing firm uses Amazon S3 to store sensitive client records that are subject to write-once-read-many (WORM) regulations to prevent alteration or deletion of records for a specific retention period. The firm wants to enforce immutable storage, such that even administrators cannot overwrite or delete the records during the lock duration. They also need audit-friendly enforcement to prevent accidental or malicious deletion. Which configuration of S3 Object Lock will ensure that the retention policy is strictly enforced, and no user (including root or administrators) can override or delete protected objects during the lock period?",
            options: [
                { id: 0, text: "Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period", correct: true },
                { id: 1, text: "Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period", correct: false },
                { id: 2, text: "Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period", correct: false },
                { id: 3, text: "Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 2, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 3, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 4, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Data moving between the volume and the instance is encrypted, Any snapshot created from the volume is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An enterprise is developing an internal compliance framework for its cloud infrastructure hosted on AWS. The enterprise uses AWS Organizations to group accounts under various organizational units (OUs) based on departmental function. As part of its governance controls, the security team mandates that all Amazon EC2 instances must be tagged to indicate the level of data classification — either 'confidential' or 'public'. Additionally, the organization must ensure that IAM users cannot launch EC2 instances without assigning a classification tag, nor should they be able to remove the tag from running instances. A solutions architect must design a solution to meet these compliance controls while minimizing operational overhead. Which combination of steps will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations", correct: true },
                { id: 1, text: "Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected", correct: false },
                { id: 2, text: "Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts", correct: true },
                { id: 3, text: "Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning", correct: false },
                { id: 4, text: "Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations, Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A financial data processing company runs a workload on Amazon EC2 instances that fetch and process real-time transaction batches from an Amazon SQS queue. The application needs to scale based on unpredictable message volume, which fluctuates significantly throughout the day. The system must process messages with minimal delay and no downtime, even during peak spikes. The company is seeking a solution that balances cost-efficiency with availability and elasticity. Which EC2 purchasing strategy best meets these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes", correct: false },
                { id: 1, text: "Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume", correct: true },
                { id: 2, text: "Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations", correct: false },
                { id: 3, text: "Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB for distributed in-memory cache based session management", correct: false },
                { id: 1, text: "Use Amazon Elasticache for distributed in-memory cache based session management", correct: true },
                { id: 2, text: "Use Amazon RDS for distributed in-memory cache based session management", correct: false },
                { id: 3, text: "Use Application Load Balancer sticky sessions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Elasticache for distributed in-memory cache based session management\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer sticky sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations", correct: true },
                { id: 1, text: "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed", correct: false },
                { id: 2, text: "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user", correct: false },
                { id: 3, text: "Set up an Amazon Route 53 geoproximity routing policy to route traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Route 53 geoproximity routing policy to route traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "A tech company runs a web application that includes multiple internal services deployed across Amazon EC2 instances within a VPC. These services require communication with a third-party SaaS provider's API for analytics and billing, which is also hosted on the AWS infrastructure. The company is concerned about minimizing public internet exposure while maintaining secure and reliable connectivity. The solution must ensure private access without allowing unsolicited incoming traffic from the SaaS provider. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC", correct: true },
                { id: 1, text: "Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider", correct: false },
                { id: 2, text: "Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication", correct: false },
                { id: 3, text: "Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
            options: [
                { id: 0, text: "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 1, text: "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone", correct: false },
                { id: 2, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 3, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A mobile-based e-learning platform is migrating its backend storage layer to Amazon DynamoDB to support a rapidly increasing number of student users and learning transactions. The platform must ensure seamless availability and minimal disruption for a global user base. The DynamoDB design must provide low-latency performance, high availability, and automatic fault tolerance across geographies with the lowest possible operational overhead and cost. Which solution will fulfill these needs in the most cost-efficient manner?",
            options: [
                { id: 0, text: "Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions", correct: false },
                { id: 1, text: "Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions", correct: false },
                { id: 2, text: "Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability", correct: true },
                { id: 3, text: "Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output", correct: false },
                { id: 1, text: "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments", correct: true },
                { id: 2, text: "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
            options: [
                { id: 0, text: "Both the Auto Scaling group and Application Load Balancer are using ALB based health check", correct: false },
                { id: 1, text: "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check", correct: false },
                { id: 2, text: "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check", correct: false },
                { id: 3, text: "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Both the Auto Scaling group and Application Load Balancer are using ALB based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A fintech company recently conducted a security audit and discovered that some IAM roles and Amazon S3 buckets might be unintentionally shared with external accounts or publicly accessible. The security team wants to identify these overly permissive resources and ensure that only intended principals (within their AWS Organization or specific AWS accounts) have access. They need a solution that can analyze IAM policies and resource policies to detect unintended access paths to AWS resources such as S3 buckets, IAM roles, KMS keys, and SNS topics. Which solution should the team use to meet this requirement?",
            options: [
                { id: 0, text: "Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment", correct: false },
                { id: 1, text: "Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization", correct: true },
                { id: 2, text: "Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access", correct: false },
                { id: 3, text: "Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "An enterprise runs a critical Oracle database workload in its on-premises environment. The company now plans to replicate both existing records and continuous transactional changes to a managed Oracle environment in AWS. The target database will run on Amazon RDS for Oracle. Data transfer volume is expected to fluctuate throughout the day, and the team wants the solution to provision compute resources automatically based on actual workload requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected", correct: false },
                { id: 1, text: "Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.", correct: false },
                { id: 2, text: "Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.", correct: false },
                { id: 3, text: "Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two)",
            options: [
                { id: 0, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions", correct: true },
                { id: 1, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region", correct: false },
                { id: 2, text: "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage", correct: false },
                { id: 3, text: "Use cross-Region Read Replicas", correct: true },
                { id: 4, text: "Use the database cloning feature of the Amazon RDS Database cluster", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions, Use cross-Region Read Replicas\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the database cloning feature of the Amazon RDS Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A DevOps team is tasked with enabling secure and temporary SSH access to Amazon EC2 instances for developers during deployments. The team wants to avoid distributing long-term SSH key pairs and instead prefers ephemeral access that can be audited and revoked immediately after the session ends. The team wants direct access via the AWS Management Console. What do you recommend?",
            options: [
                { id: 0, text: "Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address", correct: true },
                { id: 1, text: "Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint", correct: false },
                { id: 2, text: "Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions", correct: false },
                { id: 3, text: "Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through", correct: false },
                { id: 1, text: "Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances", correct: false },
                { id: 2, text: "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through", correct: true },
                { id: 3, text: "Use Application Load Balancer geo match statement listing the countries that you want to block", correct: false },
                { id: 4, text: "Use AWS WAF geo match statement listing the countries that you want to block", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through, Use AWS WAF geo match statement listing the countries that you want to block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer geo match statement listing the countries that you want to block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A digital design company has migrated its project archiving platform to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group that spans multiple Availability Zones. Designers upload and retrieve high-resolution image files from a shared file system, which is currently configured to use Amazon EFS Standard-IA. Metadata for these files is stored and indexed in an Amazon RDS for PostgreSQL database. The company's cloud engineering team has been asked to optimize storage costs for the image archive without compromising reliability. They are open to refactoring the application to use managed AWS services when necessary. Which solution offers the most cost-effective architecture?",
            options: [
                { id: 0, text: "Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path", correct: false },
                { id: 1, text: "Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost", correct: false },
                { id: 2, text: "Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 1, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 3, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A data analytics team at a global media firm is building a new analytics platform to process large volumes of both historical and real-time data. This data is stored in Amazon S3. The team wants to implement a serverless solution that allows them to query the data directly using SQL. Additionally, the solution must ensure that all data is encrypted at rest and automatically replicated to another AWS Region to support business continuity. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data", correct: true },
                { id: 2, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls", correct: false },
                { id: 1, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
                { id: 2, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls", correct: true },
                { id: 3, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company hosts a Microsoft SQL Server database on Amazon EC2 instances with attached Amazon EBS volumes. The operations team takes daily snapshots of these EBS volumes as backups. However, a recent incident occurred in which an automated script designed to clean up expired snapshots accidentally deleted all available snapshots, leading to potential data loss. The company wants to improve the backup strategy to avoid permanent data loss while still ensuring that old snapshots are eventually removed to optimize cost. A solutions architect needs to implement a mechanism that prevents immediate and irreversible deletion of snapshots. Which solution will best meet these requirements with the least development effort?",
            options: [
                { id: 0, text: "Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots", correct: true },
                { id: 1, text: "Set up the IAM policy of the user to deny EBS snapshot deletion", correct: false },
                { id: 2, text: "Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery", correct: false },
                { id: 3, text: "Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up the IAM policy of the user to deny EBS snapshot deletion: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 1, text: "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
            options: [
                { id: 0, text: "Use long polling to handle message processing failures", correct: false },
                { id: 1, text: "Use a dead-letter queue to handle message processing failures", correct: true },
                { id: 2, text: "Use a temporary queue to handle message processing failures", correct: false },
                { id: 3, text: "Use short polling to handle message processing failures", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a dead-letter queue to handle message processing failures\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use long polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a temporary queue to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use short polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting aProvisionedThroughputExceededExceptionexception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
            options: [
                { id: 0, text: "Decrease the Stream retention duration", correct: false },
                { id: 1, text: "Use batch messages", correct: true },
                { id: 2, text: "Increase the number of shards", correct: false },
                { id: 3, text: "Use Exponential Backoff", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use batch messages\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Decrease the Stream retention duration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the number of shards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Exponential Backoff: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two)",
            options: [
                { id: 0, text: "Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target", correct: false },
                { id: 1, text: "Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica", correct: true },
                { id: 2, text: "Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target", correct: true },
                { id: 3, text: "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes", correct: false },
                { id: 4, text: "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica, Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "An enterprise SaaS provider is currently operating a legacy web application hosted on a single Amazon EC2 instance within a public subnet. The same instance also hosts a MySQL database. DNS records for the application are configured through Amazon Route 53. As part of a modernization initiative, the company wants to rearchitect this application for high availability and scalability. In addition, the company wants to improve read performance on the database layer to handle increasing user traffic. Which combination of solutions will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: false },
                { id: 1, text: "Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages", correct: false },
                { id: 2, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: true },
                { id: 3, text: "Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions", correct: false },
                { id: 4, text: "Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly, Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
            options: [
                { id: 0, text: "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 1, text: "Use Amazon Athena to run SQL based analytics against Amazon S3 data", correct: true },
                { id: 2, text: "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 3, text: "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Athena to run SQL based analytics against Amazon S3 data\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A media streaming company expects a major increase in user activity during the launch of a highly anticipated live event. The streaming platform is deployed on AWS and uses Amazon EC2 instances for the application layer and Amazon RDS for persistent storage. The operations team needs to proactively monitor system performance to ensure a smooth user experience during the event. Their monitoring setup must provide data visibility with intervals of no more than 2 minutes, and the team prefers a solution that is quick to implement and low-maintenance. Which solution should the team implement?",
            options: [
                { id: 0, text: "Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena", correct: false },
                { id: 1, text: "Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics", correct: false },
                { id: 2, text: "Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics", correct: false },
                { id: 3, text: "Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
            options: [
                { id: 0, text: "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications", correct: true },
                { id: 1, text: "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications", correct: false },
                { id: 2, text: "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates", correct: false },
                { id: 3, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 1, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
                { id: 3, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A streaming service provider collects user experience feedback through embedded feedback forms in their mobile and web apps. Feedback submissions frequently spike to thousands per hour during content launches or service outages. Currently, the feedback is sent via email to the operations team for manual review. The company now wants to automate feedback collection and sentiment analysis so that insights can be generated quickly and stored for a full year for trend analysis. Which solution provides the most scalable and automated approach to meet these requirements?",
            options: [
                { id: 0, text: "Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item", correct: true },
                { id: 1, text: "Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table", correct: false },
                { id: 2, text: "Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months", correct: false },
                { id: 3, text: "Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
            options: [
                { id: 0, text: "Use Amazon Route 53 with Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Use AWS Firewall Manager with CloudFront distribution", correct: false },
                { id: 2, text: "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution", correct: true },
                { id: 3, text: "Use AWS Security Hub with Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Firewall Manager with CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Security Hub with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A financial analytics firm runs performance-intensive modeling software on Amazon EC2 instances backed by Amazon EBS volumes. The production data resides on EBS volumes attached to EC2 instances in the same AWS Region where the testing environment is hosted. To maintain data integrity, any changes made during testing must not affect production data. The development team needs to frequently create clones of this production data for simulations. The modeling software requires high and consistent I/O performance, and the firm wants to minimize the time required to provision test data. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment", correct: true },
                { id: 1, text: "Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance", correct: false },
                { id: 2, text: "Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations", correct: false },
                { id: 3, text: "Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A multinational logistics company operates its shipment tracking platform from Amazon EC2 instances deployed in the AWS us-west-2 Region. The platform exposes a set of APIs over HTTPS, which are used by logistics partners and customers around the world to retrieve real-time tracking data. The company has observed that users from Europe and Asia experience latency issues and inconsistent API response times when accessing the service. As a cloud architect, you have been tasked to propose the most cost-effective solution to improve performance for these international users without migrating the application. Which solution should you recommend?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups", correct: true },
                { id: 1, text: "Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2", correct: false },
                { id: 2, text: "Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region", correct: false },
                { id: 3, text: "Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A media company operates a web application that enables users to upload photos. These uploads are stored in an Amazon S3 bucket located in the eu-west-2 Region. To enhance performance and provide secure access under a custom domain name, the company wants to integrate Amazon CloudFront for uploads to the S3 bucket. The architecture must support secure HTTPS connections using a custom domain, and the upload process must ensure optimal speed and security. Which combination of actions will fulfill these requirements? (Select two)",
            options: [
                { id: 0, text: "Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution", correct: false },
                { id: 1, text: "Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations", correct: false },
                { id: 2, text: "Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL", correct: false },
                { id: 3, text: "Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)", correct: true },
                { id: 4, text: "Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC), Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A digital media company runs its content rendering service on Amazon EC2 instances that are registered with an Application Load Balancer (ALB) using IP-based target groups. The company relies on AWS Systems Manager to manage and patch these instances regularly. According to new compliance requirements, EC2 instances must be safely removed from production traffic during patching to prevent user disruption and maintain application integrity. However, during the most recent patch cycle, the operations team noticed application failures and API timeouts, even though patching succeeded on the instances. You are asked to suggest a reliable and scalable way to ensure safe patching while preserving service availability. Which solution will best meet the new compliance and operational requirements? (Select two)",
            options: [
                { id: 0, text: "Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window", correct: true },
                { id: 1, text: "Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata", correct: false },
                { id: 2, text: "Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching", correct: true },
                { id: 3, text: "Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes", correct: false },
                { id: 4, text: "Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window, Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "Use Amazon ElastiCache for Memcached", correct: false },
                { id: 2, text: "Use AWS Global Accelerator", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Redis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ElastiCache for Redis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache for Memcached: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test7: [
        {
            id: 1,
            text: "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
            options: [
                { id: 0, text: "On-Demand Instances", correct: false },
                { id: 1, text: "Spot Instances", correct: false },
                { id: 2, text: "Dedicated Instances", correct: true },
                { id: 3, text: "Dedicated Hosts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
            options: [
                { id: 0, text: "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication", correct: false },
                { id: 1, text: "Use the Amazon EC2 instances private IP for the replication", correct: true },
                { id: 2, text: "Create a Private Link between the two Amazon EC2 instances", correct: false },
                { id: 3, text: "Use an Elastic Fabric Adapter (EFA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Amazon EC2 instances private IP for the replication\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between the two Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Elastic Fabric Adapter (EFA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
            options: [
                { id: 0, text: "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: true },
                { id: 1, text: "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity", correct: true },
                { id: 2, text: "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted", correct: false },
                { id: 3, text: "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: false },
                { id: 4, text: "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification, A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
            options: [
                { id: 0, text: "Create a policy and assign it manually to the 50 users", correct: false },
                { id: 1, text: "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA", correct: false },
                { id: 2, text: "Create a group, attach the policy to the group and place the users in the group", correct: true },
                { id: 3, text: "Update the Amazon S3 bucket policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a group, attach the policy to the group and place the users in the group\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a policy and assign it manually to the 50 users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Update the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
            options: [
                { id: 0, text: "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation", correct: false },
                { id: 1, text: "You can specify instance store volumes for an instance when you launch or restart it", correct: false },
                { id: 2, text: "An instance store is a network storage type", correct: false },
                { id: 3, text: "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved", correct: true },
                { id: 4, text: "You can't detach an instance store volume from one instance and attach it to a different instance", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved, You can't detach an instance store volume from one instance and attach it to a different instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can specify instance store volumes for an instance when you launch or restart it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance store is a network storage type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
            options: [
                { id: 0, text: "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process", correct: false },
                { id: 1, text: "Use AWS CLI to run the user data scripts only once while launching the instance", correct: false },
                { id: 2, text: "Run the custom scripts as user data scripts on the Amazon EC2 instances", correct: true },
                { id: 3, text: "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Run the custom scripts as user data scripts on the Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CLI to run the user data scripts only once while launching the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the custom scripts as instance metadata scripts on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions. Which solution best meets these requirements while ensuring the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3", correct: true },
                { id: 1, text: "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx", correct: false },
                { id: 2, text: "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes", correct: false },
                { id: 3, text: "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application. Which solution will most effectively address the performance issues with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users", correct: true },
                { id: 1, text: "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket", correct: false },
                { id: 2, text: "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency", correct: false },
                { id: 3, text: "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
            options: [
                { id: 0, text: "Use AWS Shield Advanced and setup a rate-based rule", correct: false },
                { id: 1, text: "Configure Sticky Sessions on the Application Load Balancer", correct: false },
                { id: 2, text: "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule", correct: true },
                { id: 3, text: "Define a network access control list (network ACL) on your Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Shield Advanced and setup a rate-based rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Sticky Sessions on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define a network access control list (network ACL) on your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: false },
                { id: 1, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 3, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
            options: [
                { id: 0, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "AWS Global Accelerator", correct: false },
                { id: 2, text: "Amazon ElastiCache for Redis", correct: false },
                { id: 3, text: "Amazon ElastiCache for Memcached", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon ElastiCache for Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache for Redis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
            options: [
                { id: 0, text: "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 1, text: "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 2, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2", correct: true },
                { id: 3, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
            options: [
                { id: 0, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: true },
                { id: 3, text: "Amazon Redshift", correct: false },
                { id: 4, text: "Amazon DynamoDB Accelerator (DAX)", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon ElastiCache, Amazon DynamoDB Accelerator (DAX)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
            options: [
                { id: 0, text: "Create a Spot Fleet request", correct: false },
                { id: 1, text: "Assign an Amazon EC2 Instance Role to perform the necessary API calls", correct: true },
                { id: 2, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1", correct: true },
                { id: 3, text: "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group", correct: false },
                { id: 4, text: "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it", correct: true },
                { id: 5, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are: Assign an Amazon EC2 Instance Role to perform the necessary API calls, Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1, Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Spot Fleet request: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
            options: [
                { id: 0, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 1, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 2, text: "Active Directory Connector", correct: false },
                { id: 3, text: "AWS Transit Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Transit Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon Kinesis Data Streams to ingest the data", correct: true },
                { id: 1, text: "Set up AWS Fargate with Amazon ECS to process the data", correct: true },
                { id: 2, text: "Set up AWS Database Migration Service (AWS DMS) to ingest the data", correct: false },
                { id: 3, text: "Set up AWS Lambda with AWS Step Functions to process the data", correct: false },
                { id: 4, text: "Provision Amazon EC2 instances in an Auto Scaling group to process the data", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Set up Amazon Kinesis Data Streams to ingest the data, Set up AWS Fargate with Amazon ECS to process the data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up AWS Database Migration Service (AWS DMS) to ingest the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Lambda with AWS Step Functions to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision Amazon EC2 instances in an Auto Scaling group to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work. Which solution should the company implement to meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access", correct: false },
                { id: 1, text: "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically", correct: false },
                { id: 2, text: "Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets", correct: true },
                { id: 3, text: "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure", correct: true },
                { id: 1, text: "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones", correct: false },
                { id: 2, text: "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.", correct: false },
                { id: 3, text: "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency. Which solution best satisfies these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint", correct: false },
                { id: 1, text: "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway", correct: false },
                { id: 2, text: "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances", correct: false },
                { id: 3, text: "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
            options: [
                { id: 0, text: "Elastic Fabric Adapter (EFA)", correct: true },
                { id: 1, text: "Elastic IP Address (EIP)", correct: false },
                { id: 2, text: "Elastic Network Adapter (ENA)", correct: false },
                { id: 3, text: "Elastic Network Interface (ENI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Elastic Fabric Adapter (EFA)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Elastic IP Address (EIP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Adapter (ENA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Interface (ENI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
            options: [
                { id: 0, text: "enableVpcHostnames", correct: false },
                { id: 1, text: "enableVpcSupport", correct: false },
                { id: 2, text: "enableDnsHostnames", correct: true },
                { id: 3, text: "enableDnsSupport", correct: true },
                { id: 4, text: "enableDnsDomain", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: enableDnsHostnames, enableDnsSupport\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- enableVpcHostnames: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableVpcSupport: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableDnsDomain: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Migrate the Amazon Redshift underlying storage to Amazon S3 IA", correct: false },
                { id: 1, text: "Analyze the cold data with Amazon Athena", correct: true },
                { id: 2, text: "Create a smaller Amazon Redshift Cluster with the cold data", correct: false },
                { id: 3, text: "Move the data to Amazon S3 Glacier Deep Archive after 30 days", correct: false },
                { id: 4, text: "Move the data to Amazon S3 Standard IA after 30 days", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Analyze the cold data with Amazon Athena, Move the data to Amazon S3 Standard IA after 30 days\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Migrate the Amazon Redshift underlying storage to Amazon S3 IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a smaller Amazon Redshift Cluster with the cold data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move the data to Amazon S3 Glacier Deep Archive after 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region. The firm has defined strict DR objectives: Recovery Point Objective (RPO): ≤ 5 minutes Recovery Time Objective (RTO): ≤ 15 minutes Leadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
            options: [
                { id: 0, text: "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes", correct: false },
                { id: 1, text: "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover", correct: false },
                { id: 2, text: "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events", correct: false },
                { id: 3, text: "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
            options: [
                { id: 0, text: "Amazon Aurora Serverless", correct: true },
                { id: 1, text: "Amazon DynamoDB with On-Demand Capacity", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: false },
                { id: 3, text: "Amazon DynamoDB with Provisioned Capacity and Auto Scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Aurora Serverless\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB with On-Demand Capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB with Provisioned Capacity and Auto Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
            options: [
                { id: 0, text: "Set up AWS Direct Connect", correct: false },
                { id: 1, text: "Set up an AWS Site-to-Site VPN connection", correct: true },
                { id: 2, text: "Set up an Internet Gateway between the on-premises data center and AWS cloud", correct: false },
                { id: 3, text: "Set up a bastion host on Amazon EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an AWS Site-to-Site VPN connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Internet Gateway between the on-premises data center and AWS cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host on Amazon EC2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
            options: [
                { id: 0, text: "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group", correct: false },
                { id: 1, text: "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy", correct: true },
                { id: 2, text: "Make the Amazon S3 bucket public", correct: false },
                { id: 3, text: "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make the Amazon S3 bucket public: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed. Which solution meets these requirements most cost-effectively?",
            options: [
                { id: 0, text: "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation", correct: false },
                { id: 1, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: false },
                { id: 2, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: true },
                { id: 3, text: "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Amazon EFS Infrequent Access", correct: true },
                { id: 1, text: "Amazon EFS Standard", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Standard-IA", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EFS Infrequent Access\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon EFS Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
            options: [
                { id: 0, text: "AWS Firewall Manager", correct: false },
                { id: 1, text: "AWS Shield Advanced", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: true },
                { id: 3, text: "AWS Web Application Firewall (AWS WAF)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon GuardDuty\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Firewall Manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Web Application Firewall (AWS WAF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Run on AWS Lambda", correct: false },
                { id: 1, text: "Run on an Application Load Balancer", correct: false },
                { id: 2, text: "Run on Amazon EMR", correct: false },
                { id: 3, text: "Run on a Spot Instance with a persistent request type", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Run on a Spot Instance with a persistent request type\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run on AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application. Which combination of steps will enable secure S3 integration for this workload? (Select two)",
            options: [
                { id: 0, text: "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3", correct: true },
                { id: 1, text: "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID", correct: false },
                { id: 2, text: "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access", correct: false },
                { id: 3, text: "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate", correct: true },
                { id: 4, text: "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3, Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage. Which solution will help improve the application’s responsiveness and scalability during peak load periods?",
            options: [
                { id: 0, text: "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue", correct: false },
                { id: 1, text: "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput", correct: false },
                { id: 2, text: "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue", correct: true },
                { id: 3, text: "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Store the images using the Amazon S3 Standard-IA storage class", correct: false },
                { id: 1, text: "Store the images using the Amazon S3 Intelligent-Tiering storage class", correct: true },
                { id: 2, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
                { id: 3, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Store the images using the Amazon S3 Intelligent-Tiering storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Store the images using the Amazon S3 Standard-IA storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold. What is the MOST operationally efficient way to achieve this?",
            options: [
                { id: 0, text: "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders", correct: true },
                { id: 1, text: "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly", correct: false },
                { id: 2, text: "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops", correct: false },
                { id: 3, text: "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) temporary queues", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS) dead-letter queues", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) delay queues", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS) FIFO queues", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Simple Queue Service (Amazon SQS) temporary queues\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) dead-letter queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) delay queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
            options: [
                { id: 0, text: "Create a Read Replica in another Region as the Master database and point the analytics workload there", correct: false },
                { id: 1, text: "Create a Read Replica in the same Region as the Master database and point the analytics workload there", correct: true },
                { id: 2, text: "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database", correct: false },
                { id: 3, text: "Migrate the analytics application to AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Read Replica in the same Region as the Master database and point the analytics workload there\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Read Replica in another Region as the Master database and point the analytics workload there: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the analytics application to AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans. Which combination of savings plans will satisfy these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly", correct: false },
                { id: 1, text: "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances", correct: true },
                { id: 2, text: "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads", correct: false },
                { id: 3, text: "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type", correct: false },
                { id: 4, text: "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances, Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
            options: [
                { id: 0, text: "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: true },
                { id: 1, text: "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 2, text: "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 3, text: "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
            options: [
                { id: 0, text: "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes", correct: false },
                { id: 1, text: "Storage class analysis only provides recommendations for Standard to Standard IA classes", correct: true },
                { id: 2, text: "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes", correct: false },
                { id: 3, text: "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Storage class analysis only provides recommendations for Standard to Standard IA classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
            options: [
                { id: 0, text: "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby", correct: true },
                { id: 1, text: "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database", correct: false },
                { id: 2, text: "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync", correct: false },
                { id: 3, text: "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason", correct: true },
                { id: 4, text: "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby, Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practices—like mandatory logging and guardrails—when new accounts are created. The firm prefers a low-maintenance, governance-driven setup. Which solution best meets these goals while minimizing operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually", correct: false },
                { id: 1, text: "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance", correct: false },
                { id: 2, text: "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs", correct: false },
                { id: 3, text: "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
            options: [
                { id: 0, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region", correct: false },
                { id: 1, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region", correct: true },
                { id: 2, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region", correct: false },
                { id: 3, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection. Which configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
            options: [
                { id: 0, text: "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center", correct: true },
                { id: 1, text: "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect", correct: false },
                { id: 2, text: "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing", correct: false },
                { id: 3, text: "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
            options: [
                { id: 0, text: "Require HTTPS for communication between Amazon CloudFront and your S3 origin", correct: false },
                { id: 1, text: "Require HTTPS for communication between Amazon CloudFront and your custom origin", correct: false },
                { id: 2, text: "Use Amazon CloudFront signed URLs", correct: true },
                { id: 3, text: "Use Amazon CloudFront signed cookies", correct: true },
                { id: 4, text: "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Amazon CloudFront signed URLs, Use Amazon CloudFront signed cookies\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Require HTTPS for communication between Amazon CloudFront and your S3 origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Require HTTPS for communication between Amazon CloudFront and your custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in read request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 1, text: "Use AWS Global Accelerator", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use AWS Direct Connect", correct: false },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
            options: [
                { id: 0, text: "VPC Route Table", correct: false },
                { id: 1, text: "Amazon Inspector", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: false },
                { id: 3, text: "AWS Shield Advanced", correct: true },
                { id: 4, text: "AWS Web Application Firewall (AWS WAF)", correct: true },
                { id: 5, text: "VPC Security Group", correct: true },
            ],
            correctAnswers: [3, 4, 5],
            explanation: "The correct answers are: AWS Shield Advanced, AWS Web Application Firewall (AWS WAF), VPC Security Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- VPC Route Table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Inspector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon GuardDuty: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon MQ", correct: true },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) Standard", correct: false },
                { id: 3, text: "Amazon SQS FIFO (First-In-First-Out)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SQS FIFO (First-In-First-Out): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 48,
            text: "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning. Which solution will meet these requirements in the MOST efficient and scalable way?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically", correct: false },
                { id: 1, text: "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time", correct: true },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals", correct: false },
                { id: 3, text: "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "VPC Endpoint", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "AWS VPN CloudHub", correct: true },
                { id: 3, text: "Software VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic", correct: true },
                { id: 1, text: "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 2, text: "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store", correct: false },
                { id: 1, text: "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones", correct: false },
                { id: 2, text: "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances", correct: false },
                { id: 3, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 52,
            text: "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
            options: [
                { id: 0, text: "AWS Transit Gateway", correct: true },
                { id: 1, text: "VPC Peering Connection", correct: false },
                { id: 2, text: "Virtual private gateway (VGW)", correct: false },
                { id: 3, text: "AWS PrivateLink", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Transit Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Peering Connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Virtual private gateway (VGW): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS PrivateLink: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring. Which combination of actions will best support this migration approach? (Select three)",
            options: [
                { id: 0, text: "Launch a cutover instance after completing testing and confirming that replication is up-to-date", correct: true },
                { id: 1, text: "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use", correct: false },
                { id: 2, text: "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover", correct: true },
                { id: 3, text: "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs", correct: true },
                { id: 4, text: "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations", correct: false },
                { id: 5, text: "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation", correct: false },
            ],
            correctAnswers: [0, 2, 3],
            explanation: "The correct answers are: Launch a cutover instance after completing testing and confirming that replication is up-to-date, Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover, Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
            options: [
                { id: 0, text: "Throughput Optimized HDD Amazon EBS volumes", correct: false },
                { id: 1, text: "Provisioned IOPS SSD Amazon EBS volumes", correct: true },
                { id: 2, text: "General-purpose SSD-based Amazon EBS volumes", correct: false },
                { id: 3, text: "Cold HDD Amazon EBS volumes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD Amazon EBS volumes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Throughput Optimized HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- General-purpose SSD-based Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 1, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
            options: [
                { id: 0, text: "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS", correct: false },
                { id: 1, text: "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index", correct: false },
                { id: 2, text: "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS", correct: true },
                { id: 3, text: "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone", correct: false },
                { id: 1, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: false },
                { id: 2, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: true },
                { id: 3, text: "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
            options: [
                { id: 0, text: "Client Side Encryption", correct: true },
                { id: 1, text: "Server-side encryption with AWS KMS keys (SSE-KMS)", correct: false },
                { id: 2, text: "Server-side encryption with customer-provided keys (SSE-C)", correct: false },
                { id: 3, text: "Server-side encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Client Side Encryption\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption with AWS KMS keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root user’s email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users. Which solution meets these requirements in the most secure and scalable way?",
            options: [
                { id: 0, text: "Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications", correct: true },
                { id: 1, text: "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts", correct: false },
                { id: 2, text: "Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses", correct: false },
                { id: 3, text: "Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
            options: [
                { id: 0, text: "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC", correct: false },
                { id: 1, text: "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC", correct: true },
                { id: 2, text: "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC", correct: false },
                { id: 3, text: "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 1, text: "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 2, text: "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 3, text: "Use delay queues to postpone the delivery of new messages to the queue for a few seconds", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use delay queues to postpone the delivery of new messages to the queue for a few seconds\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 1, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 2, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue", correct: true },
                { id: 3, text: "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort. What is the most efficient solution to meet this goal with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule", correct: false },
                { id: 1, text: "Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed", correct: true },
                { id: 2, text: "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds", correct: false },
                { id: 3, text: "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
            options: [
                { id: 0, text: "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica", correct: true },
                { id: 1, text: "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
                { id: 2, text: "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica", correct: false },
                { id: 3, text: "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets", correct: false },
                { id: 1, text: "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled", correct: true },
                { id: 2, text: "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket", correct: false },
                { id: 3, text: "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test8: [
        {
            id: 1,
            text: "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)", correct: false },
                { id: 2, text: "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)\n\nAmazon Kinesis is designed for real-time streaming data processing, making it ideal for IoT data ingestion and analytics. It can handle high-throughput, real-time data streams and process them for analytics. Amazon SNS provides push-based notifications to mobile applications, which is exactly what's needed to notify device owners. This combination allows for real-time data processing followed by immediate notification delivery to mobile apps.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis with Amazon Simple Email Service (Amazon SES): SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS): SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
            options: [
                { id: 0, text: "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: true },
                { id: 1, text: "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 2, text: "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 3, text: "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost\n\nLaunch templates are the recommended and modern way to configure Auto Scaling groups with mixed instance types and purchasing options. They support both On-Demand and Spot Instances across multiple instance types, allowing for cost optimization while maintaining performance. Launch configurations are legacy and don't support mixed instance types or Spot Instances with the same flexibility. You cannot use launch configurations for mixed instance types with Spot Instances - only launch templates support this feature.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
            options: [
                { id: 0, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type", correct: false },
                { id: 1, text: "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed", correct: true },
                { id: 2, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance", correct: false },
                { id: 3, text: "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 1, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 2, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
                { id: 3, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 4, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
            options: [
                { id: 0, text: "AWS Lambda", correct: false },
                { id: 1, text: "AWS Step Functions", correct: false },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon CloudWatch", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon Simple Notification Service (Amazon SNS), Amazon CloudWatch\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Step Functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 2, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Remove full database access for all IAM users in the organization", correct: false },
                { id: 1, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Only root user should have full database access in the organization", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to analyze API calls", correct: true },
                { id: 1, text: "Implement an IAM policy to forbid users to change Amazon S3 bucket settings", correct: false },
                { id: 2, text: "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations", correct: false },
                { id: 3, text: "Use Amazon S3 access logs to analyze user access using Athena", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS CloudTrail to analyze API calls\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Implement an IAM policy to forbid users to change Amazon S3 bucket settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 access logs to analyze user access using Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
            options: [
                { id: 0, text: "Amazon Redshift", correct: false },
                { id: 1, text: "Amazon RDS", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: true },
                { id: 3, text: "Amazon ElastiCache", correct: false },
                { id: 4, text: "AWS Lambda", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon DynamoDB, AWS Lambda\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
                { id: 3, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 weighted routing to spread traffic across different deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CodeDeploy deployment options to choose the right deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to distribute traffic across deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
                { id: 1, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 2, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Enable versioning on the Amazon S3 bucket, Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 3, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
            options: [
                { id: 0, text: "Move to Amazon Redshift", correct: false },
                { id: 1, text: "Switch application code to AWS Lambda for better performance", correct: false },
                { id: 2, text: "Setup Amazon ElastiCache in front of Amazon RDS", correct: true },
                { id: 3, text: "Setup Amazon RDS Read Replicas", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Setup Amazon ElastiCache in front of Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Move to Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch application code to AWS Lambda for better performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup Amazon RDS Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 1, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 2, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
            options: [
                { id: 0, text: "Amazon ElastiCache for Redis/Memcached", correct: true },
                { id: 1, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon DocumentDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon ElastiCache for Redis/Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DocumentDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 1, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 1, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 3, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 1, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 2, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 3, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 4, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
                { id: 5, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code, Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator", correct: false },
                { id: 1, text: "Use AWS Direct Connect", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon S3", correct: true },
                { id: 1, text: "Amazon Kinesis", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 4, text: "Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Amazon S3, Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a read replica and modify the application to use the appropriate endpoint\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Activate read-through caching on the Amazon Aurora database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the application to read from the Multi-AZ standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 2, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 3, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 4, text: "The route for the health check is misconfigured", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer, The route for the health check is misconfigured\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
            options: [
                { id: 0, text: "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated", correct: false },
                { id: 1, text: "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume", correct: true },
                { id: 2, text: "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume", correct: false },
                { id: 3, text: "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use Spot Instances", correct: false },
                { id: 1, text: "Use a Cluster placement group", correct: true },
                { id: 2, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
                { id: 1, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 2, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 3, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
            options: [
                { id: 0, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data", correct: false },
                { id: 1, text: "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object", correct: true },
                { id: 2, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data", correct: false },
                { id: 3, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
                { id: 3, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 1, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
            options: [
                { id: 0, text: "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once", correct: true },
                { id: 1, text: "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts", correct: false },
                { id: 2, text: "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs", correct: false },
                { id: 3, text: "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda", correct: false },
                { id: 1, text: "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: false },
                { id: 2, text: "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda", correct: false },
                { id: 3, text: "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon Neptune", correct: true },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon Aurora", correct: false },
                { id: 3, text: "Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
                { id: 1, text: "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 3, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
                { id: 1, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 2, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 3, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 4, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: By default, scripts entered as user data are executed with root user privileges, By default, user data runs only during the boot cycle when you first launch an instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- When an instance is running, you can update user data by using root user credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, scripts entered as user data do not have root user privileges for executing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "AWS Glue", correct: false },
                { id: 1, text: "Amazon EMR", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: true },
                { id: 3, text: "Amazon FSx for Windows File Server", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 2, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 3, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
                { id: 4, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
            options: [
                { id: 0, text: "Use Identity and Access Management (IAM) policies", correct: false },
                { id: 1, text: "Use Amazon S3 Bucket Policies", correct: true },
                { id: 2, text: "Use Security Groups", correct: false },
                { id: 3, text: "Use Access Control Lists (ACLs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon S3 Bucket Policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Identity and Access Management (IAM) policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Security Groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Access Control Lists (ACLs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 1, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 2, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 4, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 5, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are: Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix, Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 1, text: "Use AWS Glue to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 2, text: "Use AWS EMR to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS EMR to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 2, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 3, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 2, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
                { id: 3, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
            options: [
                { id: 0, text: "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs", correct: false },
                { id: 1, text: "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: true },
                { id: 2, text: "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: false },
                { id: 3, text: "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
            options: [
                { id: 0, text: "Use AWS_IAM authorization", correct: false },
                { id: 1, text: "Use Amazon Cognito User Pools", correct: true },
                { id: 2, text: "Use Amazon Cognito Identity Pools", correct: false },
                { id: 3, text: "Use AWS Lambda authorizer for Amazon API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Cognito User Pools\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS_IAM authorization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Identity Pools: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda authorizer for Amazon API Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 1, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 1, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 2, text: "NAT instance supports port forwarding", correct: true },
                { id: 3, text: "NAT gateway supports port forwarding", correct: false },
                { id: 4, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 5, text: "Security Groups can be associated with a NAT gateway", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are: NAT instance can be used as a bastion server, NAT instance supports port forwarding, Security Groups can be associated with a NAT instance\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "It is not possible to access cross-account resources", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
            options: [
                { id: 0, text: "7", correct: false },
                { id: 1, text: "3", correct: true },
                { id: 2, text: "14", correct: false },
                { id: 3, text: "15", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: 3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- 7: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 14: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 15: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 1, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
                { id: 3, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
            options: [
                { id: 0, text: "Distribute the static content through Amazon EFS", correct: false },
                { id: 1, text: "Distribute the dynamic content through Amazon EFS", correct: false },
                { id: 2, text: "Distribute the static content through Amazon S3", correct: true },
                { id: 3, text: "Distribute the dynamic content through Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Distribute the static content through Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Distribute the static content through Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Distribute the dynamic content through Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Distribute the dynamic content through Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Database Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Glue", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 1, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 2, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 3, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 4, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: Any snapshot created from the volume is encrypted, Data moving between the volume and the instance is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "Software VPN", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "VPC Endpoint", correct: false },
                { id: 3, text: "AWS VPN CloudHub", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
            options: [
                { id: 0, text: "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
                { id: 1, text: "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis", correct: false },
                { id: 2, text: "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift", correct: true },
                { id: 3, text: "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 1, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
                { id: 3, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
    test9: [
        {
            id: 1,
            text: "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
            options: [
                { id: 0, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold", correct: false },
                { id: 1, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold", correct: false },
                { id: 2, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold", correct: false },
                { id: 3, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
                { id: 1, text: "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: true },
                { id: 2, text: "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: false },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 2, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 4, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
                { id: 5, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue, Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
            options: [
                { id: 0, text: "Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint", correct: false },
                { id: 1, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: false },
                { id: 2, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: true },
                { id: 3, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: true },
                { id: 4, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint, Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 1, text: "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 2, text: "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 3, text: "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte. As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
            options: [
                { id: 0, text: "Use Amazon EFS with Bursting Throughput mode", correct: false },
                { id: 1, text: "Use Amazon EFS with Provisioned Throughput mode", correct: true },
                { id: 2, text: "Use Amazon DynamoDB table that is accessible by all ECS cluster instances", correct: false },
                { id: 3, text: "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EFS with Provisioned Throughput mode\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EFS with Bursting Throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB table that is accessible by all ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon EBS volume mounted to the Amazon ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead. What solution would best meet these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume", correct: true },
                { id: 1, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
                { id: 2, text: "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume", correct: false },
                { id: 3, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
            options: [
                { id: 0, text: "Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads", correct: true },
                { id: 1, text: "Use Amazon ElastiCache to improve the performance of compute-intensive workloads", correct: true },
                { id: 2, text: "Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads", correct: false },
                { id: 3, text: "Use Amazon ElastiCache to run highly complex JOIN queries", correct: false },
                { id: 4, text: "Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads, Use Amazon ElastiCache to improve the performance of compute-intensive workloads\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to run highly complex JOIN queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
            options: [
                { id: 0, text: "You can use an IP address as the custom source for the inbound rule", correct: false },
                { id: 1, text: "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule", correct: false },
                { id: 2, text: "You can use an Internet Gateway ID as the custom source for the inbound rule", correct: true },
                { id: 3, text: "You can use a security group as the custom source for the inbound rule", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: You can use an Internet Gateway ID as the custom source for the inbound rule\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You can use an IP address as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a security group as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A national logistics company has a dedicated AWS Direct Connect connection from its corporate data center to AWS. Within its AWS account, the company operates 25 Amazon VPCs in the same Region, each supporting different regional distribution services. The VPCs were configured with non-overlapping CIDR blocks and currently use private VIFs for Direct Connect access to on-premises resources. As the architecture scales, the company wants to enable communication across all VPCs and the on-premises environment. The solution must scale efficiently, support full-mesh connectivity, and reduce the complexity of maintaining separate private VIFs for each VPC. Which combination of solutions will best fulfill these requirements with the least amount of operational overhead? (Select two)",
            options: [
                { id: 0, text: "Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs", correct: false },
                { id: 1, text: "Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services", correct: false },
                { id: 2, text: "Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway", correct: true },
                { id: 3, text: "Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing", correct: true },
                { id: 4, text: "Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway, Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic", correct: false },
                { id: 1, text: "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic", correct: false },
                { id: 2, text: "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint", correct: true },
                { id: 3, text: "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A regional transportation authority operates a high-traffic public information portal hosted on AWS. The backend consists of Amazon EC2 instances behind an Application Load Balancer (ALB). In recent weeks, the operations team has observed intermittent slowdowns and performance issues. After investigation, the team suspect the application is being targeted by distributed denial-of-service (DDoS) attacks coming from a wide range of IP addresses. The team needs a solution that provides DDoS mitigation, detailed logs for audit purposes, and requires minimal changes to the existing architecture. Which solution best addresses these needs?",
            options: [
                { id: 0, text: "Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly", correct: false },
                { id: 1, text: "Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events", correct: true },
                { id: 2, text: "Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service", correct: false },
                { id: 1, text: "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users", correct: false },
                { id: 2, text: "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face", correct: true },
                { id: 3, text: "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A healthcare startup is modernizing its monolithic Python-based analytics application by transitioning to a microservices architecture on AWS. As a pilot, the team wants to refactor one module into a standalone microservice that can handle hundreds of requests per second. They are seeking an AWS-native solution that supports Python, scales automatically with traffic, and requires minimal infrastructure management and operational overhead to build, test, and deploy the service efficiently. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup", correct: false },
                { id: 1, text: "Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads", correct: true },
                { id: 2, text: "Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments", correct: false },
                { id: 3, text: "Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A company has its application servers in the public subnet that connect to the database instances in the private subnet. For regular maintenance, the database instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
            options: [
                { id: 0, text: "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC", correct: false },
                { id: 1, text: "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables", correct: false },
                { id: 2, text: "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC", correct: false },
                { id: 3, text: "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Egress-only internet gateway for the resources in the private subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
            options: [
                { id: 0, text: "General Purpose SSD (gp2)", correct: false },
                { id: 1, text: "Provisioned IOPS SSD (io1)", correct: true },
                { id: 2, text: "Throughput Optimized HDD (st1)", correct: false },
                { id: 3, text: "Cold HDD (sc1)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD (io1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose SSD (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Throughput Optimized HDD (st1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD (sc1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer", correct: false },
                { id: 1, text: "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions", correct: false },
                { id: 2, text: "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints", correct: true },
                { id: 3, text: "Configure Elastic IPs for each of the Application Load Balancers in each Region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Elastic IPs for each of the Application Load Balancers in each Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account. Which of the following solutions should a solutions architect configure for the necessary permissions?",
            options: [
                { id: 0, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly", correct: false },
                { id: 1, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly", correct: false },
                { id: 2, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role", correct: false },
                { id: 3, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance\n\nIAM roles provide temporary credentials and are the recommended way to grant permissions to AWS services and applications. They're more secure than access keys as credentials are automatically rotated and don't need to be stored.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
            options: [
                { id: 0, text: "Create a CNAME record", correct: true },
                { id: 1, text: "Create an A record", correct: false },
                { id: 2, text: "Create a PTR record", correct: false },
                { id: 3, text: "Create an Alias Record", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a CNAME record\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an A record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a PTR record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Alias Record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets", correct: false },
                { id: 1, text: "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets", correct: true },
                { id: 2, text: "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture", correct: false },
                { id: 3, text: "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 22,
            text: "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
            options: [
                { id: 0, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: false },
                { id: 1, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each", correct: false },
                { id: 2, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each", correct: false },
                { id: 3, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A financial services firm runs a containerized risk analytics tool in its on-premises data center using Docker. The tool depends on persistent data storage for maintaining customer simulation results and operates on a single host machine where the volume is locally mounted. The infrastructure team is looking to replatform the tool to AWS using a fully managed service because they want to avoid managing EC2 instances, volumes, or underlying servers. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually", correct: false },
                { id: 1, text: "Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically", correct: false },
                { id: 2, text: "Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage", correct: false },
                { id: 3, text: "Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 2, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 3, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
            options: [
                { id: 0, text: "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 1, text: "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 2, text: "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services", correct: true },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A financial services company is modernizing its analytics platform on AWS. Their legacy data processing scripts, built for both Windows and Linux environments, require shared access to a file system that supports Windows ACLs and SMB protocol for compatibility with existing Windows workloads. At the same time, their Linux-based applications need to read from and write to the same shared storage to maintain cross-platform consistency. The solutions architect needs to design a storage solution that allows both Windows and Linux EC2 instances to access the shared file system simultaneously, while preserving Windows-specific features like NTFS permissions and Active Directory (AD) integration. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms", correct: false },
                { id: 1, text: "Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances", correct: false },
                { id: 2, text: "Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads", correct: false },
                { id: 3, text: "Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
            options: [
                { id: 0, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume", correct: true },
                { id: 1, text: "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected", correct: false },
                { id: 2, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 1, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 2, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
            options: [
                { id: 0, text: "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets", correct: false },
                { id: 1, text: "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data", correct: false },
                { id: 2, text: "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively", correct: true },
                { id: 3, text: "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in theus-west-2region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of theus-west-2region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
            options: [
                { id: 0, text: "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts", correct: false },
                { id: 1, text: "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 2, text: "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 3, text: "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
            options: [
                { id: 0, text: "Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions", correct: false },
                { id: 1, text: "Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions", correct: true },
                { id: 2, text: "Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy", correct: false },
                { id: 3, text: "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use a simple scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 1, text: "Use a target tracking scaling policy based on a custom Amazon SQS queue metric", correct: true },
                { id: 2, text: "Use a step scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 3, text: "Use a scheduled scaling policy based on a custom Amazon SQS queue metric", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a target tracking scaling policy based on a custom Amazon SQS queue metric\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a simple scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a step scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a scheduled scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 1, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
                { id: 2, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
            options: [
                { id: 0, text: "Use SQS message timer to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 1, text: "Use SQS long polling to retrieve messages from your Amazon SQS queues", correct: true },
                { id: 2, text: "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 3, text: "Use SQS short polling to retrieve messages from your Amazon SQS queues", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use SQS long polling to retrieve messages from your Amazon SQS queues\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use SQS message timer to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS visibility timeout to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS short polling to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
            options: [
                { id: 0, text: "Service control policy (SCP) affects service-linked roles", correct: false },
                { id: 1, text: "Service control policy (SCP) does not affect service-linked role", correct: true },
                { id: 2, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action", correct: false },
                { id: 3, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action", correct: true },
                { id: 4, text: "Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts", correct: true },
                { id: 5, text: "Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts", correct: false },
            ],
            correctAnswers: [1, 3, 4],
            explanation: "The correct answers are: Service control policy (SCP) does not affect service-linked role, If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action, Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Service control policy (SCP) affects service-linked roles: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
            options: [
                { id: 0, text: "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: false },
                { id: 1, text: "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ", correct: false },
                { id: 2, text: "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: true },
                { id: 3, text: "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
            options: [
                { id: 0, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot", correct: false },
                { id: 1, text: "You cannot share an Amazon Machine Image (AMI) with another AWS account", correct: false },
                { id: 2, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot", correct: true },
                { id: 3, text: "You can share an Amazon Machine Image (AMI) with another AWS account", correct: true },
                { id: 4, text: "You cannot copy an Amazon Machine Image (AMI) across AWS Regions", correct: false },
                { id: 5, text: "You can copy an Amazon Machine Image (AMI) across AWS Regions", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot, You can share an Amazon Machine Image (AMI) with another AWS account, You can copy an Amazon Machine Image (AMI) across AWS Regions\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot share an Amazon Machine Image (AMI) with another AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot copy an Amazon Machine Image (AMI) across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
            options: [
                { id: 0, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy", correct: false },
                { id: 1, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy", correct: true },
                { id: 2, text: "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy", correct: false },
                { id: 3, text: "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 1, text: "Security Groups can be associated with a NAT gateway", correct: false },
                { id: 2, text: "NAT gateway supports port forwarding", correct: false },
                { id: 3, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 4, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 5, text: "NAT instance supports port forwarding", correct: true },
            ],
            correctAnswers: [0, 4, 5],
            explanation: "The correct answers are: Security Groups can be associated with a NAT instance, NAT instance can be used as a bastion server, NAT instance supports port forwarding\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 1, text: "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones", correct: true },
                { id: 2, text: "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 3, text: "Add Amazon EventBridge to decouple the complex architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon EventBridge to decouple the complex architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Glue", correct: false },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Database Migration Service (AWS DMS)", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
            options: [
                { id: 0, text: "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 1, text: "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 2, text: "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 3, text: "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 1, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs", correct: false },
                { id: 2, text: "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data. How will you implement this requirement without adding the overhead of splitting the data into logical groups?",
            options: [
                { id: 0, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: true },
                { id: 1, text: "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data", correct: false },
                { id: 2, text: "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: false },
                { id: 3, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data", correct: false },
                { id: 1, text: "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data", correct: true },
                { id: 2, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data", correct: false },
                { id: 3, text: "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
            options: [
                { id: 0, text: "Amazon Cloud Directory", correct: false },
                { id: 1, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 2, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 3, text: "Active Directory Connector", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Cloud Directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 3, text: "File Gateway Configuration of AWS Storage Gateway", correct: true },
                { id: 4, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon FSx for Windows File Server, File Gateway Configuration of AWS Storage Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic Block Store (Amazon EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A startup is building a serverless microservices architecture where client applications (web and mobile) authenticate users via a third-party OIDC-compliant identity provider. The backend APIs must validate JSON Web Tokens (JWTs) issued by this provider, enforce scope-based access control, and be cost-effective with minimal latency. The development team wants to use a fully managed service that supports JWT validation natively, without writing custom authentication logic. Which solution should the team implement to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider", correct: true },
                { id: 1, text: "Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer", correct: false },
                { id: 2, text: "Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens", correct: false },
                { id: 3, text: "Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects", correct: true },
                { id: 2, text: "Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy", correct: false },
                { id: 4, text: "Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects, Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A media streaming startup is building a set of backend APIs that will be consumed by external mobile applications. To prevent API abuse, protect downstream resources, and ensure fair usage across clients, the architecture must enforce rate limiting and throttling on a per-client basis. The team also wants to define usage quotas and apply different limits to different API consumers. Which solution should the team implement to enforce rate limiting and usage quotas at the API layer?",
            options: [
                { id: 0, text: "Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances", correct: false },
                { id: 1, text: "Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits", correct: false },
                { id: 2, text: "Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client", correct: true },
                { id: 3, text: "Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling", correct: true },
                { id: 1, text: "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas", correct: false },
                { id: 2, text: "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling", correct: false },
                { id: 3, text: "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A geospatial analytics firm recently completed a large-scale seafloor imaging project and used AWS Snowball Edge Storage Optimized devices to collect and transfer over 150 TB of raw sonar data. The firm uses a high-performance computing (HPC) cluster on AWS to process and analyze massive datasets to identify natural resource formations. The processing workloads require sub-millisecond latency, high-throughput, fast and parallel file access to the imported dataset once the Snowball Edge devices are returned to AWS and the data is ingested. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel", correct: false },
                { id: 1, text: "Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data", correct: true },
                { id: 2, text: "Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access", correct: false },
                { id: 3, text: "Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
            options: [
                { id: 0, text: "Use Amazon FSx for Windows File Server as a shared storage solution", correct: true },
                { id: 1, text: "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies", correct: false },
                { id: 2, text: "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution", correct: false },
                { id: 3, text: "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon FSx for Windows File Server as a shared storage solution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic File System (Amazon EFS) as a shared storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway of AWS Storage Gateway to create a hybrid storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 1, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
            options: [
                { id: 0, text: "If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery", correct: false },
                { id: 1, text: "Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance", correct: false },
                { id: 2, text: "During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained", correct: false },
                { id: 3, text: "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery", correct: true },
                { id: 4, text: "A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If your instance has a public IPv4 address, it retains the public IPv4 address after recovery, A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A software company manages a fleet of Amazon EC2 instances that support internal analytics applications. These instances use an IAM role with custom policies to connect to Amazon RDS and AWS Secrets Manager for secure access to credentials and database endpoints. The IT operations team wants to implement a centralized patch management solution that simplifies compliance and security tasks. Their goal is to automate OS patching across EC2 instances without disrupting the running applications. Which approach will allow the company to meet these goals with the least administrative overhead?",
            options: [
                { id: 0, text: "Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts", correct: false },
                { id: 1, text: "Enable Default Host Management Configuration in AWS Systems Manager Quick Setup", correct: true },
                { id: 2, text: "Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features", correct: false },
                { id: 3, text: "Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Default Host Management Configuration in AWS Systems Manager Quick Setup\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
            options: [
                { id: 0, text: "If a spot request is persistent, then it is opened again after your Spot Instance is interrupted", correct: true },
                { id: 1, text: "Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: true },
                { id: 2, text: "When you cancel an active spot request, it terminates the associated instance as well", correct: false },
                { id: 3, text: "Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: false },
                { id: 4, text: "If a spot request is persistent, then it is opened again after you stop the Spot Instance", correct: false },
                { id: 5, text: "When you cancel an active spot request, it does not terminate the associated instance", correct: true },
            ],
            correctAnswers: [0, 1, 5],
            explanation: "The correct answers are: If a spot request is persistent, then it is opened again after your Spot Instance is interrupted, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, When you cancel an active spot request, it does not terminate the associated instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- When you cancel an active spot request, it terminates the associated instance as well: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a spot request is persistent, then it is opened again after you stop the Spot Instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
            options: [
                { id: 0, text: "Internet Gateway (I1)", correct: true },
                { id: 1, text: "Route Table (R1)", correct: false },
                { id: 2, text: "Subnet (S1)", correct: false },
                { id: 3, text: "Network Address Translation (NAT) instance (N1)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Internet Gateway (I1)\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Route Table (R1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subnet (S1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Network Address Translation (NAT) instance (N1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 3, text: "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
            options: [
                { id: 0, text: "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job", correct: true },
                { id: 1, text: "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script", correct: false },
                { id: 2, text: "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
                { id: 3, text: "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions", correct: false },
                { id: 1, text: "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions", correct: true },
                { id: 2, text: "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions", correct: false },
                { id: 3, text: "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
            options: [
                { id: 0, text: "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data", correct: false },
                { id: 1, text: "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written", correct: false },
                { id: 2, text: "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written", correct: false },
                { id: 3, text: "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
            options: [
                { id: 0, text: "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ", correct: false },
                { id: 1, text: "Create a read replica and connect the report generation tool/application to it", correct: true },
                { id: 2, text: "Migrate from General Purpose SSD to magnetic storage to enhance IOPS", correct: false },
                { id: 3, text: "Increase the size of Amazon RDS instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a read replica and connect the report generation tool/application to it\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate from General Purpose SSD to magnetic storage to enhance IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the size of Amazon RDS instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test10: [
        {
            id: 1,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 2, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 3, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
                { id: 4, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
            options: [
                { id: 0, text: "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions", correct: true },
                { id: 2, text: "Use Amazon EC2 user data to speed up the installation process", correct: false },
                { id: 3, text: "Store the installation files in Amazon S3 for quicker retrieval", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to speed up the installation process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the installation files in Amazon S3 for quicker retrieval: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
            options: [
                { id: 0, text: "Set up Amazon DynamoDB table with a global secondary index", correct: false },
                { id: 1, text: "Set up Amazon DynamoDB table in the on-demand capacity mode", correct: true },
                { id: 2, text: "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled", correct: false },
                { id: 3, text: "Set up Amazon DynamoDB global table in the provisioned capacity mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up Amazon DynamoDB table in the on-demand capacity mode\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon DynamoDB table with a global secondary index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB global table in the provisioned capacity mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
            options: [
                { id: 0, text: "EXAMPLE.COM", correct: false },
                { id: 1, text: "example.test.com", correct: false },
                { id: 2, text: "test.example.com", correct: true },
                { id: 3, text: "example.com", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: test.example.com\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- EXAMPLE.COM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.test.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 1, text: "Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
                { id: 3, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
            options: [
                { id: 0, text: "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels", correct: true },
                { id: 1, text: "Use AWS Global Accelerator for the VPN connection to maximize the throughput", correct: false },
                { id: 2, text: "Use Transfer Acceleration for the VPN connection to maximize the throughput", correct: false },
                { id: 3, text: "Create a virtual private gateway with equal cost multipath routing and multiple channels", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transfer Acceleration for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway with equal cost multipath routing and multiple channels: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Dedicated hosts for the minimum capacity", correct: false },
                { id: 1, text: "Set the minimum capacity to 3", correct: false },
                { id: 2, text: "Use Reserved Instances (RIs) for the minimum capacity", correct: true },
                { id: 3, text: "Set the minimum capacity to 2", correct: true },
                { id: 4, text: "Set the minimum capacity to 1", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Reserved Instances (RIs) for the minimum capacity, Set the minimum capacity to 2\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Dedicated hosts for the minimum capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 1: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
            options: [
                { id: 0, text: "Use a wildcard Secure Sockets Layer certificate (SSL certificate)", correct: false },
                { id: 1, text: "Use Secure Sockets Layer certificate (SSL certificate) with SNI", correct: true },
                { id: 2, text: "Change the Elastic Load Balancing (ELB) SSL Security Policy", correct: false },
                { id: 3, text: "Use an HTTP to HTTPS redirect", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Secure Sockets Layer certificate (SSL certificate) with SNI\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a wildcard Secure Sockets Layer certificate (SSL certificate): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the Elastic Load Balancing (ELB) SSL Security Policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an HTTP to HTTPS redirect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Spot Instances", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon EC2 Reserved Instances (RIs)", correct: false },
                { id: 4, text: "Amazon EC2 On-Demand Instances", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Amazon EC2 Spot Instances, Amazon Simple Queue Service (Amazon SQS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Reserved Instances (RIs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
            options: [
                { id: 0, text: "The Time To Live (TTL) is still in effect", correct: true },
                { id: 1, text: "The health checks are failing", correct: false },
                { id: 2, text: "The Alias Record is misconfigured", correct: false },
                { id: 3, text: "The CNAME Record is misconfigured", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The Time To Live (TTL) is still in effect\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The health checks are failing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Alias Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CNAME Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size10.0.1.0/24and the Auto Scaling group is deployed in a subnet of size10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
            options: [
                { id: 0, text: "Add a rule to authorize the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "Add a rule to authorize the CIDR 10.0.1.0/24", correct: false },
                { id: 2, text: "Add a rule to authorize the security group of the Auto Scaling group", correct: false },
                { id: 3, text: "Add a rule to authorize the CIDR 10.0.4.0/22", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Add a rule to authorize the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Add a rule to authorize the CIDR 10.0.1.0/24: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the security group of the Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the CIDR 10.0.4.0/22: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A fintech startup hosts its real-time transaction metadata in Amazon DynamoDB tables. During a recent system maintenance event, a junior engineer accidentally deleted a production table, resulting in major service downtime and irreversible data loss. Leadership has mandated an immediate solution that prevents future data loss from human error, while requiring minimal ongoing maintenance or manual effort from the engineering team. Which approach best addresses these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Enable deletion protection on DynamoDB tables", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3", correct: false },
                { id: 2, text: "Enable point-in-time recovery (PITR) on each DynamoDB table", correct: false },
                { id: 3, text: "Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable deletion protection on DynamoDB tables\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable point-in-time recovery (PITR) on each DynamoDB table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare provider is experiencing rapid data growth in its on-premises servers due to increased patient imaging and record retention requirements. The organization wants to extend its storage capacity to AWS in a way that preserves quick access to critical records, including from its local file systems. The company must optimize bandwidth usage during migration and avoid any retrieval fees or delays when accessing the data in the cloud. The provider wants a hybrid cloud solution that requires minimal application reconfiguration, allows frequent local access to key datasets, and ensures that cloud storage costs remain predictable without paying extra for data retrieval. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates", correct: false },
                { id: 1, text: "Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement", correct: false },
                { id: 2, text: "Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3", correct: true },
                { id: 3, text: "Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A genomics research firm is processing sporadic bursts of data-intensive workloads using Amazon EC2 instances. The shared storage must support unpredictable spikes in file operations, but the average daily throughput demand remains relatively low. The team has selected Amazon Elastic File System (EFS) for its scalability and wants to ensure optimal cost and performance during bursts without provisioning throughput manually. Which approach should the team take to best meet these requirements?",
            options: [
                { id: 0, text: "Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads", correct: false },
                { id: 1, text: "Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class", correct: true },
                { id: 2, text: "Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode", correct: false },
                { id: 3, text: "Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use a Cluster placement group", correct: true },
                { id: 1, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 2, text: "Use Spot Instances", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
            options: [
                { id: 0, text: "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment", correct: true },
                { id: 1, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration", correct: false },
                { id: 2, text: "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration", correct: false },
                { id: 3, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
            options: [
                { id: 0, text: "Use a Network Address Translation gateway (NAT gateway)", correct: false },
                { id: 1, text: "Use a VPC peering connection", correct: true },
                { id: 2, text: "Use an AWS Direct Connect connection", correct: false },
                { id: 3, text: "Use an Internet Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a VPC peering connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use a Network Address Translation gateway (NAT gateway): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Direct Connect connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Internet Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 18,
            text: "A transportation logistics company runs a shipment tracking application on Amazon EC2 instances with an Amazon Aurora MySQL database cluster. The application is experiencing rapid growth due to increased demand from mobile app users querying package delivery statuses. Although the compute layer (EC2) has remained stable, the Aurora DB cluster is under growing read pressure, especially from frequent repeated queries about package locations and delivery history. The company added an Aurora read replica, which temporarily alleviated the load, but read traffic continues to spike as user queries grow. The company wants to reduce the repeated reads pressure on the DB cluster. Which solution will best meet these requirements in a cost-effective manner?",
            options: [
                { id: 0, text: "Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas", correct: false },
                { id: 1, text: "Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database", correct: true },
                { id: 2, text: "Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions", correct: false },
                { id: 3, text: "Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect – Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
            options: [
                { id: 0, text: "Use field level encryption in Amazon CloudFront to protect sensitive data for specific content", correct: true },
                { id: 1, text: "Amazon CloudFront can route to multiple origins based on the price class", correct: false },
                { id: 2, text: "Use geo restriction to configure Amazon CloudFront for high-availability and failover", correct: false },
                { id: 3, text: "Amazon CloudFront can route to multiple origins based on the content type", correct: true },
                { id: 4, text: "Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content", correct: false },
                { id: 5, text: "Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover", correct: true },
            ],
            correctAnswers: [0, 3, 5],
            explanation: "The correct answers are: Use field level encryption in Amazon CloudFront to protect sensitive data for specific content, Amazon CloudFront can route to multiple origins based on the content type, Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon CloudFront can route to multiple origins based on the price class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use geo restriction to configure Amazon CloudFront for high-availability and failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
            options: [
                { id: 0, text: "Amazon ElastiCache", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 3, text: "Amazon Neptune", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Neptune: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
            options: [
                { id: 0, text: "Pilot Light", correct: false },
                { id: 1, text: "Warm Standby", correct: true },
                { id: 2, text: "Multi Site", correct: false },
                { id: 3, text: "Backup and Restore", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Warm Standby\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Pilot Light: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi Site: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Backup and Restore: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
            options: [
                { id: 0, text: "Access control list (ACL)", correct: false },
                { id: 1, text: "Trust policy", correct: true },
                { id: 2, text: "Permissions boundary", correct: false },
                { id: 3, text: "AWS Organizations Service Control Policies (SCP)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Trust policy\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Access control list (ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Permissions boundary: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Organizations Service Control Policies (SCP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon Kinesis", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon S3", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Amazon DynamoDB, Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
                { id: 1, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 2, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 3, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 4, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 5, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are: Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold, By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
            options: [
                { id: 0, text: "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years", correct: true },
                { id: 1, text: "Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes", correct: false },
                { id: 2, text: "Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs", correct: false },
                { id: 3, text: "Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
            options: [
                { id: 0, text: "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases", correct: false },
                { id: 1, text: "Use AWS CloudFormation to manage Amazon RDS databases", correct: true },
                { id: 2, text: "Attach an IAM policy to interns preventing them from creating an Amazon RDS database", correct: false },
                { id: 3, text: "Store your recommendations in a custom AWS Trusted Advisor rule", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation to manage Amazon RDS databases\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy to interns preventing them from creating an Amazon RDS database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store your recommendations in a custom AWS Trusted Advisor rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator", correct: true },
                { id: 1, text: "Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer", correct: false },
                { id: 3, text: "Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs):AZ-AandAZ-B. Cross-zone load balancing is disabled.AZ-Ahas four targets andAZ-Bhas six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
            options: [
                { id: 0, text: "Each of the four targets in AZ-A receives 12.5% of the traffic", correct: true },
                { id: 1, text: "Each of the six targets in AZ-B receives 10% of the traffic", correct: false },
                { id: 2, text: "Each of the four targets in AZ-A receives 10% of the traffic", correct: false },
                { id: 3, text: "Each of the four targets in AZ-A receives 8% of the traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Each of the four targets in AZ-A receives 12.5% of the traffic\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Each of the six targets in AZ-B receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 8% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 30,
            text: "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
            options: [
                { id: 0, text: "Use Amazon RDS Read Replicas", correct: true },
                { id: 1, text: "Use Amazon DynamoDB", correct: false },
                { id: 2, text: "Use Amazon ElastiCache", correct: false },
                { id: 3, text: "Use Amazon RDS Multi-AZ feature", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon RDS Read Replicas\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Multi-AZ feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A global pharmaceutical company operates a hybrid cloud network. Its primary AWS workloads run in the us-west-2 Region, connected to its on-premises data center via an AWS Direct Connect connection. After acquiring a biotech firm headquartered in Europe, the company must integrate the biotech’s workloads, which are hosted in several VPCs in the eu-central-1 Region and connected to the biotech's on-premises facility through a separate Direct Connect link. All CIDR blocks are non-overlapping, and the business requires full connectivity between both data centers and all VPCs across the two Regions. The company also wants a scalable solution that minimizes manual network configuration and long-term operational overhead. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication", correct: false },
                { id: 1, text: "Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions", correct: true },
                { id: 2, text: "Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic", correct: false },
                { id: 3, text: "Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon OpenSearch Service", correct: false },
                { id: 1, text: "Amazon Redshift", correct: false },
                { id: 2, text: "Amazon Neptune", correct: true },
                { id: 3, text: "Amazon Aurora", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
            options: [
                { id: 0, text: "Change the Amazon EC2 instance type to something much smaller", correct: false },
                { id: 1, text: "Keep the Amazon EBS volume to io1 and reduce the IOPS", correct: false },
                { id: 2, text: "Convert the Amazon EC2 instance EBS volume to gp2", correct: true },
                { id: 3, text: "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Convert the Amazon EC2 instance EBS volume to gp2\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Change the Amazon EC2 instance type to something much smaller: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Keep the Amazon EBS volume to io1 and reduce the IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs).AZ-Ahas 3 Amazon EC2 instances andAZ-Bhas 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
            options: [
                { id: 0, text: "A random instance in the AZ-A will be terminated", correct: false },
                { id: 1, text: "A random instance will be terminated in AZ-B", correct: false },
                { id: 2, text: "An instance in the AZ-A will be created", correct: false },
                { id: 3, text: "The instance with the oldest launch template or launch configuration will be terminated in AZ-B", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The instance with the oldest launch template or launch configuration will be terminated in AZ-B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- A random instance in the AZ-A will be terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A random instance will be terminated in AZ-B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance in the AZ-A will be created: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
            options: [
                { id: 0, text: "Use a Classic Load Balancer with an Auto Scaling Group", correct: false },
                { id: 1, text: "Use an Application Load Balancer with an Auto Scaling Group", correct: false },
                { id: 2, text: "Use a Network Load Balancer with an Auto Scaling Group", correct: true },
                { id: 3, text: "Use an Auto Scaling Group with Dynamic Elastic IPs attachment", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use a Network Load Balancer with an Auto Scaling Group\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a Classic Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling Group with Dynamic Elastic IPs attachment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
            options: [
                { id: 0, text: "Use a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 1, text: "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 2, text: "Use permissions boundary to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 3, text: "Use a bucket policy to grant permission to users in its account as well as to users in another account", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a bucket policy to grant permission to users in its account as well as to users in another account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use permissions boundary to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "You have an Amazon S3 bucket that contains files in two different folders -s3://my-bucket/imagesands3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
            options: [
                { id: 0, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days", correct: false },
                { id: 1, text: "Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days", correct: false },
                { id: 2, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days", correct: false },
                { id: 3, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days", correct: true },
                { id: 4, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days, Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
            options: [
                { id: 0, text: "Deploy AWS Lambda in a VPC", correct: false },
                { id: 1, text: "Attach an AWS Identity and Access Management (IAM) role to AWS Lambda", correct: true },
                { id: 2, text: "Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL", correct: true },
                { id: 3, text: "Restrict the Amazon RDS database security group to the AWS Lambda's security group", correct: false },
                { id: 4, text: "Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Attach an AWS Identity and Access Management (IAM) role to AWS Lambda, Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Lambda in a VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Restrict the Amazon RDS database security group to the AWS Lambda's security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A ride-hailing startup has launched a mobile app that matches passengers with nearby drivers based on real-time GPS coordinates. The application backend uses an Amazon RDS for PostgreSQL instance with read replicas to store the latitude and longitude of drivers and passengers. As the service scales, the backend experiences performance bottlenecks during peak hours, especially when thousands of updates and reads occur per second to keep location data current. The company expects its user base to double in the next few months and needs a high-performance, scalable solution that can handle frequent write and read operations with minimal latency. What do you recommend?",
            options: [
                { id: 0, text: "Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration", correct: false },
                { id: 1, text: "Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards", correct: false },
                { id: 2, text: "Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy", correct: true },
                { id: 3, text: "Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 1, text: "The route for the health check is misconfigured", correct: true },
                { id: 2, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 3, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 4, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: The route for the health check is misconfigured, The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A digital media platform is preparing to launch a new interactive content service that is expected to receive sudden spikes in user engagement, especially during live events and media releases. The backend uses an Amazon Aurora PostgreSQL Serverless v2 cluster to handle dynamic workloads. The architecture must be capable of scaling both compute and storage performance to maintain low latency and avoid bottlenecks under load. The engineering team is evaluating storage configuration options and wants a solution that will scale automatically with traffic, optimize I/O performance, and remain cost-effective without manual provisioning or tuning. Which configuration will best meet these requirements?",
            options: [
                { id: 0, text: "Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage", correct: false },
                { id: 1, text: "Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes", correct: false },
                { id: 2, text: "Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks", correct: false },
                { id: 3, text: "Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 2, text: "Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon MQ", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
            options: [
                { id: 0, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time", correct: false },
                { id: 1, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3", correct: true },
                { id: 2, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3", correct: false },
                { id: 3, text: "Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
            options: [
                { id: 0, text: "Latency-based routing", correct: false },
                { id: 1, text: "Geolocation routing", correct: false },
                { id: 2, text: "Geoproximity routing", correct: true },
                { id: 3, text: "Weighted routing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Geoproximity routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Latency-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Geolocation routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Weighted routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A financial services company is implementing two separate data retention policies to comply with regulatory standards: Policy A: Critical transaction records must be immediately available for audit and must not be deleted or overwritten for 7 years. Policy B: Archived compliance data must be stored in a low-cost, long-term storage solution and locked from deletion or modification for at least 10 years. As a solutions architect, which combination of AWS features should you recommend to enforce these policies effectively?",
            options: [
                { id: 0, text: "Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B", correct: false },
                { id: 1, text: "Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B", correct: true },
                { id: 2, text: "Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention", correct: false },
                { id: 3, text: "Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
            options: [
                { id: 0, text: "Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables", correct: false },
                { id: 1, text: "Contact AWS support to map your VPC with subnet", correct: false },
                { id: 2, text: "Check if the security groups allow ping from the source", correct: true },
                { id: 3, text: "Disable Source / Destination check on the Amazon EC2 instance", correct: false },
                { id: 4, text: "Check if the route table is configured with internet gateway", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Check if the security groups allow ping from the source, Check if the route table is configured with internet gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Contact AWS support to map your VPC with subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable Source / Destination check on the Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
            options: [
                { id: 0, text: "Enable Amazon API Gateway Caching", correct: true },
                { id: 1, text: "Switch to using an Application Load Balancer", correct: false },
                { id: 2, text: "Add Amazon Aurora Read Replicas", correct: false },
                { id: 3, text: "Enable AWS Lambda In Memory Caching", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon API Gateway Caching\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Switch to using an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Aurora Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Lambda In Memory Caching: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy?",
            options: [
                { id: 0, text: "It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket", correct: false },
                { id: 1, text: "It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket", correct: true },
                { id: 2, text: "It ensures Amazon EC2 instances that have inherited a security group can access the bucket", correct: false },
                { id: 3, text: "It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures Amazon EC2 instances that have inherited a security group can access the bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Egress Only Internet Gateway as a backup connection", correct: false },
                { id: 1, text: "Use AWS Site-to-Site VPN as a backup connection", correct: true },
                { id: 2, text: "Use AWS Direct Connect connection as a primary connection", correct: true },
                { id: 3, text: "Use AWS Site-to-Site VPN as a primary connection", correct: false },
                { id: 4, text: "Use AWS Direct Connect connection as a backup connection", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use AWS Site-to-Site VPN as a backup connection, Use AWS Direct Connect connection as a primary connection\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Egress Only Internet Gateway as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Site-to-Site VPN as a primary connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect connection as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this AWS CloudFormation snippet do? (Select three)",
            options: [
                { id: 0, text: "It lets traffic flow from one IP on port 22", correct: true },
                { id: 1, text: "It configures a security group's outbound rules", correct: false },
                { id: 2, text: "It configures a security group's inbound rules", correct: true },
                { id: 3, text: "It configures the inbound rules of a network access control list (network ACL)", correct: false },
                { id: 4, text: "It only allows the IP 0.0.0.0 to reach HTTP", correct: false },
                { id: 5, text: "It allows any IP to pass through on the HTTP port", correct: true },
                { id: 6, text: "It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1", correct: false },
            ],
            correctAnswers: [0, 2, 5],
            explanation: "The correct answers are: It lets traffic flow from one IP on port 22, It configures a security group's inbound rules, It allows any IP to pass through on the HTTP port\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- It configures a security group's outbound rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It configures the inbound rules of a network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It only allows the IP 0.0.0.0 to reach HTTP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "An e-commerce analytics company is preparing to archive several years of transaction records and customer analytics reports in Amazon S3 for long-term storage. To meet compliance requirements, the archived data must be encrypted at rest. Additionally, the solution must be cost-effective and ensure that key rotation occurs automatically every 12 months to comply with the company’s internal data governance policy. Which solution will meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket", correct: true },
                { id: 1, text: "Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow", correct: false },
                { id: 2, text: "Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings", correct: false },
                { id: 3, text: "Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 1, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 2, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 3, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
                { id: 4, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
            options: [
                { id: 0, text: "Select an Elastic Inference accelerator while launching Amazon EC2 instances", correct: false },
                { id: 1, text: "Select the appropriate capacity reservation while launching Amazon EC2 instances", correct: false },
                { id: 2, text: "Select dedicated instance tenancy while launching Amazon EC2 instances", correct: false },
                { id: 3, text: "Select a cluster placement group while launching Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Select a cluster placement group while launching Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Select an Elastic Inference accelerator while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select the appropriate capacity reservation while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select dedicated instance tenancy while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
            options: [
                { id: 0, text: "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure", correct: true },
                { id: 1, text: "Schedule manual backups using Redis append-only file (AOF)", correct: false },
                { id: 2, text: "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure", correct: false },
                { id: 3, text: "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Schedule manual backups using Redis append-only file (AOF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Schedule daily automatic backups at a time when you expect low resource utilization for your cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 1, text: "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks", correct: true },
                { id: 2, text: "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 3, text: "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 56,
            text: "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
            options: [
                { id: 0, text: "Use AWS Snowball", correct: false },
                { id: 1, text: "Use multi-part upload feature of Amazon S3", correct: true },
                { id: 2, text: "Use AWS Direct Connect to provide extra bandwidth", correct: false },
                { id: 3, text: "Use Amazon S3 Versioning", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use multi-part upload feature of Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Snowball: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to provide extra bandwidth: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Versioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 57,
            text: "A media company operates a video rendering pipeline on Amazon EKS, where containerized jobs are scheduled using Kubernetes deployments. The application experiences bursty traffic patterns, particularly during peak streaming hours. The platform uses the Kubernetes Horizontal Pod Autoscaler (HPA) to scale pods based on CPU utilization. A solutions architect observes that the total number of EC2 worker nodes remains constant during traffic spikes, even when all nodes are at maximum resource utilization. The company needs a solution that enables automatic scaling of the underlying compute infrastructure when pod demand exceeds cluster capacity. Which solution should the architect implement to resolve this issue with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure", correct: false },
                { id: 1, text: "Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands", correct: true },
                { id: 2, text: "Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group", correct: false },
                { id: 3, text: "Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two)",
            options: [
                { id: 0, text: "Use Amazon EMR for serverless orchestration of the containerized services", correct: false },
                { id: 1, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 3, text: "Use Amazon SageMaker for serverless orchestration of the containerized services", correct: false },
                { id: 4, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services, Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EMR for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
            options: [
                { id: 0, text: "Use an Amazon CloudFront distribution in front of your website", correct: false },
                { id: 1, text: "Use an Auto Scaling Group", correct: true },
                { id: 2, text: "Use an Amazon Route 53 Multi Value record", correct: false },
                { id: 3, text: "Deploy the website on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon CloudFront distribution in front of your website: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Route 53 Multi Value record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the website on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A streaming media company operates a high-traffic content delivery platform on AWS. The application backend is deployed on Amazon EC2 instances within an Auto Scaling group across multiple Availability Zones in a VPC. The team has observed that workloads follow predictable usage patterns, such as higher viewership on weekends and in the evenings, along with occasional real-time spikes due to viral content. To reduce cost and improve responsiveness, the team wants an automated scaling approach that can forecast future demand using historical usage patterns, scale in advance based on those predictions, and react quickly to unplanned usage surges in real time. Which scaling strategy should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments", correct: false },
                { id: 1, text: "Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes", correct: true },
                { id: 2, text: "Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours", correct: false },
                { id: 3, text: "Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture", correct: false },
                { id: 1, text: "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture", correct: false },
                { id: 3, text: "Use Amazon EventBridge to decouple the system architecture", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EventBridge to decouple the system architecture\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) for effective decoupling of system architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A research organization is running a high-performance computing (HPC) workload using Amazon EC2 instances that are distributed across multiple Availability Zones (AZs) within a single AWS Region. The workload requires access to a shared file system with the lowest possible latency for frequent reads and writes.The team decides to use Amazon Elastic File System (Amazon EFS) for its scalability and simplicity. To ensure optimal performance and reduce network latency, the solution architect must design the architecture so that each EC2 instance can access the file system with the least possible delay. Which of the following is the most appropriate solution to meet these requirements?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones", correct: false },
                { id: 1, text: "Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target", correct: false },
                { id: 2, text: "Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances", correct: false },
                { id: 3, text: "Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A global insurance company is modernizing its infrastructure by migrating multiple line-of-business applications from its on-premises data centers to AWS. These applications will be deployed across several AWS accounts, all governed under a centralized AWS Organizations structure. The company manages all user identities, groups, and access policies within its on-premises Microsoft Active Directory and wants to continue doing so. The goal is to enable seamless single sign-in across all AWS accounts without duplicating user identity stores or manually provisioning accounts. Which solution best meets these requirements in the most operationally efficient manner?",
            options: [
                { id: 0, text: "Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts", correct: false },
                { id: 1, text: "Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider", correct: false },
                { id: 2, text: "Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts", correct: true },
                { id: 3, text: "Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Streams", correct: false },
                { id: 1, text: "Use Amazon DynamoDB DAX", correct: true },
                { id: 2, text: "Use Amazon DynamoDB Global Tables", correct: false },
                { id: 3, text: "Use Amazon ElastiCache", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon DynamoDB DAX\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test11: [
        {
            id: 1,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
            options: [
                { id: 0, text: "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch", correct: true },
                { id: 1, text: "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3", correct: false },
                { id: 2, text: "Disable the Termination from the Auto Scaling Group any time a user reports an issue", correct: false },
                { id: 3, text: "Make a snapshot of the Amazon EC2 instance just before it gets terminated", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable the Termination from the Auto Scaling Group any time a user reports an issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make a snapshot of the Amazon EC2 instance just before it gets terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too", correct: false },
                { id: 1, text: "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data", correct: false },
                { id: 2, text: "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures", correct: true },
                { id: 3, text: "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
            options: [
                { id: 0, text: "The application will be down until the primary database has recovered itself", correct: false },
                { id: 1, text: "The URL to access the database will change to the standby database", correct: false },
                { id: 2, text: "An email will be sent to the System Administrator asking for manual intervention", correct: false },
                { id: 3, text: "The CNAME record will be updated to point to the standby database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The CNAME record will be updated to point to the standby database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- The application will be down until the primary database has recovered itself: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The URL to access the database will change to the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An email will be sent to the System Administrator asking for manual intervention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A healthcare startup runs a lightweight reporting application on a single Amazon EC2 On-Demand instance. The application is designed to be stateless, fault-tolerant, and optimized for fast rendering of analytics dashboards. During major health events or news cycles, the team observes latency issues and occasional 5xx errors due to traffic spikes. To meet growing demand without over-provisioning resources during off-peak hours, the company wants to implement a cost-effective, scalable solution that ensures consistent performance even under unpredictable load. Which approach best meets the requirements while minimizing costs?",
            options: [
                { id: 0, text: "Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%", correct: false },
                { id: 1, text: "Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly", correct: false },
                { id: 2, text: "Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load", correct: false },
                { id: 3, text: "Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two)",
            options: [
                { id: 0, text: "The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic", correct: true },
                { id: 1, text: "The subnet has been configured to be public and has no access to the internet", correct: false },
                { id: 2, text: "The instance's subnet is not associated with any route table", correct: false },
                { id: 3, text: "The route table in the instance’s subnet should have a route to an Internet Gateway", correct: true },
                { id: 4, text: "The instance's subnet is associated with multiple route tables with conflicting configurations", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic, The route table in the instance’s subnet should have a route to an Internet Gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The subnet has been configured to be public and has no access to the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is not associated with any route table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is associated with multiple route tables with conflicting configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A digital content production company has transitioned all of its media assets to Amazon S3 in an effort to reduce storage costs. However, the rendering engine used in production continues to run in an on-premises data center and requires frequent and low-latency access to large media files. The company wants to implement a storage solution that maintains application performance while keeping costs low. Which approach should the company choose to meet these requirements in the most cost-effective way?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket", correct: false },
                { id: 1, text: "Set up an Amazon S3 File Gateway to provide storage for the on-premises application", correct: true },
                { id: 2, text: "Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel", correct: false },
                { id: 3, text: "Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 File Gateway to provide storage for the on-premises application\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
            options: [
                { id: 0, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 1, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation", correct: true },
                { id: 2, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A digital media startup allows users to submit images through its web portal. These images are uploaded directly into an Amazon S3 bucket. On average, around 200 images are uploaded daily. The company wants to automatically generate a smaller preview version (thumbnail) of each new image and store the resulting thumbnails in a separate Amazon S3 bucket. The team prefers a design that is low-cost, requires minimal infrastructure management, and automatically reacts to new uploads. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail", correct: false },
                { id: 1, text: "Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket", correct: true },
                { id: 2, text: "Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket", correct: false },
                { id: 3, text: "Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A digital publishing platform stores large volumes of media assets (such as images and documents) in an Amazon S3 bucket. These assets are accessed frequently during business hours by internal editors and content delivery tools. The company has strict encryption policies and currently uses AWS KMS to handle server-side encryption. The cloud operations team notices that AWS KMS request costs are increasing significantly due to the high frequency of object uploads and accesses. The team is now looking for a way to maintain the same encryption method but reduce the cost of KMS usage, especially for frequent access patterns. Which solution meets the company's encryption and cost optimization goals?",
            options: [
                { id: 0, text: "Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object", correct: true },
                { id: 1, text: "Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control", correct: false },
                { id: 2, text: "Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges", correct: false },
                { id: 3, text: "Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
            options: [
                { id: 0, text: "The database user credentials (username and password) configured for the application are incorrect", correct: false },
                { id: 1, text: "The database user credentials (username and password) configured for the application do not have the required privilege for the given database", correct: false },
                { id: 2, text: "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers", correct: true },
                { id: 3, text: "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The database user credentials (username and password) configured for the application are incorrect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The database user credentials (username and password) configured for the application do not have the required privilege for the given database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A biomedical research firm operates a file exchange system for external research partners to upload and download experimental data. Currently, the system runs on two Amazon EC2 Linux instances, each configured with Elastic IP addresses to allow access from trusted IPs. File transfers use the SFTP protocol, and Linux user accounts are manually provisioned to enforce file-level access control. Data is stored on a shared file system mounted to both EC2 instances. The firm wants to modernize the solution to a fully managed, serverless model with high IOPS, fine-grained user permission control, and strict IP-based access restrictions. They also want to reduce operational overhead without sacrificing performance or security. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings", correct: false },
                { id: 1, text: "Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses", correct: false },
                { id: 2, text: "Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF", correct: false },
                { id: 3, text: "Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "An organization operates a legacy reporting tool hosted on an Amazon EC2 instance located within a public subnet of a VPC. This tool aggregates scanned PDF reports from field devices and temporarily stores them on an attached Amazon EBS volume. At the end of each day, the tool transfers the accumulated files to an Amazon S3 bucket for archival. A solutions architect identifies that the files are being uploaded over the internet using S3's public endpoint. To improve security and avoid exposing data traffic to the public internet, the architect needs to reconfigure the setup so that uploads to Amazon S3 occur privately without using the public S3 endpoint. Which solution will fulfill these requirements?",
            options: [
                { id: 0, text: "Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data", correct: false },
                { id: 1, text: "Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint", correct: true },
                { id: 2, text: "Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately", correct: false },
                { id: 3, text: "Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A fintech company currently operates a real-time search and analytics platform on-premises. This platform ingests streaming data from multiple data-producing systems and provides immediate search capabilities and interactive visualizations for end users. As part of its cloud migration strategy, the company wants to rearchitect the solution using AWS-native services. Which of the following represents the most efficient solution?",
            options: [
                { id: 0, text: "Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations", correct: false },
                { id: 1, text: "Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights", correct: false },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data", correct: false },
                { id: 3, text: "Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A global e-commerce platform currently operates its order processing system in a single on-premises data center located in Europe. As the company grows its customer base across Asia and North America, it plans to deploy the application across multiple AWS Regions to improve availability and reduce latency. The company requires that updates to the central order database be completed in under one second with global consistency. The application layer will be deployed separately in each Region, but the order management data must remain centrally managed and globally synchronized. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region", correct: false },
                { id: 1, text: "Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.", correct: false },
                { id: 2, text: "Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions", correct: false },
                { id: 3, text: "Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose", correct: false },
                { id: 1, text: "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 2, text: "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 3, text: "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
            options: [
                { id: 0, text: "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed", correct: false },
                { id: 1, text: "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page", correct: false },
                { id: 2, text: "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: true },
                { id: 3, text: "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A retail enterprise is expanding its hybrid IT infrastructure and plans to securely connect its on-premises corporate network to its AWS environment. The company wants to ensure that all data exchanged between on-premises systems and AWS is encrypted at both the network and session layers. Additionally, the solution must incorporate granular security controls that restrict unnecessary or unauthorized access between the cloud and on-premises environments. A solutions architect must recommend a scalable and secure approach that supports these goals. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems", correct: true },
                { id: 1, text: "Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed", correct: false },
                { id: 2, text: "Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies", correct: false },
                { id: 3, text: "Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
            options: [
                { id: 0, text: "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type", correct: true },
                { id: 1, text: "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type", correct: false },
                { id: 2, text: "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type", correct: false },
                { id: 3, text: "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
            options: [
                { id: 0, text: "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service", correct: false },
                { id: 1, text: "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage", correct: false },
                { id: 2, text: "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises", correct: false },
                { id: 3, text: "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure Amazon EFS to provide a fast, cost-effective and sharable storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
            options: [
                { id: 0, text: "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option", correct: false },
                { id: 3, text: "Use Amazon EC2 instances with Instance Store as the storage option", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 instances with Instance Store as the storage option\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 3, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A logistics company runs a two-step job handling process on AWS. The first step quickly receives job submissions from clients, while the second step requires longer processing time to complete each job. Currently, both steps run on separate Amazon EC2 Auto Scaling groups. However, during high-demand hours, the job processing stage falls behind, and there is concern that jobs may be lost due to instance termination during scaling events. A solutions architect needs to design a more scalable and reliable architecture that preserves job data and accommodates fluctuating demand in both stages. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue", correct: true },
                { id: 1, text: "Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue", correct: false },
                { id: 2, text: "Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior", correct: false },
                { id: 3, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
                { id: 1, text: "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: true },
                { id: 2, text: "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: false },
                { id: 3, text: "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A global enterprise is modernizing its hybrid IT infrastructure to improve both availability and network performance. The company operates a TCP-based application hosted on Amazon EC2 instances that are deployed across multiple AWS Regions, while a secondary UDP-based component of the application is hosted in its on-premises data centers. These application components must be accessed by customers around the world with minimal latency and consistent uptime. Which combination of options should a solutions architect implement for the given use case? (Select two)",
            options: [
                { id: 0, text: "Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers", correct: true },
                { id: 1, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups", correct: false },
                { id: 2, text: "Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover", correct: false },
                { id: 3, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups", correct: true },
                { id: 4, text: "Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers, Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A SaaS analytics company is deploying a microservices-based application on Amazon ECS using the Fargate launch type. The application requires access to a shared, POSIX-compliant file system that is available across multiple Availability Zones for redundancy and availability. To meet compliance requirements, the system must support regional backups and cross-Region data recovery with a recovery point objective (RPO) of no more than 8 hours. A backup strategy will be implemented using AWS Backup to automate replication across Regions. As the lead cloud architect, you are evaluating file storage solutions that align with these requirements. Which option best meets the application’s availability, durability, and RPO objectives?",
            options: [
                { id: 0, text: "Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically", correct: false },
                { id: 1, text: "Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata", correct: false },
                { id: 2, text: "Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region", correct: false },
                { id: 3, text: "Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
                { id: 1, text: "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets", correct: false },
                { id: 2, text: "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection", correct: false },
                { id: 3, text: "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances", correct: false },
                { id: 4, text: "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files, Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A financial auditing firm uses Amazon S3 to store sensitive client records that are subject to write-once-read-many (WORM) regulations to prevent alteration or deletion of records for a specific retention period. The firm wants to enforce immutable storage, such that even administrators cannot overwrite or delete the records during the lock duration. They also need audit-friendly enforcement to prevent accidental or malicious deletion. Which configuration of S3 Object Lock will ensure that the retention policy is strictly enforced, and no user (including root or administrators) can override or delete protected objects during the lock period?",
            options: [
                { id: 0, text: "Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period", correct: true },
                { id: 1, text: "Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period", correct: false },
                { id: 2, text: "Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period", correct: false },
                { id: 3, text: "Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 2, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 3, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 4, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Data moving between the volume and the instance is encrypted, Any snapshot created from the volume is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An enterprise is developing an internal compliance framework for its cloud infrastructure hosted on AWS. The enterprise uses AWS Organizations to group accounts under various organizational units (OUs) based on departmental function. As part of its governance controls, the security team mandates that all Amazon EC2 instances must be tagged to indicate the level of data classification — either 'confidential' or 'public'. Additionally, the organization must ensure that IAM users cannot launch EC2 instances without assigning a classification tag, nor should they be able to remove the tag from running instances. A solutions architect must design a solution to meet these compliance controls while minimizing operational overhead. Which combination of steps will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations", correct: true },
                { id: 1, text: "Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected", correct: false },
                { id: 2, text: "Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts", correct: true },
                { id: 3, text: "Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning", correct: false },
                { id: 4, text: "Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations, Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A financial data processing company runs a workload on Amazon EC2 instances that fetch and process real-time transaction batches from an Amazon SQS queue. The application needs to scale based on unpredictable message volume, which fluctuates significantly throughout the day. The system must process messages with minimal delay and no downtime, even during peak spikes. The company is seeking a solution that balances cost-efficiency with availability and elasticity. Which EC2 purchasing strategy best meets these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes", correct: false },
                { id: 1, text: "Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume", correct: true },
                { id: 2, text: "Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations", correct: false },
                { id: 3, text: "Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB for distributed in-memory cache based session management", correct: false },
                { id: 1, text: "Use Amazon Elasticache for distributed in-memory cache based session management", correct: true },
                { id: 2, text: "Use Amazon RDS for distributed in-memory cache based session management", correct: false },
                { id: 3, text: "Use Application Load Balancer sticky sessions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Elasticache for distributed in-memory cache based session management\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer sticky sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations", correct: true },
                { id: 1, text: "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed", correct: false },
                { id: 2, text: "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user", correct: false },
                { id: 3, text: "Set up an Amazon Route 53 geoproximity routing policy to route traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Route 53 geoproximity routing policy to route traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "A tech company runs a web application that includes multiple internal services deployed across Amazon EC2 instances within a VPC. These services require communication with a third-party SaaS provider's API for analytics and billing, which is also hosted on the AWS infrastructure. The company is concerned about minimizing public internet exposure while maintaining secure and reliable connectivity. The solution must ensure private access without allowing unsolicited incoming traffic from the SaaS provider. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC", correct: true },
                { id: 1, text: "Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider", correct: false },
                { id: 2, text: "Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication", correct: false },
                { id: 3, text: "Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
            options: [
                { id: 0, text: "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 1, text: "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone", correct: false },
                { id: 2, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 3, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A mobile-based e-learning platform is migrating its backend storage layer to Amazon DynamoDB to support a rapidly increasing number of student users and learning transactions. The platform must ensure seamless availability and minimal disruption for a global user base. The DynamoDB design must provide low-latency performance, high availability, and automatic fault tolerance across geographies with the lowest possible operational overhead and cost. Which solution will fulfill these needs in the most cost-efficient manner?",
            options: [
                { id: 0, text: "Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions", correct: false },
                { id: 1, text: "Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions", correct: false },
                { id: 2, text: "Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability", correct: true },
                { id: 3, text: "Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output", correct: false },
                { id: 1, text: "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments", correct: true },
                { id: 2, text: "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
            options: [
                { id: 0, text: "Both the Auto Scaling group and Application Load Balancer are using ALB based health check", correct: false },
                { id: 1, text: "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check", correct: false },
                { id: 2, text: "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check", correct: false },
                { id: 3, text: "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Both the Auto Scaling group and Application Load Balancer are using ALB based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A fintech company recently conducted a security audit and discovered that some IAM roles and Amazon S3 buckets might be unintentionally shared with external accounts or publicly accessible. The security team wants to identify these overly permissive resources and ensure that only intended principals (within their AWS Organization or specific AWS accounts) have access. They need a solution that can analyze IAM policies and resource policies to detect unintended access paths to AWS resources such as S3 buckets, IAM roles, KMS keys, and SNS topics. Which solution should the team use to meet this requirement?",
            options: [
                { id: 0, text: "Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment", correct: false },
                { id: 1, text: "Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization", correct: true },
                { id: 2, text: "Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access", correct: false },
                { id: 3, text: "Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "An enterprise runs a critical Oracle database workload in its on-premises environment. The company now plans to replicate both existing records and continuous transactional changes to a managed Oracle environment in AWS. The target database will run on Amazon RDS for Oracle. Data transfer volume is expected to fluctuate throughout the day, and the team wants the solution to provision compute resources automatically based on actual workload requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected", correct: false },
                { id: 1, text: "Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.", correct: false },
                { id: 2, text: "Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.", correct: false },
                { id: 3, text: "Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two)",
            options: [
                { id: 0, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions", correct: true },
                { id: 1, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region", correct: false },
                { id: 2, text: "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage", correct: false },
                { id: 3, text: "Use cross-Region Read Replicas", correct: true },
                { id: 4, text: "Use the database cloning feature of the Amazon RDS Database cluster", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions, Use cross-Region Read Replicas\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the database cloning feature of the Amazon RDS Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A DevOps team is tasked with enabling secure and temporary SSH access to Amazon EC2 instances for developers during deployments. The team wants to avoid distributing long-term SSH key pairs and instead prefers ephemeral access that can be audited and revoked immediately after the session ends. The team wants direct access via the AWS Management Console. What do you recommend?",
            options: [
                { id: 0, text: "Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address", correct: true },
                { id: 1, text: "Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint", correct: false },
                { id: 2, text: "Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions", correct: false },
                { id: 3, text: "Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through", correct: false },
                { id: 1, text: "Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances", correct: false },
                { id: 2, text: "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through", correct: true },
                { id: 3, text: "Use Application Load Balancer geo match statement listing the countries that you want to block", correct: false },
                { id: 4, text: "Use AWS WAF geo match statement listing the countries that you want to block", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through, Use AWS WAF geo match statement listing the countries that you want to block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer geo match statement listing the countries that you want to block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A digital design company has migrated its project archiving platform to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group that spans multiple Availability Zones. Designers upload and retrieve high-resolution image files from a shared file system, which is currently configured to use Amazon EFS Standard-IA. Metadata for these files is stored and indexed in an Amazon RDS for PostgreSQL database. The company's cloud engineering team has been asked to optimize storage costs for the image archive without compromising reliability. They are open to refactoring the application to use managed AWS services when necessary. Which solution offers the most cost-effective architecture?",
            options: [
                { id: 0, text: "Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path", correct: false },
                { id: 1, text: "Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost", correct: false },
                { id: 2, text: "Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 1, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 3, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A data analytics team at a global media firm is building a new analytics platform to process large volumes of both historical and real-time data. This data is stored in Amazon S3. The team wants to implement a serverless solution that allows them to query the data directly using SQL. Additionally, the solution must ensure that all data is encrypted at rest and automatically replicated to another AWS Region to support business continuity. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data", correct: true },
                { id: 2, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls", correct: false },
                { id: 1, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
                { id: 2, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls", correct: true },
                { id: 3, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company hosts a Microsoft SQL Server database on Amazon EC2 instances with attached Amazon EBS volumes. The operations team takes daily snapshots of these EBS volumes as backups. However, a recent incident occurred in which an automated script designed to clean up expired snapshots accidentally deleted all available snapshots, leading to potential data loss. The company wants to improve the backup strategy to avoid permanent data loss while still ensuring that old snapshots are eventually removed to optimize cost. A solutions architect needs to implement a mechanism that prevents immediate and irreversible deletion of snapshots. Which solution will best meet these requirements with the least development effort?",
            options: [
                { id: 0, text: "Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots", correct: true },
                { id: 1, text: "Set up the IAM policy of the user to deny EBS snapshot deletion", correct: false },
                { id: 2, text: "Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery", correct: false },
                { id: 3, text: "Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up the IAM policy of the user to deny EBS snapshot deletion: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 1, text: "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
            options: [
                { id: 0, text: "Use long polling to handle message processing failures", correct: false },
                { id: 1, text: "Use a dead-letter queue to handle message processing failures", correct: true },
                { id: 2, text: "Use a temporary queue to handle message processing failures", correct: false },
                { id: 3, text: "Use short polling to handle message processing failures", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a dead-letter queue to handle message processing failures\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use long polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a temporary queue to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use short polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting aProvisionedThroughputExceededExceptionexception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
            options: [
                { id: 0, text: "Decrease the Stream retention duration", correct: false },
                { id: 1, text: "Use batch messages", correct: true },
                { id: 2, text: "Increase the number of shards", correct: false },
                { id: 3, text: "Use Exponential Backoff", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use batch messages\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Decrease the Stream retention duration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the number of shards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Exponential Backoff: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two)",
            options: [
                { id: 0, text: "Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target", correct: false },
                { id: 1, text: "Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica", correct: true },
                { id: 2, text: "Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target", correct: true },
                { id: 3, text: "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes", correct: false },
                { id: 4, text: "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica, Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "An enterprise SaaS provider is currently operating a legacy web application hosted on a single Amazon EC2 instance within a public subnet. The same instance also hosts a MySQL database. DNS records for the application are configured through Amazon Route 53. As part of a modernization initiative, the company wants to rearchitect this application for high availability and scalability. In addition, the company wants to improve read performance on the database layer to handle increasing user traffic. Which combination of solutions will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: false },
                { id: 1, text: "Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages", correct: false },
                { id: 2, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: true },
                { id: 3, text: "Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions", correct: false },
                { id: 4, text: "Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly, Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
            options: [
                { id: 0, text: "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 1, text: "Use Amazon Athena to run SQL based analytics against Amazon S3 data", correct: true },
                { id: 2, text: "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 3, text: "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Athena to run SQL based analytics against Amazon S3 data\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A media streaming company expects a major increase in user activity during the launch of a highly anticipated live event. The streaming platform is deployed on AWS and uses Amazon EC2 instances for the application layer and Amazon RDS for persistent storage. The operations team needs to proactively monitor system performance to ensure a smooth user experience during the event. Their monitoring setup must provide data visibility with intervals of no more than 2 minutes, and the team prefers a solution that is quick to implement and low-maintenance. Which solution should the team implement?",
            options: [
                { id: 0, text: "Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena", correct: false },
                { id: 1, text: "Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics", correct: false },
                { id: 2, text: "Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics", correct: false },
                { id: 3, text: "Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
            options: [
                { id: 0, text: "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications", correct: true },
                { id: 1, text: "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications", correct: false },
                { id: 2, text: "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates", correct: false },
                { id: 3, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 1, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
                { id: 3, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A streaming service provider collects user experience feedback through embedded feedback forms in their mobile and web apps. Feedback submissions frequently spike to thousands per hour during content launches or service outages. Currently, the feedback is sent via email to the operations team for manual review. The company now wants to automate feedback collection and sentiment analysis so that insights can be generated quickly and stored for a full year for trend analysis. Which solution provides the most scalable and automated approach to meet these requirements?",
            options: [
                { id: 0, text: "Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item", correct: true },
                { id: 1, text: "Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table", correct: false },
                { id: 2, text: "Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months", correct: false },
                { id: 3, text: "Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
            options: [
                { id: 0, text: "Use Amazon Route 53 with Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Use AWS Firewall Manager with CloudFront distribution", correct: false },
                { id: 2, text: "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution", correct: true },
                { id: 3, text: "Use AWS Security Hub with Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Firewall Manager with CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Security Hub with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A financial analytics firm runs performance-intensive modeling software on Amazon EC2 instances backed by Amazon EBS volumes. The production data resides on EBS volumes attached to EC2 instances in the same AWS Region where the testing environment is hosted. To maintain data integrity, any changes made during testing must not affect production data. The development team needs to frequently create clones of this production data for simulations. The modeling software requires high and consistent I/O performance, and the firm wants to minimize the time required to provision test data. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment", correct: true },
                { id: 1, text: "Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance", correct: false },
                { id: 2, text: "Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations", correct: false },
                { id: 3, text: "Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A multinational logistics company operates its shipment tracking platform from Amazon EC2 instances deployed in the AWS us-west-2 Region. The platform exposes a set of APIs over HTTPS, which are used by logistics partners and customers around the world to retrieve real-time tracking data. The company has observed that users from Europe and Asia experience latency issues and inconsistent API response times when accessing the service. As a cloud architect, you have been tasked to propose the most cost-effective solution to improve performance for these international users without migrating the application. Which solution should you recommend?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups", correct: true },
                { id: 1, text: "Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2", correct: false },
                { id: 2, text: "Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region", correct: false },
                { id: 3, text: "Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A media company operates a web application that enables users to upload photos. These uploads are stored in an Amazon S3 bucket located in the eu-west-2 Region. To enhance performance and provide secure access under a custom domain name, the company wants to integrate Amazon CloudFront for uploads to the S3 bucket. The architecture must support secure HTTPS connections using a custom domain, and the upload process must ensure optimal speed and security. Which combination of actions will fulfill these requirements? (Select two)",
            options: [
                { id: 0, text: "Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution", correct: false },
                { id: 1, text: "Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations", correct: false },
                { id: 2, text: "Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL", correct: false },
                { id: 3, text: "Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)", correct: true },
                { id: 4, text: "Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC), Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A digital media company runs its content rendering service on Amazon EC2 instances that are registered with an Application Load Balancer (ALB) using IP-based target groups. The company relies on AWS Systems Manager to manage and patch these instances regularly. According to new compliance requirements, EC2 instances must be safely removed from production traffic during patching to prevent user disruption and maintain application integrity. However, during the most recent patch cycle, the operations team noticed application failures and API timeouts, even though patching succeeded on the instances. You are asked to suggest a reliable and scalable way to ensure safe patching while preserving service availability. Which solution will best meet the new compliance and operational requirements? (Select two)",
            options: [
                { id: 0, text: "Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window", correct: true },
                { id: 1, text: "Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata", correct: false },
                { id: 2, text: "Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching", correct: true },
                { id: 3, text: "Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes", correct: false },
                { id: 4, text: "Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window, Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "Use Amazon ElastiCache for Memcached", correct: false },
                { id: 2, text: "Use AWS Global Accelerator", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Redis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ElastiCache for Redis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache for Memcached: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test12: [
        {
            id: 1,
            text: "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
            options: [
                { id: 0, text: "On-Demand Instances", correct: false },
                { id: 1, text: "Spot Instances", correct: false },
                { id: 2, text: "Dedicated Instances", correct: true },
                { id: 3, text: "Dedicated Hosts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
            options: [
                { id: 0, text: "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication", correct: false },
                { id: 1, text: "Use the Amazon EC2 instances private IP for the replication", correct: true },
                { id: 2, text: "Create a Private Link between the two Amazon EC2 instances", correct: false },
                { id: 3, text: "Use an Elastic Fabric Adapter (EFA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Amazon EC2 instances private IP for the replication\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between the two Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Elastic Fabric Adapter (EFA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
            options: [
                { id: 0, text: "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: true },
                { id: 1, text: "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity", correct: true },
                { id: 2, text: "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted", correct: false },
                { id: 3, text: "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: false },
                { id: 4, text: "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification, A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
            options: [
                { id: 0, text: "Create a policy and assign it manually to the 50 users", correct: false },
                { id: 1, text: "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA", correct: false },
                { id: 2, text: "Create a group, attach the policy to the group and place the users in the group", correct: true },
                { id: 3, text: "Update the Amazon S3 bucket policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a group, attach the policy to the group and place the users in the group\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a policy and assign it manually to the 50 users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Update the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
            options: [
                { id: 0, text: "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation", correct: false },
                { id: 1, text: "You can specify instance store volumes for an instance when you launch or restart it", correct: false },
                { id: 2, text: "An instance store is a network storage type", correct: false },
                { id: 3, text: "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved", correct: true },
                { id: 4, text: "You can't detach an instance store volume from one instance and attach it to a different instance", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved, You can't detach an instance store volume from one instance and attach it to a different instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can specify instance store volumes for an instance when you launch or restart it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance store is a network storage type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
            options: [
                { id: 0, text: "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process", correct: false },
                { id: 1, text: "Use AWS CLI to run the user data scripts only once while launching the instance", correct: false },
                { id: 2, text: "Run the custom scripts as user data scripts on the Amazon EC2 instances", correct: true },
                { id: 3, text: "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Run the custom scripts as user data scripts on the Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CLI to run the user data scripts only once while launching the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the custom scripts as instance metadata scripts on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions. Which solution best meets these requirements while ensuring the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3", correct: true },
                { id: 1, text: "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx", correct: false },
                { id: 2, text: "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes", correct: false },
                { id: 3, text: "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application. Which solution will most effectively address the performance issues with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users", correct: true },
                { id: 1, text: "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket", correct: false },
                { id: 2, text: "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency", correct: false },
                { id: 3, text: "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
            options: [
                { id: 0, text: "Use AWS Shield Advanced and setup a rate-based rule", correct: false },
                { id: 1, text: "Configure Sticky Sessions on the Application Load Balancer", correct: false },
                { id: 2, text: "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule", correct: true },
                { id: 3, text: "Define a network access control list (network ACL) on your Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Shield Advanced and setup a rate-based rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Sticky Sessions on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define a network access control list (network ACL) on your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: false },
                { id: 1, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 3, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
            options: [
                { id: 0, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "AWS Global Accelerator", correct: false },
                { id: 2, text: "Amazon ElastiCache for Redis", correct: false },
                { id: 3, text: "Amazon ElastiCache for Memcached", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon ElastiCache for Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache for Redis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
            options: [
                { id: 0, text: "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 1, text: "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 2, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2", correct: true },
                { id: 3, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
            options: [
                { id: 0, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: true },
                { id: 3, text: "Amazon Redshift", correct: false },
                { id: 4, text: "Amazon DynamoDB Accelerator (DAX)", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon ElastiCache, Amazon DynamoDB Accelerator (DAX)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
            options: [
                { id: 0, text: "Create a Spot Fleet request", correct: false },
                { id: 1, text: "Assign an Amazon EC2 Instance Role to perform the necessary API calls", correct: true },
                { id: 2, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1", correct: true },
                { id: 3, text: "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group", correct: false },
                { id: 4, text: "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it", correct: true },
                { id: 5, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are: Assign an Amazon EC2 Instance Role to perform the necessary API calls, Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1, Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Spot Fleet request: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
            options: [
                { id: 0, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 1, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 2, text: "Active Directory Connector", correct: false },
                { id: 3, text: "AWS Transit Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Transit Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon Kinesis Data Streams to ingest the data", correct: true },
                { id: 1, text: "Set up AWS Fargate with Amazon ECS to process the data", correct: true },
                { id: 2, text: "Set up AWS Database Migration Service (AWS DMS) to ingest the data", correct: false },
                { id: 3, text: "Set up AWS Lambda with AWS Step Functions to process the data", correct: false },
                { id: 4, text: "Provision Amazon EC2 instances in an Auto Scaling group to process the data", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Set up Amazon Kinesis Data Streams to ingest the data, Set up AWS Fargate with Amazon ECS to process the data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up AWS Database Migration Service (AWS DMS) to ingest the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Lambda with AWS Step Functions to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision Amazon EC2 instances in an Auto Scaling group to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work. Which solution should the company implement to meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access", correct: false },
                { id: 1, text: "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically", correct: false },
                { id: 2, text: "Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets", correct: true },
                { id: 3, text: "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure", correct: true },
                { id: 1, text: "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones", correct: false },
                { id: 2, text: "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.", correct: false },
                { id: 3, text: "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency. Which solution best satisfies these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint", correct: false },
                { id: 1, text: "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway", correct: false },
                { id: 2, text: "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances", correct: false },
                { id: 3, text: "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
            options: [
                { id: 0, text: "Elastic Fabric Adapter (EFA)", correct: true },
                { id: 1, text: "Elastic IP Address (EIP)", correct: false },
                { id: 2, text: "Elastic Network Adapter (ENA)", correct: false },
                { id: 3, text: "Elastic Network Interface (ENI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Elastic Fabric Adapter (EFA)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Elastic IP Address (EIP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Adapter (ENA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Interface (ENI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
            options: [
                { id: 0, text: "enableVpcHostnames", correct: false },
                { id: 1, text: "enableVpcSupport", correct: false },
                { id: 2, text: "enableDnsHostnames", correct: true },
                { id: 3, text: "enableDnsSupport", correct: true },
                { id: 4, text: "enableDnsDomain", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: enableDnsHostnames, enableDnsSupport\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- enableVpcHostnames: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableVpcSupport: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableDnsDomain: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Migrate the Amazon Redshift underlying storage to Amazon S3 IA", correct: false },
                { id: 1, text: "Analyze the cold data with Amazon Athena", correct: true },
                { id: 2, text: "Create a smaller Amazon Redshift Cluster with the cold data", correct: false },
                { id: 3, text: "Move the data to Amazon S3 Glacier Deep Archive after 30 days", correct: false },
                { id: 4, text: "Move the data to Amazon S3 Standard IA after 30 days", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Analyze the cold data with Amazon Athena, Move the data to Amazon S3 Standard IA after 30 days\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Migrate the Amazon Redshift underlying storage to Amazon S3 IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a smaller Amazon Redshift Cluster with the cold data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move the data to Amazon S3 Glacier Deep Archive after 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region. The firm has defined strict DR objectives: Recovery Point Objective (RPO): ≤ 5 minutes Recovery Time Objective (RTO): ≤ 15 minutes Leadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
            options: [
                { id: 0, text: "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes", correct: false },
                { id: 1, text: "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover", correct: false },
                { id: 2, text: "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events", correct: false },
                { id: 3, text: "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
            options: [
                { id: 0, text: "Amazon Aurora Serverless", correct: true },
                { id: 1, text: "Amazon DynamoDB with On-Demand Capacity", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: false },
                { id: 3, text: "Amazon DynamoDB with Provisioned Capacity and Auto Scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Aurora Serverless\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB with On-Demand Capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB with Provisioned Capacity and Auto Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
            options: [
                { id: 0, text: "Set up AWS Direct Connect", correct: false },
                { id: 1, text: "Set up an AWS Site-to-Site VPN connection", correct: true },
                { id: 2, text: "Set up an Internet Gateway between the on-premises data center and AWS cloud", correct: false },
                { id: 3, text: "Set up a bastion host on Amazon EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an AWS Site-to-Site VPN connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Internet Gateway between the on-premises data center and AWS cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host on Amazon EC2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
            options: [
                { id: 0, text: "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group", correct: false },
                { id: 1, text: "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy", correct: true },
                { id: 2, text: "Make the Amazon S3 bucket public", correct: false },
                { id: 3, text: "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make the Amazon S3 bucket public: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed. Which solution meets these requirements most cost-effectively?",
            options: [
                { id: 0, text: "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation", correct: false },
                { id: 1, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: false },
                { id: 2, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: true },
                { id: 3, text: "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Amazon EFS Infrequent Access", correct: true },
                { id: 1, text: "Amazon EFS Standard", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Standard-IA", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EFS Infrequent Access\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon EFS Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
            options: [
                { id: 0, text: "AWS Firewall Manager", correct: false },
                { id: 1, text: "AWS Shield Advanced", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: true },
                { id: 3, text: "AWS Web Application Firewall (AWS WAF)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon GuardDuty\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Firewall Manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Web Application Firewall (AWS WAF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Run on AWS Lambda", correct: false },
                { id: 1, text: "Run on an Application Load Balancer", correct: false },
                { id: 2, text: "Run on Amazon EMR", correct: false },
                { id: 3, text: "Run on a Spot Instance with a persistent request type", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Run on a Spot Instance with a persistent request type\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run on AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application. Which combination of steps will enable secure S3 integration for this workload? (Select two)",
            options: [
                { id: 0, text: "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3", correct: true },
                { id: 1, text: "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID", correct: false },
                { id: 2, text: "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access", correct: false },
                { id: 3, text: "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate", correct: true },
                { id: 4, text: "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3, Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage. Which solution will help improve the application’s responsiveness and scalability during peak load periods?",
            options: [
                { id: 0, text: "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue", correct: false },
                { id: 1, text: "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput", correct: false },
                { id: 2, text: "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue", correct: true },
                { id: 3, text: "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Store the images using the Amazon S3 Standard-IA storage class", correct: false },
                { id: 1, text: "Store the images using the Amazon S3 Intelligent-Tiering storage class", correct: true },
                { id: 2, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
                { id: 3, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Store the images using the Amazon S3 Intelligent-Tiering storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Store the images using the Amazon S3 Standard-IA storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold. What is the MOST operationally efficient way to achieve this?",
            options: [
                { id: 0, text: "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders", correct: true },
                { id: 1, text: "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly", correct: false },
                { id: 2, text: "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops", correct: false },
                { id: 3, text: "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) temporary queues", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS) dead-letter queues", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) delay queues", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS) FIFO queues", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Simple Queue Service (Amazon SQS) temporary queues\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) dead-letter queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) delay queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
            options: [
                { id: 0, text: "Create a Read Replica in another Region as the Master database and point the analytics workload there", correct: false },
                { id: 1, text: "Create a Read Replica in the same Region as the Master database and point the analytics workload there", correct: true },
                { id: 2, text: "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database", correct: false },
                { id: 3, text: "Migrate the analytics application to AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Read Replica in the same Region as the Master database and point the analytics workload there\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Read Replica in another Region as the Master database and point the analytics workload there: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the analytics application to AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans. Which combination of savings plans will satisfy these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly", correct: false },
                { id: 1, text: "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances", correct: true },
                { id: 2, text: "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads", correct: false },
                { id: 3, text: "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type", correct: false },
                { id: 4, text: "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances, Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
            options: [
                { id: 0, text: "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: true },
                { id: 1, text: "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 2, text: "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 3, text: "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
            options: [
                { id: 0, text: "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes", correct: false },
                { id: 1, text: "Storage class analysis only provides recommendations for Standard to Standard IA classes", correct: true },
                { id: 2, text: "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes", correct: false },
                { id: 3, text: "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Storage class analysis only provides recommendations for Standard to Standard IA classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
            options: [
                { id: 0, text: "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby", correct: true },
                { id: 1, text: "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database", correct: false },
                { id: 2, text: "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync", correct: false },
                { id: 3, text: "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason", correct: true },
                { id: 4, text: "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby, Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practices—like mandatory logging and guardrails—when new accounts are created. The firm prefers a low-maintenance, governance-driven setup. Which solution best meets these goals while minimizing operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually", correct: false },
                { id: 1, text: "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance", correct: false },
                { id: 2, text: "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs", correct: false },
                { id: 3, text: "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
            options: [
                { id: 0, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region", correct: false },
                { id: 1, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region", correct: true },
                { id: 2, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region", correct: false },
                { id: 3, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection. Which configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
            options: [
                { id: 0, text: "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center", correct: true },
                { id: 1, text: "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect", correct: false },
                { id: 2, text: "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing", correct: false },
                { id: 3, text: "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
            options: [
                { id: 0, text: "Require HTTPS for communication between Amazon CloudFront and your S3 origin", correct: false },
                { id: 1, text: "Require HTTPS for communication between Amazon CloudFront and your custom origin", correct: false },
                { id: 2, text: "Use Amazon CloudFront signed URLs", correct: true },
                { id: 3, text: "Use Amazon CloudFront signed cookies", correct: true },
                { id: 4, text: "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Amazon CloudFront signed URLs, Use Amazon CloudFront signed cookies\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Require HTTPS for communication between Amazon CloudFront and your S3 origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Require HTTPS for communication between Amazon CloudFront and your custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in read request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 1, text: "Use AWS Global Accelerator", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use AWS Direct Connect", correct: false },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
            options: [
                { id: 0, text: "VPC Route Table", correct: false },
                { id: 1, text: "Amazon Inspector", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: false },
                { id: 3, text: "AWS Shield Advanced", correct: true },
                { id: 4, text: "AWS Web Application Firewall (AWS WAF)", correct: true },
                { id: 5, text: "VPC Security Group", correct: true },
            ],
            correctAnswers: [3, 4, 5],
            explanation: "The correct answers are: AWS Shield Advanced, AWS Web Application Firewall (AWS WAF), VPC Security Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- VPC Route Table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Inspector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon GuardDuty: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon MQ", correct: true },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) Standard", correct: false },
                { id: 3, text: "Amazon SQS FIFO (First-In-First-Out)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SQS FIFO (First-In-First-Out): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 48,
            text: "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning. Which solution will meet these requirements in the MOST efficient and scalable way?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically", correct: false },
                { id: 1, text: "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time", correct: true },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals", correct: false },
                { id: 3, text: "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "VPC Endpoint", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "AWS VPN CloudHub", correct: true },
                { id: 3, text: "Software VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic", correct: true },
                { id: 1, text: "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 2, text: "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store", correct: false },
                { id: 1, text: "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones", correct: false },
                { id: 2, text: "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances", correct: false },
                { id: 3, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 52,
            text: "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
            options: [
                { id: 0, text: "AWS Transit Gateway", correct: true },
                { id: 1, text: "VPC Peering Connection", correct: false },
                { id: 2, text: "Virtual private gateway (VGW)", correct: false },
                { id: 3, text: "AWS PrivateLink", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Transit Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Peering Connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Virtual private gateway (VGW): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS PrivateLink: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring. Which combination of actions will best support this migration approach? (Select three)",
            options: [
                { id: 0, text: "Launch a cutover instance after completing testing and confirming that replication is up-to-date", correct: true },
                { id: 1, text: "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use", correct: false },
                { id: 2, text: "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover", correct: true },
                { id: 3, text: "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs", correct: true },
                { id: 4, text: "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations", correct: false },
                { id: 5, text: "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation", correct: false },
            ],
            correctAnswers: [0, 2, 3],
            explanation: "The correct answers are: Launch a cutover instance after completing testing and confirming that replication is up-to-date, Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover, Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
            options: [
                { id: 0, text: "Throughput Optimized HDD Amazon EBS volumes", correct: false },
                { id: 1, text: "Provisioned IOPS SSD Amazon EBS volumes", correct: true },
                { id: 2, text: "General-purpose SSD-based Amazon EBS volumes", correct: false },
                { id: 3, text: "Cold HDD Amazon EBS volumes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD Amazon EBS volumes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Throughput Optimized HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- General-purpose SSD-based Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 1, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
            options: [
                { id: 0, text: "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS", correct: false },
                { id: 1, text: "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index", correct: false },
                { id: 2, text: "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS", correct: true },
                { id: 3, text: "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone", correct: false },
                { id: 1, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: false },
                { id: 2, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: true },
                { id: 3, text: "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
            options: [
                { id: 0, text: "Client Side Encryption", correct: true },
                { id: 1, text: "Server-side encryption with AWS KMS keys (SSE-KMS)", correct: false },
                { id: 2, text: "Server-side encryption with customer-provided keys (SSE-C)", correct: false },
                { id: 3, text: "Server-side encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Client Side Encryption\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption with AWS KMS keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root user’s email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users. Which solution meets these requirements in the most secure and scalable way?",
            options: [
                { id: 0, text: "Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications", correct: true },
                { id: 1, text: "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts", correct: false },
                { id: 2, text: "Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses", correct: false },
                { id: 3, text: "Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
            options: [
                { id: 0, text: "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC", correct: false },
                { id: 1, text: "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC", correct: true },
                { id: 2, text: "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC", correct: false },
                { id: 3, text: "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 1, text: "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 2, text: "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 3, text: "Use delay queues to postpone the delivery of new messages to the queue for a few seconds", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use delay queues to postpone the delivery of new messages to the queue for a few seconds\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 1, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 2, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue", correct: true },
                { id: 3, text: "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort. What is the most efficient solution to meet this goal with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule", correct: false },
                { id: 1, text: "Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed", correct: true },
                { id: 2, text: "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds", correct: false },
                { id: 3, text: "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
            options: [
                { id: 0, text: "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica", correct: true },
                { id: 1, text: "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
                { id: 2, text: "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica", correct: false },
                { id: 3, text: "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets", correct: false },
                { id: 1, text: "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled", correct: true },
                { id: 2, text: "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket", correct: false },
                { id: 3, text: "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test13: [
        {
            id: 1,
            text: "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 2, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: true },
                { id: 3, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3", correct: false },
                { id: 1, text: "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3", correct: true },
                { id: 2, text: "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
            options: [
                { id: 0, text: "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 1, text: "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 2, text: "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 3, text: "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 4, text: "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements, Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 1, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
            options: [
                { id: 0, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS", correct: true },
                { id: 1, text: "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS", correct: false },
                { id: 2, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS", correct: false },
                { id: 3, text: "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
            options: [
                { id: 0, text: "Different versions of a single object can have different retention modes and periods", correct: true },
                { id: 1, text: "The bucket default settings will override any explicit retention mode or period you request on an object version", correct: false },
                { id: 2, text: "You cannot place a retention period on an object version through a bucket default setting", correct: false },
                { id: 3, text: "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", correct: true },
                { id: 4, text: "When you use bucket default settings, you specify a Retain Until Date for the object version", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Different versions of a single object can have different retention modes and periods, When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The bucket default settings will override any explicit retention mode or period you request on an object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot place a retention period on an object version through a bucket default setting: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When you use bucket default settings, you specify a Retain Until Date for the object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 1, text: "AWS Glue", correct: false },
                { id: 2, text: "Amazon EMR", correct: false },
                { id: 3, text: "Amazon FSx for Lustre", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
            options: [
                { id: 0, text: "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering => Amazon S3 Standard", correct: true },
                { id: 2, text: "Amazon S3 Standard-IA => Amazon S3 One Zone-IA", correct: false },
                { id: 3, text: "Amazon S3 One Zone-IA => Amazon S3 Standard-IA", correct: true },
                { id: 4, text: "Amazon S3 Standard => Amazon S3 Intelligent-Tiering", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Amazon S3 Intelligent-Tiering => Amazon S3 Standard, Amazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA => Amazon S3 One Zone-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
            options: [
                { id: 0, text: "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources", correct: false },
                { id: 1, text: "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI", correct: false },
                { id: 2, text: "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions", correct: false },
                { id: 3, text: "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
            options: [
                { id: 0, text: "Configure the security group for the Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)", correct: false },
                { id: 2, text: "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 3, text: "Configure the security group on the Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure the security group for the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the security group on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway with AWS Lambda", correct: false },
                { id: 1, text: "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Leverage Amazon QuickSight with Amazon Redshift", correct: false },
                { id: 3, text: "Leverage Amazon Athena with Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway with AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon QuickSight with Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Athena with Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor", correct: false },
                { id: 1, text: "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets", correct: true },
                { id: 3, text: "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
            options: [
                { id: 0, text: "Encrypt the access keys and save them on Amazon S3", correct: false },
                { id: 1, text: "Create a strong password for the AWS account root user", correct: true },
                { id: 2, text: "Enable Multi Factor Authentication (MFA) for the AWS account root user account", correct: true },
                { id: 3, text: "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future", correct: false },
                { id: 4, text: "Create AWS account root user access keys and share those keys only with the business owner", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Create a strong password for the AWS account root user, Enable Multi Factor Authentication (MFA) for the AWS account root user account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the access keys and save them on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create AWS account root user access keys and share those keys only with the business owner: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
            options: [
                { id: 0, text: "Grant maximum privileges to avoid assigning privileges again", correct: false },
                { id: 1, text: "Use user credentials to provide access specific permissions for Amazon EC2 instances", correct: false },
                { id: 2, text: "Create a minimum number of accounts and share these account credentials among employees", correct: false },
                { id: 3, text: "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users", correct: true },
                { id: 4, text: "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users, Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Grant maximum privileges to avoid assigning privileges again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use user credentials to provide access specific permissions for Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a minimum number of accounts and share these account credentials among employees: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 1, text: "Only root user should have full database access in the organization", correct: false },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Remove full database access for all IAM users in the organization", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance", correct: true },
                { id: 1, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched", correct: false },
                { id: 2, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application", correct: true },
                { id: 3, text: "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously", correct: false },
                { id: 4, text: "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance, As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time", correct: false },
                { id: 1, text: "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly", correct: false },
                { id: 2, text: "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores", correct: true },
                { id: 3, text: "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
            options: [
                { id: 0, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate", correct: true },
                { id: 1, text: "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages", correct: false },
                { id: 2, text: "Use Amazon SQS standard queue to process the messages", correct: false },
                { id: 3, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS standard queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
            options: [
                { id: 0, text: "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs", correct: true },
                { id: 1, text: "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches", correct: false },
                { id: 2, text: "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share", correct: false },
                { id: 3, text: "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
            options: [
                { id: 0, text: "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket", correct: false },
                { id: 1, text: "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)", correct: true },
                { id: 2, text: "Upload the compressed file using multipart upload", correct: false },
                { id: 3, text: "Upload the compressed file in a single operation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file using multipart upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file in a single operation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
            options: [
                { id: 0, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance", correct: false },
                { id: 1, text: "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance", correct: true },
                { id: 2, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance", correct: false },
                { id: 3, text: "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
            options: [
                { id: 0, text: "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution", correct: true },
                { id: 1, text: "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: true },
                { id: 2, text: "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 3, text: "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 4, text: "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution, Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection", correct: true },
                { id: 1, text: "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region", correct: false },
                { id: 2, text: "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region", correct: false },
                { id: 3, text: "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes", correct: true },
                { id: 3, text: "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
            options: [
                { id: 0, text: "The engineering team needs to provision more servers running the Amazon SNS service", correct: false },
                { id: 1, text: "The engineering team needs to provision more servers running the AWS Lambda service", correct: false },
                { id: 2, text: "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit", correct: true },
                { id: 3, text: "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The engineering team needs to provision more servers running the Amazon SNS service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The engineering team needs to provision more servers running the AWS Lambda service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
            options: [
                { id: 0, text: "General Purpose Solid State Drive (gp2)", correct: false },
                { id: 1, text: "Throughput Optimized Hard disk drive (st1)", correct: true },
                { id: 2, text: "Instance Store", correct: false },
                { id: 3, text: "Cold Hard disk drive (sc1)", correct: true },
                { id: 4, text: "Provisioned IOPS Solid state drive (io1)", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Throughput Optimized Hard disk drive (st1), Cold Hard disk drive (sc1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose Solid State Drive (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance Store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned IOPS Solid state drive (io1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
            options: [
                { id: 0, text: "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 1, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 2, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: true },
                { id: 3, text: "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
            options: [
                { id: 0, text: "Host-based Routing", correct: false },
                { id: 1, text: "Path-based Routing", correct: true },
                { id: 2, text: "HTTP header-based routing", correct: false },
                { id: 3, text: "Query string parameter-based routing", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Path-based Routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host-based Routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- HTTP header-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Query string parameter-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time", correct: false },
                { id: 1, text: "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: true },
                { id: 2, text: "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling", correct: false },
                { id: 3, text: "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
            options: [
                { id: 0, text: "Use AWS Lambda function to run the workflow processes", correct: false },
                { id: 1, text: "Use Amazon EC2 on-demand instances to run the workflow processes", correct: false },
                { id: 2, text: "Use Amazon EC2 reserved instances to run the workflow processes", correct: false },
                { id: 3, text: "Use Amazon EC2 spot instances to run the workflow processes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 spot instances to run the workflow processes\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Lambda function to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
            options: [
                { id: 0, text: "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability", correct: false },
                { id: 1, text: "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency", correct: false },
                { id: 2, text: "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed", correct: true },
                { id: 3, text: "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously", correct: true },
                { id: 4, text: "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed, Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
            options: [
                { id: 0, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: true },
                { id: 1, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 2, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 3, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
            options: [
                { id: 0, text: "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 1, text: "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables", correct: true },
                { id: 2, text: "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 3, text: "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
            options: [
                { id: 0, text: "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue", correct: false },
                { id: 1, text: "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service", correct: true },
                { id: 2, text: "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again", correct: false },
                { id: 3, text: "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy", correct: false },
                { id: 4, text: "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service, Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 3, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet", correct: false },
                { id: 1, text: "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block", correct: false },
                { id: 2, text: "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC", correct: true },
                { id: 3, text: "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
            options: [
                { id: 0, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 1, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 2, text: "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 3, text: "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse", correct: false },
                { id: 1, text: "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes", correct: false },
                { id: 2, text: "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks", correct: false },
                { id: 3, text: "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
            options: [
                { id: 0, text: "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services", correct: false },
                { id: 1, text: "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC", correct: true },
                { id: 2, text: "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services", correct: false },
                { id: 3, text: "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
            options: [
                { id: 0, text: "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior", correct: false },
                { id: 1, text: "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration", correct: false },
                { id: 2, text: "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data", correct: true },
                { id: 3, text: "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands", correct: true },
                { id: 4, text: "Use Amazon Aurora Global Database to replicate data across regions for compatibility", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data, Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Aurora Global Database to replicate data across regions for compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
            options: [
                { id: 0, text: "Use VPC security groups to control the network traffic to and from your file system", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system", correct: false },
                { id: 2, text: "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system", correct: false },
                { id: 3, text: "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance", correct: false },
                { id: 4, text: "Use an IAM policy to control access for clients who can mount your file system with the required permissions", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use VPC security groups to control the network traffic to and from your file system, Use an IAM policy to control access for clients who can mount your file system with the required permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 1, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
                { id: 2, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket, Enable versioning on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
            options: [
                { id: 0, text: "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively", correct: false },
                { id: 1, text: "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures", correct: false },
                { id: 2, text: "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements", correct: false },
                { id: 3, text: "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
            options: [
                { id: 0, text: "The junior scientist does not need to pay any transfer charges for the image upload", correct: true },
                { id: 1, text: "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload", correct: false },
                { id: 2, text: "The junior scientist only needs to pay S3TA transfer charges for the image upload", correct: false },
                { id: 3, text: "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The junior scientist does not need to pay any transfer charges for the image upload\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay Amazon S3 transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
            options: [
                { id: 0, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 1, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon S3 Glacier Deep Archive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "It is not possible to access cross-account resources", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
            options: [
                { id: 0, text: "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EFS mount points", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with access to Amazon S3 based storage", correct: false },
                { id: 3, text: "Use Instance Store based Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Instance Store based Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EFS mount points: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with access to Amazon S3 based storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
            options: [
                { id: 0, text: "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies", correct: false },
                { id: 1, text: "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods", correct: true },
                { id: 3, text: "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
            options: [
                { id: 0, text: "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class", correct: false },
                { id: 1, text: "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class", correct: false },
                { id: 2, text: "Store the intermediary query results in Amazon S3 Standard storage class", correct: true },
                { id: 3, text: "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Store the intermediary query results in Amazon S3 Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow", correct: false },
                { id: 2, text: "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards", correct: false },
                { id: 3, text: "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
            options: [
                { id: 0, text: "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%", correct: false },
                { id: 2, text: "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%", correct: true },
                { id: 3, text: "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point", correct: true },
                { id: 1, text: "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution", correct: false },
                { id: 2, text: "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers", correct: true },
                { id: 1, text: "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers", correct: false },
                { id: 2, text: "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia", correct: false },
                { id: 3, text: "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
            options: [
                { id: 0, text: "AWS Global Accelerator", correct: true },
                { id: 1, text: "AWS Elastic Load Balancing (ELB)", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon CloudFront", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Global Accelerator\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- AWS Elastic Load Balancing (ELB): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance", correct: false },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB", correct: false },
                { id: 2, text: "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB", correct: true },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Microsoft SQL Server on AWS", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: false },
                { id: 3, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Microsoft SQL Server on AWS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume", correct: false },
                { id: 2, text: "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required", correct: true },
                { id: 3, text: "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks: SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.\n- Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
            options: [
                { id: 0, text: "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud", correct: true },
                { id: 1, text: "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
            options: [
                { id: 0, text: "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B", correct: true },
                { id: 1, text: "1 Amazon EC2 instance and 2 AMIs exist in Region B", correct: false },
                { id: 2, text: "1 Amazon EC2 instance and 1 AMI exist in Region B", correct: false },
                { id: 3, text: "1 Amazon EC2 instance and 1 snapshot exist in Region B", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- 1 Amazon EC2 instance and 2 AMIs exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 AMI exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 snapshot exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
            options: [
                { id: 0, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 1, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: true },
                { id: 2, text: "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 3, text: "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 1, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
            options: [
                { id: 0, text: "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 1, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: true },
                { id: 2, text: "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 3, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
            options: [
                { id: 0, text: "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets", correct: false },
                { id: 1, text: "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations", correct: true },
                { id: 2, text: "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files", correct: false },
                { id: 3, text: "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test14: [
        {
            id: 1,
            text: "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database. Which of the following would you recommend to securely share the database with the auditor?",
            options: [
                { id: 0, text: "Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access", correct: false },
                { id: 1, text: "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket", correct: false },
                { id: 2, text: "Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket", correct: false },
                { id: 3, text: "Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored. What do you recommend?",
            options: [
                { id: 0, text: "Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is", correct: false },
                { id: 1, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID", correct: true },
                { id: 2, text: "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is", correct: false },
                { id: 3, text: "Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "\"An enterprise organization is expanding its cloud footprint and needs to centralize its security event data from various AWS accounts and services. The goal is to evaluate security posture across all environments and improve threat detection and response — without requiring significant custom code or manual integration. Which solution will fulfill these needs with the least development effort?",
            options: [
                { id: 0, text: "Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake", correct: true },
                { id: 1, text: "Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard", correct: false },
                { id: 2, text: "Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis", correct: false },
                { id: 3, text: "Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A manufacturing analytics company has a large collection of automated scripts that perform data cleanup, validation, and system integration tasks. These scripts are currently run by a local Linux cron scheduler and have an execution time of up to 30 minutes. The company wants to migrate these scripts to AWS without significant changes, and would prefer a containerized, serverless architecture that automatically scales and can respond to event-based triggers in the future. The solution must minimize infrastructure management. Which solution will best meet these requirements with minimal refactoring and operational overhead?",
            options: [
                { id: 0, text: "Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions", correct: false },
                { id: 1, text: "Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS", correct: true },
                { id: 2, text: "Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate", correct: false },
                { id: 3, text: "Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
            options: [
                { id: 0, text: "A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive", correct: false },
                { id: 1, text: "The health check grace period for the instance has not expired", correct: true },
                { id: 2, text: "The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)", correct: false },
                { id: 3, text: "The instance has failed the Elastic Load Balancing (ELB) health check status", correct: true },
                { id: 4, text: "A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks", correct: false },
                { id: 5, text: "The instance maybe in Impaired status", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: The health check grace period for the instance has not expired, The instance has failed the Elastic Load Balancing (ELB) health check status, The instance maybe in Impaired status\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located inus-east-1region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time. As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)",
            options: [
                { id: 0, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53", correct: false },
                { id: 1, text: "Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region", correct: false },
                { id: 2, text: "Create Amazon Aurora read replicas in the eu-west-1 region", correct: true },
                { id: 3, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53", correct: false },
                { id: 4, text: "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Create Amazon Aurora read replicas in the eu-west-1 region, Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 7,
            text: "Which of the following IAM policies provides read-only access to the Amazon S3 bucketmybucketand its content?",
            options: [
                { id: 0, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }", correct: true },
                { id: 1, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }", correct: false },
                { id: 2, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }", correct: false },
                { id: 3, text: "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 weighted routing to spread traffic across different deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CodeDeploy deployment options to choose the right deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to distribute traffic across deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures. Which is the MOST cost-optimal solution for this workload?",
            options: [
                { id: 0, text: "Run the workload on Reserved Instances (RI)", correct: false },
                { id: 1, text: "Run the workload on a Spot Fleet", correct: true },
                { id: 2, text: "Run the workload on Spot Instances", correct: false },
                { id: 3, text: "Run the workload on Dedicated Hosts", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Run the workload on a Spot Fleet\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run the workload on Reserved Instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the workload on Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the workload on Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos. Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)",
            options: [
                { id: 0, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos", correct: true },
                { id: 1, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos", correct: false },
                { id: 2, text: "Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos", correct: true },
                { id: 3, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos", correct: false },
                { id: 4, text: "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos, Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 11,
            text: "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use Amazon EC2 Meta-Data", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that", correct: false },
                { id: 2, text: "Use Amazon EC2 User-Data", correct: false },
                { id: 3, text: "Use Amazon EC2 Instance Hibernate", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 Instance Hibernate\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Meta-Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 User-Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "An enterprise is building a secure business intelligence API using Amazon API Gateway to serve internal users with confidential analytics data. The API must be accessible only from a set of trusted IP addresses that are part of the organization's internal network ranges. No external IP traffic should be able to invoke the API. A solutions architect must design this access control mechanism with the least operational complexity. What should the architect do to meet these requirements?",
            options: [
                { id: 0, text: "Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list", correct: true },
                { id: 1, text: "Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs", correct: false },
                { id: 2, text: "Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses", correct: false },
                { id: 3, text: "Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms: Which is the MOST operationally efficient solution?",
            options: [
                { id: 0, text: "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation", correct: true },
                { id: 1, text: "Server-side encryption (SSE-S3) with automatic key rotation", correct: false },
                { id: 2, text: "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation", correct: false },
                { id: 3, text: "Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption (SSE-S3) with automatic key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available. As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
            options: [
                { id: 0, text: "Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)", correct: false },
                { id: 1, text: "Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)", correct: false },
                { id: 2, text: "Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)", correct: true },
                { id: 3, text: "Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ)us-east-1aas it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ)us-east-1alike so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour. Which of the following instances would be terminated per the default termination policy?",
            options: [
                { id: 0, text: "Instance C", correct: false },
                { id: 1, text: "Instance A", correct: false },
                { id: 2, text: "Instance B", correct: true },
                { id: 3, text: "Instance D", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instance B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Instance C: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance A: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance D: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform. Which of the following solutions would have the LEAST amount of downtime?",
            options: [
                { id: 0, text: "Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3", correct: false },
                { id: 1, text: "Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3", correct: true },
                { id: 2, text: "Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer", correct: false },
                { id: 3, text: "Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?",
            options: [
                { id: 0, text: "Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager", correct: true },
                { id: 1, text: "Create a VPC peering connection between all virtual private cloud (VPCs)", correct: false },
                { id: 2, text: "Create a Private Link between all the Amazon EC2 instances", correct: false },
                { id: 3, text: "Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a VPC peering connection between all virtual private cloud (VPCs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between all the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner. What could be the reason for this denial of permission for the bucket owner?",
            options: [
                { id: 0, text: "By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster", correct: true },
                { id: 1, text: "When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures", correct: false },
                { id: 2, text: "The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress", correct: false },
                { id: 3, text: "When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS. How should you configure the security groups? (Select three)",
            options: [
                { id: 0, text: "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443", correct: true },
                { id: 1, text: "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80", correct: false },
                { id: 2, text: "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80", correct: false },
                { id: 3, text: "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80", correct: true },
                { id: 4, text: "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432", correct: true },
                { id: 5, text: "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432", correct: false },
            ],
            correctAnswers: [0, 3, 4],
            explanation: "The correct answers are: The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443, The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80, The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved. As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
            options: [
                { id: 0, text: "Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations", correct: false },
                { id: 1, text: "Remove any overlapping namespaces for the private and public hosted zones", correct: false },
                { id: 2, text: "Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken", correct: false },
                { id: 3, text: "Enable DNS hostnames and DNS resolution for private hosted zones", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable DNS hostnames and DNS resolution for private hosted zones\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove any overlapping namespaces for the private and public hosted zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An enterprise uses a centralized Amazon S3 bucket to store logs and reports generated by multiple analytics services. Each service writes to and reads from a dedicated prefix (folder path) in the bucket. The company wants to enforce fine-grained access control so that each service can access only its own prefix, without being able to see or modify other services' data. The solution must support scalable and maintainable permissions management with minimal operational overhead. Which approach will best meet these requirements?",
            options: [
                { id: 0, text: "Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services", correct: false },
                { id: 1, text: "Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket", correct: true },
                { id: 2, text: "Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags", correct: false },
                { id: 3, text: "Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
            options: [
                { id: 0, text: "Enable encryption on the Amazon RDS database using the AWS Console", correct: false },
                { id: 1, text: "Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database", correct: false },
                { id: 2, text: "Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ", correct: false },
                { id: 3, text: "Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable encryption on the Amazon RDS database using the AWS Console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 23,
            text: "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Purchase 70 on-demand instances and 30 reserved instances", correct: false },
                { id: 1, text: "Purchase 70 reserved instances (RIs) and 30 spot instances", correct: true },
                { id: 2, text: "Purchase 70 reserved instances and 30 on-demand instances", correct: false },
                { id: 3, text: "Purchase 70 on-demand instances and 30 spot instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Purchase 70 reserved instances (RIs) and 30 spot instances\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Purchase 70 on-demand instances and 30 reserved instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 70 reserved instances and 30 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 70 on-demand instances and 30 spot instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 24,
            text: "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience. The company is looking at alternate database options and migrating database engines if required. What would you suggest?",
            options: [
                { id: 0, text: "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora", correct: true },
                { id: 1, text: "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump", correct: false },
                { id: 2, text: "Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database", correct: false },
                { id: 3, text: "Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A media production studio is building a content rendering and editing platform on AWS. The editing workstations and rendering tools require access to shared files over the SMB (Server Message Block) protocol. The studio wants a managed storage solution that is simple to set up, integrates easily with SMB clients, and minimizes ongoing operational tasks. Which solution will best meet the requirements with the LEAST administrative overhead?",
            options: [
                { id: 0, text: "Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled", correct: false },
                { id: 1, text: "Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients", correct: false },
                { id: 2, text: "Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs", correct: false },
                { id: 3, text: "Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live. Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
            options: [
                { id: 0, text: "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration", correct: false },
                { id: 1, text: "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration", correct: false },
                { id: 2, text: "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration", correct: true },
                { id: 3, text: "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
                { id: 3, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage. As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
            options: [
                { id: 0, text: "Enable storage auto-scaling for Amazon RDS MySQL", correct: true },
                { id: 1, text: "Create read replica for Amazon RDS MySQL", correct: false },
                { id: 2, text: "Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required", correct: false },
                { id: 3, text: "Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable storage auto-scaling for Amazon RDS MySQL\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create read replica for Amazon RDS MySQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic. As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
            options: [
                { id: 0, text: "Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer", correct: false },
                { id: 1, text: "Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution", correct: false },
                { id: 2, text: "Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 1, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
                { id: 2, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 3, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A multinational logistics company is migrating its core systems to AWS. As part of this migration, the company has built an Amazon S3–based data lake to ingest and analyze supply chain data from external carriers and vendors. While some vendors have adopted the company’s modern REST-based APIs for S3 uploads, others operate legacy systems that rely exclusively on SFTP for file transfers. These vendors are unable or unwilling to modify their workflows to support S3 APIs. The company wants to provide these vendors with an SFTP-compatible solution that allows direct uploads to Amazon S3, and must use fully managed AWS services to avoid managing any infrastructure. It must also support identity federation so that internal teams can map vendor access securely to specific S3 buckets or prefixes. Which combination of options will provide a scalable and low-maintenance solution for this use case? (Select two)",
            options: [
                { id: 0, text: "Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access", correct: true },
                { id: 1, text: "Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions", correct: true },
                { id: 2, text: "Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes", correct: false },
                { id: 3, text: "Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint", correct: false },
                { id: 4, text: "Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access, Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas. Which of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
            options: [
                { id: 0, text: "There are data transfer charges for replicating data across AWS Regions", correct: true },
                { id: 1, text: "There are data transfer charges for replicating data within the same Availability Zone (AZ)", correct: false },
                { id: 2, text: "There are no data transfer charges for replicating data across AWS Regions", correct: false },
                { id: 3, text: "There are data transfer charges for replicating data within the same AWS Region", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: There are data transfer charges for replicating data across AWS Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- There are data transfer charges for replicating data within the same Availability Zone (AZ): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- There are no data transfer charges for replicating data across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- There are data transfer charges for replicating data within the same AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves theAdministratorAccessmanaged policy. How should you proceed?",
            options: [
                { id: 0, text: "For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves", correct: true },
                { id: 1, text: "Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy", correct: false },
                { id: 2, text: "Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves", correct: false },
                { id: 3, text: "Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
                { id: 1, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 2, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 3, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 4, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: By default, user data runs only during the boot cycle when you first launch an instance, By default, scripts entered as user data are executed with root user privileges\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- By default, scripts entered as user data do not have root user privileges for executing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When an instance is running, you can update user data by using root user credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A wildlife research organization uses IoT-based motion sensors attached to thousands of migrating animals to monitor their movement across regions. Every few minutes, a sensor checks for significant movement and sends updated location data to a backend application running on Amazon EC2 instances spread across multiple Availability Zones in a single AWS Region. Recently, an unexpected surge in motion data overwhelmed the application, leading to lost location records with no mechanism to replay missed data. A solutions architect must redesign the ingestion mechanism to prevent future data loss and to minimize operational overhead. What should the solutions architect do to meet these requirements?",
            options: [
                { id: 0, text: "Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages", correct: false },
                { id: 1, text: "Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages", correct: true },
                { id: 2, text: "Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically", correct: false },
                { id: 3, text: "Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 36,
            text: "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key. Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
            options: [
                { id: 0, text: "Server-Side Encryption with Customer-Provided Keys (SSE-C)", correct: true },
                { id: 1, text: "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3", correct: false },
                { id: 2, text: "Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)", correct: false },
                { id: 3, text: "Server-Side Encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Server-Side Encryption with Customer-Provided Keys (SSE-C)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-Side Encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A company has historically operated only in theus-east-1region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into theus-west-1AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions. Which of the following represents the best solution to address these requirements?",
            options: [
                { id: 0, text: "Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region", correct: false },
                { id: 1, text: "Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets", correct: false },
                { id: 2, text: "Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region", correct: false },
                { id: 3, text: "Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "A media publishing company is migrating its legacy content management application to AWS. Currently, the application and its MySQL database run on a single on-premises virtual machine, which creates a single point of failure and limits scalability. As traffic has increased due to growing reader engagement and video uploads, the company needs to redesign the solution to ensure automatic scaling, high availability, and separation of application and database layers. The company wants to continue using a MySQL-compatible engine and needs a cost-effective, managed solution that minimizes operational overhead. Which AWS architecture will best fulfill these requirements?",
            options: [
                { id: 0, text: "Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database", correct: false },
                { id: 1, text: "Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries", correct: false },
                { id: 2, text: "Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage", correct: false },
                { id: 3, text: "Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 39,
            text: "Consider the following policy associated with an IAM group containing several users: Which of the following options is correct?",
            options: [
                { id: 0, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200", correct: false },
                { id: 1, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200", correct: true },
                { id: 2, text: "Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200", correct: false },
                { id: 3, text: "Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution. Which of the following represents the MOST cost-optimal and high-performance solution?",
            options: [
                { id: 0, text: "Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution", correct: true },
                { id: 1, text: "Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin", correct: false },
                { id: 2, text: "Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin", correct: false },
                { id: 3, text: "Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A video conferencing platform serves users worldwide through a globally distributed deployment of Amazon EC2 instances behind Network Load Balancers (NLBs) in several AWS Regions. The platform's architecture currently allows clients to connect to any Region via public endpoints, depending on how DNS resolves. However, users in regions far from the load balancers frequently experience high latency and slow connection times, especially during session initiation. The company wants to optimize the experience for global users by reducing end-to-end latency and load time while keeping the existing NLBs and EC2-based application infrastructure in place. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution", correct: false },
                { id: 1, text: "Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances", correct: false },
                { id: 2, text: "Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy", correct: false },
                { id: 3, text: "Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?",
            options: [
                { id: 0, text: "General Purpose", correct: false },
                { id: 1, text: "Bursting Throughput", correct: false },
                { id: 2, text: "Provisioned Throughput", correct: false },
                { id: 3, text: "Max I/O", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Max I/O\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Bursting Throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned Throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 44,
            text: "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
            options: [
                { id: 0, text: "Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)", correct: true },
                { id: 1, text: "Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand", correct: false },
                { id: 2, text: "Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand", correct: false },
                { id: 3, text: "Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "What does this IAM policy do?",
            options: [
                { id: 0, text: "It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region", correct: false },
                { id: 1, text: "It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region", correct: false },
                { id: 2, text: "It allows running Amazon EC2 instances anywhere but in the eu-west-1 region", correct: false },
                { id: 3, text: "It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows running Amazon EC2 instances anywhere but in the eu-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A financial institution is transitioning its critical back-office systems to AWS. These systems currently rely on Microsoft SQL Server databases hosted on on-premises infrastructure. The data is highly sensitive and subject to regulatory compliance. The organization wants to enhance security and minimize database management tasks as part of the migration. Which solution will best meet these goals with the least operational burden?",
            options: [
                { id: 0, text: "Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data", correct: false },
                { id: 1, text: "Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key", correct: true },
                { id: 2, text: "Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption", correct: false },
                { id: 3, text: "Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 47,
            text: "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture. As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)",
            options: [
                { id: 0, text: "For security group B: Add an inbound rule that allows traffic only from security group A on port 443", correct: false },
                { id: 1, text: "For security group B: Add an inbound rule that allows traffic only from all sources on port 1433", correct: false },
                { id: 2, text: "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443", correct: false },
                { id: 3, text: "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433", correct: true },
                { id: 4, text: "For security group B: Add an inbound rule that allows traffic only from security group A on port 1433", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433, For security group B: Add an inbound rule that allows traffic only from security group A on port 1433\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- For security group B: Add an inbound rule that allows traffic only from security group A on port 443: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- For security group B: Add an inbound rule that allows traffic only from all sources on port 1433: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation. Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
            options: [
                { id: 0, text: "Build a shared services Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 1, text: "Use VPCs connected with AWS Direct Connect", correct: false },
                { id: 2, text: "Use Fully meshed VPC Peering connection", correct: false },
                { id: 3, text: "Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Build a shared services Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPCs connected with AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Fully meshed VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 2, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 3, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this IAM policy do?",
            options: [
                { id: 0, text: "It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block", correct: false },
                { id: 1, text: "It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block", correct: true },
                { id: 2, text: "It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block", correct: false },
                { id: 3, text: "It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A financial services company runs a Kubernetes-based microservices application in its on-premises data center. The application uses the Advanced Message Queuing Protocol (AMQP) to interact with a message queue. The company is experiencing rapid growth and its on-prem infrastructure cannot scale fast enough. The company wants to migrate the application to AWS with minimal code changes and reduce infrastructure management overhead. The messaging component must continue using AMQP, and the solution should offer high scalability and low operational effort. Which combination of options will together meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes", correct: true },
                { id: 1, text: "Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic", correct: false },
                { id: 3, text: "Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery", correct: false },
                { id: 4, text: "Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes, Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances. As a solutions architect, which of the following actions would you recommend to stop the attacks?",
            options: [
                { id: 0, text: "Create a deny rule for the malicious IP in the Security Groups associated with each of the instances", correct: false },
                { id: 1, text: "Create an IP match condition in the AWS WAF to block the malicious IP address", correct: true },
                { id: 2, text: "Create a ticket with AWS support to take action against the malicious IP", correct: false },
                { id: 3, text: "Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an IP match condition in the AWS WAF to block the malicious IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a deny rule for the malicious IP in the Security Groups associated with each of the instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a ticket with AWS support to take action against the malicious IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A SaaS company is modernizing one of its legacy web applications by migrating it to AWS. The company aims to improve the availability of the application during both normal and peak traffic periods. Additionally, the company wants to implement protection against common web exploits and malicious traffic. The architecture must be scalable and integrate AWS WAF to secure incoming traffic. Which solution will best meet these requirements with high availability and minimal configuration complexity?",
            options: [
                { id: 0, text: "Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection", correct: false },
                { id: 1, text: "Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering", correct: false },
                { id: 2, text: "Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic", correct: false },
                { id: 3, text: "Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
            options: [
                { id: 0, text: "Create an AWS Snowball job and target a Amazon S3 Glacier Vault", correct: false },
                { id: 1, text: "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day", correct: true },
                { id: 2, text: "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day", correct: false },
                { id: 3, text: "Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Snowball job and target a Amazon S3 Glacier Vault: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 1, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute. Which of the following options would you recommend?",
            options: [
                { id: 0, text: "Set up an Amazon Aurora provisioned Database cluster", correct: false },
                { id: 1, text: "Set up an Amazon Aurora Global Database cluster", correct: true },
                { id: 2, text: "Set up an Amazon Aurora multi-master Database cluster", correct: false },
                { id: 3, text: "Set up an Amazon Aurora serverless Database cluster", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon Aurora Global Database cluster\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon Aurora provisioned Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Aurora multi-master Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Aurora serverless Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a read replica and modify the application to use the appropriate endpoint\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Activate read-through caching on the Amazon Aurora database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the application to read from the Multi-AZ standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed. Which of the following options represents the best solution for the given requirements?",
            options: [
                { id: 0, text: "Amazon Elastic File System (EFS) Standard–IA storage class", correct: true },
                { id: 1, text: "Amazon Elastic Block Store (EBS)", correct: false },
                { id: 2, text: "Amazon Elastic File System (EFS) Standard storage class", correct: false },
                { id: 3, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Elastic File System (EFS) Standard–IA storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic Block Store (EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic File System (EFS) Standard storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global media agency is developing a cultural analysis project to explore how major sports stories have evolved over the last five years. The team has collected thousands of archived news bulletins and magazine spreads stored in PDF format. These documents are rich in unstructured text and come from various sources with differing layouts and font styles. The agency wants to better understand how public tone and narrative have shifted over time. The team has chosen to use Amazon Textract for its ability to accurately extract printed and scanned text from complex PDF layouts. They need a solution that can then analyze the emotional tone and subject matter of the extracted text with the least possible operational burden, using fully managed AWS services where possible. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.", correct: false },
                { id: 1, text: "Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.", correct: false },
                { id: 2, text: "Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.", correct: true },
                { id: 3, text: "Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which of the following actions meets the given requirements?",
            options: [
                { id: 0, text: "Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user", correct: false },
                { id: 1, text: "Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account", correct: false },
                { id: 2, text: "Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled", correct: false },
                { id: 3, text: "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A mobile app allows users to submit photos, which are stored in an Amazon S3 bucket. Currently, a batch of Amazon EC2 Spot Instances is launched nightly to process all the day’s uploads. Each photo requires approximately 3 minutes and 512 MB of memory to process. To improve responsiveness and minimize costs, the company wants to shift to near real-time image processing that begins as soon as an image is uploaded. Which solution will provide the MOST cost-effective and scalable architecture to meet these new requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt", correct: false },
                { id: 1, text: "Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously", correct: true },
                { id: 2, text: "Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image", correct: false },
                { id: 3, text: "Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)",
            options: [
                { id: 0, text: "Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation", correct: true },
                { id: 1, text: "Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format", correct: false },
                { id: 2, text: "Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format", correct: true },
                { id: 3, text: "Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation", correct: false },
                { id: 4, text: "Create an AWS Lambda function based job to delete the raw zone data after 1 day", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation, Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Lambda function based job to delete the raw zone data after 1 day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines. Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
            options: [
                { id: 0, text: "Spot Instances", correct: false },
                { id: 1, text: "On-Demand Instances", correct: false },
                { id: 2, text: "Dedicated Hosts", correct: false },
                { id: 3, text: "Dedicated Instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend. Which of the following is the correct outcome during the maintenance window?",
            options: [
                { id: 0, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade", correct: false },
                { id: 1, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete", correct: true },
                { id: 2, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade", correct: false },
                { id: 3, text: "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud. What do you recommend as a replacement for the DFSR?",
            options: [
                { id: 0, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 3, text: "Amazon FSx for Lustre", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test15: [
        {
            id: 1,
            text: "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
            options: [
                { id: 0, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold", correct: false },
                { id: 1, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold", correct: false },
                { id: 2, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold", correct: false },
                { id: 3, text: "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 2,
            text: "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
                { id: 1, text: "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: true },
                { id: 2, text: "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket", correct: false },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 2, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 4, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
                { id: 5, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue, Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
            options: [
                { id: 0, text: "Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint", correct: false },
                { id: 1, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: false },
                { id: 2, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint", correct: true },
                { id: 3, text: "Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: true },
                { id: 4, text: "Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint, Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
            options: [
                { id: 0, text: "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 1, text: "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 2, text: "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: false },
                { id: 3, text: "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte. As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
            options: [
                { id: 0, text: "Use Amazon EFS with Bursting Throughput mode", correct: false },
                { id: 1, text: "Use Amazon EFS with Provisioned Throughput mode", correct: true },
                { id: 2, text: "Use Amazon DynamoDB table that is accessible by all ECS cluster instances", correct: false },
                { id: 3, text: "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EFS with Provisioned Throughput mode\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EFS with Bursting Throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB table that is accessible by all ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon EBS volume mounted to the Amazon ECS cluster instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead. What solution would best meet these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume", correct: true },
                { id: 1, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
                { id: 2, text: "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume", correct: false },
                { id: 3, text: "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
            options: [
                { id: 0, text: "Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads", correct: true },
                { id: 1, text: "Use Amazon ElastiCache to improve the performance of compute-intensive workloads", correct: true },
                { id: 2, text: "Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads", correct: false },
                { id: 3, text: "Use Amazon ElastiCache to run highly complex JOIN queries", correct: false },
                { id: 4, text: "Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads, Use Amazon ElastiCache to improve the performance of compute-intensive workloads\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to run highly complex JOIN queries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
            options: [
                { id: 0, text: "You can use an IP address as the custom source for the inbound rule", correct: false },
                { id: 1, text: "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule", correct: false },
                { id: 2, text: "You can use an Internet Gateway ID as the custom source for the inbound rule", correct: true },
                { id: 3, text: "You can use a security group as the custom source for the inbound rule", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: You can use an Internet Gateway ID as the custom source for the inbound rule\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You can use an IP address as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a security group as the custom source for the inbound rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A national logistics company has a dedicated AWS Direct Connect connection from its corporate data center to AWS. Within its AWS account, the company operates 25 Amazon VPCs in the same Region, each supporting different regional distribution services. The VPCs were configured with non-overlapping CIDR blocks and currently use private VIFs for Direct Connect access to on-premises resources. As the architecture scales, the company wants to enable communication across all VPCs and the on-premises environment. The solution must scale efficiently, support full-mesh connectivity, and reduce the complexity of maintaining separate private VIFs for each VPC. Which combination of solutions will best fulfill these requirements with the least amount of operational overhead? (Select two)",
            options: [
                { id: 0, text: "Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs", correct: false },
                { id: 1, text: "Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services", correct: false },
                { id: 2, text: "Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway", correct: true },
                { id: 3, text: "Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing", correct: true },
                { id: 4, text: "Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway, Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic", correct: false },
                { id: 1, text: "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic", correct: false },
                { id: 2, text: "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint", correct: true },
                { id: 3, text: "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A regional transportation authority operates a high-traffic public information portal hosted on AWS. The backend consists of Amazon EC2 instances behind an Application Load Balancer (ALB). In recent weeks, the operations team has observed intermittent slowdowns and performance issues. After investigation, the team suspect the application is being targeted by distributed denial-of-service (DDoS) attacks coming from a wide range of IP addresses. The team needs a solution that provides DDoS mitigation, detailed logs for audit purposes, and requires minimal changes to the existing architecture. Which solution best addresses these needs?",
            options: [
                { id: 0, text: "Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly", correct: false },
                { id: 1, text: "Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events", correct: true },
                { id: 2, text: "Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups", correct: false },
                { id: 3, text: "Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service", correct: false },
                { id: 1, text: "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users", correct: false },
                { id: 2, text: "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face", correct: true },
                { id: 3, text: "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A healthcare startup is modernizing its monolithic Python-based analytics application by transitioning to a microservices architecture on AWS. As a pilot, the team wants to refactor one module into a standalone microservice that can handle hundreds of requests per second. They are seeking an AWS-native solution that supports Python, scales automatically with traffic, and requires minimal infrastructure management and operational overhead to build, test, and deploy the service efficiently. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup", correct: false },
                { id: 1, text: "Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads", correct: true },
                { id: 2, text: "Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments", correct: false },
                { id: 3, text: "Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A company has its application servers in the public subnet that connect to the database instances in the private subnet. For regular maintenance, the database instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
            options: [
                { id: 0, text: "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC", correct: false },
                { id: 1, text: "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables", correct: false },
                { id: 2, text: "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC", correct: false },
                { id: 3, text: "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Egress-only internet gateway for the resources in the private subnet of the VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 16,
            text: "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
            options: [
                { id: 0, text: "General Purpose SSD (gp2)", correct: false },
                { id: 1, text: "Provisioned IOPS SSD (io1)", correct: true },
                { id: 2, text: "Throughput Optimized HDD (st1)", correct: false },
                { id: 3, text: "Cold HDD (sc1)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD (io1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose SSD (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Throughput Optimized HDD (st1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD (sc1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer", correct: false },
                { id: 1, text: "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions", correct: false },
                { id: 2, text: "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints", correct: true },
                { id: 3, text: "Configure Elastic IPs for each of the Application Load Balancers in each Region", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Elastic IPs for each of the Application Load Balancers in each Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account. Which of the following solutions should a solutions architect configure for the necessary permissions?",
            options: [
                { id: 0, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly", correct: false },
                { id: 1, text: "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly", correct: false },
                { id: 2, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role", correct: false },
                { id: 3, text: "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance\n\nIAM roles provide temporary credentials and are the recommended way to grant permissions to AWS services and applications. They're more secure than access keys as credentials are automatically rotated and don't need to be stored.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
            options: [
                { id: 0, text: "Create a CNAME record", correct: true },
                { id: 1, text: "Create an A record", correct: false },
                { id: 2, text: "Create a PTR record", correct: false },
                { id: 3, text: "Create an Alias Record", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a CNAME record\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an A record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a PTR record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Alias Record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 21,
            text: "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets", correct: false },
                { id: 1, text: "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets", correct: true },
                { id: 2, text: "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture", correct: false },
                { id: 3, text: "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 22,
            text: "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
            options: [
                { id: 0, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: false },
                { id: 1, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each", correct: false },
                { id: 2, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each", correct: false },
                { id: 3, text: "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A financial services firm runs a containerized risk analytics tool in its on-premises data center using Docker. The tool depends on persistent data storage for maintaining customer simulation results and operates on a single host machine where the volume is locally mounted. The infrastructure team is looking to replatform the tool to AWS using a fully managed service because they want to avoid managing EC2 instances, volumes, or underlying servers. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually", correct: false },
                { id: 1, text: "Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically", correct: false },
                { id: 2, text: "Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage", correct: false },
                { id: 3, text: "Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 2, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 3, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
            options: [
                { id: 0, text: "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 1, text: "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services", correct: false },
                { id: 2, text: "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services", correct: true },
                { id: 3, text: "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A financial services company is modernizing its analytics platform on AWS. Their legacy data processing scripts, built for both Windows and Linux environments, require shared access to a file system that supports Windows ACLs and SMB protocol for compatibility with existing Windows workloads. At the same time, their Linux-based applications need to read from and write to the same shared storage to maintain cross-platform consistency. The solutions architect needs to design a storage solution that allows both Windows and Linux EC2 instances to access the shared file system simultaneously, while preserving Windows-specific features like NTFS permissions and Active Directory (AD) integration. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms", correct: false },
                { id: 1, text: "Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances", correct: false },
                { id: 2, text: "Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads", correct: false },
                { id: 3, text: "Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
            options: [
                { id: 0, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume", correct: true },
                { id: 1, text: "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected", correct: false },
                { id: 2, text: "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 1, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 2, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 29,
            text: "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
            options: [
                { id: 0, text: "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets", correct: false },
                { id: 1, text: "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data", correct: false },
                { id: 2, text: "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively", correct: true },
                { id: 3, text: "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in theus-west-2region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of theus-west-2region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
            options: [
                { id: 0, text: "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts", correct: false },
                { id: 1, text: "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 2, text: "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts", correct: false },
                { id: 3, text: "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
            options: [
                { id: 0, text: "Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions", correct: false },
                { id: 1, text: "Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions", correct: true },
                { id: 2, text: "Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy", correct: false },
                { id: 3, text: "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 32,
            text: "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use a simple scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 1, text: "Use a target tracking scaling policy based on a custom Amazon SQS queue metric", correct: true },
                { id: 2, text: "Use a step scaling policy based on a custom Amazon SQS queue metric", correct: false },
                { id: 3, text: "Use a scheduled scaling policy based on a custom Amazon SQS queue metric", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a target tracking scaling policy based on a custom Amazon SQS queue metric\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a simple scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a step scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a scheduled scaling policy based on a custom Amazon SQS queue metric: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 1, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
                { id: 2, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
            options: [
                { id: 0, text: "Use SQS message timer to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 1, text: "Use SQS long polling to retrieve messages from your Amazon SQS queues", correct: true },
                { id: 2, text: "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues", correct: false },
                { id: 3, text: "Use SQS short polling to retrieve messages from your Amazon SQS queues", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use SQS long polling to retrieve messages from your Amazon SQS queues\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use SQS message timer to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS visibility timeout to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use SQS short polling to retrieve messages from your Amazon SQS queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
            options: [
                { id: 0, text: "Service control policy (SCP) affects service-linked roles", correct: false },
                { id: 1, text: "Service control policy (SCP) does not affect service-linked role", correct: true },
                { id: 2, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action", correct: false },
                { id: 3, text: "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action", correct: true },
                { id: 4, text: "Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts", correct: true },
                { id: 5, text: "Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts", correct: false },
            ],
            correctAnswers: [1, 3, 4],
            explanation: "The correct answers are: Service control policy (SCP) does not affect service-linked role, If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action, Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Service control policy (SCP) affects service-linked roles: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
            options: [
                { id: 0, text: "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: false },
                { id: 1, text: "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ", correct: false },
                { id: 2, text: "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ", correct: true },
                { id: 3, text: "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 38,
            text: "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
            options: [
                { id: 0, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot", correct: false },
                { id: 1, text: "You cannot share an Amazon Machine Image (AMI) with another AWS account", correct: false },
                { id: 2, text: "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot", correct: true },
                { id: 3, text: "You can share an Amazon Machine Image (AMI) with another AWS account", correct: true },
                { id: 4, text: "You cannot copy an Amazon Machine Image (AMI) across AWS Regions", correct: false },
                { id: 5, text: "You can copy an Amazon Machine Image (AMI) across AWS Regions", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot, You can share an Amazon Machine Image (AMI) with another AWS account, You can copy an Amazon Machine Image (AMI) across AWS Regions\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot share an Amazon Machine Image (AMI) with another AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot copy an Amazon Machine Image (AMI) across AWS Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
            options: [
                { id: 0, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy", correct: false },
                { id: 1, text: "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy", correct: true },
                { id: 2, text: "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy", correct: false },
                { id: 3, text: "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 1, text: "Security Groups can be associated with a NAT gateway", correct: false },
                { id: 2, text: "NAT gateway supports port forwarding", correct: false },
                { id: 3, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 4, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 5, text: "NAT instance supports port forwarding", correct: true },
            ],
            correctAnswers: [0, 4, 5],
            explanation: "The correct answers are: Security Groups can be associated with a NAT instance, NAT instance can be used as a bastion server, NAT instance supports port forwarding\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 1, text: "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones", correct: true },
                { id: 2, text: "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones", correct: false },
                { id: 3, text: "Add Amazon EventBridge to decouple the complex architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon EventBridge to decouple the complex architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 42,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Glue", correct: false },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Database Migration Service (AWS DMS)", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 43,
            text: "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
            options: [
                { id: 0, text: "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 1, text: "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 2, text: "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com", correct: false },
                { id: 3, text: "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 44,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 1, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs", correct: false },
                { id: 2, text: "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data. How will you implement this requirement without adding the overhead of splitting the data into logical groups?",
            options: [
                { id: 0, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: true },
                { id: 1, text: "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data", correct: false },
                { id: 2, text: "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data", correct: false },
                { id: 3, text: "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
            options: [
                { id: 0, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data", correct: false },
                { id: 1, text: "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data", correct: true },
                { id: 2, text: "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data", correct: false },
                { id: 3, text: "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
            options: [
                { id: 0, text: "Amazon Cloud Directory", correct: false },
                { id: 1, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 2, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 3, text: "Active Directory Connector", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Cloud Directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Amazon Elastic File System (Amazon EFS)", correct: false },
                { id: 2, text: "Amazon Simple Storage Service (Amazon S3)", correct: false },
                { id: 3, text: "File Gateway Configuration of AWS Storage Gateway", correct: true },
                { id: 4, text: "Amazon Elastic Block Store (Amazon EBS)", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon FSx for Windows File Server, File Gateway Configuration of AWS Storage Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Elastic File System (Amazon EFS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Storage Service (Amazon S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Elastic Block Store (Amazon EBS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A startup is building a serverless microservices architecture where client applications (web and mobile) authenticate users via a third-party OIDC-compliant identity provider. The backend APIs must validate JSON Web Tokens (JWTs) issued by this provider, enforce scope-based access control, and be cost-effective with minimal latency. The development team wants to use a fully managed service that supports JWT validation natively, without writing custom authentication logic. Which solution should the team implement to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider", correct: true },
                { id: 1, text: "Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer", correct: false },
                { id: 2, text: "Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens", correct: false },
                { id: 3, text: "Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects", correct: true },
                { id: 2, text: "Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution", correct: false },
                { id: 3, text: "Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy", correct: false },
                { id: 4, text: "Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects, Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A media streaming startup is building a set of backend APIs that will be consumed by external mobile applications. To prevent API abuse, protect downstream resources, and ensure fair usage across clients, the architecture must enforce rate limiting and throttling on a per-client basis. The team also wants to define usage quotas and apply different limits to different API consumers. Which solution should the team implement to enforce rate limiting and usage quotas at the API layer?",
            options: [
                { id: 0, text: "Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances", correct: false },
                { id: 1, text: "Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits", correct: false },
                { id: 2, text: "Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client", correct: true },
                { id: 3, text: "Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling", correct: true },
                { id: 1, text: "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas", correct: false },
                { id: 2, text: "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling", correct: false },
                { id: 3, text: "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 53,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A geospatial analytics firm recently completed a large-scale seafloor imaging project and used AWS Snowball Edge Storage Optimized devices to collect and transfer over 150 TB of raw sonar data. The firm uses a high-performance computing (HPC) cluster on AWS to process and analyze massive datasets to identify natural resource formations. The processing workloads require sub-millisecond latency, high-throughput, fast and parallel file access to the imported dataset once the Snowball Edge devices are returned to AWS and the data is ingested. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel", correct: false },
                { id: 1, text: "Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data", correct: true },
                { id: 2, text: "Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access", correct: false },
                { id: 3, text: "Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
            options: [
                { id: 0, text: "Use Amazon FSx for Windows File Server as a shared storage solution", correct: true },
                { id: 1, text: "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies", correct: false },
                { id: 2, text: "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution", correct: false },
                { id: 3, text: "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon FSx for Windows File Server as a shared storage solution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic File System (Amazon EFS) as a shared storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use File Gateway of AWS Storage Gateway to create a hybrid storage solution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 1, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
            options: [
                { id: 0, text: "If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery", correct: false },
                { id: 1, text: "Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance", correct: false },
                { id: 2, text: "During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained", correct: false },
                { id: 3, text: "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery", correct: true },
                { id: 4, text: "A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If your instance has a public IPv4 address, it retains the public IPv4 address after recovery, A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A software company manages a fleet of Amazon EC2 instances that support internal analytics applications. These instances use an IAM role with custom policies to connect to Amazon RDS and AWS Secrets Manager for secure access to credentials and database endpoints. The IT operations team wants to implement a centralized patch management solution that simplifies compliance and security tasks. Their goal is to automate OS patching across EC2 instances without disrupting the running applications. Which approach will allow the company to meet these goals with the least administrative overhead?",
            options: [
                { id: 0, text: "Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts", correct: false },
                { id: 1, text: "Enable Default Host Management Configuration in AWS Systems Manager Quick Setup", correct: true },
                { id: 2, text: "Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features", correct: false },
                { id: 3, text: "Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Default Host Management Configuration in AWS Systems Manager Quick Setup\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
            options: [
                { id: 0, text: "If a spot request is persistent, then it is opened again after your Spot Instance is interrupted", correct: true },
                { id: 1, text: "Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: true },
                { id: 2, text: "When you cancel an active spot request, it terminates the associated instance as well", correct: false },
                { id: 3, text: "Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated", correct: false },
                { id: 4, text: "If a spot request is persistent, then it is opened again after you stop the Spot Instance", correct: false },
                { id: 5, text: "When you cancel an active spot request, it does not terminate the associated instance", correct: true },
            ],
            correctAnswers: [0, 1, 5],
            explanation: "The correct answers are: If a spot request is persistent, then it is opened again after your Spot Instance is interrupted, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, When you cancel an active spot request, it does not terminate the associated instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- When you cancel an active spot request, it terminates the associated instance as well: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- If a spot request is persistent, then it is opened again after you stop the Spot Instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
            options: [
                { id: 0, text: "Internet Gateway (I1)", correct: true },
                { id: 1, text: "Route Table (R1)", correct: false },
                { id: 2, text: "Subnet (S1)", correct: false },
                { id: 3, text: "Network Address Translation (NAT) instance (N1)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Internet Gateway (I1)\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Route Table (R1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subnet (S1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Network Address Translation (NAT) instance (N1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
            options: [
                { id: 0, text: "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 1, text: "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing", correct: false },
                { id: 3, text: "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
            options: [
                { id: 0, text: "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job", correct: true },
                { id: 1, text: "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script", correct: false },
                { id: 2, text: "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
                { id: 3, text: "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
            options: [
                { id: 0, text: "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions", correct: false },
                { id: 1, text: "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions", correct: true },
                { id: 2, text: "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions", correct: false },
                { id: 3, text: "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
            options: [
                { id: 0, text: "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data", correct: false },
                { id: 1, text: "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written", correct: false },
                { id: 2, text: "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written", correct: false },
                { id: 3, text: "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
            options: [
                { id: 0, text: "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ", correct: false },
                { id: 1, text: "Create a read replica and connect the report generation tool/application to it", correct: true },
                { id: 2, text: "Migrate from General Purpose SSD to magnetic storage to enhance IOPS", correct: false },
                { id: 3, text: "Increase the size of Amazon RDS instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a read replica and connect the report generation tool/application to it\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate from General Purpose SSD to magnetic storage to enhance IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the size of Amazon RDS instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
    ],
    test16: [
        {
            id: 1,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 2, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 3, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
                { id: 4, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
            options: [
                { id: 0, text: "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions", correct: false },
                { id: 1, text: "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions", correct: true },
                { id: 2, text: "Use Amazon EC2 user data to speed up the installation process", correct: false },
                { id: 3, text: "Store the installation files in Amazon S3 for quicker retrieval", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to speed up the installation process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the installation files in Amazon S3 for quicker retrieval: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
            options: [
                { id: 0, text: "Set up Amazon DynamoDB table with a global secondary index", correct: false },
                { id: 1, text: "Set up Amazon DynamoDB table in the on-demand capacity mode", correct: true },
                { id: 2, text: "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled", correct: false },
                { id: 3, text: "Set up Amazon DynamoDB global table in the provisioned capacity mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up Amazon DynamoDB table in the on-demand capacity mode\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon DynamoDB table with a global secondary index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon DynamoDB global table in the provisioned capacity mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
            options: [
                { id: 0, text: "EXAMPLE.COM", correct: false },
                { id: 1, text: "example.test.com", correct: false },
                { id: 2, text: "test.example.com", correct: true },
                { id: 3, text: "example.com", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: test.example.com\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- EXAMPLE.COM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.test.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- example.com: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 1, text: "Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
                { id: 3, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
            options: [
                { id: 0, text: "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels", correct: true },
                { id: 1, text: "Use AWS Global Accelerator for the VPN connection to maximize the throughput", correct: false },
                { id: 2, text: "Use Transfer Acceleration for the VPN connection to maximize the throughput", correct: false },
                { id: 3, text: "Create a virtual private gateway with equal cost multipath routing and multiple channels", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Transfer Acceleration for the VPN connection to maximize the throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway with equal cost multipath routing and multiple channels: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Dedicated hosts for the minimum capacity", correct: false },
                { id: 1, text: "Set the minimum capacity to 3", correct: false },
                { id: 2, text: "Use Reserved Instances (RIs) for the minimum capacity", correct: true },
                { id: 3, text: "Set the minimum capacity to 2", correct: true },
                { id: 4, text: "Set the minimum capacity to 1", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Reserved Instances (RIs) for the minimum capacity, Set the minimum capacity to 2\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Dedicated hosts for the minimum capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set the minimum capacity to 1: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 8,
            text: "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
            options: [
                { id: 0, text: "Use a wildcard Secure Sockets Layer certificate (SSL certificate)", correct: false },
                { id: 1, text: "Use Secure Sockets Layer certificate (SSL certificate) with SNI", correct: true },
                { id: 2, text: "Change the Elastic Load Balancing (ELB) SSL Security Policy", correct: false },
                { id: 3, text: "Use an HTTP to HTTPS redirect", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Secure Sockets Layer certificate (SSL certificate) with SNI\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a wildcard Secure Sockets Layer certificate (SSL certificate): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the Elastic Load Balancing (ELB) SSL Security Policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an HTTP to HTTPS redirect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Spot Instances", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon EC2 Reserved Instances (RIs)", correct: false },
                { id: 4, text: "Amazon EC2 On-Demand Instances", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Amazon EC2 Spot Instances, Amazon Simple Queue Service (Amazon SQS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Reserved Instances (RIs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
            options: [
                { id: 0, text: "The Time To Live (TTL) is still in effect", correct: true },
                { id: 1, text: "The health checks are failing", correct: false },
                { id: 2, text: "The Alias Record is misconfigured", correct: false },
                { id: 3, text: "The CNAME Record is misconfigured", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The Time To Live (TTL) is still in effect\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The health checks are failing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Alias Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CNAME Record is misconfigured: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size10.0.1.0/24and the Auto Scaling group is deployed in a subnet of size10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
            options: [
                { id: 0, text: "Add a rule to authorize the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "Add a rule to authorize the CIDR 10.0.1.0/24", correct: false },
                { id: 2, text: "Add a rule to authorize the security group of the Auto Scaling group", correct: false },
                { id: 3, text: "Add a rule to authorize the CIDR 10.0.4.0/22", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Add a rule to authorize the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Add a rule to authorize the CIDR 10.0.1.0/24: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the security group of the Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add a rule to authorize the CIDR 10.0.4.0/22: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A fintech startup hosts its real-time transaction metadata in Amazon DynamoDB tables. During a recent system maintenance event, a junior engineer accidentally deleted a production table, resulting in major service downtime and irreversible data loss. Leadership has mandated an immediate solution that prevents future data loss from human error, while requiring minimal ongoing maintenance or manual effort from the engineering team. Which approach best addresses these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Enable deletion protection on DynamoDB tables", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3", correct: false },
                { id: 2, text: "Enable point-in-time recovery (PITR) on each DynamoDB table", correct: false },
                { id: 3, text: "Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable deletion protection on DynamoDB tables\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable point-in-time recovery (PITR) on each DynamoDB table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare provider is experiencing rapid data growth in its on-premises servers due to increased patient imaging and record retention requirements. The organization wants to extend its storage capacity to AWS in a way that preserves quick access to critical records, including from its local file systems. The company must optimize bandwidth usage during migration and avoid any retrieval fees or delays when accessing the data in the cloud. The provider wants a hybrid cloud solution that requires minimal application reconfiguration, allows frequent local access to key datasets, and ensures that cloud storage costs remain predictable without paying extra for data retrieval. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates", correct: false },
                { id: 1, text: "Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement", correct: false },
                { id: 2, text: "Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3", correct: true },
                { id: 3, text: "Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A genomics research firm is processing sporadic bursts of data-intensive workloads using Amazon EC2 instances. The shared storage must support unpredictable spikes in file operations, but the average daily throughput demand remains relatively low. The team has selected Amazon Elastic File System (EFS) for its scalability and wants to ensure optimal cost and performance during bursts without provisioning throughput manually. Which approach should the team take to best meet these requirements?",
            options: [
                { id: 0, text: "Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads", correct: false },
                { id: 1, text: "Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class", correct: true },
                { id: 2, text: "Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode", correct: false },
                { id: 3, text: "Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use a Cluster placement group", correct: true },
                { id: 1, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 2, text: "Use Spot Instances", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
            options: [
                { id: 0, text: "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment", correct: true },
                { id: 1, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration", correct: false },
                { id: 2, text: "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration", correct: false },
                { id: 3, text: "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
            options: [
                { id: 0, text: "Use a Network Address Translation gateway (NAT gateway)", correct: false },
                { id: 1, text: "Use a VPC peering connection", correct: true },
                { id: 2, text: "Use an AWS Direct Connect connection", correct: false },
                { id: 3, text: "Use an Internet Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a VPC peering connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use a Network Address Translation gateway (NAT gateway): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Direct Connect connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Internet Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 18,
            text: "A transportation logistics company runs a shipment tracking application on Amazon EC2 instances with an Amazon Aurora MySQL database cluster. The application is experiencing rapid growth due to increased demand from mobile app users querying package delivery statuses. Although the compute layer (EC2) has remained stable, the Aurora DB cluster is under growing read pressure, especially from frequent repeated queries about package locations and delivery history. The company added an Aurora read replica, which temporarily alleviated the load, but read traffic continues to spike as user queries grow. The company wants to reduce the repeated reads pressure on the DB cluster. Which solution will best meet these requirements in a cost-effective manner?",
            options: [
                { id: 0, text: "Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas", correct: false },
                { id: 1, text: "Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database", correct: true },
                { id: 2, text: "Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions", correct: false },
                { id: 3, text: "Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect – Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
            options: [
                { id: 0, text: "Use field level encryption in Amazon CloudFront to protect sensitive data for specific content", correct: true },
                { id: 1, text: "Amazon CloudFront can route to multiple origins based on the price class", correct: false },
                { id: 2, text: "Use geo restriction to configure Amazon CloudFront for high-availability and failover", correct: false },
                { id: 3, text: "Amazon CloudFront can route to multiple origins based on the content type", correct: true },
                { id: 4, text: "Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content", correct: false },
                { id: 5, text: "Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover", correct: true },
            ],
            correctAnswers: [0, 3, 5],
            explanation: "The correct answers are: Use field level encryption in Amazon CloudFront to protect sensitive data for specific content, Amazon CloudFront can route to multiple origins based on the content type, Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon CloudFront can route to multiple origins based on the price class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use geo restriction to configure Amazon CloudFront for high-availability and failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
            options: [
                { id: 0, text: "Amazon ElastiCache", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 3, text: "Amazon Neptune", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Neptune: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
            options: [
                { id: 0, text: "Pilot Light", correct: false },
                { id: 1, text: "Warm Standby", correct: true },
                { id: 2, text: "Multi Site", correct: false },
                { id: 3, text: "Backup and Restore", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Warm Standby\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Pilot Light: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi Site: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Backup and Restore: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
            options: [
                { id: 0, text: "Access control list (ACL)", correct: false },
                { id: 1, text: "Trust policy", correct: true },
                { id: 2, text: "Permissions boundary", correct: false },
                { id: 3, text: "AWS Organizations Service Control Policies (SCP)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Trust policy\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Access control list (ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Permissions boundary: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Organizations Service Control Policies (SCP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 24,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon Kinesis", correct: false },
                { id: 1, text: "Amazon DynamoDB", correct: true },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon S3", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Amazon DynamoDB, Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
                { id: 1, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 2, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 3, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 4, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 5, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are: Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold, By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
            options: [
                { id: 0, text: "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years", correct: true },
                { id: 1, text: "Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes", correct: false },
                { id: 2, text: "Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs", correct: false },
                { id: 3, text: "Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
            options: [
                { id: 0, text: "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases", correct: false },
                { id: 1, text: "Use AWS CloudFormation to manage Amazon RDS databases", correct: true },
                { id: 2, text: "Attach an IAM policy to interns preventing them from creating an Amazon RDS database", correct: false },
                { id: 3, text: "Store your recommendations in a custom AWS Trusted Advisor rule", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS CloudFormation to manage Amazon RDS databases\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy to interns preventing them from creating an Amazon RDS database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store your recommendations in a custom AWS Trusted Advisor rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 28,
            text: "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator", correct: true },
                { id: 1, text: "Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer", correct: false },
                { id: 3, text: "Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs):AZ-AandAZ-B. Cross-zone load balancing is disabled.AZ-Ahas four targets andAZ-Bhas six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
            options: [
                { id: 0, text: "Each of the four targets in AZ-A receives 12.5% of the traffic", correct: true },
                { id: 1, text: "Each of the six targets in AZ-B receives 10% of the traffic", correct: false },
                { id: 2, text: "Each of the four targets in AZ-A receives 10% of the traffic", correct: false },
                { id: 3, text: "Each of the four targets in AZ-A receives 8% of the traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Each of the four targets in AZ-A receives 12.5% of the traffic\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Each of the six targets in AZ-B receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 10% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Each of the four targets in AZ-A receives 8% of the traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 30,
            text: "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
            options: [
                { id: 0, text: "Use Amazon RDS Read Replicas", correct: true },
                { id: 1, text: "Use Amazon DynamoDB", correct: false },
                { id: 2, text: "Use Amazon ElastiCache", correct: false },
                { id: 3, text: "Use Amazon RDS Multi-AZ feature", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon RDS Read Replicas\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Multi-AZ feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 31,
            text: "A global pharmaceutical company operates a hybrid cloud network. Its primary AWS workloads run in the us-west-2 Region, connected to its on-premises data center via an AWS Direct Connect connection. After acquiring a biotech firm headquartered in Europe, the company must integrate the biotech’s workloads, which are hosted in several VPCs in the eu-central-1 Region and connected to the biotech's on-premises facility through a separate Direct Connect link. All CIDR blocks are non-overlapping, and the business requires full connectivity between both data centers and all VPCs across the two Regions. The company also wants a scalable solution that minimizes manual network configuration and long-term operational overhead. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication", correct: false },
                { id: 1, text: "Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions", correct: true },
                { id: 2, text: "Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic", correct: false },
                { id: 3, text: "Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon OpenSearch Service", correct: false },
                { id: 1, text: "Amazon Redshift", correct: false },
                { id: 2, text: "Amazon Neptune", correct: true },
                { id: 3, text: "Amazon Aurora", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
            options: [
                { id: 0, text: "Change the Amazon EC2 instance type to something much smaller", correct: false },
                { id: 1, text: "Keep the Amazon EBS volume to io1 and reduce the IOPS", correct: false },
                { id: 2, text: "Convert the Amazon EC2 instance EBS volume to gp2", correct: true },
                { id: 3, text: "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Convert the Amazon EC2 instance EBS volume to gp2\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Change the Amazon EC2 instance type to something much smaller: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Keep the Amazon EBS volume to io1 and reduce the IOPS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs).AZ-Ahas 3 Amazon EC2 instances andAZ-Bhas 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
            options: [
                { id: 0, text: "A random instance in the AZ-A will be terminated", correct: false },
                { id: 1, text: "A random instance will be terminated in AZ-B", correct: false },
                { id: 2, text: "An instance in the AZ-A will be created", correct: false },
                { id: 3, text: "The instance with the oldest launch template or launch configuration will be terminated in AZ-B", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The instance with the oldest launch template or launch configuration will be terminated in AZ-B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- A random instance in the AZ-A will be terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A random instance will be terminated in AZ-B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance in the AZ-A will be created: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
            options: [
                { id: 0, text: "Use a Classic Load Balancer with an Auto Scaling Group", correct: false },
                { id: 1, text: "Use an Application Load Balancer with an Auto Scaling Group", correct: false },
                { id: 2, text: "Use a Network Load Balancer with an Auto Scaling Group", correct: true },
                { id: 3, text: "Use an Auto Scaling Group with Dynamic Elastic IPs attachment", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use a Network Load Balancer with an Auto Scaling Group\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a Classic Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Application Load Balancer with an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling Group with Dynamic Elastic IPs attachment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
            options: [
                { id: 0, text: "Use a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 1, text: "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 2, text: "Use permissions boundary to grant permission to users in its account as well as to users in another account", correct: false },
                { id: 3, text: "Use a bucket policy to grant permission to users in its account as well as to users in another account", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a bucket policy to grant permission to users in its account as well as to users in another account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use permissions boundary to grant permission to users in its account as well as to users in another account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "You have an Amazon S3 bucket that contains files in two different folders -s3://my-bucket/imagesands3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
            options: [
                { id: 0, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days", correct: false },
                { id: 1, text: "Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days", correct: false },
                { id: 2, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days", correct: false },
                { id: 3, text: "Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days", correct: true },
                { id: 4, text: "Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days, Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
            options: [
                { id: 0, text: "Deploy AWS Lambda in a VPC", correct: false },
                { id: 1, text: "Attach an AWS Identity and Access Management (IAM) role to AWS Lambda", correct: true },
                { id: 2, text: "Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL", correct: true },
                { id: 3, text: "Restrict the Amazon RDS database security group to the AWS Lambda's security group", correct: false },
                { id: 4, text: "Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Attach an AWS Identity and Access Management (IAM) role to AWS Lambda, Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Lambda in a VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Restrict the Amazon RDS database security group to the AWS Lambda's security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A ride-hailing startup has launched a mobile app that matches passengers with nearby drivers based on real-time GPS coordinates. The application backend uses an Amazon RDS for PostgreSQL instance with read replicas to store the latitude and longitude of drivers and passengers. As the service scales, the backend experiences performance bottlenecks during peak hours, especially when thousands of updates and reads occur per second to keep location data current. The company expects its user base to double in the next few months and needs a high-performance, scalable solution that can handle frequent write and read operations with minimal latency. What do you recommend?",
            options: [
                { id: 0, text: "Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration", correct: false },
                { id: 1, text: "Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards", correct: false },
                { id: 2, text: "Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy", correct: true },
                { id: 3, text: "Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 1, text: "The route for the health check is misconfigured", correct: true },
                { id: 2, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 3, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 4, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: The route for the health check is misconfigured, The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "A digital media platform is preparing to launch a new interactive content service that is expected to receive sudden spikes in user engagement, especially during live events and media releases. The backend uses an Amazon Aurora PostgreSQL Serverless v2 cluster to handle dynamic workloads. The architecture must be capable of scaling both compute and storage performance to maintain low latency and avoid bottlenecks under load. The engineering team is evaluating storage configuration options and wants a solution that will scale automatically with traffic, optimize I/O performance, and remain cost-effective without manual provisioning or tuning. Which configuration will best meet these requirements?",
            options: [
                { id: 0, text: "Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage", correct: false },
                { id: 1, text: "Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes", correct: false },
                { id: 2, text: "Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks", correct: false },
                { id: 3, text: "Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Aurora’s autoscaling to handle demand spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 2, text: "Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Amazon MQ", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
            options: [
                { id: 0, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time", correct: false },
                { id: 1, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3", correct: true },
                { id: 2, text: "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3", correct: false },
                { id: 3, text: "Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 44,
            text: "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
            options: [
                { id: 0, text: "Latency-based routing", correct: false },
                { id: 1, text: "Geolocation routing", correct: false },
                { id: 2, text: "Geoproximity routing", correct: true },
                { id: 3, text: "Weighted routing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Geoproximity routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Latency-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Geolocation routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Weighted routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A financial services company is implementing two separate data retention policies to comply with regulatory standards: Policy A: Critical transaction records must be immediately available for audit and must not be deleted or overwritten for 7 years. Policy B: Archived compliance data must be stored in a low-cost, long-term storage solution and locked from deletion or modification for at least 10 years. As a solutions architect, which combination of AWS features should you recommend to enforce these policies effectively?",
            options: [
                { id: 0, text: "Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B", correct: false },
                { id: 1, text: "Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B", correct: true },
                { id: 2, text: "Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention", correct: false },
                { id: 3, text: "Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 46,
            text: "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
            options: [
                { id: 0, text: "Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables", correct: false },
                { id: 1, text: "Contact AWS support to map your VPC with subnet", correct: false },
                { id: 2, text: "Check if the security groups allow ping from the source", correct: true },
                { id: 3, text: "Disable Source / Destination check on the Amazon EC2 instance", correct: false },
                { id: 4, text: "Check if the route table is configured with internet gateway", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Check if the security groups allow ping from the source, Check if the route table is configured with internet gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Contact AWS support to map your VPC with subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable Source / Destination check on the Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
            options: [
                { id: 0, text: "Enable Amazon API Gateway Caching", correct: true },
                { id: 1, text: "Switch to using an Application Load Balancer", correct: false },
                { id: 2, text: "Add Amazon Aurora Read Replicas", correct: false },
                { id: 3, text: "Enable AWS Lambda In Memory Caching", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon API Gateway Caching\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Switch to using an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Aurora Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Lambda In Memory Caching: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy?",
            options: [
                { id: 0, text: "It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket", correct: false },
                { id: 1, text: "It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket", correct: true },
                { id: 2, text: "It ensures Amazon EC2 instances that have inherited a security group can access the bucket", correct: false },
                { id: 3, text: "It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures Amazon EC2 instances that have inherited a security group can access the bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Use Egress Only Internet Gateway as a backup connection", correct: false },
                { id: 1, text: "Use AWS Site-to-Site VPN as a backup connection", correct: true },
                { id: 2, text: "Use AWS Direct Connect connection as a primary connection", correct: true },
                { id: 3, text: "Use AWS Site-to-Site VPN as a primary connection", correct: false },
                { id: 4, text: "Use AWS Direct Connect connection as a backup connection", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use AWS Site-to-Site VPN as a backup connection, Use AWS Direct Connect connection as a primary connection\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Egress Only Internet Gateway as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Site-to-Site VPN as a primary connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect connection as a backup connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "What does this AWS CloudFormation snippet do? (Select three)",
            options: [
                { id: 0, text: "It lets traffic flow from one IP on port 22", correct: true },
                { id: 1, text: "It configures a security group's outbound rules", correct: false },
                { id: 2, text: "It configures a security group's inbound rules", correct: true },
                { id: 3, text: "It configures the inbound rules of a network access control list (network ACL)", correct: false },
                { id: 4, text: "It only allows the IP 0.0.0.0 to reach HTTP", correct: false },
                { id: 5, text: "It allows any IP to pass through on the HTTP port", correct: true },
                { id: 6, text: "It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1", correct: false },
            ],
            correctAnswers: [0, 2, 5],
            explanation: "The correct answers are: It lets traffic flow from one IP on port 22, It configures a security group's inbound rules, It allows any IP to pass through on the HTTP port\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- It configures a security group's outbound rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It configures the inbound rules of a network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It only allows the IP 0.0.0.0 to reach HTTP: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 51,
            text: "An e-commerce analytics company is preparing to archive several years of transaction records and customer analytics reports in Amazon S3 for long-term storage. To meet compliance requirements, the archived data must be encrypted at rest. Additionally, the solution must be cost-effective and ensure that key rotation occurs automatically every 12 months to comply with the company’s internal data governance policy. Which solution will meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket", correct: true },
                { id: 1, text: "Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow", correct: false },
                { id: 2, text: "Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings", correct: false },
                { id: 3, text: "Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucket’s default encryption to use the customer managed key. Migrate the data to the S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 1, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 2, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 3, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
                { id: 4, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
            options: [
                { id: 0, text: "Select an Elastic Inference accelerator while launching Amazon EC2 instances", correct: false },
                { id: 1, text: "Select the appropriate capacity reservation while launching Amazon EC2 instances", correct: false },
                { id: 2, text: "Select dedicated instance tenancy while launching Amazon EC2 instances", correct: false },
                { id: 3, text: "Select a cluster placement group while launching Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Select a cluster placement group while launching Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Select an Elastic Inference accelerator while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select the appropriate capacity reservation while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Select dedicated instance tenancy while launching Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
            options: [
                { id: 0, text: "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure", correct: true },
                { id: 1, text: "Schedule manual backups using Redis append-only file (AOF)", correct: false },
                { id: 2, text: "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure", correct: false },
                { id: 3, text: "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Schedule manual backups using Redis append-only file (AOF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Schedule daily automatic backups at a time when you expect low resource utilization for your cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
            options: [
                { id: 0, text: "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 1, text: "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks", correct: true },
                { id: 2, text: "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
                { id: 3, text: "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 56,
            text: "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
            options: [
                { id: 0, text: "Use AWS Snowball", correct: false },
                { id: 1, text: "Use multi-part upload feature of Amazon S3", correct: true },
                { id: 2, text: "Use AWS Direct Connect to provide extra bandwidth", correct: false },
                { id: 3, text: "Use Amazon S3 Versioning", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use multi-part upload feature of Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Snowball: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to provide extra bandwidth: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Versioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 57,
            text: "A media company operates a video rendering pipeline on Amazon EKS, where containerized jobs are scheduled using Kubernetes deployments. The application experiences bursty traffic patterns, particularly during peak streaming hours. The platform uses the Kubernetes Horizontal Pod Autoscaler (HPA) to scale pods based on CPU utilization. A solutions architect observes that the total number of EC2 worker nodes remains constant during traffic spikes, even when all nodes are at maximum resource utilization. The company needs a solution that enables automatic scaling of the underlying compute infrastructure when pod demand exceeds cluster capacity. Which solution should the architect implement to resolve this issue with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure", correct: false },
                { id: 1, text: "Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands", correct: true },
                { id: 2, text: "Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group", correct: false },
                { id: 3, text: "Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two)",
            options: [
                { id: 0, text: "Use Amazon EMR for serverless orchestration of the containerized services", correct: false },
                { id: 1, text: "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services", correct: true },
                { id: 3, text: "Use Amazon SageMaker for serverless orchestration of the containerized services", correct: false },
                { id: 4, text: "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services, Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EMR for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
            options: [
                { id: 0, text: "Use an Amazon CloudFront distribution in front of your website", correct: false },
                { id: 1, text: "Use an Auto Scaling Group", correct: true },
                { id: 2, text: "Use an Amazon Route 53 Multi Value record", correct: false },
                { id: 3, text: "Deploy the website on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an Amazon CloudFront distribution in front of your website: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon Route 53 Multi Value record: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the website on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A streaming media company operates a high-traffic content delivery platform on AWS. The application backend is deployed on Amazon EC2 instances within an Auto Scaling group across multiple Availability Zones in a VPC. The team has observed that workloads follow predictable usage patterns, such as higher viewership on weekends and in the evenings, along with occasional real-time spikes due to viral content. To reduce cost and improve responsiveness, the team wants an automated scaling approach that can forecast future demand using historical usage patterns, scale in advance based on those predictions, and react quickly to unplanned usage surges in real time. Which scaling strategy should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments", correct: false },
                { id: 1, text: "Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes", correct: true },
                { id: 2, text: "Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours", correct: false },
                { id: 3, text: "Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
            options: [
                { id: 0, text: "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture", correct: false },
                { id: 1, text: "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture", correct: false },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture", correct: false },
                { id: 3, text: "Use Amazon EventBridge to decouple the system architecture", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EventBridge to decouple the system architecture\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Elastic Load Balancing (ELB) for effective decoupling of system architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A research organization is running a high-performance computing (HPC) workload using Amazon EC2 instances that are distributed across multiple Availability Zones (AZs) within a single AWS Region. The workload requires access to a shared file system with the lowest possible latency for frequent reads and writes.The team decides to use Amazon Elastic File System (Amazon EFS) for its scalability and simplicity. To ensure optimal performance and reduce network latency, the solution architect must design the architecture so that each EC2 instance can access the file system with the least possible delay. Which of the following is the most appropriate solution to meet these requirements?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones", correct: false },
                { id: 1, text: "Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target", correct: false },
                { id: 2, text: "Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances", correct: false },
                { id: 3, text: "Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A global insurance company is modernizing its infrastructure by migrating multiple line-of-business applications from its on-premises data centers to AWS. These applications will be deployed across several AWS accounts, all governed under a centralized AWS Organizations structure. The company manages all user identities, groups, and access policies within its on-premises Microsoft Active Directory and wants to continue doing so. The goal is to enable seamless single sign-in across all AWS accounts without duplicating user identity stores or manually provisioning accounts. Which solution best meets these requirements in the most operationally efficient manner?",
            options: [
                { id: 0, text: "Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts", correct: false },
                { id: 1, text: "Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider", correct: false },
                { id: 2, text: "Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts", correct: true },
                { id: 3, text: "Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Streams", correct: false },
                { id: 1, text: "Use Amazon DynamoDB DAX", correct: true },
                { id: 2, text: "Use Amazon DynamoDB Global Tables", correct: false },
                { id: 3, text: "Use Amazon ElastiCache", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon DynamoDB DAX\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon DynamoDB Global Tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test17: [
        {
            id: 1,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 2,
            text: "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
            options: [
                { id: 0, text: "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch", correct: true },
                { id: 1, text: "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3", correct: false },
                { id: 2, text: "Disable the Termination from the Auto Scaling Group any time a user reports an issue", correct: false },
                { id: 3, text: "Make a snapshot of the Amazon EC2 instance just before it gets terminated", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Disable the Termination from the Auto Scaling Group any time a user reports an issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make a snapshot of the Amazon EC2 instance just before it gets terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 3,
            text: "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
            options: [
                { id: 0, text: "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too", correct: false },
                { id: 1, text: "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data", correct: false },
                { id: 2, text: "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures", correct: true },
                { id: 3, text: "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
            options: [
                { id: 0, text: "The application will be down until the primary database has recovered itself", correct: false },
                { id: 1, text: "The URL to access the database will change to the standby database", correct: false },
                { id: 2, text: "An email will be sent to the System Administrator asking for manual intervention", correct: false },
                { id: 3, text: "The CNAME record will be updated to point to the standby database", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The CNAME record will be updated to point to the standby database\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- The application will be down until the primary database has recovered itself: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The URL to access the database will change to the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An email will be sent to the System Administrator asking for manual intervention: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A healthcare startup runs a lightweight reporting application on a single Amazon EC2 On-Demand instance. The application is designed to be stateless, fault-tolerant, and optimized for fast rendering of analytics dashboards. During major health events or news cycles, the team observes latency issues and occasional 5xx errors due to traffic spikes. To meet growing demand without over-provisioning resources during off-peak hours, the company wants to implement a cost-effective, scalable solution that ensures consistent performance even under unpredictable load. Which approach best meets the requirements while minimizing costs?",
            options: [
                { id: 0, text: "Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%", correct: false },
                { id: 1, text: "Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly", correct: false },
                { id: 2, text: "Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load", correct: false },
                { id: 3, text: "Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 6,
            text: "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two)",
            options: [
                { id: 0, text: "The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic", correct: true },
                { id: 1, text: "The subnet has been configured to be public and has no access to the internet", correct: false },
                { id: 2, text: "The instance's subnet is not associated with any route table", correct: false },
                { id: 3, text: "The route table in the instance’s subnet should have a route to an Internet Gateway", correct: true },
                { id: 4, text: "The instance's subnet is associated with multiple route tables with conflicting configurations", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic, The route table in the instance’s subnet should have a route to an Internet Gateway\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nInternet Gateways enable communication between resources in your VPC and the internet. They're required for public subnets and must be attached to the VPC and referenced in route tables.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The subnet has been configured to be public and has no access to the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is not associated with any route table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The instance's subnet is associated with multiple route tables with conflicting configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A digital content production company has transitioned all of its media assets to Amazon S3 in an effort to reduce storage costs. However, the rendering engine used in production continues to run in an on-premises data center and requires frequent and low-latency access to large media files. The company wants to implement a storage solution that maintains application performance while keeping costs low. Which approach should the company choose to meet these requirements in the most cost-effective way?",
            options: [
                { id: 0, text: "Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket", correct: false },
                { id: 1, text: "Set up an Amazon S3 File Gateway to provide storage for the on-premises application", correct: true },
                { id: 2, text: "Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel", correct: false },
                { id: 3, text: "Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 File Gateway to provide storage for the on-premises application\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
            options: [
                { id: 0, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 1, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation", correct: true },
                { id: 2, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "A digital media startup allows users to submit images through its web portal. These images are uploaded directly into an Amazon S3 bucket. On average, around 200 images are uploaded daily. The company wants to automatically generate a smaller preview version (thumbnail) of each new image and store the resulting thumbnails in a separate Amazon S3 bucket. The team prefers a design that is low-cost, requires minimal infrastructure management, and automatically reacts to new uploads. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail", correct: false },
                { id: 1, text: "Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket", correct: true },
                { id: 2, text: "Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket", correct: false },
                { id: 3, text: "Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A digital publishing platform stores large volumes of media assets (such as images and documents) in an Amazon S3 bucket. These assets are accessed frequently during business hours by internal editors and content delivery tools. The company has strict encryption policies and currently uses AWS KMS to handle server-side encryption. The cloud operations team notices that AWS KMS request costs are increasing significantly due to the high frequency of object uploads and accesses. The team is now looking for a way to maintain the same encryption method but reduce the cost of KMS usage, especially for frequent access patterns. Which solution meets the company's encryption and cost optimization goals?",
            options: [
                { id: 0, text: "Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object", correct: true },
                { id: 1, text: "Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control", correct: false },
                { id: 2, text: "Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges", correct: false },
                { id: 3, text: "Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
            options: [
                { id: 0, text: "The database user credentials (username and password) configured for the application are incorrect", correct: false },
                { id: 1, text: "The database user credentials (username and password) configured for the application do not have the required privilege for the given database", correct: false },
                { id: 2, text: "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers", correct: true },
                { id: 3, text: "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The database user credentials (username and password) configured for the application are incorrect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The database user credentials (username and password) configured for the application do not have the required privilege for the given database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A biomedical research firm operates a file exchange system for external research partners to upload and download experimental data. Currently, the system runs on two Amazon EC2 Linux instances, each configured with Elastic IP addresses to allow access from trusted IPs. File transfers use the SFTP protocol, and Linux user accounts are manually provisioned to enforce file-level access control. Data is stored on a shared file system mounted to both EC2 instances. The firm wants to modernize the solution to a fully managed, serverless model with high IOPS, fine-grained user permission control, and strict IP-based access restrictions. They also want to reduce operational overhead without sacrificing performance or security. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings", correct: false },
                { id: 1, text: "Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses", correct: false },
                { id: 2, text: "Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF", correct: false },
                { id: 3, text: "Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "An organization operates a legacy reporting tool hosted on an Amazon EC2 instance located within a public subnet of a VPC. This tool aggregates scanned PDF reports from field devices and temporarily stores them on an attached Amazon EBS volume. At the end of each day, the tool transfers the accumulated files to an Amazon S3 bucket for archival. A solutions architect identifies that the files are being uploaded over the internet using S3's public endpoint. To improve security and avoid exposing data traffic to the public internet, the architect needs to reconfigure the setup so that uploads to Amazon S3 occur privately without using the public S3 endpoint. Which solution will fulfill these requirements?",
            options: [
                { id: 0, text: "Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data", correct: false },
                { id: 1, text: "Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint", correct: true },
                { id: 2, text: "Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately", correct: false },
                { id: 3, text: "Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A fintech company currently operates a real-time search and analytics platform on-premises. This platform ingests streaming data from multiple data-producing systems and provides immediate search capabilities and interactive visualizations for end users. As part of its cloud migration strategy, the company wants to rearchitect the solution using AWS-native services. Which of the following represents the most efficient solution?",
            options: [
                { id: 0, text: "Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations", correct: false },
                { id: 1, text: "Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights", correct: false },
                { id: 2, text: "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data", correct: false },
                { id: 3, text: "Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 15,
            text: "A global e-commerce platform currently operates its order processing system in a single on-premises data center located in Europe. As the company grows its customer base across Asia and North America, it plans to deploy the application across multiple AWS Regions to improve availability and reduce latency. The company requires that updates to the central order database be completed in under one second with global consistency. The application layer will be deployed separately in each Region, but the order management data must remain centrally managed and globally synchronized. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region", correct: false },
                { id: 1, text: "Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.", correct: false },
                { id: 2, text: "Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions", correct: false },
                { id: 3, text: "Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
            options: [
                { id: 0, text: "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose", correct: false },
                { id: 1, text: "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 2, text: "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications", correct: false },
                { id: 3, text: "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
            options: [
                { id: 0, text: "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed", correct: false },
                { id: 1, text: "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page", correct: false },
                { id: 2, text: "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: true },
                { id: 3, text: "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 18,
            text: "A retail enterprise is expanding its hybrid IT infrastructure and plans to securely connect its on-premises corporate network to its AWS environment. The company wants to ensure that all data exchanged between on-premises systems and AWS is encrypted at both the network and session layers. Additionally, the solution must incorporate granular security controls that restrict unnecessary or unauthorized access between the cloud and on-premises environments. A solutions architect must recommend a scalable and secure approach that supports these goals. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems", correct: true },
                { id: 1, text: "Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed", correct: false },
                { id: 2, text: "Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies", correct: false },
                { id: 3, text: "Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 19,
            text: "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
            options: [
                { id: 0, text: "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type", correct: true },
                { id: 1, text: "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type", correct: false },
                { id: 2, text: "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type", correct: false },
                { id: 3, text: "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
            options: [
                { id: 0, text: "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service", correct: false },
                { id: 1, text: "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage", correct: false },
                { id: 2, text: "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises", correct: false },
                { id: 3, text: "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure Amazon EFS to provide a fast, cost-effective and sharable storage service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 21,
            text: "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
            options: [
                { id: 0, text: "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option", correct: false },
                { id: 3, text: "Use Amazon EC2 instances with Instance Store as the storage option", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 instances with Instance Store as the storage option\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 22,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 3, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 23,
            text: "A logistics company runs a two-step job handling process on AWS. The first step quickly receives job submissions from clients, while the second step requires longer processing time to complete each job. Currently, both steps run on separate Amazon EC2 Auto Scaling groups. However, during high-demand hours, the job processing stage falls behind, and there is concern that jobs may be lost due to instance termination during scaling events. A solutions architect needs to design a more scalable and reliable architecture that preserves job data and accommodates fluctuating demand in both stages. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue", correct: true },
                { id: 1, text: "Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue", correct: false },
                { id: 2, text: "Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior", correct: false },
                { id: 3, text: "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
                { id: 1, text: "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: true },
                { id: 2, text: "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage", correct: false },
                { id: 3, text: "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "A global enterprise is modernizing its hybrid IT infrastructure to improve both availability and network performance. The company operates a TCP-based application hosted on Amazon EC2 instances that are deployed across multiple AWS Regions, while a secondary UDP-based component of the application is hosted in its on-premises data centers. These application components must be accessed by customers around the world with minimal latency and consistent uptime. Which combination of options should a solutions architect implement for the given use case? (Select two)",
            options: [
                { id: 0, text: "Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers", correct: true },
                { id: 1, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups", correct: false },
                { id: 2, text: "Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover", correct: false },
                { id: 3, text: "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups", correct: true },
                { id: 4, text: "Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers, Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "A SaaS analytics company is deploying a microservices-based application on Amazon ECS using the Fargate launch type. The application requires access to a shared, POSIX-compliant file system that is available across multiple Availability Zones for redundancy and availability. To meet compliance requirements, the system must support regional backups and cross-Region data recovery with a recovery point objective (RPO) of no more than 8 hours. A backup strategy will be implemented using AWS Backup to automate replication across Regions. As the lead cloud architect, you are evaluating file storage solutions that align with these requirements. Which option best meets the application’s availability, durability, and RPO objectives?",
            options: [
                { id: 0, text: "Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically", correct: false },
                { id: 1, text: "Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata", correct: false },
                { id: 2, text: "Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region", correct: false },
                { id: 3, text: "Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
                { id: 1, text: "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets", correct: false },
                { id: 2, text: "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection", correct: false },
                { id: 3, text: "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances", correct: false },
                { id: 4, text: "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files, Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A financial auditing firm uses Amazon S3 to store sensitive client records that are subject to write-once-read-many (WORM) regulations to prevent alteration or deletion of records for a specific retention period. The firm wants to enforce immutable storage, such that even administrators cannot overwrite or delete the records during the lock duration. They also need audit-friendly enforcement to prevent accidental or malicious deletion. Which configuration of S3 Object Lock will ensure that the retention policy is strictly enforced, and no user (including root or administrators) can override or delete protected objects during the lock period?",
            options: [
                { id: 0, text: "Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period", correct: true },
                { id: 1, text: "Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period", correct: false },
                { id: 2, text: "Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period", correct: false },
                { id: 3, text: "Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 2, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 3, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 4, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [2, 3, 5],
            explanation: "The correct answers are: Data moving between the volume and the instance is encrypted, Any snapshot created from the volume is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An enterprise is developing an internal compliance framework for its cloud infrastructure hosted on AWS. The enterprise uses AWS Organizations to group accounts under various organizational units (OUs) based on departmental function. As part of its governance controls, the security team mandates that all Amazon EC2 instances must be tagged to indicate the level of data classification — either 'confidential' or 'public'. Additionally, the organization must ensure that IAM users cannot launch EC2 instances without assigning a classification tag, nor should they be able to remove the tag from running instances. A solutions architect must design a solution to meet these compliance controls while minimizing operational overhead. Which combination of steps will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations", correct: true },
                { id: 1, text: "Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected", correct: false },
                { id: 2, text: "Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts", correct: true },
                { id: 3, text: "Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning", correct: false },
                { id: 4, text: "Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations, Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A financial data processing company runs a workload on Amazon EC2 instances that fetch and process real-time transaction batches from an Amazon SQS queue. The application needs to scale based on unpredictable message volume, which fluctuates significantly throughout the day. The system must process messages with minimal delay and no downtime, even during peak spikes. The company is seeking a solution that balances cost-efficiency with availability and elasticity. Which EC2 purchasing strategy best meets these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes", correct: false },
                { id: 1, text: "Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume", correct: true },
                { id: 2, text: "Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations", correct: false },
                { id: 3, text: "Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB for distributed in-memory cache based session management", correct: false },
                { id: 1, text: "Use Amazon Elasticache for distributed in-memory cache based session management", correct: true },
                { id: 2, text: "Use Amazon RDS for distributed in-memory cache based session management", correct: false },
                { id: 3, text: "Use Application Load Balancer sticky sessions", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Elasticache for distributed in-memory cache based session management\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS for distributed in-memory cache based session management: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer sticky sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 34,
            text: "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations", correct: true },
                { id: 1, text: "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed", correct: false },
                { id: 2, text: "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user", correct: false },
                { id: 3, text: "Set up an Amazon Route 53 geoproximity routing policy to route traffic", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon Route 53 geoproximity routing policy to route traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 35,
            text: "A tech company runs a web application that includes multiple internal services deployed across Amazon EC2 instances within a VPC. These services require communication with a third-party SaaS provider's API for analytics and billing, which is also hosted on the AWS infrastructure. The company is concerned about minimizing public internet exposure while maintaining secure and reliable connectivity. The solution must ensure private access without allowing unsolicited incoming traffic from the SaaS provider. Which solution will best meet these requirements?",
            options: [
                { id: 0, text: "Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC", correct: true },
                { id: 1, text: "Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider", correct: false },
                { id: 2, text: "Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication", correct: false },
                { id: 3, text: "Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
            options: [
                { id: 0, text: "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 1, text: "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone", correct: false },
                { id: 2, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6", correct: false },
                { id: 3, text: "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A mobile-based e-learning platform is migrating its backend storage layer to Amazon DynamoDB to support a rapidly increasing number of student users and learning transactions. The platform must ensure seamless availability and minimal disruption for a global user base. The DynamoDB design must provide low-latency performance, high availability, and automatic fault tolerance across geographies with the lowest possible operational overhead and cost. Which solution will fulfill these needs in the most cost-efficient manner?",
            options: [
                { id: 0, text: "Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions", correct: false },
                { id: 1, text: "Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions", correct: false },
                { id: 2, text: "Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability", correct: true },
                { id: 3, text: "Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 38,
            text: "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output", correct: false },
                { id: 1, text: "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments", correct: true },
                { id: 2, text: "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes", correct: false },
                { id: 3, text: "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
            options: [
                { id: 0, text: "Both the Auto Scaling group and Application Load Balancer are using ALB based health check", correct: false },
                { id: 1, text: "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check", correct: false },
                { id: 2, text: "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check", correct: false },
                { id: 3, text: "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Both the Auto Scaling group and Application Load Balancer are using ALB based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 40,
            text: "A fintech company recently conducted a security audit and discovered that some IAM roles and Amazon S3 buckets might be unintentionally shared with external accounts or publicly accessible. The security team wants to identify these overly permissive resources and ensure that only intended principals (within their AWS Organization or specific AWS accounts) have access. They need a solution that can analyze IAM policies and resource policies to detect unintended access paths to AWS resources such as S3 buckets, IAM roles, KMS keys, and SNS topics. Which solution should the team use to meet this requirement?",
            options: [
                { id: 0, text: "Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment", correct: false },
                { id: 1, text: "Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization", correct: true },
                { id: 2, text: "Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access", correct: false },
                { id: 3, text: "Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 41,
            text: "An enterprise runs a critical Oracle database workload in its on-premises environment. The company now plans to replicate both existing records and continuous transactional changes to a managed Oracle environment in AWS. The target database will run on Amazon RDS for Oracle. Data transfer volume is expected to fluctuate throughout the day, and the team wants the solution to provision compute resources automatically based on actual workload requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected", correct: false },
                { id: 1, text: "Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.", correct: false },
                { id: 2, text: "Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.", correct: false },
                { id: 3, text: "Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 42,
            text: "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two)",
            options: [
                { id: 0, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions", correct: true },
                { id: 1, text: "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region", correct: false },
                { id: 2, text: "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage", correct: false },
                { id: 3, text: "Use cross-Region Read Replicas", correct: true },
                { id: 4, text: "Use the database cloning feature of the Amazon RDS Database cluster", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions, Use cross-Region Read Replicas\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the database cloning feature of the Amazon RDS Database cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A DevOps team is tasked with enabling secure and temporary SSH access to Amazon EC2 instances for developers during deployments. The team wants to avoid distributing long-term SSH key pairs and instead prefers ephemeral access that can be audited and revoked immediately after the session ends. The team wants direct access via the AWS Management Console. What do you recommend?",
            options: [
                { id: 0, text: "Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address", correct: true },
                { id: 1, text: "Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint", correct: false },
                { id: 2, text: "Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions", correct: false },
                { id: 3, text: "Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through", correct: false },
                { id: 1, text: "Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances", correct: false },
                { id: 2, text: "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through", correct: true },
                { id: 3, text: "Use Application Load Balancer geo match statement listing the countries that you want to block", correct: false },
                { id: 4, text: "Use AWS WAF geo match statement listing the countries that you want to block", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through, Use AWS WAF geo match statement listing the countries that you want to block\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Application Load Balancer geo match statement listing the countries that you want to block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A digital design company has migrated its project archiving platform to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group that spans multiple Availability Zones. Designers upload and retrieve high-resolution image files from a shared file system, which is currently configured to use Amazon EFS Standard-IA. Metadata for these files is stored and indexed in an Amazon RDS for PostgreSQL database. The company's cloud engineering team has been asked to optimize storage costs for the image archive without compromising reliability. They are open to refactoring the application to use managed AWS services when necessary. Which solution offers the most cost-effective architecture?",
            options: [
                { id: 0, text: "Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path", correct: false },
                { id: 1, text: "Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost", correct: false },
                { id: 2, text: "Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 1, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 3, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A data analytics team at a global media firm is building a new analytics platform to process large volumes of both historical and real-time data. This data is stored in Amazon S3. The team wants to implement a serverless solution that allows them to query the data directly using SQL. Additionally, the solution must ensure that all data is encrypted at rest and automatically replicated to another AWS Region to support business continuity. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 1, text: "Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data", correct: true },
                { id: 2, text: "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data", correct: false },
                { id: 3, text: "Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls", correct: false },
                { id: 1, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
                { id: 2, text: "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls", correct: true },
                { id: 3, text: "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A company hosts a Microsoft SQL Server database on Amazon EC2 instances with attached Amazon EBS volumes. The operations team takes daily snapshots of these EBS volumes as backups. However, a recent incident occurred in which an automated script designed to clean up expired snapshots accidentally deleted all available snapshots, leading to potential data loss. The company wants to improve the backup strategy to avoid permanent data loss while still ensuring that old snapshots are eventually removed to optimize cost. A solutions architect needs to implement a mechanism that prevents immediate and irreversible deletion of snapshots. Which solution will best meet these requirements with the least development effort?",
            options: [
                { id: 0, text: "Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots", correct: true },
                { id: 1, text: "Set up the IAM policy of the user to deny EBS snapshot deletion", correct: false },
                { id: 2, text: "Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery", correct: false },
                { id: 3, text: "Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots\n\nRDS snapshots are point-in-time backups stored in S3. They can be used to restore databases, create new instances, or copy databases across regions.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up the IAM policy of the user to deny EBS snapshot deletion: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 50,
            text: "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 1, text: "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
            options: [
                { id: 0, text: "Use long polling to handle message processing failures", correct: false },
                { id: 1, text: "Use a dead-letter queue to handle message processing failures", correct: true },
                { id: 2, text: "Use a temporary queue to handle message processing failures", correct: false },
                { id: 3, text: "Use short polling to handle message processing failures", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a dead-letter queue to handle message processing failures\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use long polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a temporary queue to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use short polling to handle message processing failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting aProvisionedThroughputExceededExceptionexception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
            options: [
                { id: 0, text: "Decrease the Stream retention duration", correct: false },
                { id: 1, text: "Use batch messages", correct: true },
                { id: 2, text: "Increase the number of shards", correct: false },
                { id: 3, text: "Use Exponential Backoff", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use batch messages\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Decrease the Stream retention duration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the number of shards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Exponential Backoff: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two)",
            options: [
                { id: 0, text: "Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target", correct: false },
                { id: 1, text: "Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica", correct: true },
                { id: 2, text: "Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target", correct: true },
                { id: 3, text: "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes", correct: false },
                { id: 4, text: "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica, Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 54,
            text: "An enterprise SaaS provider is currently operating a legacy web application hosted on a single Amazon EC2 instance within a public subnet. The same instance also hosts a MySQL database. DNS records for the application are configured through Amazon Route 53. As part of a modernization initiative, the company wants to rearchitect this application for high availability and scalability. In addition, the company wants to improve read performance on the database layer to handle increasing user traffic. Which combination of solutions will meet these requirements? (Select two)",
            options: [
                { id: 0, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: false },
                { id: 1, text: "Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages", correct: false },
                { id: 2, text: "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly", correct: true },
                { id: 3, text: "Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions", correct: false },
                { id: 4, text: "Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly, Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 55,
            text: "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
            options: [
                { id: 0, text: "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 1, text: "Use Amazon Athena to run SQL based analytics against Amazon S3 data", correct: true },
                { id: 2, text: "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks", correct: false },
                { id: 3, text: "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Athena to run SQL based analytics against Amazon S3 data\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A media streaming company expects a major increase in user activity during the launch of a highly anticipated live event. The streaming platform is deployed on AWS and uses Amazon EC2 instances for the application layer and Amazon RDS for persistent storage. The operations team needs to proactively monitor system performance to ensure a smooth user experience during the event. Their monitoring setup must provide data visibility with intervals of no more than 2 minutes, and the team prefers a solution that is quick to implement and low-maintenance. Which solution should the team implement?",
            options: [
                { id: 0, text: "Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena", correct: false },
                { id: 1, text: "Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics", correct: false },
                { id: 2, text: "Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics", correct: false },
                { id: 3, text: "Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
            options: [
                { id: 0, text: "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications", correct: true },
                { id: 1, text: "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications", correct: false },
                { id: 2, text: "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates", correct: false },
                { id: 3, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 58,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 1, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
                { id: 3, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 59,
            text: "A streaming service provider collects user experience feedback through embedded feedback forms in their mobile and web apps. Feedback submissions frequently spike to thousands per hour during content launches or service outages. Currently, the feedback is sent via email to the operations team for manual review. The company now wants to automate feedback collection and sentiment analysis so that insights can be generated quickly and stored for a full year for trend analysis. Which solution provides the most scalable and automated approach to meet these requirements?",
            options: [
                { id: 0, text: "Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item", correct: true },
                { id: 1, text: "Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table", correct: false },
                { id: 2, text: "Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months", correct: false },
                { id: 3, text: "Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item\n\nAmazon SQS is a message queuing service, but it's pull-based and not ideal for real-time push notifications to mobile apps. SNS is better suited for push notifications to mobile applications as it supports mobile push notification services.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
            options: [
                { id: 0, text: "Use Amazon Route 53 with Amazon CloudFront distribution", correct: false },
                { id: 1, text: "Use AWS Firewall Manager with CloudFront distribution", correct: false },
                { id: 2, text: "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution", correct: true },
                { id: 3, text: "Use AWS Security Hub with Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Firewall Manager with CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Security Hub with Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "A financial analytics firm runs performance-intensive modeling software on Amazon EC2 instances backed by Amazon EBS volumes. The production data resides on EBS volumes attached to EC2 instances in the same AWS Region where the testing environment is hosted. To maintain data integrity, any changes made during testing must not affect production data. The development team needs to frequently create clones of this production data for simulations. The modeling software requires high and consistent I/O performance, and the firm wants to minimize the time required to provision test data. Which solution should a solutions architect recommend to meet these requirements?",
            options: [
                { id: 0, text: "Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment", correct: true },
                { id: 1, text: "Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance", correct: false },
                { id: 2, text: "Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations", correct: false },
                { id: 3, text: "Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A multinational logistics company operates its shipment tracking platform from Amazon EC2 instances deployed in the AWS us-west-2 Region. The platform exposes a set of APIs over HTTPS, which are used by logistics partners and customers around the world to retrieve real-time tracking data. The company has observed that users from Europe and Asia experience latency issues and inconsistent API response times when accessing the service. As a cloud architect, you have been tasked to propose the most cost-effective solution to improve performance for these international users without migrating the application. Which solution should you recommend?",
            options: [
                { id: 0, text: "Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups", correct: true },
                { id: 1, text: "Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2", correct: false },
                { id: 2, text: "Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region", correct: false },
                { id: 3, text: "Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "A media company operates a web application that enables users to upload photos. These uploads are stored in an Amazon S3 bucket located in the eu-west-2 Region. To enhance performance and provide secure access under a custom domain name, the company wants to integrate Amazon CloudFront for uploads to the S3 bucket. The architecture must support secure HTTPS connections using a custom domain, and the upload process must ensure optimal speed and security. Which combination of actions will fulfill these requirements? (Select two)",
            options: [
                { id: 0, text: "Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution", correct: false },
                { id: 1, text: "Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations", correct: false },
                { id: 2, text: "Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL", correct: false },
                { id: 3, text: "Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)", correct: true },
                { id: 4, text: "Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC), Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 64,
            text: "A digital media company runs its content rendering service on Amazon EC2 instances that are registered with an Application Load Balancer (ALB) using IP-based target groups. The company relies on AWS Systems Manager to manage and patch these instances regularly. According to new compliance requirements, EC2 instances must be safely removed from production traffic during patching to prevent user disruption and maintain application integrity. However, during the most recent patch cycle, the operations team noticed application failures and API timeouts, even though patching succeeded on the instances. You are asked to suggest a reliable and scalable way to ensure safe patching while preserving service availability. Which solution will best meet the new compliance and operational requirements? (Select two)",
            options: [
                { id: 0, text: "Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window", correct: true },
                { id: 1, text: "Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata", correct: false },
                { id: 2, text: "Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching", correct: true },
                { id: 3, text: "Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes", correct: false },
                { id: 4, text: "Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window, Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "Use Amazon ElastiCache for Memcached", correct: false },
                { id: 2, text: "Use AWS Global Accelerator", correct: false },
                { id: 3, text: "Use Amazon ElastiCache for Redis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon ElastiCache for Redis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon ElastiCache for Memcached: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
    ],
    test18: [
        {
            id: 1,
            text: "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
            options: [
                { id: 0, text: "On-Demand Instances", correct: false },
                { id: 1, text: "Spot Instances", correct: false },
                { id: 2, text: "Dedicated Instances", correct: true },
                { id: 3, text: "Dedicated Hosts", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Dedicated Instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- On-Demand Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Dedicated Hosts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
            options: [
                { id: 0, text: "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication", correct: false },
                { id: 1, text: "Use the Amazon EC2 instances private IP for the replication", correct: true },
                { id: 2, text: "Create a Private Link between the two Amazon EC2 instances", correct: false },
                { id: 3, text: "Use an Elastic Fabric Adapter (EFA)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Amazon EC2 instances private IP for the replication\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Private Link between the two Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Elastic Fabric Adapter (EFA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 3,
            text: "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
            options: [
                { id: 0, text: "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: true },
                { id: 1, text: "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity", correct: true },
                { id: 2, text: "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted", correct: false },
                { id: 3, text: "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification", correct: false },
                { id: 4, text: "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification, A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 4,
            text: "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
            options: [
                { id: 0, text: "Create a policy and assign it manually to the 50 users", correct: false },
                { id: 1, text: "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA", correct: false },
                { id: 2, text: "Create a group, attach the policy to the group and place the users in the group", correct: true },
                { id: 3, text: "Update the Amazon S3 bucket policy", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create a group, attach the policy to the group and place the users in the group\n\nIAM policies define permissions using JSON. They can be attached to users, groups, or roles. Policies specify what actions are allowed or denied on which resources under what conditions.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a policy and assign it manually to the 50 users: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Update the Amazon S3 bucket policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 5,
            text: "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
            options: [
                { id: 0, text: "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation", correct: false },
                { id: 1, text: "You can specify instance store volumes for an instance when you launch or restart it", correct: false },
                { id: 2, text: "An instance store is a network storage type", correct: false },
                { id: 3, text: "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved", correct: true },
                { id: 4, text: "You can't detach an instance store volume from one instance and attach it to a different instance", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved, You can't detach an instance store volume from one instance and attach it to a different instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can specify instance store volumes for an instance when you launch or restart it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- An instance store is a network storage type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
            options: [
                { id: 0, text: "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process", correct: false },
                { id: 1, text: "Use AWS CLI to run the user data scripts only once while launching the instance", correct: false },
                { id: 2, text: "Run the custom scripts as user data scripts on the Amazon EC2 instances", correct: true },
                { id: 3, text: "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Run the custom scripts as user data scripts on the Amazon EC2 instances\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CLI to run the user data scripts only once while launching the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run the custom scripts as instance metadata scripts on the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 7,
            text: "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions. Which solution best meets these requirements while ensuring the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3", correct: true },
                { id: 1, text: "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx", correct: false },
                { id: 2, text: "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes", correct: false },
                { id: 3, text: "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application. Which solution will most effectively address the performance issues with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users", correct: true },
                { id: 1, text: "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket", correct: false },
                { id: 2, text: "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency", correct: false },
                { id: 3, text: "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
            options: [
                { id: 0, text: "Use AWS Shield Advanced and setup a rate-based rule", correct: false },
                { id: 1, text: "Configure Sticky Sessions on the Application Load Balancer", correct: false },
                { id: 2, text: "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule", correct: true },
                { id: 3, text: "Define a network access control list (network ACL) on your Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use AWS Shield Advanced and setup a rate-based rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Sticky Sessions on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define a network access control list (network ACL) on your Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 10,
            text: "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
            options: [
                { id: 0, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: false },
                { id: 1, text: "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 2, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer", correct: false },
                { id: 3, text: "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
            options: [
                { id: 0, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 1, text: "AWS Global Accelerator", correct: false },
                { id: 2, text: "Amazon ElastiCache for Redis", correct: false },
                { id: 3, text: "Amazon ElastiCache for Memcached", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon ElastiCache for Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache for Redis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 12,
            text: "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
            options: [
                { id: 0, text: "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 1, text: "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2", correct: false },
                { id: 2, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2", correct: true },
                { id: 3, text: "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
            options: [
                { id: 0, text: "Amazon Relational Database Service (Amazon RDS)", correct: false },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: true },
                { id: 3, text: "Amazon Redshift", correct: false },
                { id: 4, text: "Amazon DynamoDB Accelerator (DAX)", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon ElastiCache, Amazon DynamoDB Accelerator (DAX)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Relational Database Service (Amazon RDS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
            options: [
                { id: 0, text: "Create a Spot Fleet request", correct: false },
                { id: 1, text: "Assign an Amazon EC2 Instance Role to perform the necessary API calls", correct: true },
                { id: 2, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1", correct: true },
                { id: 3, text: "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group", correct: false },
                { id: 4, text: "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it", correct: true },
                { id: 5, text: "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are: Assign an Amazon EC2 Instance Role to perform the necessary API calls, Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1, Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Spot Fleet request: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 15,
            text: "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
            options: [
                { id: 0, text: "Simple Active Directory (Simple AD)", correct: false },
                { id: 1, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: true },
                { id: 2, text: "Active Directory Connector", correct: false },
                { id: 3, text: "AWS Transit Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Simple Active Directory (Simple AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Active Directory Connector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Transit Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 16,
            text: "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
            options: [
                { id: 0, text: "Set up Amazon Kinesis Data Streams to ingest the data", correct: true },
                { id: 1, text: "Set up AWS Fargate with Amazon ECS to process the data", correct: true },
                { id: 2, text: "Set up AWS Database Migration Service (AWS DMS) to ingest the data", correct: false },
                { id: 3, text: "Set up AWS Lambda with AWS Step Functions to process the data", correct: false },
                { id: 4, text: "Provision Amazon EC2 instances in an Auto Scaling group to process the data", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Set up Amazon Kinesis Data Streams to ingest the data, Set up AWS Fargate with Amazon ECS to process the data\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up AWS Database Migration Service (AWS DMS) to ingest the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up AWS Lambda with AWS Step Functions to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision Amazon EC2 instances in an Auto Scaling group to process the data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 17,
            text: "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work. Which solution should the company implement to meet these requirements with the least operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access", correct: false },
                { id: 1, text: "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically", correct: false },
                { id: 2, text: "Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets", correct: true },
                { id: 3, text: "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure", correct: true },
                { id: 1, text: "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones", correct: false },
                { id: 2, text: "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.", correct: false },
                { id: 3, text: "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency. Which solution best satisfies these requirements in the most cost-effective manner?",
            options: [
                { id: 0, text: "Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint", correct: false },
                { id: 1, text: "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway", correct: false },
                { id: 2, text: "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances", correct: false },
                { id: 3, text: "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 20,
            text: "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
            options: [
                { id: 0, text: "Elastic Fabric Adapter (EFA)", correct: true },
                { id: 1, text: "Elastic IP Address (EIP)", correct: false },
                { id: 2, text: "Elastic Network Adapter (ENA)", correct: false },
                { id: 3, text: "Elastic Network Interface (ENI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Elastic Fabric Adapter (EFA)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Elastic IP Address (EIP): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Adapter (ENA): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Network Interface (ENI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
            options: [
                { id: 0, text: "enableVpcHostnames", correct: false },
                { id: 1, text: "enableVpcSupport", correct: false },
                { id: 2, text: "enableDnsHostnames", correct: true },
                { id: 3, text: "enableDnsSupport", correct: true },
                { id: 4, text: "enableDnsDomain", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: enableDnsHostnames, enableDnsSupport\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- enableVpcHostnames: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableVpcSupport: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- enableDnsDomain: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
            options: [
                { id: 0, text: "Migrate the Amazon Redshift underlying storage to Amazon S3 IA", correct: false },
                { id: 1, text: "Analyze the cold data with Amazon Athena", correct: true },
                { id: 2, text: "Create a smaller Amazon Redshift Cluster with the cold data", correct: false },
                { id: 3, text: "Move the data to Amazon S3 Glacier Deep Archive after 30 days", correct: false },
                { id: 4, text: "Move the data to Amazon S3 Standard IA after 30 days", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Analyze the cold data with Amazon Athena, Move the data to Amazon S3 Standard IA after 30 days\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Migrate the Amazon Redshift underlying storage to Amazon S3 IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a smaller Amazon Redshift Cluster with the cold data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Move the data to Amazon S3 Glacier Deep Archive after 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 23,
            text: "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region. The firm has defined strict DR objectives: Recovery Point Objective (RPO): ≤ 5 minutes Recovery Time Objective (RTO): ≤ 15 minutes Leadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
            options: [
                { id: 0, text: "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes", correct: false },
                { id: 1, text: "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover", correct: false },
                { id: 2, text: "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events", correct: false },
                { id: 3, text: "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
            options: [
                { id: 0, text: "Amazon Aurora Serverless", correct: true },
                { id: 1, text: "Amazon DynamoDB with On-Demand Capacity", correct: false },
                { id: 2, text: "Amazon ElastiCache", correct: false },
                { id: 3, text: "Amazon DynamoDB with Provisioned Capacity and Auto Scaling", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Aurora Serverless\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB with On-Demand Capacity: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB with Provisioned Capacity and Auto Scaling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 25,
            text: "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
            options: [
                { id: 0, text: "Set up AWS Direct Connect", correct: false },
                { id: 1, text: "Set up an AWS Site-to-Site VPN connection", correct: true },
                { id: 2, text: "Set up an Internet Gateway between the on-premises data center and AWS cloud", correct: false },
                { id: 3, text: "Set up a bastion host on Amazon EC2", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up an AWS Site-to-Site VPN connection\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Set up AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Internet Gateway between the on-premises data center and AWS cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a bastion host on Amazon EC2: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 26,
            text: "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
            options: [
                { id: 0, text: "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group", correct: false },
                { id: 1, text: "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy", correct: true },
                { id: 2, text: "Make the Amazon S3 bucket public", correct: false },
                { id: 3, text: "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make the Amazon S3 bucket public: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed. Which solution meets these requirements most cost-effectively?",
            options: [
                { id: 0, text: "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation", correct: false },
                { id: 1, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: false },
                { id: 2, text: "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation", correct: true },
                { id: 3, text: "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
            options: [
                { id: 0, text: "Amazon EFS Infrequent Access", correct: true },
                { id: 1, text: "Amazon EFS Standard", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Standard-IA", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon EFS Infrequent Access\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon EFS Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 29,
            text: "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
            options: [
                { id: 0, text: "AWS Firewall Manager", correct: false },
                { id: 1, text: "AWS Shield Advanced", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: true },
                { id: 3, text: "AWS Web Application Firewall (AWS WAF)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon GuardDuty\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Firewall Manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Web Application Firewall (AWS WAF): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 30,
            text: "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Run on AWS Lambda", correct: false },
                { id: 1, text: "Run on an Application Load Balancer", correct: false },
                { id: 2, text: "Run on Amazon EMR", correct: false },
                { id: 3, text: "Run on a Spot Instance with a persistent request type", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Run on a Spot Instance with a persistent request type\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Run on AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on an Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run on Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 31,
            text: "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application. Which combination of steps will enable secure S3 integration for this workload? (Select two)",
            options: [
                { id: 0, text: "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3", correct: true },
                { id: 1, text: "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID", correct: false },
                { id: 2, text: "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access", correct: false },
                { id: 3, text: "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate", correct: true },
                { id: 4, text: "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3, Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 32,
            text: "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage. Which solution will help improve the application’s responsiveness and scalability during peak load periods?",
            options: [
                { id: 0, text: "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue", correct: false },
                { id: 1, text: "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput", correct: false },
                { id: 2, text: "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue", correct: true },
                { id: 3, text: "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 33,
            text: "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Store the images using the Amazon S3 Standard-IA storage class", correct: false },
                { id: 1, text: "Store the images using the Amazon S3 Intelligent-Tiering storage class", correct: true },
                { id: 2, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
                { id: 3, text: "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Store the images using the Amazon S3 Intelligent-Tiering storage class\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Store the images using the Amazon S3 Standard-IA storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 34,
            text: "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold. What is the MOST operationally efficient way to achieve this?",
            options: [
                { id: 0, text: "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders", correct: true },
                { id: 1, text: "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly", correct: false },
                { id: 2, text: "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops", correct: false },
                { id: 3, text: "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 35,
            text: "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) temporary queues", correct: true },
                { id: 1, text: "Amazon Simple Queue Service (Amazon SQS) dead-letter queues", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) delay queues", correct: false },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS) FIFO queues", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Simple Queue Service (Amazon SQS) temporary queues\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) dead-letter queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) delay queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 36,
            text: "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
            options: [
                { id: 0, text: "Create a Read Replica in another Region as the Master database and point the analytics workload there", correct: false },
                { id: 1, text: "Create a Read Replica in the same Region as the Master database and point the analytics workload there", correct: true },
                { id: 2, text: "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database", correct: false },
                { id: 3, text: "Migrate the analytics application to AWS Lambda", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Read Replica in the same Region as the Master database and point the analytics workload there\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a Read Replica in another Region as the Master database and point the analytics workload there: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the analytics application to AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 37,
            text: "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans. Which combination of savings plans will satisfy these requirements? (Select two)",
            options: [
                { id: 0, text: "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly", correct: false },
                { id: 1, text: "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances", correct: true },
                { id: 2, text: "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads", correct: false },
                { id: 3, text: "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type", correct: false },
                { id: 4, text: "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances, Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
            options: [
                { id: 0, text: "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: true },
                { id: 1, text: "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 2, text: "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
                { id: 3, text: "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
            options: [
                { id: 0, text: "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes", correct: false },
                { id: 1, text: "Storage class analysis only provides recommendations for Standard to Standard IA classes", correct: true },
                { id: 2, text: "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes", correct: false },
                { id: 3, text: "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Storage class analysis only provides recommendations for Standard to Standard IA classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 40,
            text: "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
            options: [
                { id: 0, text: "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby", correct: true },
                { id: 1, text: "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database", correct: false },
                { id: 2, text: "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync", correct: false },
                { id: 3, text: "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason", correct: true },
                { id: 4, text: "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby, Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 41,
            text: "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practices—like mandatory logging and guardrails—when new accounts are created. The firm prefers a low-maintenance, governance-driven setup. Which solution best meets these goals while minimizing operational overhead?",
            options: [
                { id: 0, text: "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually", correct: false },
                { id: 1, text: "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance", correct: false },
                { id: 2, text: "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs", correct: false },
                { id: 3, text: "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
            options: [
                { id: 0, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region", correct: false },
                { id: 1, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region", correct: true },
                { id: 2, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region", correct: false },
                { id: 3, text: "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection. Which configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
            options: [
                { id: 0, text: "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center", correct: true },
                { id: 1, text: "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect", correct: false },
                { id: 2, text: "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing", correct: false },
                { id: 3, text: "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
            options: [
                { id: 0, text: "Require HTTPS for communication between Amazon CloudFront and your S3 origin", correct: false },
                { id: 1, text: "Require HTTPS for communication between Amazon CloudFront and your custom origin", correct: false },
                { id: 2, text: "Use Amazon CloudFront signed URLs", correct: true },
                { id: 3, text: "Use Amazon CloudFront signed cookies", correct: true },
                { id: 4, text: "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use Amazon CloudFront signed URLs, Use Amazon CloudFront signed cookies\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Require HTTPS for communication between Amazon CloudFront and your S3 origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Require HTTPS for communication between Amazon CloudFront and your custom origin: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 45,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in read request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 1, text: "Use AWS Global Accelerator", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use AWS Direct Connect", correct: false },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 46,
            text: "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
            options: [
                { id: 0, text: "VPC Route Table", correct: false },
                { id: 1, text: "Amazon Inspector", correct: false },
                { id: 2, text: "Amazon GuardDuty", correct: false },
                { id: 3, text: "AWS Shield Advanced", correct: true },
                { id: 4, text: "AWS Web Application Firewall (AWS WAF)", correct: true },
                { id: 5, text: "VPC Security Group", correct: true },
            ],
            correctAnswers: [3, 4, 5],
            explanation: "The correct answers are: AWS Shield Advanced, AWS Web Application Firewall (AWS WAF), VPC Security Group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- VPC Route Table: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Inspector: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon GuardDuty: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
            options: [
                { id: 0, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon MQ", correct: true },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS) Standard", correct: false },
                { id: 3, text: "Amazon SQS FIFO (First-In-First-Out)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Amazon MQ\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS) Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SQS FIFO (First-In-First-Out): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 48,
            text: "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning. Which solution will meet these requirements in the MOST efficient and scalable way?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically", correct: false },
                { id: 1, text: "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time", correct: true },
                { id: 2, text: "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals", correct: false },
                { id: 3, text: "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "VPC Endpoint", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "AWS VPN CloudHub", correct: true },
                { id: 3, text: "Software VPN", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
            options: [
                { id: 0, text: "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic", correct: true },
                { id: 1, text: "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", correct: false },
                { id: 2, text: "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior", correct: false },
                { id: 3, text: "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
            options: [
                { id: 0, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store", correct: false },
                { id: 1, text: "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones", correct: false },
                { id: 2, text: "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances", correct: false },
                { id: 3, text: "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 52,
            text: "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
            options: [
                { id: 0, text: "AWS Transit Gateway", correct: true },
                { id: 1, text: "VPC Peering Connection", correct: false },
                { id: 2, text: "Virtual private gateway (VGW)", correct: false },
                { id: 3, text: "AWS PrivateLink", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Transit Gateway\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Peering Connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Virtual private gateway (VGW): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS PrivateLink: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 53,
            text: "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring. Which combination of actions will best support this migration approach? (Select three)",
            options: [
                { id: 0, text: "Launch a cutover instance after completing testing and confirming that replication is up-to-date", correct: true },
                { id: 1, text: "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use", correct: false },
                { id: 2, text: "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover", correct: true },
                { id: 3, text: "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs", correct: true },
                { id: 4, text: "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations", correct: false },
                { id: 5, text: "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation", correct: false },
            ],
            correctAnswers: [0, 2, 3],
            explanation: "The correct answers are: Launch a cutover instance after completing testing and confirming that replication is up-to-date, Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover, Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
            options: [
                { id: 0, text: "Throughput Optimized HDD Amazon EBS volumes", correct: false },
                { id: 1, text: "Provisioned IOPS SSD Amazon EBS volumes", correct: true },
                { id: 2, text: "General-purpose SSD-based Amazon EBS volumes", correct: false },
                { id: 3, text: "Cold HDD Amazon EBS volumes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Provisioned IOPS SSD Amazon EBS volumes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Throughput Optimized HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- General-purpose SSD-based Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cold HDD Amazon EBS volumes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 1, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
            options: [
                { id: 0, text: "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS", correct: false },
                { id: 1, text: "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index", correct: false },
                { id: 2, text: "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS", correct: true },
                { id: 3, text: "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone", correct: false },
                { id: 1, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: false },
                { id: 2, text: "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration", correct: true },
                { id: 3, text: "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 58,
            text: "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
            options: [
                { id: 0, text: "Client Side Encryption", correct: true },
                { id: 1, text: "Server-side encryption with AWS KMS keys (SSE-KMS)", correct: false },
                { id: 2, text: "Server-side encryption with customer-provided keys (SSE-C)", correct: false },
                { id: 3, text: "Server-side encryption with Amazon S3 managed keys (SSE-S3)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Client Side Encryption\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Server-side encryption with AWS KMS keys (SSE-KMS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with customer-provided keys (SSE-C): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Server-side encryption with Amazon S3 managed keys (SSE-S3): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 59,
            text: "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root user’s email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users. Which solution meets these requirements in the most secure and scalable way?",
            options: [
                { id: 0, text: "Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications", correct: true },
                { id: 1, text: "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts", correct: false },
                { id: 2, text: "Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses", correct: false },
                { id: 3, text: "Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 60,
            text: "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
            options: [
                { id: 0, text: "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC", correct: false },
                { id: 1, text: "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC", correct: true },
                { id: 2, text: "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC", correct: false },
                { id: 3, text: "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC\n\nRoute tables control traffic routing in VPCs. Each subnet must be associated with a route table. To allow internet access, the route table needs a route to an Internet Gateway (0.0.0.0/0 -> igw-xxx).\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
            options: [
                { id: 0, text: "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 1, text: "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 2, text: "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds", correct: false },
                { id: 3, text: "Use delay queues to postpone the delivery of new messages to the queue for a few seconds", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use delay queues to postpone the delivery of new messages to the queue for a few seconds\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 62,
            text: "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
            options: [
                { id: 0, text: "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 1, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling", correct: false },
                { id: 2, text: "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue", correct: true },
                { id: 3, text: "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort. What is the most efficient solution to meet this goal with the least operational overhead?",
            options: [
                { id: 0, text: "Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule", correct: false },
                { id: 1, text: "Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed", correct: true },
                { id: 2, text: "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds", correct: false },
                { id: 3, text: "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
            options: [
                { id: 0, text: "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica", correct: true },
                { id: 1, text: "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
                { id: 2, text: "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica", correct: false },
                { id: 3, text: "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 65,
            text: "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets", correct: false },
                { id: 1, text: "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled", correct: true },
                { id: 2, text: "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket", correct: false },
                { id: 3, text: "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test19: [
        {
            id: 1,
            text: "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances", correct: false },
                { id: 2, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: true },
                { id: 3, text: "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
            options: [
                { id: 0, text: "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3", correct: false },
                { id: 1, text: "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3", correct: true },
                { id: 2, text: "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3", correct: false },
                { id: 3, text: "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 4,
            text: "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
            options: [
                { id: 0, text: "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 1, text: "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 2, text: "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements", correct: false },
                { id: 3, text: "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements", correct: true },
                { id: 4, text: "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements, Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 5,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 1, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 6,
            text: "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
            options: [
                { id: 0, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS", correct: true },
                { id: 1, text: "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS", correct: false },
                { id: 2, text: "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS", correct: false },
                { id: 3, text: "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 7,
            text: "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
            options: [
                { id: 0, text: "Different versions of a single object can have different retention modes and periods", correct: true },
                { id: 1, text: "The bucket default settings will override any explicit retention mode or period you request on an object version", correct: false },
                { id: 2, text: "You cannot place a retention period on an object version through a bucket default setting", correct: false },
                { id: 3, text: "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", correct: true },
                { id: 4, text: "When you use bucket default settings, you specify a Retain Until Date for the object version", correct: false },
            ],
            correctAnswers: [0, 3],
            explanation: "The correct answers are: Different versions of a single object can have different retention modes and periods, When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The bucket default settings will override any explicit retention mode or period you request on an object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot place a retention period on an object version through a bucket default setting: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When you use bucket default settings, you specify a Retain Until Date for the object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 8,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: false },
                { id: 1, text: "AWS Glue", correct: false },
                { id: 2, text: "Amazon EMR", correct: false },
                { id: 3, text: "Amazon FSx for Lustre", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 9,
            text: "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
            options: [
                { id: 0, text: "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering", correct: false },
                { id: 1, text: "Amazon S3 Intelligent-Tiering => Amazon S3 Standard", correct: true },
                { id: 2, text: "Amazon S3 Standard-IA => Amazon S3 One Zone-IA", correct: false },
                { id: 3, text: "Amazon S3 One Zone-IA => Amazon S3 Standard-IA", correct: true },
                { id: 4, text: "Amazon S3 Standard => Amazon S3 Intelligent-Tiering", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Amazon S3 Intelligent-Tiering => Amazon S3 Standard, Amazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA => Amazon S3 One Zone-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 10,
            text: "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
            options: [
                { id: 0, text: "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources", correct: false },
                { id: 1, text: "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI", correct: false },
                { id: 2, text: "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions", correct: false },
                { id: 3, text: "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 11,
            text: "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
            options: [
                { id: 0, text: "Configure the security group for the Amazon EC2 instances", correct: false },
                { id: 1, text: "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)", correct: false },
                { id: 2, text: "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)", correct: true },
                { id: 3, text: "Configure the security group on the Application Load Balancer", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure the security group for the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the security group on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 12,
            text: "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway with AWS Lambda", correct: false },
                { id: 1, text: "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics", correct: true },
                { id: 2, text: "Leverage Amazon QuickSight with Amazon Redshift", correct: false },
                { id: 3, text: "Leverage Amazon Athena with Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway with AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon QuickSight with Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Athena with Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 13,
            text: "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
            options: [
                { id: 0, text: "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor", correct: false },
                { id: 1, text: "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions", correct: false },
                { id: 2, text: "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets", correct: true },
                { id: 3, text: "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 14,
            text: "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
            options: [
                { id: 0, text: "Encrypt the access keys and save them on Amazon S3", correct: false },
                { id: 1, text: "Create a strong password for the AWS account root user", correct: true },
                { id: 2, text: "Enable Multi Factor Authentication (MFA) for the AWS account root user account", correct: true },
                { id: 3, text: "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future", correct: false },
                { id: 4, text: "Create AWS account root user access keys and share those keys only with the business owner", correct: false },
            ],
            correctAnswers: [1, 2],
            explanation: "The correct answers are: Create a strong password for the AWS account root user, Enable Multi Factor Authentication (MFA) for the AWS account root user account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the access keys and save them on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create AWS account root user access keys and share those keys only with the business owner: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
            options: [
                { id: 0, text: "Grant maximum privileges to avoid assigning privileges again", correct: false },
                { id: 1, text: "Use user credentials to provide access specific permissions for Amazon EC2 instances", correct: false },
                { id: 2, text: "Create a minimum number of accounts and share these account credentials among employees", correct: false },
                { id: 3, text: "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users", correct: true },
                { id: 4, text: "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users, Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Grant maximum privileges to avoid assigning privileges again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use user credentials to provide access specific permissions for Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a minimum number of accounts and share these account credentials among employees: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 17,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 1, text: "Only root user should have full database access in the organization", correct: false },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Remove full database access for all IAM users in the organization", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 18,
            text: "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
            options: [
                { id: 0, text: "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance", correct: true },
                { id: 1, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched", correct: false },
                { id: 2, text: "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application", correct: true },
                { id: 3, text: "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously", correct: false },
                { id: 4, text: "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance, As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time", correct: false },
                { id: 1, text: "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly", correct: false },
                { id: 2, text: "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores", correct: true },
                { id: 3, text: "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 20,
            text: "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
            options: [
                { id: 0, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate", correct: true },
                { id: 1, text: "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages", correct: false },
                { id: 2, text: "Use Amazon SQS standard queue to process the messages", correct: false },
                { id: 3, text: "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS standard queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 21,
            text: "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
            options: [
                { id: 0, text: "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs", correct: true },
                { id: 1, text: "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches", correct: false },
                { id: 2, text: "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share", correct: false },
                { id: 3, text: "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
            options: [
                { id: 0, text: "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket", correct: false },
                { id: 1, text: "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)", correct: true },
                { id: 2, text: "Upload the compressed file using multipart upload", correct: false },
                { id: 3, text: "Upload the compressed file in a single operation", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file using multipart upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file in a single operation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
            options: [
                { id: 0, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance", correct: false },
                { id: 1, text: "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance", correct: true },
                { id: 2, text: "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance", correct: false },
                { id: 3, text: "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 24,
            text: "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
            options: [
                { id: 0, text: "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution", correct: true },
                { id: 1, text: "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: true },
                { id: 2, text: "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 3, text: "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
                { id: 4, text: "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution, Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 25,
            text: "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
            options: [
                { id: 0, text: "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection", correct: true },
                { id: 1, text: "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region", correct: false },
                { id: 2, text: "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region", correct: false },
                { id: 3, text: "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 26,
            text: "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
            options: [
                { id: 0, text: "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files", correct: false },
                { id: 1, text: "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering", correct: false },
                { id: 2, text: "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes", correct: true },
                { id: 3, text: "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 27,
            text: "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
            options: [
                { id: 0, text: "The engineering team needs to provision more servers running the Amazon SNS service", correct: false },
                { id: 1, text: "The engineering team needs to provision more servers running the AWS Lambda service", correct: false },
                { id: 2, text: "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit", correct: true },
                { id: 3, text: "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The engineering team needs to provision more servers running the Amazon SNS service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The engineering team needs to provision more servers running the AWS Lambda service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 28,
            text: "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
            options: [
                { id: 0, text: "General Purpose Solid State Drive (gp2)", correct: false },
                { id: 1, text: "Throughput Optimized Hard disk drive (st1)", correct: true },
                { id: 2, text: "Instance Store", correct: false },
                { id: 3, text: "Cold Hard disk drive (sc1)", correct: true },
                { id: 4, text: "Provisioned IOPS Solid state drive (io1)", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Throughput Optimized Hard disk drive (st1), Cold Hard disk drive (sc1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose Solid State Drive (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance Store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned IOPS Solid state drive (io1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
            options: [
                { id: 0, text: "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 1, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
                { id: 2, text: "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: true },
                { id: 3, text: "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
            options: [
                { id: 0, text: "Host-based Routing", correct: false },
                { id: 1, text: "Path-based Routing", correct: true },
                { id: 2, text: "HTTP header-based routing", correct: false },
                { id: 3, text: "Query string parameter-based routing", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Path-based Routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host-based Routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- HTTP header-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Query string parameter-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 31,
            text: "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
            options: [
                { id: 0, text: "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time", correct: false },
                { id: 1, text: "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: true },
                { id: 2, text: "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling", correct: false },
                { id: 3, text: "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
            options: [
                { id: 0, text: "Use AWS Lambda function to run the workflow processes", correct: false },
                { id: 1, text: "Use Amazon EC2 on-demand instances to run the workflow processes", correct: false },
                { id: 2, text: "Use Amazon EC2 reserved instances to run the workflow processes", correct: false },
                { id: 3, text: "Use Amazon EC2 spot instances to run the workflow processes", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Amazon EC2 spot instances to run the workflow processes\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Lambda function to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 33,
            text: "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
            options: [
                { id: 0, text: "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability", correct: false },
                { id: 1, text: "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency", correct: false },
                { id: 2, text: "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed", correct: true },
                { id: 3, text: "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously", correct: true },
                { id: 4, text: "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed, Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 34,
            text: "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
            options: [
                { id: 0, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: true },
                { id: 1, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 2, text: "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server", correct: false },
                { id: 3, text: "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 35,
            text: "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
            options: [
                { id: 0, text: "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 1, text: "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables", correct: true },
                { id: 2, text: "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables", correct: false },
                { id: 3, text: "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 36,
            text: "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
            options: [
                { id: 0, text: "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue", correct: false },
                { id: 1, text: "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service", correct: true },
                { id: 2, text: "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again", correct: false },
                { id: 3, text: "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy", correct: false },
                { id: 4, text: "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again", correct: true },
            ],
            correctAnswers: [1, 4],
            explanation: "The correct answers are: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service, Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 37,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 3, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 38,
            text: "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
            options: [
                { id: 0, text: "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet", correct: false },
                { id: 1, text: "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block", correct: false },
                { id: 2, text: "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC", correct: true },
                { id: 3, text: "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 39,
            text: "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
            options: [
                { id: 0, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 1, text: "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 2, text: "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: false },
                { id: 3, text: "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
            options: [
                { id: 0, text: "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse", correct: false },
                { id: 1, text: "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes", correct: false },
                { id: 2, text: "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks", correct: false },
                { id: 3, text: "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
            options: [
                { id: 0, text: "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services", correct: false },
                { id: 1, text: "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC", correct: true },
                { id: 2, text: "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services", correct: false },
                { id: 3, text: "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
            options: [
                { id: 0, text: "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior", correct: false },
                { id: 1, text: "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration", correct: false },
                { id: 2, text: "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data", correct: true },
                { id: 3, text: "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands", correct: true },
                { id: 4, text: "Use Amazon Aurora Global Database to replicate data across regions for compatibility", correct: false },
            ],
            correctAnswers: [2, 3],
            explanation: "The correct answers are: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data, Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Aurora Global Database to replicate data across regions for compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 43,
            text: "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
            options: [
                { id: 0, text: "Use VPC security groups to control the network traffic to and from your file system", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system", correct: false },
                { id: 2, text: "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system", correct: false },
                { id: 3, text: "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance", correct: false },
                { id: 4, text: "Use an IAM policy to control access for clients who can mount your file system with the required permissions", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Use VPC security groups to control the network traffic to and from your file system, Use an IAM policy to control access for clients who can mount your file system with the required permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 1, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
                { id: 2, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket, Enable versioning on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
            options: [
                { id: 0, text: "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively", correct: false },
                { id: 1, text: "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures", correct: false },
                { id: 2, text: "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements", correct: false },
                { id: 3, text: "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 46,
            text: "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
            options: [
                { id: 0, text: "The junior scientist does not need to pay any transfer charges for the image upload", correct: true },
                { id: 1, text: "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload", correct: false },
                { id: 2, text: "The junior scientist only needs to pay S3TA transfer charges for the image upload", correct: false },
                { id: 3, text: "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: The junior scientist does not need to pay any transfer charges for the image upload\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay Amazon S3 transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
            options: [
                { id: 0, text: "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)", correct: true },
                { id: 1, text: "Amazon S3 Glacier Deep Archive", correct: false },
                { id: 2, text: "Amazon S3 Standard", correct: false },
                { id: 3, text: "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon S3 Glacier Deep Archive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 48,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "It is not possible to access cross-account resources", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 49,
            text: "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
            options: [
                { id: 0, text: "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances", correct: false },
                { id: 1, text: "Use Amazon EC2 instances with Amazon EFS mount points", correct: false },
                { id: 2, text: "Use Amazon EC2 instances with access to Amazon S3 based storage", correct: false },
                { id: 3, text: "Use Instance Store based Amazon EC2 instances", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Instance Store based Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EFS mount points: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with access to Amazon S3 based storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
            options: [
                { id: 0, text: "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies", correct: false },
                { id: 1, text: "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster", correct: false },
                { id: 2, text: "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods", correct: true },
                { id: 3, text: "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
            options: [
                { id: 0, text: "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class", correct: false },
                { id: 1, text: "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class", correct: false },
                { id: 2, text: "Store the intermediary query results in Amazon S3 Standard storage class", correct: true },
                { id: 3, text: "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Store the intermediary query results in Amazon S3 Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 52,
            text: "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
            options: [
                { id: 0, text: "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team", correct: true },
                { id: 1, text: "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow", correct: false },
                { id: 2, text: "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards", correct: false },
                { id: 3, text: "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 53,
            text: "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
            options: [
                { id: 0, text: "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
                { id: 1, text: "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%", correct: false },
                { id: 2, text: "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%", correct: true },
                { id: 3, text: "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 54,
            text: "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
            options: [
                { id: 0, text: "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point", correct: true },
                { id: 1, text: "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution", correct: false },
                { id: 2, text: "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation", correct: false },
                { id: 3, text: "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
            options: [
                { id: 0, text: "Use Amazon CloudFront with a custom origin pointing to the on-premises servers", correct: true },
                { id: 1, text: "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers", correct: false },
                { id: 2, text: "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia", correct: false },
                { id: 3, text: "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 56,
            text: "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
            options: [
                { id: 0, text: "AWS Global Accelerator", correct: true },
                { id: 1, text: "AWS Elastic Load Balancing (ELB)", correct: false },
                { id: 2, text: "Amazon Route 53", correct: false },
                { id: 3, text: "Amazon CloudFront", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: AWS Global Accelerator\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- AWS Elastic Load Balancing (ELB): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 57,
            text: "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
            options: [
                { id: 0, text: "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance", correct: false },
                { id: 1, text: "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB", correct: false },
                { id: 2, text: "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB", correct: true },
                { id: 3, text: "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
            options: [
                { id: 0, text: "Amazon FSx for Windows File Server", correct: true },
                { id: 1, text: "Microsoft SQL Server on AWS", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: false },
                { id: 3, text: "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Microsoft SQL Server on AWS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
            options: [
                { id: 0, text: "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks", correct: false },
                { id: 1, text: "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume", correct: false },
                { id: 2, text: "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required", correct: true },
                { id: 3, text: "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks: SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.\n- Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 60,
            text: "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
            options: [
                { id: 0, text: "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud", correct: true },
                { id: 1, text: "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 2, text: "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud", correct: false },
                { id: 3, text: "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 61,
            text: "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
            options: [
                { id: 0, text: "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B", correct: true },
                { id: 1, text: "1 Amazon EC2 instance and 2 AMIs exist in Region B", correct: false },
                { id: 2, text: "1 Amazon EC2 instance and 1 AMI exist in Region B", correct: false },
                { id: 3, text: "1 Amazon EC2 instance and 1 snapshot exist in Region B", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- 1 Amazon EC2 instance and 2 AMIs exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 AMI exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 snapshot exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 62,
            text: "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
            options: [
                { id: 0, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 1, text: "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: true },
                { id: 2, text: "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
                { id: 3, text: "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 63,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 1, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [1, 3],
            explanation: "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
            options: [
                { id: 0, text: "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 1, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: true },
                { id: 2, text: "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }", correct: false },
                { id: 3, text: "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 65,
            text: "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
            options: [
                { id: 0, text: "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets", correct: false },
                { id: 1, text: "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations", correct: true },
                { id: 2, text: "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files", correct: false },
                { id: 3, text: "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
    ],
    test20: [
        {
            id: 1,
            text: "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 1, text: "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)", correct: false },
                { id: 2, text: "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)\n\nAmazon Kinesis is designed for real-time streaming data processing, making it ideal for IoT data ingestion and analytics. It can handle high-throughput, real-time data streams and process them for analytics. Amazon SNS provides push-based notifications to mobile applications, which is exactly what's needed to notify device owners. This combination allows for real-time data processing followed by immediate notification delivery to mobile apps.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis with Amazon Simple Email Service (Amazon SES): SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS): SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 2,
            text: "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
            options: [
                { id: 0, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days", correct: false },
                { id: 1, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days", correct: false },
                { id: 2, text: "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days", correct: true },
                { id: 3, text: "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
            domain: "Design Secure Architectures",
        },
        {
            id: 3,
            text: "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
            options: [
                { id: 0, text: "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: true },
                { id: 1, text: "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 2, text: "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
                { id: 3, text: "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost\n\nLaunch templates are the recommended and modern way to configure Auto Scaling groups with mixed instance types and purchasing options. They support both On-Demand and Spot Instances across multiple instance types, allowing for cost optimization while maintaining performance. Launch configurations are legacy and don't support mixed instance types or Spot Instances with the same flexibility. You cannot use launch configurations for mixed instance types with Spot Instances - only launch templates support this feature.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 4,
            text: "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
            options: [
                { id: 0, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type", correct: false },
                { id: 1, text: "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed", correct: true },
                { id: 2, text: "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance", correct: false },
                { id: 3, text: "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 5,
            text: "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
            options: [
                { id: 0, text: "Copy data from the source bucket to the destination bucket using the aws S3 sync command", correct: true },
                { id: 1, text: "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console", correct: false },
                { id: 2, text: "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration", correct: true },
                { id: 3, text: "Use AWS Snowball Edge device to copy the data from one Region to another Region", correct: false },
                { id: 4, text: "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 6,
            text: "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
            options: [
                { id: 0, text: "AWS Lambda", correct: false },
                { id: 1, text: "AWS Step Functions", correct: false },
                { id: 2, text: "Amazon Simple Notification Service (Amazon SNS)", correct: true },
                { id: 3, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 4, text: "Amazon CloudWatch", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon Simple Notification Service (Amazon SNS), Amazon CloudWatch\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Step Functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 7,
            text: "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
            options: [
                { id: 0, text: "Use Auto Scaling group to provide a low latency way to distribute live sports results", correct: false },
                { id: 1, text: "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results", correct: false },
                { id: 2, text: "Use Amazon CloudFront to provide a low latency way to distribute live sports results", correct: false },
                { id: 3, text: "Use AWS Global Accelerator to provide a low latency way to distribute live sports results", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 8,
            text: "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: true },
                { id: 1, text: "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 2, text: "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
                { id: 3, text: "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 9,
            text: "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
            options: [
                { id: 0, text: "Remove full database access for all IAM users in the organization", correct: false },
                { id: 1, text: "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals", correct: true },
                { id: 2, text: "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur", correct: false },
                { id: 3, text: "Only root user should have full database access in the organization", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 10,
            text: "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
            options: [
                { id: 0, text: "Use AWS CloudTrail to analyze API calls", correct: true },
                { id: 1, text: "Implement an IAM policy to forbid users to change Amazon S3 bucket settings", correct: false },
                { id: 2, text: "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations", correct: false },
                { id: 3, text: "Use Amazon S3 access logs to analyze user access using Athena", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use AWS CloudTrail to analyze API calls\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Implement an IAM policy to forbid users to change Amazon S3 bucket settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 access logs to analyze user access using Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 11,
            text: "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
            options: [
                { id: 0, text: "Amazon Redshift", correct: false },
                { id: 1, text: "Amazon RDS", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: true },
                { id: 3, text: "Amazon ElastiCache", correct: false },
                { id: 4, text: "AWS Lambda", correct: true },
            ],
            correctAnswers: [2, 4],
            explanation: "The correct answers are: Amazon DynamoDB, AWS Lambda\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 12,
            text: "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
            options: [
                { id: 0, text: "Use Amazon Route 53 weighted routing to spread traffic across different deployments", correct: false },
                { id: 1, text: "Use AWS CodeDeploy deployment options to choose the right deployment", correct: false },
                { id: 2, text: "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment", correct: true },
                { id: 3, text: "Use Elastic Load Balancing (ELB) to distribute traffic across deployments", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 weighted routing to spread traffic across different deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CodeDeploy deployment options to choose the right deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to distribute traffic across deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 13,
            text: "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
            options: [
                { id: 0, text: "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object", correct: false },
                { id: 1, text: "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager", correct: false },
                { id: 2, text: "Establish a process to get managerial approval for deleting Amazon S3 objects", correct: false },
                { id: 3, text: "Enable versioning on the Amazon S3 bucket", correct: true },
                { id: 4, text: "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Enable versioning on the Amazon S3 bucket, Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 14,
            text: "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
            options: [
                { id: 0, text: "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB", correct: true },
                { id: 1, text: "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 2, text: "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB", correct: false },
                { id: 3, text: "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 15,
            text: "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
            options: [
                { id: 0, text: "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 1, text: "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: true },
                { id: 2, text: "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
                { id: 3, text: "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 16,
            text: "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
            options: [
                { id: 0, text: "Move to Amazon Redshift", correct: false },
                { id: 1, text: "Switch application code to AWS Lambda for better performance", correct: false },
                { id: 2, text: "Setup Amazon ElastiCache in front of Amazon RDS", correct: true },
                { id: 3, text: "Setup Amazon RDS Read Replicas", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Setup Amazon ElastiCache in front of Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Move to Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch application code to AWS Lambda for better performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup Amazon RDS Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 17,
            text: "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
            options: [
                { id: 0, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3", correct: true },
                { id: 1, text: "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 2, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3", correct: false },
                { id: 3, text: "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 18,
            text: "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
            options: [
                { id: 0, text: "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 1, text: "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: true },
                { id: 2, text: "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
                { id: 3, text: "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 19,
            text: "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
            options: [
                { id: 0, text: "Amazon ElastiCache for Redis/Memcached", correct: true },
                { id: 1, text: "Amazon DynamoDB Accelerator (DAX)", correct: false },
                { id: 2, text: "Amazon DynamoDB", correct: false },
                { id: 3, text: "Amazon DocumentDB", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon ElastiCache for Redis/Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DocumentDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 20,
            text: "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
            options: [
                { id: 0, text: "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams", correct: false },
                { id: 1, text: "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams", correct: true },
                { id: 2, text: "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams", correct: false },
                { id: 3, text: "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 21,
            text: "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
            options: [
                { id: 0, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role", correct: true },
                { id: 1, text: "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account", correct: false },
                { id: 2, text: "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket", correct: false },
                { id: 3, text: "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 22,
            text: "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
            options: [
                { id: 0, text: "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 1, text: "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier", correct: false },
                { id: 2, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier", correct: true },
                { id: 3, text: "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 23,
            text: "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
            options: [
                { id: 0, text: "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images", correct: false },
                { id: 1, text: "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources", correct: true },
                { id: 2, text: "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions", correct: false },
                { id: 3, text: "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code", correct: true },
                { id: 4, text: "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package", correct: false },
                { id: 5, text: "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code, Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 24,
            text: "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
            options: [
                { id: 0, text: "Use AWS Global Accelerator", correct: false },
                { id: 1, text: "Use AWS Direct Connect", correct: false },
                { id: 2, text: "Use AWS Shield", correct: false },
                { id: 3, text: "Use Amazon CloudFront distribution in front of the Application Load Balancer", correct: true },
                { id: 4, text: "Use Amazon Aurora Replica", correct: true },
            ],
            correctAnswers: [3, 4],
            explanation: "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 25,
            text: "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
            options: [
                { id: 0, text: "Amazon S3", correct: true },
                { id: 1, text: "Amazon Kinesis", correct: false },
                { id: 2, text: "Amazon Simple Queue Service (Amazon SQS)", correct: false },
                { id: 3, text: "Amazon Simple Notification Service (Amazon SNS)", correct: false },
                { id: 4, text: "Amazon DynamoDB", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: Amazon S3, Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 26,
            text: "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
            options: [
                { id: 0, text: "Provision another Amazon Aurora database and link it to the primary database as a read replica", correct: false },
                { id: 1, text: "Set up a read replica and modify the application to use the appropriate endpoint", correct: true },
                { id: 2, text: "Activate read-through caching on the Amazon Aurora database", correct: false },
                { id: 3, text: "Configure the application to read from the Multi-AZ standby instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Set up a read replica and modify the application to use the appropriate endpoint\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Activate read-through caching on the Amazon Aurora database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the application to read from the Multi-AZ standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 27,
            text: "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
            options: [
                { id: 0, text: "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer", correct: true },
                { id: 1, text: "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted", correct: false },
                { id: 2, text: "You need to attach elastic IP address (EIP) to the Amazon EC2 instances", correct: false },
                { id: 3, text: "Your web-app has a runtime that is not supported by the Application Load Balancer", correct: false },
                { id: 4, text: "The route for the health check is misconfigured", correct: true },
            ],
            correctAnswers: [0, 4],
            explanation: "The correct answers are: The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer, The route for the health check is misconfigured\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 28,
            text: "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
            options: [
                { id: 0, text: "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated", correct: false },
                { id: 1, text: "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume", correct: true },
                { id: 2, text: "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume", correct: false },
                { id: 3, text: "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 29,
            text: "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
            options: [
                { id: 0, text: "Use Spot Instances", correct: false },
                { id: 1, text: "Use a Cluster placement group", correct: true },
                { id: 2, text: "Optimize the Amazon EC2 kernel using EC2 User Data", correct: false },
                { id: 3, text: "Use a Spread placement group", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 30,
            text: "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
            options: [
                { id: 0, text: "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region", correct: false },
                { id: 1, text: "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region", correct: true },
                { id: 2, text: "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases", correct: false },
                { id: 3, text: "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 31,
            text: "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
            options: [
                { id: 0, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data", correct: false },
                { id: 1, text: "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object", correct: true },
                { id: 2, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data", correct: false },
                { id: 3, text: "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 32,
            text: "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
            options: [
                { id: 0, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: false },
                { id: 1, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours", correct: true },
                { id: 2, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
                { id: 3, text: "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 33,
            text: "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
            options: [
                { id: 0, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used", correct: false },
                { id: 1, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests", correct: false },
                { id: 2, text: "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour", correct: false },
                { id: 3, text: "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 34,
            text: "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
            options: [
                { id: 0, text: "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database", correct: false },
                { id: 1, text: "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database", correct: false },
                { id: 2, text: "Use IAM authentication to access the database instead of the database user's access credentials", correct: false },
                { id: 3, text: "Configure Amazon RDS to use SSL for data in transit", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 35,
            text: "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
            options: [
                { id: 0, text: "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket", correct: true },
                { id: 1, text: "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket", correct: true },
                { id: 2, text: "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3", correct: false },
                { id: 3, text: "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket", correct: false },
                { id: 4, text: "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 36,
            text: "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
            options: [
                { id: 0, text: "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once", correct: true },
                { id: 1, text: "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts", correct: false },
                { id: 2, text: "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs", correct: false },
                { id: 3, text: "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 37,
            text: "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
            options: [
                { id: 0, text: "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda", correct: false },
                { id: 1, text: "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: false },
                { id: 2, text: "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda", correct: false },
                { id: 3, text: "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 38,
            text: "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
            options: [
                { id: 0, text: "Amazon Neptune", correct: true },
                { id: 1, text: "Amazon OpenSearch Service", correct: false },
                { id: 2, text: "Amazon Aurora", correct: false },
                { id: 3, text: "Amazon Redshift", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 39,
            text: "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
            options: [
                { id: 0, text: "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances", correct: false },
                { id: 1, text: "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations", correct: true },
                { id: 2, text: "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs", correct: false },
                { id: 3, text: "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 40,
            text: "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
            options: [
                { id: 0, text: "By default, scripts entered as user data are executed with root user privileges", correct: true },
                { id: 1, text: "By default, user data runs only during the boot cycle when you first launch an instance", correct: true },
                { id: 2, text: "When an instance is running, you can update user data by using root user credentials", correct: false },
                { id: 3, text: "By default, user data is executed every time an Amazon EC2 instance is re-started", correct: false },
                { id: 4, text: "By default, scripts entered as user data do not have root user privileges for executing", correct: false },
            ],
            correctAnswers: [0, 1],
            explanation: "The correct answers are: By default, scripts entered as user data are executed with root user privileges, By default, user data runs only during the boot cycle when you first launch an instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- When an instance is running, you can update user data by using root user credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, scripts entered as user data do not have root user privileges for executing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 41,
            text: "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
            options: [
                { id: 0, text: "AWS Glue", correct: false },
                { id: 1, text: "Amazon EMR", correct: false },
                { id: 2, text: "Amazon FSx for Lustre", correct: true },
                { id: 3, text: "Amazon FSx for Windows File Server", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 42,
            text: "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
            options: [
                { id: 0, text: "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup", correct: true },
                { id: 1, text: "Store the installation files in Amazon S3 so they can be quickly retrieved", correct: false },
                { id: 2, text: "Use Amazon EC2 user data to customize the dynamic installation parts at boot time", correct: true },
                { id: 3, text: "Use Amazon EC2 user data to install the application at boot time", correct: false },
                { id: 4, text: "Use AWS Elastic Beanstalk deployment caching feature", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 43,
            text: "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
            options: [
                { id: 0, text: "Use Identity and Access Management (IAM) policies", correct: false },
                { id: 1, text: "Use Amazon S3 Bucket Policies", correct: true },
                { id: 2, text: "Use Security Groups", correct: false },
                { id: 3, text: "Use Access Control Lists (ACLs)", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon S3 Bucket Policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Identity and Access Management (IAM) policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Security Groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Access Control Lists (ACLs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 44,
            text: "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
            options: [
                { id: 0, text: "Use Internet Gateway to access Amazon SQS", correct: false },
                { id: 1, text: "Use VPN connection to access Amazon SQS", correct: false },
                { id: 2, text: "Use VPC endpoint to access Amazon SQS", correct: true },
                { id: 3, text: "Use Network Address Translation (NAT) instance to access Amazon SQS", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 45,
            text: "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
            options: [
                { id: 0, text: "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region", correct: false },
                { id: 1, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region", correct: true },
                { id: 2, text: "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region", correct: false },
                { id: 3, text: "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 46,
            text: "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
            options: [
                { id: 0, text: "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue", correct: false },
                { id: 1, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second", correct: true },
                { id: 2, text: "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix", correct: true },
                { id: 3, text: "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second", correct: false },
                { id: 4, text: "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue", correct: true },
                { id: 5, text: "Convert the existing standard queue into a FIFO (First-In-First-Out) queue", correct: false },
            ],
            correctAnswers: [1, 2, 4],
            explanation: "The correct answers are: Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix, Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 47,
            text: "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
            options: [
                { id: 0, text: "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 1, text: "Use AWS Glue to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 2, text: "Use AWS EMR to replicate the data from the databases into Amazon Redshift", correct: false },
                { id: 3, text: "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS EMR to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Resilient Architectures",
        },
        {
            id: 48,
            text: "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
            options: [
                { id: 0, text: "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN", correct: false },
                { id: 1, text: "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN", correct: true },
                { id: 2, text: "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
                { id: 3, text: "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 49,
            text: "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
            options: [
                { id: 0, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
                { id: 1, text: "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: true },
                { id: 2, text: "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days", correct: false },
                { id: 3, text: "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 50,
            text: "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
            options: [
                { id: 0, text: "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs", correct: false },
                { id: 1, text: "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: true },
                { id: 2, text: "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events", correct: false },
                { id: 3, text: "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 51,
            text: "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
            options: [
                { id: 0, text: "Use AWS_IAM authorization", correct: false },
                { id: 1, text: "Use Amazon Cognito User Pools", correct: true },
                { id: 2, text: "Use Amazon Cognito Identity Pools", correct: false },
                { id: 3, text: "Use AWS Lambda authorizer for Amazon API Gateway", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Use Amazon Cognito User Pools\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS_IAM authorization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Identity Pools: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda authorizer for Amazon API Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 52,
            text: "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
            options: [
                { id: 0, text: "Use Amazon EC2 dedicated hosts", correct: true },
                { id: 1, text: "Use Amazon EC2 dedicated instances", correct: false },
                { id: 2, text: "Use Amazon EC2 on-demand instances", correct: false },
                { id: 3, text: "Use Amazon EC2 reserved instances (RI)", correct: false },
            ],
            correctAnswers: [0],
            explanation: "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 53,
            text: "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
            options: [
                { id: 0, text: "NAT instance can be used as a bastion server", correct: true },
                { id: 1, text: "NAT gateway can be used as a bastion server", correct: false },
                { id: 2, text: "NAT instance supports port forwarding", correct: true },
                { id: 3, text: "NAT gateway supports port forwarding", correct: false },
                { id: 4, text: "Security Groups can be associated with a NAT instance", correct: true },
                { id: 5, text: "Security Groups can be associated with a NAT gateway", correct: false },
            ],
            correctAnswers: [0, 2, 4],
            explanation: "The correct answers are: NAT instance can be used as a bastion server, NAT instance supports port forwarding, Security Groups can be associated with a NAT instance\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 54,
            text: "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
            options: [
                { id: 0, text: "Both IAM roles and IAM users can be used interchangeably for cross-account access", correct: false },
                { id: 1, text: "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment", correct: true },
                { id: 2, text: "It is not possible to access cross-account resources", correct: false },
                { id: 3, text: "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 55,
            text: "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
            options: [
                { id: 0, text: "7", correct: false },
                { id: 1, text: "3", correct: true },
                { id: 2, text: "14", correct: false },
                { id: 3, text: "15", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: 3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- 7: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 14: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 15: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 56,
            text: "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
            options: [
                { id: 0, text: "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3", correct: false },
                { id: 1, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3", correct: true },
                { id: 2, text: "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
                { id: 3, text: "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 57,
            text: "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
            options: [
                { id: 0, text: "Establish VPC peering connections between all VPCs", correct: false },
                { id: 1, text: "Use an internet gateway to interconnect the VPCs", correct: false },
                { id: 2, text: "Use a VPC endpoint to interconnect the VPCs", correct: false },
                { id: 3, text: "Use AWS transit gateway to interconnect the VPCs", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 58,
            text: "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
            options: [
                { id: 0, text: "Distribute the static content through Amazon EFS", correct: false },
                { id: 1, text: "Distribute the dynamic content through Amazon EFS", correct: false },
                { id: 2, text: "Distribute the static content through Amazon S3", correct: true },
                { id: 3, text: "Distribute the dynamic content through Amazon S3", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Distribute the static content through Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Distribute the static content through Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Distribute the dynamic content through Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Distribute the dynamic content through Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 59,
            text: "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
            options: [
                { id: 0, text: "AWS Schema Conversion Tool (AWS SCT)", correct: true },
                { id: 1, text: "Basic Schema Copy", correct: false },
                { id: 2, text: "AWS Database Migration Service (AWS DMS)", correct: true },
                { id: 3, text: "AWS Snowball Edge", correct: false },
                { id: 4, text: "AWS Glue", correct: false },
            ],
            correctAnswers: [0, 2],
            explanation: "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 60,
            text: "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
            options: [
                { id: 0, text: "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance", correct: false },
                { id: 1, text: "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance", correct: true },
                { id: 2, text: "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance", correct: false },
                { id: 3, text: "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 61,
            text: "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
            options: [
                { id: 0, text: "Data moving between the volume and the instance is NOT encrypted", correct: false },
                { id: 1, text: "Any snapshot created from the volume is encrypted", correct: true },
                { id: 2, text: "Any snapshot created from the volume is NOT encrypted", correct: false },
                { id: 3, text: "Data moving between the volume and the instance is encrypted", correct: true },
                { id: 4, text: "Data at rest inside the volume is NOT encrypted", correct: false },
                { id: 5, text: "Data at rest inside the volume is encrypted", correct: true },
            ],
            correctAnswers: [1, 3, 5],
            explanation: "The correct answers are: Any snapshot created from the volume is encrypted, Data moving between the volume and the instance is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 62,
            text: "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
            options: [
                { id: 0, text: "Software VPN", correct: false },
                { id: 1, text: "VPC Peering connection", correct: false },
                { id: 2, text: "VPC Endpoint", correct: false },
                { id: 3, text: "AWS VPN CloudHub", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Secure Architectures",
        },
        {
            id: 63,
            text: "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
            options: [
                { id: 0, text: "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
                { id: 1, text: "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis", correct: false },
                { id: 2, text: "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift", correct: true },
                { id: 3, text: "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift", correct: false },
            ],
            correctAnswers: [2],
            explanation: "The correct answer is: Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
        {
            id: 64,
            text: "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
            options: [
                { id: 0, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues", correct: false },
                { id: 1, text: "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues", correct: false },
                { id: 2, text: "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose", correct: false },
                { id: 3, text: "Use Enhanced Fanout feature of Amazon Kinesis Data Streams", correct: true },
            ],
            correctAnswers: [3],
            explanation: "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design High-Performing Architectures",
        },
        {
            id: 65,
            text: "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
            options: [
                { id: 0, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 1, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance", correct: true },
                { id: 2, text: "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance", correct: false },
                { id: 3, text: "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes", correct: false },
            ],
            correctAnswers: [1],
            explanation: "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
            domain: "Design Cost-Optimized Architectures",
        },
    ],
};

// Function to get all questions for a test
function getTestQuestions(testNumber) {
    const testKey = `test${testNumber}`;
    return examQuestions[testKey] || [];
}
